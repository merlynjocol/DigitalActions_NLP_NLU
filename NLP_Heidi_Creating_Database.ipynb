{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merlynjocol/DigitalActions_NLP_NLU/blob/main/NLP_Heidi_Creating_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRncrB4FsMDH"
      },
      "source": [
        "![HEIDI_banner topic modelling.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABjAAAAGMCAYAAAB0yOt+AAAAAXNSR0IArs4c6QAAAAlwSFlzAAAOxAAADsQBlSsOGwAABJZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+Cjx4OnhtcG1ldGEgeG1sbnM6eD0nYWRvYmU6bnM6bWV0YS8nPgo8cmRmOlJERiB4bWxuczpyZGY9J2h0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMnPgoKIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PScnCiAgeG1sbnM6QXR0cmliPSdodHRwOi8vbnMuYXR0cmlidXRpb24uY29tL2Fkcy8xLjAvJz4KICA8QXR0cmliOkFkcz4KICAgPHJkZjpTZXE+CiAgICA8cmRmOmxpIHJkZjpwYXJzZVR5cGU9J1Jlc291cmNlJz4KICAgICA8QXR0cmliOkNyZWF0ZWQ+MjAyMi0wMS0xMTwvQXR0cmliOkNyZWF0ZWQ+CiAgICAgPEF0dHJpYjpFeHRJZD43MWQ4ZjNhNC0wYTYzLTQ3YWItYjZjMy04ZTE0NDU0ZTYyM2Q8L0F0dHJpYjpFeHRJZD4KICAgICA8QXR0cmliOkZiSWQ+NTI1MjY1OTE0MTc5NTgwPC9BdHRyaWI6RmJJZD4KICAgICA8QXR0cmliOlRvdWNoVHlwZT4yPC9BdHRyaWI6VG91Y2hUeXBlPgogICAgPC9yZGY6bGk+CiAgIDwvcmRmOlNlcT4KICA8L0F0dHJpYjpBZHM+CiA8L3JkZjpEZXNjcmlwdGlvbj4KCiA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0nJwogIHhtbG5zOmRjPSdodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyc+CiAgPGRjOnRpdGxlPgogICA8cmRmOkFsdD4KICAgIDxyZGY6bGkgeG1sOmxhbmc9J3gtZGVmYXVsdCc+UGluayBhbmQgTXVzdGFyZCBNaW5pbWFsaXN0IENvcHl3cml0ZXIgTGlua2VkSW4gQmFubmVyPC9yZGY6bGk+CiAgIDwvcmRmOkFsdD4KICA8L2RjOnRpdGxlPgogPC9yZGY6RGVzY3JpcHRpb24+CgogPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9JycKICB4bWxuczpwZGY9J2h0dHA6Ly9ucy5hZG9iZS5jb20vcGRmLzEuMy8nPgogIDxwZGY6QXV0aG9yPmpvaGEuaHVydGFkbzwvcGRmOkF1dGhvcj4KIDwvcmRmOkRlc2NyaXB0aW9uPgoKIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PScnCiAgeG1sbnM6eG1wPSdodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvJz4KICA8eG1wOkNyZWF0b3JUb29sPkNhbnZhPC94bXA6Q3JlYXRvclRvb2w+CiA8L3JkZjpEZXNjcmlwdGlvbj4KPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KPD94cGFja2V0IGVuZD0ncic/Phvgw0EAACAASURBVHic7L1bkiQ5ki12FDD3yKzqmeFy+ME1cOv84BYod+TeEeFMT1dlRLgZoPzQJ2DmkZFV2XObJDQl0t3tgacCMDsHqkrMzFiyZMmSJUuWLFmyZMmSJUuWLFmyZMmSJUuWLPkHkvI/uwBLlixZsmTJkiVLlixZsmTJkiVLlixZsmTJkiWzLAJjyZIlS5YsWbJkyZIlS5YsWbJkyZIlS5YsWfIPJ4vAWLJkyZIlS5YsWbJkyZIlS5YsWbJkyZIlS5b8w8kiMJYsWbJkyZIlS5YsWbJkyZIlS5YsWbJkyZIl/3Cy/R//1/95cZiHTwKBmMZLhp8dYLIr5U7qAOn9TCCUOAeekiJNpXmeck0HowPcYbHGmXn4s2OeFhUQFdRSQUSgUkBEeo78+1wZ+g6Xk/PgqQZzmpdx0RkAfz8fS4kR7XdOz9pmyo8Z9g/MUlLW7/LTr+1d2pQ7g7u1sfz13iF9ykBHur8B6Oh+LaeysRR+al6GpGXXduvLVCVrv6GPwVrGPlxzai0COrqmRxBOrqJuG7bthvvtC+63F1CtKKWi1hsqbSh0A1GRy0kKzpZHjyqfhaOuuNYp+3VZZG8rBlvbpnZzzdIP4vT9meqkazxfOh/PpWO+PKl1CS28FKLhEvICjgUY1IHG/EadLpdjhl1pLXkCiMb6WSGmucB0LH5D5ijiGFd2YtK7SFbTGNqBkRs25iI523sHtwbtXDDIMoIMpo+FiIY8xzFmrZXGjLYMkh56m1u9tLxEQClF5yBtxKTH46eXCGOiqSWYfd7w3+kzLvSCppUl9e3VWNN+GeezSeYB9sE10SZz0eZ7Yl6NsoVWjFrNT8e4LJf9VKZxzmYUqihl82OmS+x9lErW+/An/Sl9WEpBrUV+o0TbMIu6U5c/0wX9c5UsQCGg2kpNFGrLNlfN5Y8aET3XoXF9jjmTtKjDWjro7NRoyGtv6I6treBrXY2xhLMQDZ9X64y3BaXep9wWcz52Rp6Haq0otXjxZM4c9cjmJ58l8txldSIAkGebrdykyj45pnKLYsBnZtOrbvNTR2sdvTNKIZRCqLiBuAIAOsva27jFcwIzjuNAaw2tNxztiDW9Ab1LSY5jx2N/R+tt7IM0lntqJ+7s5/J4g9UZBLbp1NvatMBnv2jPtPZEm/ji6r9LkTWndXu+KGktl7+jNXBnV7W4m3wMokAGDkl+RZ/vvK6d/VpbcqkUlFI8tWgLDMcAlr7g3MckZU339d6xW1ntfgIKybUvX++4v9yRn89BBCqkz0QVW92wbfp323C73XG/3VGKpCN5EbhH3YhEt328F0mTdHC01vC//6//G5YsWbJkyZIlS5YsWbLkZ8g2/uTh/wz9GPYnh9KL0jOQ8yRnqOj7Z0xmQPUafJ3T9Je1+dwAqDxFmb9Tns/W+2fKBO48AccAxXLSC/uEAsmR71TBgZMZ9HtKII1Ar/xSIPcExMp5AoGZHTyw72AFjuakB3BMClcScMEcwJu83De0doCYwVWRiAKgFsUdijfRrAk0ayalgtj1lAB8RNmGtIhTM8YZqSYN6ZLlqjfQeMvQGD4+7X5Ox/J9zOl31Ogp0UYGHmNQEtYyZ0CHWOeG3FYKBrEXQcZrnl0ifwLQnpfHdZ0Un8pzgYGWhnwptGKgqxML9inXcuoPJ/isjJxydRZrAk1hQHAakXZf7+hH8/QCedR8sm7Q2IcZ+LVxEa3Afp8ARJZWAtOt71L6ohNSZ6lS9zSG1tbONT6NB8Xj9H367eN6nAsCfFW95Ki491Fuc++flIcBm55OnrelkMNK5GnmgzROYZZPHgvj8D2P+3QLDco/5TddC8zz8Nj3ACnpUITYVYC5tY7WYn60XFjB0NYFWC6qMwZC962i1g2lkhIB7PU1QLcocUBk+tgTYSyNyGS5ql7ykzb57BpoGxm0DBTT9SmNeY44JZVBaVKdN/Ji6kfAhomUnuhizRvSk7y5R13JbnKio3jbOEGT8/P+zrMfCcBfpvlZ0xDQt4CYwARUB4zTtX6LbQapsHU1z0++XCpgbgC3kQWtNfk7hMCAzpHUC9CLr9Otd/Te0HVDQ2dGO5qQGP3A0VtKUz47M1pvaP1A5z5ME3k8xFzklYQ/r1HcQDDAvYA4s/dGKCGmfkjbBUesBJ31fVqwbQ3rHTia6WZP86nc2VrTNkrdac8pndGbEhiVlDMkJxs6dyWClMBo3XWvbhsqlCzRMWwEStRHSQIwCgGlVBTdhFFKiethfRzzhWzcAQoVFCp4+eUFty83nwvyECAImSMEqJAZZau46QYQe5bx6aSrbrQgtmyO937tog9HO7BkyZIlS5YsWbJkyZIlP0u268Mz+KZHyUA0nq79vswQ2HV+T2RATT8JmrCBCfqCmsHxPyP+fj0CjP9YEoDuCAzycEnG1hhjL/D4lpuwPh6v5Qyi6IusbzVMwKaDlSPUkkE6IIO4BHQB06LYCi+kF38MGkngTugKjPTWceAAd915XipqbdhqVwAAKAyxxDBgDQJCDLtAVe+4cCqF4yFD81oaDltTSsfToxkDjjpwnBCjJv5g3MTmT2/yoXWlPA6wUexmJ4LsphxUOMCaEfj33hv0QJLkANy9m0jblL1wc79HUgnMeja2Z130BIpbhjGMsEjAiu1mZ9VFBSNt9ygz0HsDcxdAlwxrHTvGrTnAyPqe+9S+9tbRDyE4TE8tRWLy/vAd6LYb3RFyEsAKhN4k7wbbMW+6lWdTDhyYALEgyOPcCmB1EWurAAFzLzxr/onMRpDDZOBxshazrrT91n5dSi/PAMxQ4JEcgGN0seJz/ZvWCE6Nr2c+0qHgddKApavL2U8hY+KE6AM7OQ7jqEvOF8MFsF38dt22VdTbhuM40I8D7Tjw2B94fXtH7wJceupdgMrWDrTeZP4iQlXwcdsqbvcXvHyRuQ29+0xERXaHb7Wg1KIN3oHeFdQ+0HVnflGiw8pabVd16oOx6SZdzDNkmr/S5vahTSKtJ32XrIuG+/LawGl3uR2a5qqwbvEr/F4rU++MozXPi6CkTynSLiAQ1SBhin2BKclUBsa+Nxytg1gsZgbgukh/lFJQqOoudluPCGJ2Rz4OAKB1RmtCdlHX8c5dRr6D5kZ2dbSjCZGv3x/HjnZ0tKNjfxw4Hg3t0dF3DsJCCYzG3cmM42g4diUw+BDQvDN6kz+zuqBC0i7WJFpn22hA1ngEEMXmBbNMsOvljoLbdsdWw2LB5k0b9AbeJx7VLTvcasbyLeTrfmuM/UhWcRzpsBI2Qv5oXUr0mRgGM1BJDJuqlrnLvUc/FOhnH1eqTbi9EO4v0teFCiopMaEmlv1oSoSwz4e3+wvu9xfcbjds26akQ3XywdKSY5quEhIvv95x+3KT5xoOHeHOYCUiQHASBkSomrZZ2fWm9zCD9wNHO2TO6t11Jsixhv3Yse/7aSwvWbJkyZIlS5YsWbJkyR+V7RocTAcowIUMFWdQOUANyndO6bLvhjwBDQoGZmDnjC3ZkUvU6VrSzsmPyYuPIGK7JF+j9X2S3qVLKQVDPlv0PyqZMvD/r4gWbUYB2BKiNAPrjs0YMEmOHeT+GkDLtKM9IUzXzTzkMV5QimxZdfcKhg9dpOPuhUq4cZAX7x3cmgJQBb1U9K3J9dsdtG0oqCBUdzdWKHRygLso1SVXKY8Fnr4mAsBvDbQ1DqklQFyXKZ6ZxIgx0JnFusTODMSAwd0GGEf5Z9dHAdCP37PrDweI9biURPPxAlLoU8mJp9w5SoiL71cyjnr7Vob5xMmLbuXl+K0ERYDc5O0AZlBhw1sd9LJ+8DJbX7p+BxDpl3cG94Y8SIw8lXQSAEfQ3e7qeoMFYOuKphohg5yft0YSLbNkMRIY3qQO8vFVCifJbc1eVQod0bFYSF1S0dhO1qtJM4fUOX/6JELeHpJCWLrEwMlA9jifEkX97BwRJqOUBDgbMJmLly8ecjeiLteGfU6gdM1HQnFx6A4Dx97w/v7A67dX/P777/iP//grHnuDI+uAg7IGLpPeX4tYcmzbpu6KqpIQcPdFLy93vLzc8fXrF7x8uaMWgk6vTkCWQmAUJ0X8uNWJ8/wCX1/JQWJt4dwGlOg2WwN90swrlbXn2FrDs0LqegfBkdTnYv4YjjhJbUPZey30kAjb7eZ9IwBy0bVB1pBwN+SNNywBaXkAmFG3CuraeCT56rSDxgy0BkYD+OH16VofcyHkrpa44zg6Hg8hJcw6Qnb8G5gsVhEDqNwDZBZXUGrpszP63tEPoB9pDlULAva0Lb2Ozs1dQTIAbjJH9NZk/a3V127SNdlJCut/36jAqW/I12Dy9i8gfsj6PKXlze3zsn0nt8SQ9TyRItaPhbSduj83WLp+fWGdQ6JMpRbUuskc1wHaCLQRyqY6wrLINLNeaWGxIPpTcX95we3lHlYPpbrL00ycdCQC43YXImfbULdNy1uCTDVV1GNimVVQSwETYbc001/bVRcOsZgxAsz0liEWO91djtl9B/bHjkOJ1J50qx0tdK6NbsSWLFmyZMmSJUuWLFmy5M9IEBj5Reh0WYAN15J2SHp6I+QYJz9BFjwpxfncR2XSKwywm5ClwRXFZ0qTQYrvZ3vO6wfv+XkSlhEZY3YiYgA0SYFQmtqLgcKy0xC2DxkOvLqLCgd37TZ+2ry2A97LlvILKwwBqJkZnWKnZHZTM9wHARgKVXfb0DujddsNrzsTj10ApXsDcMfGG0rdABSgVt0pCsBIm6EOsqv8rAsGThlQHEDeyZqBhxv9dzcSY7gugbhsoE6AY72pBUEuTkbTzKeJWsMEIWFAVaqXEUW5DEYGONhvePo4J5g1jFtiGO6a3YV8og3OEgDXVDmc/JbrrlKvi4N/AurzNAx9BjCwLGFrVKLZGBCg2n5bORzgzH2t7cJRZhkvEftliAGgJaEex0E0RMsI8iTmrDy9MqD+03kYG1LnbhVwNy9AF2u6HDfgWct/Z+ratoJa4yLbvWxz/bgsTOB2vsnF5hcpZz461H0gMqb0IPWl/Okgn5YrzWGRyUVNFWxn9LiDbB3LJGMCY/PtgdxHGRkOmAKMtne8v+749vsb/vNvf8O//9//jn/913/F77+/oWUOB9bXooRE7Bv0ayXcNpn33t8fqKXgvm24f9lwe9nwT//0F/zzv/wz/pd/+Rf88z//E+73G273DVvdZJ+7xkahG+uucAVzCwVYrX+k87a5BZN7aeyLNFYpGkLPaWyrSbPOrVdhrR/dPc73ukH8u+RFJjyiTHo/gM7SrqVW3F9ubnFBRtCppYUT5SlGiZOlQOxm9+cO4Faq6LQSAUcLC4ejNeztwL4fCiY3JRka9uPA43jgaHsAyE2sJl5fH2hqPdbM0sLc+zig3FMsi9GdkREdxOKeibpEPxnJJZkf8nzutlc6UQ5qzdI/GwURSVBiNpNRaQ6Nz27TlD6GsN/xeDuwP7q6W6tqEWMAfloPbd4FlPgoKLUKGVeKWr+YtYvGCGF9Ztg2ncs21O2mZGBBKZtYI9SKUoBt23C73YSw6UC5FdRbQb0LsWCUuq89Tjh2j8F1v99wu929PtUIH4hVjVi5SB8ZsVOpotDmFhcW/0T6U/So9w4jHhiMRoyOjsfvb3jsD3WnKWU5jgPHY8f7+zsejwd2dQ1m5/djx7Ef2PddrjUSTAkN1lgoYbHT3Nrk7C50yZIlS5YsWbJkyZIlS/68XLuQ+gjcz+Do5a36Uq+7GceTmgQPP6/vT+AHs+yR/O4r0ZygA4XPyIsP6IuLrbQG+D/fZZvqnvL6VMyNP/DC99EdIyQ4kknu8mnK066ST0ptb6hC9ElJdSUKkAKED19eaYhEzR9XIpUqCLLphgQUkV+rMKj6SLd6gIFOHQRxmXIcpEB1Q+kHeq8ovTqoa0CAFRX+28CqoSDxp0BO4DVRTwP/A3DhdCwB0H4PD/cbFGRnemtgDegb2izpe58k5D3Kn3espjLlOnHc4y3h1TSQPkO4qQzKBvgu5aHveLIUifYdxTQx0rW4GfKvxy5pkIDyJc8XWlcSuJRO6SPKaPlY8dOlVvw4nwFrSronN3Ius3cJ+32ii+Sea6wQjABax1kiwHv7nDB/VbukMxzAKmDEVCp7AoUvmmT8/ZzjgMeEMP20PAbykuJa+z60i34lS08SJE1j1LB5hgphS9f0ksnzJEq06xOiwfTAAVuvCo99n9ijwmY1cN2eozXCOT8qABXCdi+4t4qX/YYvv9zxy1++gEnc3ASBFU3GrPdqmxW1wgAD9/sLtlpwu2243zfc7hv+8uuv+OXrL/jy5Qvu9xu22w2bupgJlWZAYwHY1GERU6YlBLDxNtTNFTHUnYZpQuue1wBv5lNDZZLAyzAMUgIKRZBhaxxPw+YgungWiPmvM9yKQeaRDQChaR0IQox2IAhemIUU3PIh4k3AAyozqzVEY7Vc0O8ay+RIZMVxHOhNQGY/3nYcbVcy3mJRdN1R38LigpWgYLWe6OEmqMOsKeAkptW9MEGCcxTALBfzgIaQEX7Yx+P0jEhx7ugMsFg5ZlJWXMON5PUYQwF6nSRZdF7tnUBUAaoSnFwtJGoNN0weR8L+KUmx1S2uU+uLWiRWjMU3KRo7pm4b6lZxUwIjXDKpdUWBu2Uyd260KYFxk3tL2nRhrprMSsEJ71LAULKJzfUWIVsyWF/HANGNJaG92sfq2us4lNBSwp7ZB92+P7Afu5MNrG7ojuPA4/HAoS6hZmsKIzqa6mTrXdK2P+5o2eqxa1yVJUuWLFmyZMmSJUuWLPk7yCWBQYEU4AwqAvjueYznE3DjENYn3nMEfhLAyWDLS3B8epeW7x+gbn9ErpJ70gR/Os7GT5MMfAfoDYzge66G7TB1mMK3rUdfFBZXI8PO2JIQjJRPZJJ2PSeSqtO4Y29wx5NKSKdfV4RWcnNm+JUCWO6iiQFGRzt2OVcZnQXsqH1D6Zu2D3sQzihNTwTDnHMKKMG5vfvYwAncEyuI7iDp0GppJ/0sWbtaOxxg9TxpvDYILAG3UvLgix34g3UA9/EkawYOVPfLnvAdzwowQWs35xZUzjWoPGYMx0gJRUBp62O9wQiOAOiAXgiFDX2b9RKB2/kxa0+MjU2YgPk4b/pJQ11CS2XOM7AuyCCG6aeRMJYejen6MLygYaYD0bfW5+P4GuanK6sDLfBYzYl4sOOkSQwmIwjU2nzhJzdTuczWxr5x3SeQNO6elTEXFqm8/sVXDdjsM3Lzaf6CxJPIc5rjrFdxbCB9Uwzkf0LW03Qgx0UBIAG3K+GFK7jc0PGCjl/wOP4ZX3654zgSKGhzhyqt675NdwWopWLbbhJbYyPcthtu2w1fv37BL798xddfv+DrLy/quqa4e6gA3yUxNiKQMWiigai5r3xDgvajkWUDUK1pRBwC2+1Pw3wZzWUz1wiQ+xKj95LGjoj+sjWvOFhMGiiolDHwsV3uBEbv8r1DgFmtBDHjMAuHQy0YYK6azPrB4kzITnSzCunc8f76wP6+o0FjG3TovU3SbeHqKVz8sBIZB45+RFDupE4e88KJFKu+9qMvSebOCUBhVNSYD7iiYPP2ym2D8SP1JZ303bpSLEzasG6VRFjldXvYIKB9nucni8VS6IbttoGqBZu2v6qxXzaxZCBxxVRI44psFbcqRB3V4qRGrTfcbjclvljS2W7qHkpibtSymcZ6uVBMLcktWagWlK0IeVGFHMkERlNLhUwOHO3wANdBdHQchxAF4cKr+7PCcYSe2Djt6rLJiC4/1zs42DfXoTk/sbgIqwzmNqwVRkyYPso81H3QMNsIlbZhKigWJ+wf5hl4yZIlS5YsWbJkyZIl/1+RzaHqTDLYy62DOWe4FnaPgw8XLywW3NVuH9L6/gsOWxDo5G1itmg45Qe6eHly+CpiKfyU96tcjqv6p3JdgTRXKdrLY/iqeSred6cSGGAgrmIwYdBjd/J84KpU/ueuMga00JCO+C4v8aMfHgMHw6oDCpQS3FUFJsuQS7cuWp5hN77V30B6URqLfeAlY9sxe6B1gA72AKrMjMLNU2rqgircArHvEtfWSC2U+otzvdI1MzDUeSAweAJrHQDUz2jdsEjprXkeuU0ZVxoZ4HAGkTIqVoiAUgbwcUht6g5riTkbBifgyqyWpt23pwS1PwcAGqkmVtYEArNY0ZTCQsYworgMoLuH+wthGSpPdGzOamjUudinHwYQ5nJnyxDLJ/Uqk7vl4dk1imdtwG4uq2XJQxElARt/Mg6Kk1Pm93+utxUr2t5zK2eNIkevL+b5AcS3vfw+OKbJy9rLxsFYb2s1UsTexpaN+VmELAtQ1gBWv3fII9YMoqlgBTqv2SQq5/OKd0XqAKn/nplh6DmLG7DVTYNwH/j1119R64b9aApKs+u2kHeJkPHiChgr/vJlR/imu8u/fnnBly9f8HK/Y6s3cafjAahZ0O8C+USubp5ZO/J8wdqPRsqwt0xA0+ztFTvxbQH2jf1ESOZI0UY0fqf5PAhA8XKw6sTg4hCmWrL2hzEY+Q55Zvad5OLSqTtIHEC0AsgKMnfufuwwkNpJCHZLh9473l/f8XjsEWegw13/tBRfoBkwne49NPh2xMJR90m1SPsaIB1TO8zyysjpYVbwdUn6q6CgM4GsHYc1LdLUwZe0WJ+n0uRoFg3+p13thEdehHNPqg6ZeycP+k2kVhQ31HoTwq1WlEpBXui4qWVTd09GcmxCYGxCYJgVRaGCum243+5h6WHXK0FihIi7JpvmMxsXzBCLPwhRJXsVpOyd1dKBIxC6xIg44jHByAGWc4/H7nEjnMRQ4uA4DuzHgW4EGneNTdHUxZO6f2rNrSFMD4Q4M/dS0qHu+smtPfS8smRG1GRLkrCWYZ8nLldWW3eeLbtLlixZsmTJkiVLlixZ8gdkG4EwfeFNoCvxk7cQA5wUfJjBedsl7ySGiwFe9IkXnAtwVPMcyoj8Yv1M0lUJaAvsJ9Un/cb0M99+KtpTYqWkJniCGObLDV14UpmMoeZLRvywPAWXzxjsdTtfZcjI8JSdN5DLyh0NG4AXD5dneFTOkr5YR1mmzcpDWciAWQwQq76km6VE/DngAgbQ0fsBNEmjlgomAYkKF8+jH+rTGeZygx1IM3DAy5TAAVYwR8CzcUwN5IwCXaNbDU46F2nm0lv6kkawe8MoMOBI78/i0IPHxbA2JXQqAmUF8ifpKXg26NgTnQn91XbIwc8TMJ132X/XTZFfblov8Fukm3Qg43AFEl8iau3/ex2nicEBdNO/NCcE5p3alnPZ7J40pthS5Yu65jakqBNZTa3HY6yYRYEX0AAj00MD5uxOHSvWFw7WM5Qkntou6aBDxZTyiiqJ9D5e7Per5RRF+0fbZEAz9Inndpv1IhEQU5FyKnIeHHHk84UUV2a9sXlpcDN0UYQhOSvDRFKcSYzpzkS+gjTg7lZxv9/RWsdf/rJju91lN3sPix2wkVBB3Oa5pVZJQ3z4b6ib7FR/ud/xcrvjdhOf/hY7w8tVAHRSRHbSyWlw8nwuE1uqU0NMg1TP+EtWASB9Trho6XSNcC1ZAW28lJijEcA+weI2xRIlGLsA/mwBln23e8N+NLztu/vytzgGx9HQjo7mAbCNsBDS42gdrR3DDnlmib/0/vqO/bGjIwJyO3isJLkF6+6MlG9X11Bh+UckFjulV1ejzIvbPOREhq+1SiJxbnNAnGZJ8GnpI511fIrO8xvFfMhQXYk5xFy2GVmtA8PB8JgTcv+qLptlhbpsMmuZWooQcjcJem0kQ6nFyQsjMGpRK4pCKFX0/7Zt2LYtxcAgIQpvd3+OtPw8z1pBpaAdSkwgyABvE21Cj3NxiDvDXrsTGJ3Z9aG1Jq6ajsMJLu7yfNF6w3HseH88ZEOC9WWPAODiBkoIjHZYDJMWpJrFr2jdXUg5+YCG7htidL13YqKhc9O4OjGGjUhyMizXm4WEy1MaxaQaw/P68WDJkiVLlixZsmTJkiVL/pBcx8AwGZEXF9tpJ0DFNcATF0+fPyDEOaDrZ28ycAfujuAMv8QLOvnO4ScF5Pw1/TBQ9lNFY4ccn6U9HuRnJz+WDGQSBKCfkzHQY77PsvbvlN5CbYdmDzBt6Ff2rEEEpBdouSY3ou2klpsjeCWfAhEDAQwOIKW7FzGMxFwUwV/KDeArRGDdRe1gj5IcvTN472hkOz4FuHCQtiN8maf/HSg+dUAQEQP5wqE7hu0ABggkckS/zwSJjAAeVY5Mv8Mtk4Oo1kRe3Gi9DD5mO4xMJiTP93DgkAK8imysByZbFLM4KgQqsz7nT/Z0HGdOAZIBw9Os/Bp4FoD4rOgCnnLBYH3FUbsTiefWQPqzkCH9UioLbp0kwOoApG1oDFowEVqz66ELTY7/fZ5KO+M5lZWtvxUO1jxsjnM/9blERjrlsYhBMQLIvxDS9jK8M+PLBN19retAnhRcX6f6WkOeDBK87RFtPxRJdcLIF4o+oYt0bN3wdhqyMz3xCqrVi/XDuQ2uRvpg2QFKv8nXIC89per4GJbzvcl4IxbXNVu948vLF/zTP/1FAHN1HQQEmJyDEEsi7POH6E9RIDdAWwGGDaA20jTu83m00mnTgqXcSzm1BZWicQlkjUBhQ1/VmibaK3bZk+stiJSyC+U46Y3VS3e9GzYOhoLE5vpJ2qqpmx63nvMiBah8KPB7tEPd84gVxWPf8fb+nlz5aByDI4HFTXa0C7gcQbk9RkCe1xHriMc1YOju/O4B2UH6rEOmhAToulZKHZZowDjDeTBh0GtdENPsinGqd53qei0BpGC9dzyHAtt4cf1ukb/OBbVWvLy8oGjcCbu2VNHJ6H/RYYk/UXG733C/3XG7WwDt6jFabvc7ttvNdciCeNcqlpPiNqpKO5n1hhIhlTR+ha+XOj5K0VgQbdA2e4ZoveOhAaw7NN6IWky6Dh0HGksck9YO9N49tobcY1YXGutE0zOyKjb/SAAAIABJREFUIAdVHwgMtueh5gSG5cEN4dXR5xIlDO05gtOmCCUw2Kxx043S9ULc+XOZtoZYAxZXpbymWoPFcjetIin+y5IlS5YsWbJkyZIlS5b8LNnYX54xvdkaLEPnuwxU5BaH0g7Bk1wcupLTC0/e2T8nmQGiDHgP9+YbMriYM70uw7ke84VD4qfyXV2sMEwc4vEcphfM78nZnVYCa+mimjy2wXVujpada6EvyDMoPIBKnyy9AfWxuzYDoZJPJi8uA7HrG3ehsDaRF/gGUB1A4a7YnWA18YLPGkCTiNDoUDLEdjY7Bqx5WrNc15DTf0FgWP9y1CUBD1bX2DmbALcrcNx/SgalkgCT3u9GaliFaWq7qBCPKIRn4zE1nAiM+tvmaddkszBJnR9YOSvQoTpDCFCTBBAJvQmoU0o2/SZr166AmvqDscCmXBC78628spPZgPiofgQRDzCO3W0Tsts1xxRHcL4nwIdzepgAw0FtafqWd/+PoK4RUMlzTJTBdzWba54ENA36Jv1r7nJcrxzshtf92aA1PXo2rVvds9jyMNYwsNnT3BqFVx1JcyTYvQvlIL+20zuDbj6368Wuv16wuCF6M1+Qa3DWy/hGQIrBIkcUhs/95/eMAeJzGXuXTi5VANhivvprifN2m84TtRQQ1UhQQfDYcW3EVtFAyNqOYNmp3Zu7r+vood9EHhcg63BszqbzGCsFVCvErZVZbzBILcti2Q3djomfYp2yScFyFkVVa4SYnAKklYsMHG49fPkfveGxP5Jvf7WGUKLCAmbLn1hdHIcSGI8d7wOBofExWrjs6W61wegdGqB7Iu3TwlHrhkLVYwlYX5q1YZBeapVS4EC7KhWc2GEhappugacSaxyLSvn48h6cFmhZe033NXg3J5Mcc7vot1EqoxIA03xlg+9+u+Hr16/Ybje3aCC1LtqUrChF4q/UUlFvFdvthvv9jpeXF9xfwnpo2yS+xXaTANv22OFzCNkaVZN+WZPp7+z5zT/lR+9dlxIZPxYQfd937PuON3X91XAIgdGMhEqERGt4HDv2/YHWDtQSBIbEQ2luvXMcu7qQ0rm4i8VNb5Le/hCdFQsH9ngWOai2FJi8voXGemfxlYA7nMBIGx7smVCegTiR4NqfXRqbdI3N7jM/yDA2iSwCY8mSJUuWLFmyZMmSJT9RNiADqAhg+Ecko7ync3/gHr0x72SMcvIIFCSxl2xL368g+jCnPyaGln3yUnBCeXFR9ZxWwTlwxSnBdE/sqPVfGWhPRY12U9DLT8R9FkDSd/bqX9dAlO4L3YueypGJB5rayNqB4oAAOnFfJmWILt3uJwBUrnMQDgzZEGwBOLW+FBuDDWPzcB2Bo+n3eD3nHkCvldcwJWYDiXlo5th1Gvc44MaM2Zd+BhSsznrXWOncn1oAaZ+KaGZOfeElUtBn1D2y4kz5cEJ8AwazmAOKn7Ddn3ZLuwrk9AJodGgk7U73caxesLL7ML+fAjwrJLECDFKzUlLKy8gXvx1qpKEHsksMQSfz7nkta0a9KecRhwtZW7B7RHJ1YCOwrmcdA7fzuHRcN5Xd2ivuk+8lULoAF1MRRpDZnFAh2pxi/I4j1AZJHDTbsdNsPU9BuT98TFH0n9XRMdmpbXwABonn8UlS43uQc5/fjY4cKgIGw20UqKAowcDWyIM5U95xbmW3OcKsAKfCnjos8pV5NWyAAnW19jEAUmLNNMicK/FaxBpDdj+TjzWpvJGuU2XtZyIyJFB2R0MmIVlmSo1NJLRL2p1ehAwteYwCaE3A9roZYFxAHhhagXcto5GV7PN6dK8tJZ2jnAx21zcCEMs6046Gfd8V/BVXTdxZYkK0ptYU6kbHXPTorvh9P7A3CXydx2RLMQOCiJC6tW55q9tAtpnaqEAShWSANV5EJrhLjbmWM5nBANEGUEUpEvTdJmxZEqKd0sosv1v0gU8vzB7gG0SgEgSINDQh/ERZP2rfWh9T1RgoBVtRCwZsQooRVA+KW/qUItY8VcmE27Y5QVFqxVZqWDxsVQgHdcMk1hCEbdtwu900dkVRa0fLJywmzFpCqlI0JpNaUaolC0Nca7FZvrT3cAOmesEtrBvMDaQ940DbtxRxQ8WspJC6Afv99294fX3F67dXPPYdjB7zoVre7PuOx+ORArgfMoZNB2DPT3DLHwvi7c+mSTfFXVTT9STWRxOzVI25PubU/AwyTkjxHEWweCVpPYHoCldbIKdzQKxt9t2I2nmZTnd7hKCr2FJLlixZsmTJkiVLlixZ8gclXEidECpcHUjAhqGYfyDX0z00AZ8zHvrxbq4B5Jt+D6nNxzkBYvMdDqLPQKmW5/KuD4SmL4FMDUWM3etXjXtCDVNpFAhxMOR5ezmAGygl/CU47XJlDUxp3508Qko/A9QKaGUSQ6TKhdn3vZUj9etIfAj0erXze2iKVBqBmAIYZrs/dT+nzxMqbTqtiiG7/aMlY7epXZZbQu4tVNS9e4BP8Gt75MEMMgsFmN4OhRnGxFVQdylOAqtPY0QDGQ86HnC0ZTf2Qc7AdCK1jTXcgBhTqmikEUE/o/2KAduuteGKiBLgmesgWAgFOTSPO0NHTwdFh07N432s5EMmPTLuMswrWd/hvZs2H8NwLtJ7FTo81wcG8idigK7mLlIdzmDVmKq7zMrXpGnC9H2cTcb7fb5IgZRDb4fiRC0MufX+TACq1Z+SZYI27LDJN7fOUDcOrtf6yjAxL5ONOldQKxgcz2WZEXLABznM3rZzWwGia0ZwjuG6p/Yl0iaLg0aYTvgeJCUtmFlFkZFLyQKSAepd5j6YNZgVhnxemxLXcutv3aXtO6oHfU4Eq45P11eCEh9CbPga7IS1EsxBDfm01hkaCDgCUQeILymZmycjCSzuxLEbqcDqTkesIx7vDxz7gX1P1hVNAhZL7IimBIR835X02JWgQGo9MCINdQOVm6Zb30F01vtEJ4NStkQ+AGwuCTsGQooKgRSsBkMB9wpwibWpxNzElm8iMgis5Ynuy/vmLdi1kbpGFNiYgx7zwNVKVlj8iFo3IRFqwa3eJIYESeBqIRwkbkrdNg+cLcHhN9zvN2y3G263m1sN3aqSIOY2Sl1EGSlGhTxWhVgMFm8z6xy2tuKI/yHEggSwPhTcD+LKSKvDCamjHx6U2lx9tV0JL+bxrzO2bcPLy4vnY+TWb7/9hm/ffsfr6xuO/fDnFtuQ0ZXAeH9/IAhxjZXBoui27hGntc6IO+vVvJamcU1BUce0Cqil5bympIkrHbJ7mNL4TunFA1B2P3gtIw9xcS2PR2n8b8mSJUuWLFmyZMmSJUt+imxAAmVGpFPkBChmsZ3m9Py60zvMs5caszr4MMMfks+9Pg2o5t9H/KXxR/L4XjvMyOKz6z973BG7dCqn60qi/5cRAMzwKJ1eZwF0VZPr8ly7HntSr+xqa9i1n+pwkZSDxZi7xNIg5KsiVQMl8170FNB5qi0lgNV3SNKk21fVnY5z+t/ArqFlc9tnAN8AjAlEnz/5SfM+k1Msbi/vBzomyE3k2xVAThWxent3M2LnpwFdujM1b3r3gM+DZdO5YWn6ZZCShT74PtbyRGeRtMUAdiLIduEZaIKCxRGkGyDFMp9nfjJUOJWXI5b5nMw8DJWwIS5ecLFISlYPlmrq6zExHj5kR3fBKFHIoQ2m8pnlBBnoHp2elGIohKYT+jNYz/gwVn3jogAiqY3WLCc7p7QYaur208pHeVQChOR651RHBaVJd2Z3VxIhGFKRvSZKPBAXmJUHezuOnT80DQPw8D0KENtu/FwssnEDnTbS/EmiB01JAcViJV5EH+d6J7k7o2sw7NbM5U3sNhcXTt0tL8SVk1hVmCsmsbI4NKBxl0DFGvTY41UouN2MuLC0ocGve9NgyPJnrjFLIpOFFiowG8Kxl9SSjwpK2UCQ0M2xnCTShoS4kDgFXVWjiPWfBra2OQCkIQG4q4s6O5A6heMH6TqjYZvSHCgWOWMcByEKLL4E3IKmituqraJu4q6JEhFRa3UyY6s3iTVBm5LvelzjUIjLp+Kkxu22CXFRq+dfKemb6RhM1VWHGRpnJM2ZnAgL0xc210kG+ne8vr/h/f0t6Z3qjbkOOyQ+ROsRQL0dR5AYmmnENBH92qoQGFIURmtCYry+fsPb+5tYWBwHbB4Iy0FozIsjjV/Rc9sAQrouZavYkPzMYhOxDjbkeRDjuu63f7Roz08J6bnsdESPDw8C6ppvuGheQX/mU/qSJUuWLFmyZMmSJUuWfE42ohHEGYDk9C40g3E0/E/zOw5OYO0AetIQKPe0W31I42PAe7bMsNfFZ+5bLlLCd9DLizJ9Xs7OZz5/5+B/Azihimc3MKlNeXxpHiwmpmY9BUp39IGHvAVLmaITzO/LT8QClM7lyvXxXYIAxM8zn/VxePFnnGv5vOe/s9EQGFJSEIKSBYanHmUYAGy/LqWSy3zKP48iTui0F2C6duxluy+FDhmzGcavflJqh4/6Lg8LqwPB6z/X+SOJVjIdvABVKGVpgDEQoOxQQWuH2M0aLSl5ZcuLjyqX40lcXXV1UPQ+98W0uzX3K0XRbc6j6bpJY0Z1sfJNt4QlQ5o9aUovgfm5PLZDXwDG1DepoUl3mM+NEHOF1V92nV/L0DraFuMYO2uujfmkqIPC2u/TjDdeStreVo/BI98VBEepreW+IPbZzztp5m1BSMzBWBoybTSXQ2SaKfcyht3PbLvnB1JE/tgtqTIpZKnBJlY9W2Mn/jwOtM9FzxjwHe/sO+ANWDbDgKMHgeCucMxdT+tiQdEa9v0Itz19jBnBgAdBPg4hJ8wKwwNjp6DGrIRI7xZfQrXOYgNoAOxO0HL1gURhtfZiWIBxvd/6MA0RaQr2vqJStakY4SKugEtB1lrbd8+dIcGvAZQy9g+Ruw3qpp+Wve+KV3pq6CftP7c6E3LBLCK2212JCULdlFBQImMrG7btJq6c1J1TLXZeLCuKBtMWqwgLgi0um0pJ8SrU2oOInMzwRYTG+pgVRecg64UfYxx8AAy3nvD+sxgPFrC6BwkGAJ0bfvvtN/z+7Xfrqehv7uhNdUfJDNfR1gb9AyMIjNaxtwO1Vtxvd+8vsxp6f3/Hvj/c0sfWPSePAC+rjFudE5JF6Ti3Tc9eef5kuFvG00I+3ERp2pvmr4t7KK2OAOmjXF6DXdGmvNKcwkkppzxOM2j68WzfyZIlS5YsWbJkyZIlS5b8Gbm2wDAZdjjPINWFK5dZJv/lLn/XlxuHnk/ZZuDyz6T+edH8fsjyYrp3fgl+KgquZNRx3mKf0FF/wQ1fKQ7iBOpKAWqSwXWknkzk3ow1AwHsflQt4vxyzchETII7xxfuMQmvzwSN/13E6u34rn5xLAFwouXU0zQGpD6713qWaQI2DSx3MMERMG+t6zbIrUOQoMOkoHVKilIw+KuiTTjy5Si6uG9QBepgMt/7c8LRxp8fnlc9nlGUlHA6NWDhTBcHPysXBXXFzRYJHCjVQMaNeZ3gdOu6qbG9jUzfnuhSbmMfXwndJ5TYiZ/S9XFJSHEmkAb5mJboX0lNncH9a5nCyCYrKK1Xibllqrx+GtBmN5NjbWRAn05MnH3xeCIXswZpXAevvF4yWQtJXjmN4sRDTo8S6eEuyvyS5J7J+tLNgazWJdbdod9CIQhTviRpj9YaVqcAQu23HTFA2OIFCGHQdWc8cPSIESGAcAQobkdy3bTvHmciA8iyA54crDYXUV0Jita6Bxa3QMoWWNksPbztE0gspACjE7sLq97kPirVA9XEmpUmMA+Mk136kMxRRWNBWBwPMCwaOqspH3vbqttGJcgKqQUN2RghoCuJNqxwMfd4nAqK3xZbhGqytCCxgthuN9xfvqJuRd0zVSUxJPj1TeNNbHeNO7GJeyi3vDBiQtMzawxx+wQJrq3uowgRE4WU9HBSSecLI7/QAKYWbicR/ddMZ46kG0ZctK7B5Xu4e+oyb/be8de//gf+9tvfXKezfvbeJc0u7si6WQUlMmR2H9Vaw94OcX+1bTqqiqkUjmMX/VQLHycvAO8bc2c1DLVJ3EI1BloMReR78hyT3WbmeSvfOz7dXtiRwbQ9zxa25j2fme354uSk6nk4Cxo+zsVesmTJkiVLlixZsmTJkp8kYYHh79MXUCjR+NLlGEi8vQx+9DHhdaddXDykOdw3fGf87LehcWfa+cXzU+Dy92QGS3F+AQz/5OOJnP9sqfAcZqXUXvr9A9Ik2lm/eXkNnLGgzQkoN1CGZQerv0cza114AuctnkH4WWe9P+N/nIEBGAhp5cHYiJlwmVrlCoN2XZ361FUvgwrp1gB8WYFcO6u7E404ycCFf591Vr4XsxhKu27BQd6czUPClmQEgzMIqWXwPhvFyArb9x2/hwIL4DcMUR4qFu3oVR0+r4RTmqH84kpnqEHa1fosxeDkrjT/fFsGgOYg2EEGlVFfrxGoswTu6ACzE0JWmMml1UUI7CkTbXWdiGMs23gJKwzXHeR+oHSfHpvGnuemW9mZxxgA49bwXHbXALUgGPV7BvJS8UTFiHyIkf3zHdyzzhrQbHORXRdZmm/5UxNCiYU8bVkjFSVY8nql6dhc4EQMj8mOAXwVkLWZljXXqnMjUlwCgrj0sXFvxWQAXLx/yyYgdZAV1k7FS2Xrs1mUeF9YUxh6TkBvjMdjd6sKvVgBXduh3oed8F0DCJvbpt47OtSCojMeR8N+xHW9jwSFxJSQ+BRmZWFugMylj7nTsRgY3CNOhh9z3Qx98e5FqAFrYzJL3youC0YBFV13iNCpgHvxMR4uhMQaRlSAfJTY0tIeza+39quVUKoQBkZJSysB3ax8uEoZSK0+SL7fNnGzFEREkAGl1ogzoZ/F4lWotcS2bW4ZUbaC2+2O+8sXVC2PkxP6uWmQ7XqrYX1RgqQwKxG3psuxqbzxtX2sn9DR+xFxSI5DdAw9LCeauAc79j3FJ5E/IQV2cRVmFjfN3EB1HH10AeXPDtwlHsXbN59YOjIh0cEepDvuNbdlzKzB7aM/O8RNGtGBY9+H+RMEt0QyayQL5m7zRZC9+n3Q12l+utjQMc554xxIg9aniSw9c8xk8inN86NEfmI4leej4+x1/kD4O+eXLFmyZMmSJUuWLFmy5CdIWGCMX9KHASsOH8Tx5Nb/mUunH5MBgsPf97VoRrmeFOd0iH6gjmeg/Y/Jn2nTqQjfC3ygQJngh6RgIEN2KLNutlYigQKA909zocAfxzMxfZHAlLH32c/R+dqc/hUwcJkHpftzmlDQ34KxDumxg+45KHjkmSwfDFhPoPmJv9AyzOSYBa9+pmd+0eBaIu2ot/wVCD41iREwXuLRwZbczw76jqnr7+yP+0dEy2VfRwTX6nVul/w7dCmBsR+SAR8UJ1ksnCxaniQ3x4UY9Yfgu6cTwRDdP8JS0sYpsUQUDy2uADmlNKL/zmUlVvcyBn5zTo9AaD6vG1gcYyi3Afkdc3tNo3PI/3EcaP1IYzZ2nwtYK2kHERNtVko6BtHBzhF7YKqpW1OEJrhSKXmQxoABb1oOCfZhY4qnlOHH2YgTTd/AUADqmqaHBy6W8VQI4oJH/+AxB+TTpt0MN1of1Lu4+jFEVMhhiqU2zU1Sf2kHZkiwYB8bEjlhP97x229v4s6JYxf6vu8OIh+H7Hbvah3RWUDk/TjweDwcfG6HAMzf3h542w8nq8xag1va6e5Vo2FvgunsMO9QkXYCaR3GLimoIBTXjxx3RFYctQ4Bo1AVAog0SgXHfMFMSlaYNYBYagQRZapiVhOEdjQ83h8AxxgkItzuBRuTkgZFy1zA3CQfBggVTJsTF6RxI379+gu+3F/cBVMtRlYU3JRsuN/uYjVxU9dPdvwux2stQlBsxY9b8G5S6yGqkWfdqsfJkOVJ+4VJLUKkTdqxo7fQFQui3o6mumKBshse+473xzv2fZf4EGaNw0I+cGPsjwP7Q+6z4OqNO/b9gV1jSki8E1bSS+NY8KGWFeG+zEgHRhPLDu9/0xfOR3CyRGCZH+M5VXu0FtBWwczYj32aCyjie4h/Mslf43NAy1dKQdlEh21uY0yKfGlxQTbUBynp0dpD1CT3lXb/R/YTz+TPPHn62v2D55YsWbJkyZIlS5YsWbLkZ8sGopOHo8ATA0hmnl4Qebp4BgV/4M1msIj4KUTID8jHGOapfsOh6ficUsAuz18hM/Az5xMXRUr2Zm/uX84xQCy/GaSjdH4EhXOPGeA0Qt3q+icFsc3Xm5sF29EeO9un1++hKRLEddKdq/rP95+rSYCDoaa7NAFq9tbN2iYMBemV2LFdl1aeXA3O/1HKK4PRQ7OfAWsDmu30HIPGi2iJXTTJOMLOehm/Fdqm3K/52kDxsvrNWdIVcv5EvP+dMtFSDrfTKc0MmF+l+aMy3/O5NEyb7R4MXTiCskYpzJBSoh5SR3Eug6fJAXplsMt2dGv6bAGdgbBgGvIrygepf/9TUHPrZEYpBl4OrZ9+JDLG54wy6VaaGTYAXFN/Rpu4pYWb4+nudatqj33glman0Gsa8iLkMST1neG9oXF9HlNziFQWu3waj1ZyjryYbXc3ULU9ichdEBEDKKRxQIJIAkngcCNdYte4drGRD+9HIkoUnmcBd1ldLBloKoSB+Op3AkHdPlmf7o8d37694jiOcKXDPYgLDXgsO+N7WEwwa8DsBCR3IW32zjisvLobH52VSCGAq7RXSeQUrsQWW4K4WSNdWrSvnWAqrivAuMaxndeGNMsNJgaTEs6gIeQJq+4RkfShlQFJR41QA4H+Sck3mIUD4XZ/wW27oW5iGeGKqnO4Bb7ebjcJhq16Uqjgy5cveLndIxi2xpUolVCrkhVqbWFEhRFgFkjb9M6sNkBFCb8OVssatjYkAJTilbRY6013zELmOCLWQ/Og2Af248Cxi6XF0Q51FSZuwo4m17hLJm5O9rbWcDwOIcGQdUuICzbd7Rw6yBpW3QlrIAJg6yCj6DeyRcpiCtnIPa1/cs0wj5OMVSGh9DlqerYrBFQqbpBFqkOUgotLP1b45H6xsWK2urUl15mK6WQcep7G5XM1DR8XQkPMuc9KLFPXDgGvZt8lS5YsWbJkyZIlS5Ys+XvJxpdIw/w6whdvUh8D82kj6+ck3u7+MeRTZZ/aZSZ10okAP//Eq95AZORU7TT7yT9rCONQIZFsiyYOWEgBOqA74D2TFwzWHZ9R59j9HmCtgcFD9qcd+DNYGfcYRG6AaSFCyfCZg5VIfZXc9FDSZK1ejgfhfsy1wNm9VPgqN6CZBxUeCQa9PlFDg1/2sfnhwXcvOjIDC1eS43E4nGxEiQPNKbPUNJ8hEJ9dc0VCGJF1mfeTdK9+X1nhXJXxe0TFlYWHntHfeYRm4GxKB2Ndxx3m4w3OOUzFZQduR+Is+/yHa0yBezXnqT18PJ6RLLJ66CfVlH6u/4ftRqOe2rVEqLSh6nATF0dl3ngs48hAaZu3+tj23n4FCJTZK6eJuNMbxb8zgPlcZ4MMUhc6+SYd4xYs2Uoj40XmOBMJbnxzgNkS77o7290hIb5bYGkjErpaHJjlwvv7Ox6PB44e17TW1GJC3DNxE5OL1jraIem1XeJPtF2DYHteFiBb9KkbgHwcHtC4swUfjnbj1H4MVpd0Eieh3G8gB+11tiSACkBQsF4BdwGSx4eKsyWTtrP2i1vSsO56H8oEx4hHESD7aDsOKAlEEr+BzBMYQ8lb8v4rZh2j1gtFyQc7dr/f8fWXX7FtN1TaUNR11P32gvvtjnorQjLQJkQECXFxv99xv9+x3W7SPg7AQywravVYEkaMCDlS3JJCmoaG/vBYJKl/OkPdMB1OOAhBoPrBB45jx+MhhMOuLr666UfSBdGzQ48J8XCoDu27XGe6EwGxU4dQzDlFSYHeGsBdp6o0p7GNrhSXxYY2wt1afnhkMDoRuEwu756M9ULff8ISArK4dRKme6oGS5cHg6LkSkenLkQZS123uglRowRSaOaTfFMNzLCR87ybSIxsJXdS/qcBKZ7IT9oMFE9BS5YsWbJkyZIlS5YsWfJfL9vkneYshgQgHJvIpZTOXcnF6w7pz2FLvB3+B3st4tOXP5cYYWR0fuClUlyKZNDxIwuMWWg6+SRfsh3z0OC9af88QQPr6g9OQD6M6MD0Ek+QndvnN3DnB76HNNjFrqMMzOmR7YOfyYZTQt/Py4CUIjU3wMn0VjAZbZUJ9EQibJDaxtNNRXPger4uLh/qFyxROgQDuzCAHOQnNa/LJnHKx288udWeAXIvW0R0mK1HTpZcjshQLn7KJulQAu0HOImiKCOwbx8GCAW4P19DV8e9Lmmv9jwvET0BiyjV7aryGYgb9eAkDJC5ClFwzD5JiQnzqW9EAlnaOV0a63wqiyQLIxaBTHJy6O3UST7+BwDalA2y675H2QIVH/OOtUM1/9ngT7vlz/cC4i5HAMjxbACePigsrWKgKV32ATPACTiX+dV+hwUGcAC0p/vEIsFiAJh7rs4SO8KCFHf1829BprtZRrSO/XjgcewDwdF7093vsVMeZnmhO+plx3xHPzr6nggSjUNggLGTKQrGukswSt2qc2q4H+OkY0X+uMDnjTwXmfIxaRtOcwL4tEblPqOsFxxEmVi0yLJjsUVMP8yyDCDc6Y5exK0XqXumUovHH8nWELVWbLUm10tqAeTWDQW3+x1fvnzFVm8oZVOLB8Kt3iXgs1pkFNK4ElRwu91wf3nB/XbDtt2in7rFh2h4b4evF2Wem9K6IcSXWjZ0CW4tgdWjHU0fLW5Ja0ow7GpFwXLMLCja0QcdlVglRmgcSm6IjjJrmkcEYO9GfGnMlGF4pa51a5jeYswEIp/6P9vHRBqDlmRrDH/2mC86C9taeTphX2Ix594HItW+CQHDbrUQVnJyv8XBMssnI92++0z30elTsa2cHyf5eelIC8UPy3kET8exClt5AAAgAElEQVT/WLJLlixZsmTJkiVLlixZ8mnZwt9uOjq+SU63BOBgMPdwOF0270T2F9oJh8z5fNZ11M92MfWsllcvkLM7nGEj81XigtBeXjFfr/DZ+NvBRR4u9N2Nw+fVS+8MrkbwYn/h5+Hs2LuT6ygvFOV79Luna+BWstrwa3JzBIQ1xjrwxph+BOid60QXaqbbb53seSbh7srAuXCB4/Xp5rpG3SMprqe5Q6G6AKO9JNDgrRlzjisuhs3wiw3JNsAI7Dv6fWd5dqXB0Qce3wIj3n1ZAsclQ08H2iHh0oO1wXTuJDRey8j/ha7I5xT7IeEtYeXSPbURs7d2GArqu7ztYs7V8f840kiJUjR63MMY8hnCKuiQEMIB4oukX4/xnIt92FgcQDhVHAPUx/nWyDRrK0r3WGbF75mJM6R2B8INkVeIT19DCACxWDs5mWCNkYGyWVeuQbSsd+ZeyFxFiQ5bHxUQVfjgc2AuVVl3TAs4XUClni14TN+VGLB5V8B+hv1z4NgAZSMjzJ2M+fFXELh3JTBax2EAcw8Cox1GYmi8gb7j6If72O8sabbDQGPtJ9v4zjpPdjiRwXt3104WdDl3sVisaEOS7FKHzlPEFsS6u1uusS+EiI45MY5Hu5Niwkm7rJzjJJ4VIdnJpZnGMiE42VKpOolXqga/1mDYqABVlj6uFVvdJNbETSxCNrO2KEI03LZtsJqz+ocbqBteXl5Qy4ZSgrDYyoatVG8bIy+c9HhRF1N1cyuHox3Y24HX11fsj4eQfaHw0tJDIPNEerSGoze3jojg1vGcJEHW9ZrehMhqDQc3jUnRhODKpBkLKWJEG7tVkP5BgP1uwbuzriPnP60DDLVOYCU5bEKc16Y01vODwzSCZczFXE5m0ZKeG2JApzspyniaZZgAm+MsaPzApch9XcfRuM6pvtjCou1v+ymkKmQde53/1ZEnzEBsE+DzaZqvfZ6+Xfi90GcfindDXuxSctdT+pIlS5YsWbJkyZIlS5b8VNmenskvO7Nrlwy6KYB0BUl9Vq6vzMCHAXs/+g72d3qr+rAQV3mewbyfVYQzsGgv2ClvjyDb56vjtgng5dPL88UbsF37tMrPGkotNqD+rP9Qr1LyCKUBxtMFT0F7mn4kBMGNHZI7jxz0WfA/jTMwdykb4CpI40jejTvK/0BlozkNLDBsKPv0N1DJftNcyFTRp9ll6iolPBXofOSi2AZypjZ3qx7HZyKl0Gl2UNR9d1/VZWA4YOjYULiBLBgwL4r2e1aT2cqBp/qc0CfbyV0dEHcuIeXjYBdNaWh06LDIULCPLFYLjToMhYFzX1OcmdtodJ1lLt8IQEs3cpoDxs9B0jWhK1PhxqaJ9K4ucdJDAxL7bXP/lPQ35seAEg7SjqwxKYpszw8A2NYRBdkdNGb2XeZGJjQOkLkdBx7qcqe15nEojn7g0N8tgdBHS25+zI1US0B167JbXmMI+O77HnEtRJEIiclxQqMzhLA41EqDOSyybM7R20oPiwMbhTGyA2i+6kHmAotqE0yszJ3kZRvv5imtNMsiSmHupozglbTdYsJiRpQqbrtAHoPCCYQboeixagTGtqHe9Pe2acDsirsFv7Z6qBIYoSUuwjYhIsqWXEspYQJyMN9cTpVS8HK/48vLF2ybBPA+DrG24J1x9IZ9f+D3129q+aGt4QQBu2undrQgvTQeyUODr4ebstALJ8fMQuKQ35kMkTgqCKKEkyUFeCQFrD1sDnBSAyNobWM/Ea0MwMJJBIFhJ2IkO7kwWwoaOzdYwamLStPptOwmnmCcoE6KR5fn4975IUFJN2LXeZuDpMykbZQS0TXY1+hhwhyfSzCd8ueJoSyZvLi4aZDPPFekteF0+yefva4m7e9M+UuWLFmyZMmSJUuWLFnyM2UbTe4NfMhwAwIYzeIneLjj+x6Ir+T8EpXx9yeXfCe5GdCDvvReg/Au5prpR14Mh0M0vhNm4PMz4m/BA7Z7KSPAmHfOZqR7LsuYqAMq+cV/fsmfimY7/zldeP1azaf2dhBVUzhlPxEGAtqdG0IhhQSeEoygsR21PFckxcAI9aDoc47y5bL6d0qgdLLO8HLark2tlJEY4drK6pXolu90Mvl/oUpDMOjpAsqExlQHyxvpFpehaWga80/uyYfoycjPO3CtK3lURXef4vnbbwO5si6P1jZncnWsTlTv7PaJ50pdgLjRxwyYG6KAfaeqEsw0x9yM0Jiaf5xyss5Nqs9g2VGsB6wphnytvxA6FWW+QJjYiAu7mTBaM4zlYu+sKR29VXbGlyhgTjZG+NTMV5pCEPImxangSMeuEd0sPoZ8+DBSjAnbha3WGWaJpEGpYUGqFWxt6ve/ZYKBNaYEB9DcjkPiVRwHDrWQ4NaxO5Dcfae7WF2EBUZP59gsNXpHQ7gHGv56UzBUXT5xgv0TadA7gy1IcwZ6bZ4oQRaDS5qLgOiw06o4SlfVTJYxrneJFAlMmhzvNM1knwst9gS8r92dUzHyomiMASEh6lbV4kKCWpctyIO6FZSbEBx12ySQ9lZRNiEzbjeJWVJqwZeXF7zc766rZiHCGq+hlIpaNE+SPC1Whf1ru1hDIFmB3LYbttsNtVQnBrr2/b4/8Pu33/Gff/sbqCqBYaRY62hqjeMB1ruSGGrB8TiMwAhyzVyX9d7UbVi4HrN4DGzB2TvU2gBJv7rXXZaqAOk/+5gSuhFz4fDYM07SFzdegfNIA9rmE1YSw8a+3T09oc2TYuYATiRF5F+GVMQCxYnAvEKQzU+RhgX0NiLc17RnHMNURkvX3biezKDznP6duuZ8ZrG15XScP+xvnr7EWj+lTfPBJUuWLFmyZMmSJUuWLPn5snXAX/biFcTc4ejbyY+QBz8i33vxSpcN+NknypNhmh8qSH5r++Su+ROIay/OjgkojJMDjth2/8uynOHX578+W0ABNT4rHEyFgwB8+Xb+h0oEd8aQ+3TAWdMO4ekF3EBhGsBX6y/2+6cMgyih0T2EaYuDfslf9EBqlGSZQMAzcmD4/wQiP2+rS22zOnm2NF6ofuMDs6ZJ+b+vw55mKlq4inmSjGEf83FFMR308WoHeSM+zUmBIslY8NeJuALGviJA+jtPHKpJXs7IP9zXZKLrWSNQ+sjuqXz/LUznIl2eulP00YNtp3JeY1pjeWZ9ZnQQ9QFenjU3AL1QlQFIhIF90m5OFg1uQMad8HMpTe9mQo+IJK6z70wmJ0i8/0vBIAZ4E4WOAXp/iX5ygDnNRQxEsOz49JgBh+xal5gBEdT6cezYjz2A3wQCiwuehn0/sHtw4yAzDiUf3NWO/TElsFrd9HDocbe81J0VIASK10jLLlW1OUuukT60tShcgBmISykYeU0BqyUPcW0j7VtjTRLmE32Y1sm0bHCz55YQetXRhHCpFajVgnYH2SSWCjnmRJAS26YunWpR909KPJgVRamoNX5XIy2qWELI/ZoHiruQMksNFICrWT4VJysi5sXN0//y5QUvLy9qncDiYsmsBQioVaw8ehP3Y3UiMMDAToTjOHzQde54HA/wm+jB4/HA+9s7Xl+/4fXtDb/9/jv+23//b/j3v/4HqraTWcu4KzHVVbY4JmzkQ3fXZEZACPmh5U9u37qeG6Zr56fSoFZ3kNbdxS2eaNBF7/1pHc0urKYT6Cxxo2p20zVcZgRKLmTSuVmytcOYCsxScJZhdnyy1BJp36oee9wbHUitH+jcA98nm9mjdY0Ecldklh9LWwzk8ZNyhFun6Rrf5PHRcxVFvzLwsSVFWvuu5vfviK0t+RGNGa4vf2zT0pIlS5YsWbJkyZIlS5b8mIgLqen9w0zoJyT04tKLl6YffJdhRdyGHfNX8n3899N55ST56r3Obzhndun+JKWX85pf7E4v3VckBl/94POpqzJegMyOaNpLegLQWP0znAHRlC/D/a/bsQBfU94OiGYQOr5kfMJBEk4XzqSCvy0r2D3VLbsfORefr9siERyn9BzANhA69NLKfAWyjMGGxxgO8/lTmwFjw5yLBS1Euu/KAiHAYN8dXaQBv2fF42W8QNid2Lhsy8h+xF5ouMYtavx4so7xYB6Wn0FBFAf8M3337o3r5p36ub3s8wpo4ZS2NGUmL1Iefr+NYgUMjZSkuI45+mMAl0+lGxs3dDqVnABytyZRgijiXKcROGO73juKp7yT7pwARdsRj+k7HOQ+oAGiT0Cd5jnN6z7K2Wo8zi3c2H32u8sbQYrVxZOA6a3Lda0HsbDvQlSYFYXFEHh7vOOxP3zO82DbSmC0o8m9e7KY4ACPjRCJ+UBAX3c1Y/OjEy5hRRZraWpjCqJCjHrGtpOvZoViBKX1V7jQkm5JY5QBCRUhXzzgOeC6HXNjLKjVsrcg00ZQaHlvvYMB1BpkgrhwEtKBqt5DEty6KqlRasHtdsftdsO2qSVFCXdPls62VU/byItt23C/3bFt6srJia8oOjOjgdEtngeFjjKzWnHc3Irj5eULvry8qGuvhl6KxIrQOWnbKnrreH97BxiJwAAM5DcLHE5umkASE2Pfd7y+vuHt9RXfvn3D6+srfvv9N/zbv/0P/Md//tXrIbEXVNe5oycrHIATsJ3WaQfH4TpsE6GdIp2zi/ZbEF8FadCBiGNJ5WjX0+xo+j6QnTqK7bDfJyPC2r+QT8oxtmE3zZaXV5lHfSQQfTrO52fFPKecZbyaNI0I4g5/zAAIHaTkRV6N7HqErg15pbUst93YnNdyceLZE1/kqUQ559pxuu7jPD/r0jJP7Uktta34aTmXLFmyZMmSJUuWLFmy5GfLdvm+N7yJ/T0kv6jrq9GfijL4B0vB44seOWnz0U3PLzEg5LRL0QCIqwJgPmUgVZ8ujZdU3wH+jPzwl1oDPwwA4XNZ/J44biQAD/dontM7u9xw8bpsb77f8bEcgYVTsgqgwYHbVEbPNnaE552QUvMWYBdy28mO06Hskem5XqlKn3nhzyD61fVSjrwj3aEn/ccDthhpWCGuy0wGmDvIl9vDum/sh2gzS8tA6e9WU7GauHZQiYs0wmURJ0CbJAAvK8AmMFWArgaaaw4ZogkQJ4HqBIwxKsjvsikmdqPn9lNrM0MA/X67KUiyER5Kum2dxg7NpWmCtPwG3FGA+BmN9fKQp2XWQCVBVJls8jszVkjDGdcsKy+peyumDkDc8ti4Ji+DAXazVYZ9jwJkMFG6mGDBgIVwQLisEdw2BSSWuxz8Z8L72wOvr29oR9frIzaFxZBotjO9KZGh545jx74rgdGgwHAP11I6Z4b7qIgJEEGOI3D3+BfdJP3evV1DN6Pd2Y4TD1OL/9BpwNwURQZG3Mh8TF0bmqODzcInpmXyqaTWGnEeQEnX059phVowwAkHIxkIVIrHoHj58oL7XQJUl5IJjOLjPQg9cxFVtCwvHjy7bjXcRBWDheHpmRWFWWGI+ydpqBwjRCwTjiCpusZzSGuexaHYtju2reJWN7R9R9MYJofGmLD4EdZu769v+Lf/8W/oraG6lYlYc1h8C4l/YmTXgd4bAODxeOD19RXvb+94e3vH2/sbXt9e8fv7K462S6gZQMgLD8rewehedhlC560P3nd6WTGrEIrzGVIfHmXY/8MMOku8l3TanjuYYv1PWg1RF593fX4hApEEObdA4nkeilgaUvGS1qeYfDCNFbgO+/KBmG8iPtL3nxvztMUs8WlE36zs6dpOqCyLk+lr2IbpJwlxl1vUyv8p96Onen500Qfp5UUgVryhj/+MhYSp3JzCrBWLwliyZMmSJUuWLFmyZMnfWy6DeFP6375dvgINO8Mn+d4bDSEYBP57vAQ9LfXFdSKD5b8du7jj5D7he+c/SWAkWBbnINoXtw+/aHqrNOBtMJ9Id3L6lcENBfnsOgfJLPbDR2W4KqQAp1dtll+MMyB++cYMGDpwOjW4tgFg6KCBHTlWxSnZdGBslQlCuqj4uU62e38ERIbApedKpXsVdD8BxhNQ42A7gJLBbgOWMuxw1iMjL+byONDxSbzjuUXKgPb6uPKjWrSx74sCYhPBlEyeXCtJ/gvywtppcleUFWoAb+O7JXiuSyC+nA89Q57URY8bZORryKY7cgJmsDfKfZEsH4zw6KSlmPU8dzEQBGzWH5vNU9nN+sVdVs1EjIwasRIY5omuU4IRbYz3xjgGcoAUFBX//s3d5DR0DTxtoPEY80FIgrfXd/z++yvaEW5zclyJ3jkCYnchSNwNz3FIvICmxIlF/6VEMkzksuevVe2IuSpwXzp/d3CwXFrlZcueAUtOY5cIGtg5zxfs7SvkHinBF0SUu/1xc44Ac+t2w8uXFxSUwWWgc3tuKaH5Vgk6Xyz2w20bCIxaK3799Vd8+fKirqCqB7U2ayFGBKI2FRJwuOLFCIybuHKiEouI9Huf5sew5rBrusdz6E48HPuBY9+xGxGRXHW13rA/HkARCxAjMPbHV+yPd7W2ObAfu7gc6x2lFHz58oLXb6/4H//239H2Q9qBpL3rtqHUosSFECjtONB6kCD7vuP9/R37Y8f744F9f+B9f+BQKyKvaxdFYBs34NDVCbQeLfziVCxpaR60e4bpXsjESNKIJltGbIEnKZNNYPpn49rzTSmbRYKXoaiLN9i4plHvPdEOdodMsVYRIfIaak0wkm6wpRgn5kFG7n/4EYQ+M5iLuDSkKE2x+Vn75HI5TGSK1TLPlHMNxsLN5549RT1Za6ZDpxIOi8N18j/yrH391DLQNj+Q2pIlS5YsWbJkyZIlS5b8MdmegsUmH7zpnKHRH70iZ2w+Nf7fJRl8ieCxIW6R8QkCI79EzxKWHd9vdXgq08s+W56XhbE3++vkXE8sBkG6jqbrhmJcpze7gHhW9LMbhgTiP7lZfL9DARdLKL1uKwAOC5A8J/bBmAiwYiqHfQ7EwKgbV6mN3wyZ4um8haDWvIeyjUDXCIZOMTwQ5MWzev08CXSdyNSKIDvXc8YK8Mu2XgVAFUz7KPkM0jugZ62YANI5FdcLRfBwdkNm5f9oJ+15pKadyMl3+wjSAR8pV/Rh1CcTHd6impy7gck6o/cNVhMXdQMUYCQIiAcD8/UKdZnUE9Dqu+A56NX348DjaDDgnRluEdFSkOJmBEZnDYDdhvRYd6U/Hjve397RWldSI0gKcReF6R72fm6t4TjiPnYWIFtJxFw8uHk69ad3ytCGgsOK26xhE7ldewrEC9dluuhITv1uvWX9X6igsMWUUIuHUlChQaZLAVGFE4AgCVx9v8mxjmgDRZ+pFCUISrh/KhW1bOraqfiucyMwvv7yC768vIgFRs33mxVIx3E0HBZnxAiYQqhbQd2quomqCZ8ntNbAgJMfA1ZOhFo1WHtXV2HccLQdezuwP3YcDwl0fbTDXTExix48Hg+UUnB7uWNTl1T744H98Y739wcejwce+45jf6BxE0uT+x1vb2/463/+O47jEKKIhFAx8oZbdwuQ42geM8XqcBxCruzHrtYaXUiBQU9UrylVeNCZGM+yx2OcM8zaRNTPSNu0xhi4bmRCsBGDFMAQ+wD6XRetDFPZc0l1A4rh5fH8E1GAfOz4fCaWX7P7x5F0j3qYcdFcpFOTpUuuHpFoutbj9AAyX1NcY2Mwng+M8OjDmmD19zbjaManZf0vk78/qfDZJ9ElS5YsWbJkyZIlS5Ys+Rmyycsn+4vinwYyP9gV91RGvPfHs7rMkZ+eOQPE6Yy9lH6i/L5zV35MyRuIqSD2nN5MXiDA5Y+b4XuNlBrTAyV/fMflab/PwE5JrwzXGwDK/jY7gvdx5VDPDOZeYYb+5XsA8pNzyeIi8jjnFT1ApyNg9qC5Od0Avq+1yF2zpHRnYiuDJ5jBy6GCAX8TXeiQnaUMfgcIHgB1uDY7W0ik7FKskSv64HtutGaXVFFi+eyqk9bKlO/znejRZqSIkPB/CcpPDR9YcIKfzNJh7Bg1ksgwlbTRrGc86MQs87lod9uP7Mg2R9Ij90hTlnqPpkFWTkrpEUDDHJMAuIwiepKjLgmgCd99zGk8GL8aO+IRbpuaALMeaFiPQ/Xq29s73h+7piPpGpjbWsdx9AiKrUB0O8yaomtQ7SAwBARuvutecUOxttA5aZ4zrZK9d3ALIiUARnj7xHwN1cUA942AMPIoW0U4QNxtPHe9ZtQFzmAmx70D2kehewSIpYSNbx0DdSvYlFiopYKoYqubBLcuN9z0e62bEBxFSY0qlhOh8NH/ZMG206cExVYCo4SFhAxJuebry1fc73d37WQuoGxOsRgij8e7BMXu5g6OPAi3xb8gkHM8RnTJvQ/p49bDfZYuNs0Isd6wt13+3h94vD9Uzw53F8YsOvT+eAdRwcsXiaNx2254+/IVr1+/4O31DW/vb3h/f8fj8Y7WD4CA++2Gx77jb//5N7TWtemksBaLA0rYhBVGWBNlqyKrGzMD9YKYnadKVw9KQ1nnwkRAeOwBJaW6ujIjtpgpoaseUyOvt5psGXQ36aitJ2z6GQXNY810i5mGfDp3hBvB8/ppZcvWQ+Oil547dAw5UTjxG8/kvE6mcuv9hdK1aW21aUCqaONUCsLdJ5J43rOE5jVB16xc3mHuORf6zwudvnh2ZnHzU+RnlHXJkiVLlixZsmTJkiVLflCSC6kzHEv4B35XGd/6/8uF018uTlyQQMWfUEYDCX78RkCRhnQw3qp9R3J6yeZ0FgoEdmZUvVVe6uOl2HfY05zPH5UZ4H1SsYuszJe6XGEoGCDO5KcbvM5zQuFShhjumYh8d72V8iMrgedIi4+rRFbRDIAM116fZQTYauXTZGFtaBYYc3yWCS6DuSj6KcPJwWCM5blIPALv0uBOx8BIMgAbGUBDYi4sAwVeOWDHIfb7NJkRgKeEzKcaQfOxNh/64CJdYQhgJMUwf4VCeJqSTBEQrRSYlUV0XwBuntipgQ2cg+x4ZqC3tCMbBjuyx6bondQ1k+xkb4fuKu/NSYcmZhDozPjt9294e3uHEZ4dEmjbiIhjF+DZ7pVYFQYAK1FhVhXq8iis2boApE4axHRGJYJYmwWCj+cMGGYQNpDh1EdjXxEs7oeNLW0rknY5hIYDnPiiy7SERIl8YzaYyAtTcpLxXAqhoKKWiu12w22T2BOVBIS/3+54ub3g5faCbbth224gdQG1bRsi6LKQBTEXCMlRqpEWklc1AqNuQKHBgKSQ/L3cv+B2UwLDxmghoAC9dRz7gcf7O6iYe6+mfclOekicixoEkepq7x2Pfce3tze0owm51VpYALk1j5Bge3vg0XY83t/xeHsXEkGDrhtofxxKYBQhMKTdNrx/+QVvX77i92+/4fX1G97e3vD+eMN+HGAwtlrRO+P98RCSzpm9IDDM8VEzl2bHEUQFKAKO57nFgnebvqqCeqBrTutNBtZ1hWGbO2y9UWUKt1PK9NimCC13jHPTiR5gvc+bprqMkRS1ok1guDOxMb+HGzZ24mogKDCuy1nMY5WX9rwU50b5LonxmedVKgAyGWHLB3StLQARm6cvJXYhRCMPt8DGVpizPCnUTGL4qf+CB9if9Vg2Jfo/6dF7yZIlS5YsWbJkyZIl/z+VzQGwYXsY+5slDf6TMXz3V1ZOhvW2a+0Jw5DBu9htHddlv+iUXrKzb3nx/jOBgHopoWB4MWcaymIv4ERjlYZS5vJMZ2l4dR2z8sqkg/Fy3oaMOCqO7IPfQdoU74C1f04lOgU+H91IcL7HX/7z22x6G0+1AyTYNcsXgKBuQ/Ra20FJJBAL96RCGdafA1YPTQsCg7Jrp1SCp0CEgYoO+oz6xdE9cFTC6p8VRtvCQNARGSkBcKtpEgEBeri+adwGjqTnGBT2PxUausqjIdB1na++zyMqen9Ebk/GQDktijvolKIC08BQrqgcR5u5Ck1uNsgANLWYIAJD3dK4Gp57lyE7t1FKKrTmtxEqBECW2Ajaf11JGhCYigJwdv8E0jEN7db1G9v4ynqTQEW2ekQHq+5noDL0SL6lQOSEkThKwZhzM3geHG3aIfEvGEWBRAUxHdAPSwOLM8HdYgYAbLvAu8WRaDhaG3eHMySGwNHc3ZPdZ5YTBh53tcTIMQle3x947EfqRy0bNN+m17Lsyu8sx3rrblXh2KuBozrHsA5mOZfHKsAtSM6rOZqI4KGdzSKgSBv2rvMHRzsa0O6DR5Xc4ioUdXFWqKJUAjZtdw7VIQtG7QGwN/msBaVK3AgD8i3mBBGjeFBkUsuIitttw91jR9xQS8Xt/2HvXdfkxpFkQXOAkanqnpnTM3v22/d/vf32zKWrSpkRJOD7w68gGZEpKVWlbsGqUhHBC24EQNIM7r484ekihPzl8oSqwoOJfxI4W9w+ieVMkPoEKdflcsFyWVzEyPed1ju2tqVWlLbo3HFbr1h4QS8aiFsfC1pr2NYNv7/8jl9//RXbuuK23rCuq8TE0Po/PUl58z3rdrvhen3Fy+srXl5exeWSuWXSdrf4KF0FLRPD1tuKbV3BSbgwwr5bPYiw3q7SJqXit+U3XC6LiB/rDZvGwGhN+ocFlo6YKDGvbpsS89o6XQU3TlYXRBI4PlvuERHE7xk8LUuXrN+me/M4M+bnqnRn2k3qlG+qh9/HOwmBoIF14jg+sXDDeEi+I6Qi2uDVNFK9NAEZzv2kPKlZbLxZBfLtmKxsua2QLDn2NdzPCCfPaeWsolam8X7q54/mp3HeEEOIh/PSw8IxzZO8R6QHi0PGYxH45BCfLznvupvZF4K9N01MTExMTExMTExMTPwRcAsMMlcAvqpv91I2EMPpFdFWygJp9bH5Ot4jgtgOqxQZvtLP/ZszozjHp/8Ob6Sx3blA/ec04LLVSokGGsj1fCj7OVLe1BZAItIQhAYpITqYJcR544pn/W1vnEza9rty799XOV5kg/7IL7gYLltYbOxe0pUo8X1+mY0gsBaX1d8MVrEo9Q0lGJ10JSEoyItJx7x38IEyurUAACAASURBVJ6SLCOc9jk7LbLytIXnyNdmR1woY366VjCxB07rqM/r6Jfmp5uj79nKd5LjaUgrnZt7rrYXOF+XOPasv54SRbpKdix/tF/wbcf6jkGij1SUWKuwBjFW9zimXsXFsoLo15Hoya57mIWYdYWHwu2M+T/n1CZGqoM1Fob+SUBhWdnMWwdvzecH6a4ydjqxu1EZAoFTmpdyV4Ftio41XgYyOhRAkbRN9SwFKHVcVUwAqMhYYRVsigpUJdm5MKVrmPoeKNwkqeumDpE9u4svSq52jS3B9snue79t4qKpdwlqva4b2iqfEj/gmnz3S9vfrjfc1psLC+zCglpVWJkSWWuBizfuKs0STMChAl3FTL5iHYCLi13dVMmkOThvkTbvLG2YbjqDJYGSlxZkvLP2Wz2uEKGiQKMiiGjDLNYDALatOfFtsTqenp7w/KwCBrN53xP3QgCWsmDxINZiydBax7puPjQs1sKyLGIp8fQJT08X1MuCZRELiaUuWKoIEJfLRWJQVF2Nb21WihyziICxLJLnZbngsjxJLIpSPRaF887MuCwLPn36hHVd8fr6qkKREvSl4NOnT3h+fkYp1fvu1jYNQH0DX/Vq6pjpzLher2itqZhSffwTgK2J+6i///p3/Od//ideX1/x+vKKl5fPYskABgrh+flZXGEp4c3M7jrqer3ierV+KYGxe1oIYFZvbPcdhgTT7urmyQcuS0Bmn42A7XYUus9iVQFAY2mjWqtaR2g6WUxxoQQuHEnZ8r3ePqUTcdM5er9qIuF881jGMoymw+430kwPcGnajmn95H7tjxBy7v6OEUmzpzWkr20i7qh6aIPDAo9dm4y3z3ge8EUy4j7Lugfl1QMP6/++nX790r2U7DnoboL2bGT3x7R99/1+Gidp+ufbJ90vWjxbvSed9+N95ZqYmJiYmJiYmJiYmPgoLCWtOjWLivzyyP5yhuGdxVceK9E4EALOXyeXJ0YEI8hDIzadE0cQOPKr+5kGNuIfGAWNVD5bNe10djDrlvHIZ5K+4NmnpcXFW8UP9RXX+iJtpG0UbmyC/ANBxHlhfdW2ZqqrN8NVg+7xyibXI4cXyJw/HcgBSqtf/bU2v3jv2pOGcnoiMAHKc92LL/K6f1I+3eKceA5RbF3iPslzqCnF+ZwIryG3QzMlYuykdNlCwVntzCUMSVFqmpHMP5TVsvb+l4mFw5GIq532JuJbPpMv8XT9KBHeTlDdoZ9Gv+bq0puqn83DBaMYtojtychhTNNc8Ws6WeC0esS6abHUsJX4ZOlyFxdSuiq31AWljK3jrnJUcAr3K/mwmIsYEhgYAKiWFH8DTjKmVnW3VJ4WhUuYwbURN7V80Kax+qW5sDcTa8LSzLjDpmS6kLNNxQtG4xBIXTxoXd3YdLdMkXPZSWtxASXBhrdbCnRs3nFUqNjWDdu26YryEIe8n7BeU475rncCdwItT1hq9Dc/y8WPcQ6T9IoSkNY/k8sZ7z95XrTOlS3nUgwVcIQyJrV0IEIlESKedcwLMV382ltZmYGnpyc8PT3p/JT6p/r0r0XcOhVzh1SLtyG5OyaxqliWBZfLBU+XZzw9icVDuFESawBCBZWCZSmoSwENXYzcQqMOVh0VpSx+f3BrmiQwUSn4/fffXRxo6RiC1PNyuQxz5bZtLiJcr1cfv9Y/b7cbtq1FjAztiwDcIuLl8wt+++13FUIktsS6ruhgUCl4erqoCyk9l816Y8W6rXJstzgSPe6bCBHMbl92X/VpLdTl4dOvpd4bsnWnHZB6rPcr6+tDl97fF+6APK5K9C2fV1IRT1M43jpgVchnnNhBnJ318OeY752Mx4Pi3jVszcR8GusPs8sPQ/mh8jRb3TWmON7qj5YYH433pa89886hX1LC8ZKcPet9WXrfFz9OSSYmJiYmJiYmJiYm/nmxRLDYeDVPGgEGawHoy7PzO0b2kZNctsKcjAMI1hDyGsqxglZXZyu/6C/69rJofrSPoPtvyEZWIL30DkymlNndDwQLNrykivug4vUatgOwNb+9h8BhR5Wzl93kHmIgyQ7lN64kkwGcdzgR/milfbzyBnFiJIO7xXH2ZiTOrXzhPsdOtr1HMiEcCjCyfJSOGo4P3mkkdrw/neQTbXHEwQXXAKM3z/vTIRDvUFocfg0MWigBx/IdqwYUwhkRNDLGh61w2mpHdLqFQx4TbHuN8D0hloZVyHG1CnRVcSLwOI+TXD0tUBD/41U3wppTC4wWOtqUPgd1X+FctEoiYXYlpqsGNNY5YjdOBpGSx2shK7pNxDC3RAA0BoC7RPO/XXMOl4eUnA8houtKa4kdYW73SGMChFXFtoWo0JI7Jztva00DE4uA0cBoHFZpFlS7NXEZZen3xmOsCA98LILIdpOV9aUuIKriIo7ZSWO3isg+742Ly+y6t2oBFcZyuaAsiwob7G1trqtCxA73MKzzl1vY+GcZBGq/lsO1lTSyy/kcR2X/28QGCSZdXcTI+4kIF3XXFFZC5DFZwvVTuF0qtYJ0u8SRiD/JZ8HlsqgFxQWlRNkAiOVMY1wuFculRpPvRPDRcorEFZeKVNu2+Z/1GbG2gAgD6+oBsHuT629Cio9LhosOIjzcBksbZrEykbgUycIKo+smsQCyvqtl6mrNUQi3qwbBtsvL5OUyESTmMMoaks9fNnW4WF1s7hlm2APyvSE/owyHZwHcBBTrdzkG0nBbMksd3dB3+2z1O5U7Jdtlfyz5G2f9nDgsmPlB8COWaWJiYmJiYmJiYmJi4p8FS2MlGYzwMzdBWcDIK8uMPNfjRrI5EcRZ5BgY4eNLuXPBj3mIb0cSVDh5kBpfiDOBx4mQsE16vis0kXT+/k1VOCFksnzgMsHuRT7/FqKFTqQEHhNMOYyLRHuqjMkYehKPddzX/QsqOuZ/uv8sxV15viTnkYE6X1npja350NgP7tBkqWQZnMZCyu9QXHq7CjT+YADmhD/xZnFgXK79AmBkhnAgXixgdFq5PLSY5TWIB1GAHN8iDlECkQYqcjyagFLEDoNZLUFAbpFjrqeo1HROIta9Q45kuxWW3KWbzUc6szGh9b0rMHUZA47gwF1+M9gtGMKSoquAoMGpt+Z1d4HAhIetuXjh4oMSxiJgdA+C3QERMVhiLRih3FsE0w6riTQd+bEc4oZaNfTGIOph0QFzwTQi5pnxisnFgATCZQCwuAbR2YhZVs0TD/Ns7r9iZaAiQdkLEHZ5Ld3d2CLtpgihwQSQkgUIEjdeYRmhVhQlrCHEuqKgLgvqUkcBwwQQjS9hrsAiz0hj/2cihlhjJPLezqeGhn4IbpxJdu4uM8g+DSTcVAhbbxJrwkSMNQWUbiZseBwU2V6ruL9i6+FdLDBut5v/ScyQ7tY4W+suYJhgzzoIZCECPOZG90DvLe5FTGL5oxaL9pjgliM6xvZzg3ZRHY/HNjqbMLNIfwaLTzRMX2zPKd4qQ1/Ls3oIejswwkpy8tgTExMTExMTExMTExMTEx+KpfVtXIw4uLYx8sSWFtLIL6hPfkJxcg/pLHcrMKx0VNpy4J2Tq6nEJGZRY0/CM8bjHiPIEOc9diTDKVV+zpHs1JZztmLUBigd/xh71wi5bNnVlFlg7MmvgSAngsXsyFYiTtZ4sdKKV9uOsXVJTz5qOnaBThp0f9yp1UlYJMheI6vSFcniQbYwsb7CUbvHrh7MiiHSDjdJY7HNk5UJQQPZBSD5UYq0B/FA3FExcUhIWcA4wcOe5IVJJWUl1Tsny4axLnYaW4MNSdg4VFdtJlxYIG0KW5p9qUMOoMOl3wfpNguLPfnnPYsJVKAe29L8k/umk8MWpFvmHs/LXaHtBAz7NMswJL4dIgz0psKJtakKBY0t6LURrSpodDknAlur9UVjWX2+bt4+rFYarTexhNhaxKlolqaJDT1WzPeeBAxLS+phpK+7oerp2ml1TTgWSxGruV6rzsKGE8INnl0LsulNx6WudFcDFm9ei29BHmA8RrBkzRLTwbel674TB5CCSZcSVhlAuuYpDemiIlZIUG0Nml0rai1xbsHg2slEBQl+XVzIqKWCKp2IF5REinKYvszaYu+aSOK/REDtZVnuChUWKySnYe6/TKiyoNJtE5Fg05gmN48dsWLdNtxuK7oKB9kiw/oJmMVypOjcBICbuHJa1WJjXVcwtxAnmNH63lJH+g+Hr7SwzEjB2n3qIkLjplO1lQXRXtjfc3hsJ+SFErHNr4N3rLg2b90H7v2UqSXnmfqw/fk9iOD34vGBRrXffcJfiD9KCHn7AepPBKd/f/CiTkxMTExMTExMTExMTHw3LNu2DiSTLXq1F8Ugv0dSwQ0rKAc3ZA+0GYx4IoGNpTBu40wD8DfVO3SureLMIsibsEoZeUJj3sFUa73L+Nqc89GV3Hn/ni69hzMiGAgiZyA8domO6+DvWGBkqYKgq9ePQodnwEG+UEo30jpU/o0a5pq+FxSiAACho3J/05IxUhyQXD4oOW4rg+Pcc5uOvC1I7ZRTtLRzwnfEmby6fChyCaKLLCC2pa1tfKd/D0jdLyi0qIcRhcd4KIPi49d/PNsqmPqdjn8jgO9pbhFPPtF2qf7eh4qlVUARpWAUMQqN5dEMDtShTR1GXjIAFIzqkxHwQeQ7aZpKxgjXL1tv2NQKzawqWAnkrW1Yt1WJ46aukdSNk1lRdN3eJKC2BMxeY4YwEtgsMHpH3/RPXdWYa54QMpQIJqCDRD5O8WucXAbLdobHSTi6UhKXdkwEdOntrHp0jDCLwZAstvjYR8huDASxSABp0Pd8WWnIl8xdk7pOI4JbTIgrJyX2Nf9aigatzpYNIWAQ5HwTIEwgsCDZFmTa+l+hogKHpCnHVVQNjF1rxVIWF6fCcgOw2BekdRWrne79vpSCulSxdDCXYE2Er94lLbNcOB1Heq2BNogDrTWJX7FtYdXALLFM1g2324bbq8WseMVtE0uM6+2Kru7LJL0mYluPe4wLMhpHhjs0D3FJta4rALXAMJFB+1jbNvQe4ga4S78ycYYsZpT2Kwu+DRE/pM4WsyPmCOt3JhDkIe393Qf0ntL+Aty5fYW1BUXddqfYLJbdTtoTAxOD+ngL0NtrOvvevXO8V/Fpvb4nbf9HqSQTExMTExMTExMTExMTE1+PZVtXhCuO/Jk5dPaViZTf6J0YbweCnEiIMCOpQEUCXJo7HnPJwkHKBfZruGXbwOG/C1HYWIVum02BGQn6YdOhENYSieA4WenpRMywUanBO+XPxGMQ+vs6B8lh4sUgbFiV7AftV4Im4YeilLba3VbCuiUBRWpAukS+eWwHj3+yr7uX/mTr2JypnonYGQQOyTxb9xDsyrATb77/LGlP3wSGOHTIj9JfFta8H+2FDfKyGeHpZPJulbrV5FC0/EvHmCe9O4pcuKFDPzTLGkYIEvbnRLN/P2QOI6KHHpDblKNsWdC0fxhQ90Dq9in5gecsMvg8oxQmE4KrH3tMtoAwEh4cIsHQngx3geOCgAatZiXde++4thVr25RklfS7unpaNwmEvLWmgoMJGC1cQvUmdG/v6BvcTVSsLpd2aq2P7nzMrZOO0Wxpxmks6lQZrO744aOEoTHTSYQB1S69nc3SiFM/JhU2LOh1CBgU5yWLBBE6SINWyzlN23SMHWHWCyIQ5FgTVAil6vkommd0BHG59IS6d83kfVB+Z6sKiTOxJAHD+mQSTLQMtVY8PYm7qLpU1LJgqRd3w2SWLHKeChgqqop1gcV+kIDZVCWWw8abCwCbunKy2BrbtmFZlkEEMmsH7R5qLdE0FopYV6we20LjmWjslOv1huvrDbfbK663K9b1hnW94arxK1wkZQb3UYg0N1vQhQeyn0OE600sK8jqCVSqINAgYPhg1xurjPOYJ2gQ3ABzUmausazvh3CRrrMuJrD7idVlsITLgyDl8wguRtPZeLo3L9OxjG6BkfI9lEsHW88pHDGIMYy4drv8H5w8brhz6J8Je5zYVWv8PD1xfCaMf3ege8LPx+DdgtkXP5++E99cv7s96KtwT2KbMUAmJiYmJiYmJiYmJr4nlm3bABixoStjlSgoNL74MHe0RBU7yWBk0Y50KkVECyS3+u5uqrCvDA2BRP/JQgkQRDHbq1iFk6T7GtFITGQKr1jQz0wpUDrN3ISYHsAlVv0TazxPqUi2mhDRI44LUja97PlbfPz2U9IKVN6/qt57J+Tg0ocDM6lz8kJ92JTEjYFgzpwQJx6fzSXSoYZOQMneeO13vt+PTVYPewHDmfBUDS2Mx0XIFgMI0sfazkKwZp7KW/XQP8KMYiyr/lOTwKPEnZ86umbXHDqMPQ5hyMphZGJcbTj1DF8VnVtL6OTc3klAIOBSLgDJSnipPDsJbmQgASFi+Gp3I6etTYLUFxK0oFANQtHHsIqRqUScRAsj35kYpSwSILssICpgErcz7tIGSXxQUcLiS7AF0lWiv0eFVJyAptWxWdDgTVeya4DkpgTw2taIO6E++rmJW6frtuLWVrfIMBc+vYn7qHVdhzgTGYMLHAaYRQyopcZ117/egd4J6ATm6vOENW+sPyevJ0NEkxA+TwQnb0eAN2n3PrgB1L8ezUclxGpQwaKulZycJXigawuCXamiwKwWCsoi22UuLig15v9aCGUpWOqCWi4wd0xUQ7ygQkpyk09kzIzL8oTnp093XS7ZSv4cf8KEC9tmfVjaONqC1BqjmPhRKupSUCuh69iJwVVcdHTxgnmI49C2DX1l/Pbrb/j8+TNePn/Gq7l1WkXEWJYFz8/PGjw87o1bayp4bFhvG1qXeBXM0k/X26qihvY9sFgG9a5xVDbt92v6Lu4gQ4yysX3CHae52jqQB6MXcyBYsGwTrIgW6bupA5Lejq2v+aIEnSvDGsvyk+cLc1FWCu20BH2ugInRnKy72Cf0uGfeuUHGzXVX+5xXnidtwO6sFb1s2lfvCIk2bv3e6YsL3oN8zzt9qrmPw4Xdb/zzYVrW8VkN76zqSX10vpY0vry+4xl0v8neUb7v2eyc5rDHB+5Kk61VueCL+tQ7kZ9WJiYmJiYmJiYmJiYmvjcWdy9k7KN+DBYLAMCQ1cnpzwgl1tWmFkgVFL7DeWGgAoXY9wWUpPCsWQmFJE4Mb4eJvM1kXoKTNrbyXf+1la9O5PIufcpF4+FMISZGUSX7Lc/1GYqcv6d2ZMsPCEImJbHjJh9ibM2T327VwcMxRqTLv5zcdpDHwQg3MhnsZc/1CPmFDmWwnH0FLOL9OhNQRvg7WWWpKqnKRS4SF1LvQRT7pLhBwO34ph6ZYCjACXJ4C8pKTjdijuX03oM/0W3EIs4UqxtHmuAgwaP5E4utn5RaVNp3PClzFVQWUClOQHoHs/7l40RJzZ2AYZ9mRUHcQSQCBqi6wEEFGtOghDWF92WzajABqYvswqSkPQCS1esSoDoFv3bf+gxuElui966ErZC2Zhnhbdchx7IQvkYE+2r1Zm6eVMBwklcFDA+e3XBrG1YWEdfc38WKdBExxNojzQmZIB57DgpVLJWibfR6WdgJcNl1PY5rnghuEIl4wR1F4woUKjFPkfcQbROO2B4+9xHM/ZGtHi/FrCDMfVNx64XBUoIo3DTZH4VQUC9C/jMqGMXjRRCJSFFrQS2Lilgk5H0llMUEjJLmlxCxluWC58szSqm5kdTyRmJA2D3L81PLCguUbTEkWF0hefro6CxunQDodRax46rCQ1iZRXsB0HglHW7ZwBqPom349ddf8ftvv+Hl5cUFjHUVAaIWsRAxSxQTq5qWcb1p3+0NnS1QdkNbTdAAYvhbO2mMFGZ0zrEuunL85CKG9S/vJ2kOkk60nwc5/el4sN8WOD11QtXA/FQ1OhitJ2wUJFGF2Cwu/EYYfxx/+9Jl/XkkjscjPd/Msp7z4CmNEzAPieT7/WCBsX8MeCPfyN9HwG7LlyGy0Xr/KKxyeu45bKfdMafIN9D91rTtC+rLu8+DaPXFiHtuurFj33u/yQ7i0amH+8nunHc8R05MTExMTExMTExMTPzoWJZlgRMRSlSevcsxlNTrHY2NiIw/4832AkZ7amhLE/KrSABRI+p8Ra2LF7pC3VeHCh1qL4hHLuCkoAO5aERwkIJ2ytk7XZATFCTKnaz2gVs91UOchoIDjDcy4uUj2YZBmNi7oxiKcLJxV+f97h1Z5eS2pTes9jUCC+42TMojV9GOpEKgTtF0uuKcWISKXgq4AL0Qei3oC2FbCtpCaDX+epE/SYJCvHBxw4j/6AMg9hbKfJN9miDh9WXW3wxqjNIYtTNKA+rWUFrH0htq6yhd/sCyf2nO7InQQtDGJFgcGRNoJDyy9VwrVLou5ooNBIZEv5YV4qRighWZ1KAlCRcgMWWhWPnM1iY6RKoFhTbxg6I93XuMuWeCksBOBqswwYzWRMBw101K+prQYMRrU9c5YjXRRnGjRWyAbduwbht665A4w9K+kganMSmEPnoIES2JJRaQm8Fo1NGIg1i1Dk4A1YqFjESOEbO3DLBPXbiO622LXkQyBxgRTd2ubLquSmrL3FjSlbfRoiG4kyUDUVGf/KOlgohNQewvKVC1Ba22GBR2jc0FUxYEiltTSIwIO9+EgmVZUGrB1jsaupTHBJhdMGxzW2SWF7VK4G0XbnyVPYOgwbV39SoqiMl9A562ldVcR+Vrkn93DXyOtmLbIii35fH582e8vLwcgl5b/zLxK8/7du/7/fff8fr6ittNYlFs66rB3zvM3ZWT9XZX85gXcNHMRq6lHe4Vgx8doOWv2l5imBj3JLsWPi/bPtY5EoC5ksqCvAeL9z7dwGr5Ei69zP5BP3f3wxA0d+UxcY2SvNG95uP1My7W56a4q1vZYVYaZ/e5LNTs2s8FjpN778EVDjM6AYQWzxB5Ts43EB6miomPwOligw9k5T/ietnD6eEhdf9k8Z3wJ/S52c0nJiYmJiYmJiYmJv5ILJn4Ofr0H6loEzHMdUv47W5pFVsfBAwjwsxFS+EINmtkijM0vH/Z05xNvHDy5XGlMpm+Z37yavMQRWJVqq+SY48ucEj/jCA7JUIgFPPZjlNCys9KFblb2f06wp3lAycGPxMtOxCCcInmV1IxHTOeEOJF4r7vVEgbmUgtG5SOtc0shPpWC1oVYl8CwxI6FREltP/0Kr/bUtAK0CvJOSZelAhA7NXXcrl4oQW2nk1akNw0zitzqpJZXVhKnVFUvCjMqFtHUeGi9g7q3QWP0oHaIOx2t+MYSwOKmYaoOpTJQXfxdHrR7LuZRozuVEzHYrucScyIg7KjL1IBibx/gjhi1XgfOVsFnuJD6NzQO2Nr8mdWFeLfv0VQbBcvGppZTegfLL0U3HhbN9zWm6ZnE0G4d4qpxOaTKK+R0mY5xnpcK6wevxL5DiM2GSiUxMZ8McjHQQ4czyroRBpJkCDI3GfCEMj5UNlHoJrmTCe2RcR1F38mYLibpxBFqBSgyL5SCpYqLqJKXVTAqB4PwkRds8AoKn7EZ0EtYkmRBZBSxT1UqQVrF2J/FCxiXgBCaHDLDg3SPczRTn7HPGbXwK0edFBK/9N89sS5kv/btqmQpv2rby5AeFBuH9qMzy+f8fL5xfulCWiw9Nrm1h85n7ZteHl9FbdRHrNCrSJY4lGcucNyV20ubET3ykKDj2XGcH/2eSK3u5Wr9xBnkmgQcwMP/HtO04qx4/7PBQI7luP5wO7XviW1l+VcyO8YcWxOfn/vseYZqjGKO/cLlw7/FsZVhefIbBRbKN9EvjWvCcGjNvyANv42q4uEN7WUfy7x4l52H9aeExMTExMTExMTExMTJ1jE7cb5CkZO29lYPOIIwgklYzj7aUds7yF0CI/H4F5RID7M80rL8Yu+nVLK3zhCTvvfEBdcBjFxYsey00BI5ETe/yK2X/2c6xG77rAsXp4sWuyO3f9O6Sc/THC2mndHDedSVinGtHIdmKNxsthCGIKzUk4pN0Mqzl78ICXKLZnKhK1WvPy14vMvC26Xiu2pYFtGgtMDiyMSD//QZumxq9qYddRVGyZfnvdccU4V5UroBcCiZOKh7XdtRARqHWXb8Py64S+/rygvDU/XJuJLxeCCLbtq8hgYTJ6yENsEIgkybPUjLSd7+/RUplx9a4tkRcRJANJG7z0sC0YXNpu6wQm/+Vm86Nyxbj0JGOxzgQgY3ecHcytl6XFTwrrHynnW4MYSIFkFCCNF2epDuVpBhHKU0dxc2artpq6XStFW7OT9jmGux45zSnQ4Exp8o4sURYU4IdqFcDarCZDGmRDbAjmvFhQl92stkaZOdUUFlrJQnCshSVDMHVMxC4yKSgXLou6VagWVikIVNYkUoAiIbduI1OUTFVSSmBEuXiSLh1IKqFch6n1lPY1TM8NJfNtfVBzJY4V1n1yXBkBiJNkqewmuroJX032chCcKiz4XELoICltfsbU1CRhm+UEiVKDj9eWK15fX5H5KhHpzW7W1TQWNGONNA3/fbrcUcDsJICZs96gbrO9pWYfA1af3Iut1BLC5xBoR8lW0Z8hH6bhUhjzp7Y/xfIsXIAkaMabkzOLbRxkj5WnPDZpSZ5IA7shpRV3t3JjVdoXMBRruleMh7p7q0K6PnfmM7RT1jk8TX+KYO3L/xA+Ie4993y+3iYmJiYmJiYmJiYmJiW/FUmv1Vb4WqFQQhAQQhF92B6FfEtcd7GHvstK1aZBwrrK3GrnKYt1gq6Vt9aikeeJSgoM8H8s4Yljpqv/4Kmljdm0/C11iK0htDTs9IEXGvOSz0N7Owu0Mohx7csVP2BFEyGTInWPOCPqxahhFkyiJkzO5cP7zjLhJhyUlQ3icnYUAjcnK1doLHuzHtqXg5a8LXp8XvD5X3J7MNZR8+qFxipd3LOJePRmvYdZszixi3kUxZGLP0s16yqEPjMHBNaQEqC7gUtCXiu254Xrt+Mva8dQAqgT4Cv3i5OYg0pOjdwAAIABJREFUxGkmXfst0QJQ0dy0tU1s2I1dSyMIVPve3YqhNyX1NX5AcxEhW1HkleYRD6cla4neGWvraA0icriY2dG37sKEBSl24SO572EOsbTnyvs0QyG2pE9wtJXNAYzdADHCVYlmUQiCeycmi56gAcNNpD0Sw3mcuZsmc/GkQatFwFBrCHWlV5IVBYEluPRiVg7VrS1qLaCqYlYhES2UPJZF9mlFva66L5r+ouJFMcEBKkLY6vySyxxWP7G6v7iwVqh4QGf7vqTWiHtCdFhObWMtJy6m4NfWA7djd/9huwIhhG+rxjLZRFwwQQowy4vk2nDbsG43XG+vuK03HwdB0psgyLjdVqzXWxK6rCzslkPc9frr6RYjxfKzsWbtVl0Li/YJaz1C1cUDfpOylkpzTZ6qex4CSQwxZ04gFWQ8JtXRfWG2VqF8Tdz6Qu9dZVy84Pd+HVDsdZXYLBw3EbijQCIvfzwb2Pl9qEfeh6EfHcWG/Jwx6PF2oyU7yiwoZXxHVlIoe/YYcmETPvTuxTQkHRdmvA6Hkv5xDPk/L6zR2X84yK9nGiDvxvFe/k2wh6/ds8ljmeyd+JIynj23Tv1kYmJiYmJiYmJiYuKfBEspRckJyKpWZDcZkPfHtHo5I3xa5xf3kTRtREFIKEmYz5eD98XKIgOPW9/5QifEzlCkeMmM0uzSJ4m94MTHSbp2opbfam8v03zycrxf8ffonfIgIHwYTkSM/X5/Aeez09KGkzIeKjVQ72O6xGAibEvFf//tCb//cgFbxyhyTG3Rh4THyhm8r40OZ+yvyTe+3Gc+y4kUbcOqR/gKftaV04WwPlesnxa8/tJxWRlP/7Phr587+qLuf1ASkRzEpyUhWSl5pwIGZ/cyPl6hxH7EfBDiUVazb62jd7jlBDfGujVsq7i/6ZAV701dPQUxLC56jCwWF1JKMhup3BlbgwoY3cUNs8Zgt6yA/mmZXVgIQohZAoPXWnFZFrdM0cgQelg6pwfpxfsLb21Eds6Gzk3zUjdnOmWpxy8VfeBfrD9avAwphblKkqDNZiFRF3G5BBbS/7JcUJcIhu3unADUZcFiAkapWJ4qlkvF09MFdcmu/nZfeUdME9Qyg9T90yIiiItj5uaJXJAY4hPkRrLfRCG2lHCJVdWqA0jzuXVTj4kCM2UBM4EKA8QRU6LDXZHF/NSR7zvW7yzOxLqtg8VD7x3rumn8CbW8WFfcbjf8/vl3vF5fD3OU0/dE2NS6x7Z3TpZLKt6hi7BlAzO7NwMQcUP82pbh/ml5MQvpT1zQ0cDchXfP18DGfOrH1NO9DEnEQAgS2TWVlSkff3Z/8buzuaWCUf6yxwSMrhYgJl6AOSzySPqbtyyl6TCJHm+Vxcu0M4/0stj5NjF4giEs+UGU2yZSGkpyKMKJQHkHB1eSfvefQsaHQefls+1jD3l/Qz943PxyeJ+jsT9+KN6Z9tBOXyPsTExMTExMTExMTExM/LiQBbRp5VgQJknEUIS/9SBYS7cVxiPxaORLkBoIJtASvfNOprrJnf1BvmcP/mfHjSnKluy6KtacmjBh9E1emRopZDcR0gpGWhnZHvV0kkfP8/gJZ4W1PL7kBZhwcFtEu/3y3ptYnRPtwept14T8nJxwEGSjMJG/PrgW3mzklhEvny74/MuC61NFq0ocjwzXmNNZue/kddBccP4uT/xg54m+cw/eWifE2SExBqDWGk2XaP/6SwWj4NMKLBvQh/aWhuFucR+C/OvMWNcrWmfP08SD7M7JyN/OrDEnuri+Ubc3vTV1C8XYmgQsVspShQez0uhorPErmogRMtxthbYRi0ZcFu1L5AKpkNUAdw1AnuJXmIuusMTSJlO1qTPhtvUIrA6K/p2mFRsX9yzGhHsXN0m1Ah44OgW3plLkqvXoUCYmVaqodcFyuYjgQCleRK1YlkXJYBEv3PKBJPh1rcuQl8UEsHNLlbRoIdRKKFVjKOR2oSSaJEHCGoPI5uvqVhQ2zoc2sTKUIM4pt5enOI4mu85b28B98xkTiP7ZWse2Rt/yIOoQt2HrtkrciFUsGbrHmmjIK/SzyyWxwIh+G27IpN+Ki7HsmszEts2UmKFeVk0J7p4EAcvX+hZC1Jb2GUWfaBn4uR3NA9iPwj5QUEVohNY1CRduPeH3ybheg+vFE4yWNMfyWRnyNRWBKYn2BzFM+g/bYgW/fVs9JdaHbXerE7+Jy1++31p9TmFl9/KmstoGs9SiLHbs7nWent2DQ2DZT/BhjbFrM8LQt/fl3M/t1g6EMpb5O2Ofxxc9SnwvovvePXuX3/3sM3E/SkNHEer9lTh/PvjKRpAb3S5l3XWy4Oer8/jS8vFZLb8i2xPEuOR05FRLJiYmJiYmJiYmJia+Lxb7EhwL73mTIPCUmCglAmGXUlA5mHQnmzzg6fGlTnmH81ceMiJkz2gj8Q73X5beEgFqrb5q9kDUJRLQLEvkl5FQiaSBuI6qRVyp9D6u2AWgq96hhKu0hRPdOdsylvteHQ61TgIG7Q/id766ZmJ0l8YgHni66vaEMFyfQxKJfbLLxkRqYEF4/WXBr/9ywVaN8OFBLIqi0O73vXoEX3boJh/JID1IM3elTF059ZLe94WTJ2yV8esvBSsx/uPvHWVj9KKHdjlYCF+zXIgA1q13/P7yinVVN22cXDX1rkTvhm3dsK5bWE+0Dbe24nq7CeHbgmht+sk6/oTYtXILWdnFJEILWYYxQynWAZHGXVASnbsIDoNLK4uUnkjeA8+j/aO3jm1r4KHfG6m6IycT6WlpelB0AioVDVQt+2qpKjZIHUotnrZsIxcpLvWCp6dnfPr0CU9PTx5DYklxIpyIpoJC1b+7i6jsIkwrnAWMUgvElV5YImQBw+pjq/0lzkWstvf5Nc1p0j9apJfahEoivVOZDB6LROc0c/W1XldsaxsIbBMqtrXhdl0jYLuKCa2JCHG9XbHeblhvq1hPbA232xXX66tYJew8IOX+Lf2yx7a0b6wfVKyyWA0nA5jHPLILs27pFPJGtb6Qz/E2V3GS2QLR98O9gZlRwCBUSLyP7qQ9EFYT+5J6oO6xyAfkoOGUrv2+vJG2jJeu5kaDWyQCLmoV1jX+g5Whq7WiCRmFI1+OAWqZ6k86lO0MYZ3JurjdVZPxb3iQeIswNmdbnDftTk0uxpLksr8Pyfb9zdfSCedc/PBKfQ98KZH8/crn+pV++j35A7nuD3HThG8t0nGMfRS+pX5nz98fj3PxZmJiYmJiYmJiYmJi4qOxmG8PInHiw1ScZCFiWeFMDFLXESPMh3cHEjEgRI6tFGUnkWwVcqkFVJUW4L0dRWYUwgLCwIVHAuMMR6YBANC7uCAxK5H7AkZea6urzxlC8upvew+nHOg0rUizVeW7IuyEEShx9ZjMuYczzwo0fH9EouyYGyPH+KTURjAb+URKppJd8/Q9CQis6QISkJVAaBdgXQjXp4J1oZzLaSn3295DEfzRlNHX5meBydtCeL00/J+X31D/6xWN1V1Utz6nsSZadwGD1ZridpOYABanIQhdtZ5oTd0/jQG2W9fV7r0DgyUFRGRwFaiKyKBcCFFBMXEgq0ROXBqZo9Y2Wh6PNZPGrbu9Gj9yC+lxsrMsFcvlAqWkQ5TwgOekQlbx083SwmJAmEBRVWjAAlAltYqwuBMiCJjoZCRt0f5fSYSGy+WCy+USIoIKE6VUr5AFwzbiVoJlFxeD8wiotYiQUuSTqYNB2ge6ixlWuWHOUGGYE7HPrDFMLMZJFwEjhIg09tNQz4RwxC5J7sL8d8d2k3gUNhlZ/xPrixW32xZBrXOMFO7u7mlbRVhrravAsaqAdhwze4EhCxjZsiRW8Eu9G5oKadG3cjomKjFYXCZ1ua/Z4YWqWhPaXN+jt6voTrnxEpGdr5MR6EQWvr0c5zgbM0lczlTkcDxrHbUPaKd/2H73XDfa2oHxXqRWW9x8POn/KBY/KwseXo4x3bjvHeNyIJ1j9cgpnN3qqUCeHR7cNg/xKRipz6e07HhvlnReWrRgN7dB/rG5kXdXqO/K9r343X923jiG3A+Pc9ecfzLsHv0tZja7Oh2fJ//ZO+HExMTExMTExMTExI+ARd5vEvEDclLCrCzEnYe6tDjxW0RsYkVQ1xJYlsEk4kb4Xi/usoSoCAnB40tRdiFzWNGJ44LL83fcHRFPsjK49RVe5eSmYhAv9KUvVpc2cIO6vzEC0MjEPuR1eIFNK5qze5C9m4/99pzWQdzgsf1zVU/b6U6rjEckYvT0WGsjiEUJGQGa5Aoj03bltfpXFFwvhNfngvVS0CqhHBdafxvOKv0179dvnfOBafJCWBvwenvB9ve/o90a+io+93tPAkRr4rbJXERxcvHU1OJHA3Db965EdjcRDlCCkH3bOFRiiayGrdbtdrWV7C7sLnfiPD1V3UZ1Jf88SsVO8PCe5h+Uf/h3K81iFgplkTghJYJVCzmtaZpgUNRCjKpbSZhLJw+UfSHUS1XhwAJoi6snAENbo2tfBqnVQ4pZ4eNXYx9wkMWU6lrVGsXHS2r/cEUlaZpb9U4dlKxXbB629rULW1gCS7e2uciwbiJwWeD01sf5a09N2afMe93djdlftqTYtg1tY49jYtc4n7OuaxIddE7TvtJdWGsS6L2ZWNyxt747FXhdwLD4GeM8C7VU2vom+ZbqAkJKwq+Buzt0LjrmQpub3SIGYcFHibveE+p7iwO7hZoAZ6T+IZ7CThjQBPYNMLSFz79qNRJT+vnEs495wulYEU9MuBf5TMaNlrmkecKvrwZd380nUs3iY8fLeFaTXb3jWsW84Asiij9tjGkdSOQ0rxwyfAs8NvM+Wc5beTzFPkdV8ONw6A50Uq83KvqduOeznvrFWsTZ48kH4sFoShvf136DC71dn3kEPvk2FG73bPx12AWw/yCM08r4zDoxMTExMTExMTExMfHRWNIbtiIR4L5ScyQa4wBdgEik/rxtFTCBilopsK0+jhXSYzax+jcT9sZzySLUCDx95srhHEpaGOluxFVeKakEnq90VYLNAs8ybHWvrhBWErBz05XMTV2y9APB5qtxKXzUO3Gqf+56xsjSr335O3kTZ+NsOB9krWDClNb5IYvBXg+pUqIiksWJpCIrxuMSWxyL4tu2WnC9FHQiUP9u/Mk/FpTvWEvBFcDt5Yb2ugFqZdFbWFB4sGuEyCikMccqew7XQ52djtbLZdc8EYV6Se3akYkQiaCzWBPuDMcPtB/xAQ1gbauuRyFCCWZzWQQk4SGLGknI03+XZcHT5UkDR6s1RRWLiEKLxJIgFTAInkctJl6EFYZZgpVLQVlKSrO4iGGZm8VC1m4pHzu4X7K6hCCRt4uLrXAHhs5ePzNRoS7tYy6MLPD5tjWNYdK8IIVEyJEg3aTz1Ibem7hk2lYPUN2bCBjueulAJKvQ5IITuxjRdvEl1lVcQ3FTAXoQMCz2hJyT+xmDh+DW2SVUb6YkSEdzi760sjnPs+FW6/68mUl5E+6cFHzjXB8AlGe0Ezb70SRGI7kZYob+fj/X+bCIX0EPP0jrBCbWM6VxbTca7GUhT5D0fhvTRY956Dyju9sGe6UHAYzITEmGiu2OTI8BtmDBqr/3XDmktNt3ko1+6t3vj+ZzbX7Km+ZN9ovwnmF9PF7//doBffII/u3CRaTzUa62JiYmJiYmJiYmJiYm/iwsRkQEwT8SLIICc5skO9MuEy9MJAi+G0Ssq5bVjUsm+Ixh94+RmAq3E0m8IBoJhuGlL4kRaZevctVy9q6xFvZEGAhQ9zwtBYfduqxovt1WrDfz597Quvlyv7m7j2G1tbplydYVJliIaHHB09MFT09PICqoNa/CzYRX6ARpq1+zDE77jIQkQgpeqrw0G3+TrCV6XjmbVSbrD8H22Ll+jBOBJNfckzDrG1t9y2iFsC4VTB9sefEPD8JaCl6Z8fL5ivW3K0BA6x2tbeoaKK5RMpQQkt2DbVt/7iFekB/mJHlwiPZdOoZfLRUxTASx47oRyQU69pPgINXQmA/qgsnGntWyaOyJZLVgbpaM7MdJgGQG43K54PnpGYu5YapigVFpQS1LnON/ELGipCDdRBr7Qj6pQi05ktjg1hTRvnne8VFqq8opKFIj1Yc8S1H3U3K8x2vgFMeBu8YTEjdH1MmFk9bE6uF2W7GuK7ZtVeE08i61gArE8mHdNKj1htt6i2DZ5gZKXTox9qJpEi90XFtwbA+YzRYYfkNrHeg6l7ueGS6kLJD2Xpd1awYT4iy4N4coMwhs97DbfxA3fIduIwxC+8g0mvWHnZsHmrXN2XxrN7y8cZywD+QhDQefkosPAwDb8MwFGZJ4LGbQLn+fyVkI2MFKhYYjUnaWaX5mYO+T0b6SmiXDuzzPaNXze14SfZJQMNzt91XmtC/nd2ahcZLfoVz75NmnyeOBZPn8gcRxqsPxDv5HKykj4vnh9LLcwSiKDb1a2/eja8X7734dHyDrVDqGcmKPxAPpP3y4YGGF/AE1HITaj+qP4xziW+9Ye01MTExMTExMTExMTHwrluEXJ2JcxYM9qZ6FAX8tov2LC6E0UtJVDrSVwr6quhsJms5KK23328IC486reFrNLSWw1/jkUgRC9OYgp1YpW8neehe3K9uGbV2xbhJg9vXlBS+vr2ibBKDd2oZ1u+K23tBa8xXhhYKgVS7WidFlueCyXLBcFnx6/oRPv3wCM7DUC8w3uHnm8EXD9vp778XwQNrsV36ml2MTjyiT2HIV+xlplNyJmHjkPlBMCbHjBl8sdpWsMsUJLaZy7uniJwaTkBgdwK11fH654fr5FUTkbn98db65YKvW30gJlKCeQQRmCQLt4mC6JGwxERKRKwS9jgtzpUMhYHTA3VHJLh1hnEU73V4lHgRQVZhIbtJMxDMXTho7QoS95ehezeg3ZlwuT3h+fsayLJI+qYBRTcAAoo9LvcySwy02dHCZtUbnJnZDPubMIszcP8l5Zh2SZ0HuHCKQzh9gCTZeSKwiJD5GlaDhel5rDRug11aEhd46qBCWujhtZ3EdzAri9fUV1+srXl9f3bLB4qGUAqDA56euMU5uGqh9s3lLLTE690QP2upz2UYUAhen+BImajAj2q1VUDcpMlkGWUwWMvJ6nINNnA4xaJxrvsifvF1PK8VOlHF3RzrGhtlHdzHE2mlMdydM7Wctn6PP9w0WhxmDuELaTXdE+735PhGaLjSk+f1QvgdNOEoJdvzOykRu3lDPUX5OljLoUH8eRQwOLv9IXu/rPX6e1muHQzUt/s7u7JP1Afqb72fx3pvVyfkHQ5A/AOfVeFC/fwSQPHMMclAWCr4WQ7fN7hJ3B9m9ZLfZ7rXj89ko9kED3b8pYjza+M3XLg3ADwHd+T4xMTExMTExMTExMfH9oAKGkUdpxd6eC9EXKrbviFWa+xc5WVVG4F5cwDAiUawxwr3MsDKP40UvyHVFLIdL5YtjD69R+/dJRXbhdEjarDMylJQMdyXDWS42RHyP9FegpCkQZCypWJFI2uRmxutkpBZH/cZ6UhIhUqk4PlNrHRpHiEYKAQoFqKzv65mmYj1+xyw5yctOro/mARZBQclf1zYyBTwRJCiwNQnK/Xpbcb2uoFLAJmDooUSs7o4YFYRmrqLSqvqR+LAxVjTgPAAmFAprjiBYNI5ECXdnndOafDb3Y4Sy6DjiOMfGwWVZNFj1gkLVA2e7JdLeOsEFjPg9jA1tpFoXXC6Lx6sQbYxQy4Jal2R4YdYTiaBPYggQ/bD1DY07KIaeo6hQhHyuEfgacNotE5paVWhMh+Wy4GlbDwIGM4sg+vqK2/UqcSLUOoJU9Hi6POFyuYQLKXXZ9PL5BS+vLy5g5KDYcg0glhF9k/gorWHd1iRq9GSBwdYVVJhR4jqRgpwmkz3xbGJH6Quo17Rde4tNBRYzAaMQfUKfD6LVm2ur01wc94yTc1xICLHJto/JpfTSTWBI26azw+x1wpYjj6v90bzri4dUhqT4bGPO+nhzGI4/K8FZRsfYGezlczeLGJvxPC0TZXjYzWdlzbeMU9y/U/Dp/iwzjmUjStf5vTeg01X+NHz1a3noU4j6/hFKRmrvQ5/5o2+4wxgaxybtv5xZxPjX88b7XrYX90SM47Db5x+d2ofQF7R51mIkJZsDT+WNL8PxoeCDMJ/iJiYmJiYmJiYmJib+OCznsRsAf40aVsoy9m/jnI7N7/EdsgKcq2whTq6VLF2iHRdA8Y0ZzA2d+p5PwciAnEMXioMok2kFSy3uyiqnZi+cRQlHIwdbE5/yf/3rX7Em0lCCeW/o3JTUNREiu6EhFyEIEbjc4l4slwVPlwuenp6dNLu36ji70MLuZTcddJ4GxzY2otISIgC2Or2Q+iwfgz+Lv/4uAYYtNgAZQaQ0kq9ARxDliZ0ijXVeIDrJfPVVJKHq9nrD9eWGxkCnEKh6IjEKAOjK+8ZCbPe2+X537QRzr1T0+qq5E0EDXgNENdwpad+1C7NcFizLRYlxJfH1e60Fl6dFRAjkfbISflkWLEWsImqxwNlwd2Xk/TSJeiSdw91KgcLVlAobJhwIQUqgwipgEAold1GkosqOKRuCAytZu9QFC3ESUq2MIggQMZq6lutqKbFuK663G64qQrRN4j20W1MRqfsYpxJBvcUlV8e6bri+vOJ2G907McT4ZVkuWJaLC1NNLTCu1yvW24rbehtECCDcMsm8JINNxq7EvGi6LYtDeyo+rBhYm3gnYOT2g4ia5srLNQnvqtqmRvBmjj8Tqrtbiokb/i8NSQ64N4dEDISY876GAzxKLLZ9FE8ezmW7W9XbViVnROP9cwiE7pFpTiWPd+T1GBLHCoBa7RTYWDZRwyoZ9+9RMOL0YJEv9juLeV4q3O8Be+iVdEuLL8h4zx8f9ZK3s35LwPjAm+Gh93yEC6I/Cg90jO+GdH3PxYt758V9YjiZybt7XIs3ErR4U5pOSBekz27vKM/ExMTExMTExMTExMQ/OXYChhENR4FA3IrYCjOyo+X9jxMBqCiF3eVTWB4E6XB0r5FX7CV/45mI4ngVfPOlbk9eArFS/CR/YgAVsQI4uW/plwsuT09OGspK9O5/1m5Bxo5/nj7FKvV9IG+Dpf/Ql7A3gl4rv2RKKO0+M4fCzuXtrk0iNcHsL9EeCLqzEJUqTIE54mewWHNA3RLBPLFkdyTqbqdd5RpgIQ3+/sZ1/AkgnBrj9tsV19+u6I1BdZFrwDKW7EBzh2Tu2BgAq6hEBLeeINLg1WrtQLVGn1ShoRZ12eSBqMO12rKogJG2mxVFrQX1Im6czJrI+nYhcYNUNU5FUUsk6VoU9YV2UZ8TZEyZdZYIGCFi1FpEtKkdysUDFK6fSEnqsEIh51fjr6dxLSOCFriVwJ7/NfmotY62dQ2m3nBbV7xer3h9ecH19VUEjHVDWyU+jsWzWJZltKzSMmxbw3q9Yb1JgG2LTWHXcFkuqMviAdm31tC2pvEvxJrCLcK0TcNiRixxxOVKWI4x9WT9AM9rZGgHpn2cH+ME3U+xIxF2kWb01+w+akwxxIr91ox7q63jfmT1SfeMgZg8W0G9K82j+fbkRLei0Po+TP94O43zwXeCLH/QGnN6kBLvPk/YetodS4cjk71DvtfFTSnScvHgo5jpuLc/wrD/rIO9L5v0PQt56be10TBOviK/b8EPdEPdX2kTRE+J/TvFFkuEBwd8C3aamneNnTZ2JkAMXYJzQjY1kk0NadeDvr/rpHHajxuA+90u/iYmJiYmJiYmJiYmJj4AYwyMgWBM7pwI4ct8EBrstJMXvIEso/SSd/IyxsefYT5/z6HI/ZfBc7LsmP6wejYqg1KNbGRd+c3u0sXTIEtF2FRzF+HWJakc98gx4XTL4di7wWhzHQ7uPuT7oVWIVGgZRYx9OdiUDSd7CdzVDRh3tE4az0PjIthCeED83WuQ6Z5IYgkMLoRq7xYQuIK3Cn5+Av9SQB5H4CeFEvvcGNf/fsXrf72AICIAlaL9KfWLLI4Vwi82LnfCWBbHShUrBnfJVEjiTpSKWqqfVwe3TiJ65GJCrz0ViWVRiEAs1gXhj5ywVEmXpWOg9Vj9f3QRJWUyZc3ykWaJYwok4HapRb2K0CBISFwPAA0Rz4Wlb/bOaBpQ2uI/9NbR0VEvBFoAi2HhfZfZ3TD1Zq6XJL7FbV1xu17VDdQNbRMrrbZJ3Il1FddRS11UcBpp3967BNpuHWyup7j5dUDv6G1DZwoBYwjAzTGIk4jh29OcQIAHTgcQooYen+edMfB1asQ7HZf839xJLN8QhCjPU+nAiBsSe95Fbw+sOh2KOPDHfP8O8qXYpzGKGHdPejO9byEoz+v1JSx9vp8fxaQdl7s7065W2aV0P4+PxsNa7ivzNXjUAEjj8I0kHuHDb38/Ct9NBx00BILzw8fj0vbv/4iQltcw7UTFB/Ogm5PpvFvunDJqHBMTExMTExMTExMTExNfiIVOXxvPVpwV5Lcvzv/sDjcyLFx5ZFI+CCfh2h4Q2GSvhpRWdVrGFIlIpoeT7yxsjeMZ6UVViTyrppX73hu3ZW9yyI7IGIQI2rVyJhDTKlIjoh+tGD24wdlvy+2ZBQkgCFqEdYwQ3+pDn1mJYSD73++sLmg6wFtzy5QI2BsxGLqJGF22C8ncZBV77yJerAv6/6ogLLh/8X8CaKforaOtDdgYpReUpaorM7WmANz6YSD1i1hDaIxpJ8DNamFZTMCocb5aXxSyANNFA1yb6CHWE0566xiXD+3nlg/IB1CmPs3yo2sg5wLpU4XEpZXFvzA3T4NrtEzAp3TFLRKl8SH9T9wkaX/jkZwX/UTFi9bQUgyI1sRSol4KyhIWVzlotbuQUxHDYlys64rb7Ybr7Yb1dkPfQmDYtg3rurmAlAOYA7Jal4dYFPBxYu76NgCVWQOnA62NdRyJsUjbBQFtSxecyYRSdtd6lsiZQOrj1FZEAAAgAElEQVTzNu3m2F2/jYyytE0hriJm4cP0rCKUTVKDDru3CjzNN5dnd9/yudm61NvyxcP9j8hHur9rOPcPQr6fOKn64OjhnLipY/yWr1+uEB2us5y3u2a7GxrnfSepvg/kfdzTs253fCTR/RR9+Ysy/LJ7FO8+H2X3oXc/uvP9T4DZXuaZYf/10UafS1zk+w4YuikNm7INhPx7XgJCCLR7F4mHU7RBhrEyrRgmJiYmJiYmJiYmJibeheU+O5OpgRAiTl+598vsUhrZbdR4Ds7TNLIpESbkjISKDjsxJcpwFDGOv3N5zsi72J1jkO7dQph/98g+CNSD5YX5udFyktXT3VFZWU7KZaQujoLF/uV3+M0QwYHde5OLGR0RV8SuHVk5exYwRHwQAYPRmFGUbO0mTvQQOSI+iG5vkGO5ufsdoAKvFfi/n1C8D/zc6GtDuzaUXrCUJ5RLRVmqkv0iOJAHwg5xrLrgQCpi0CBiuICRBQNdiW9umcw9VHHRY0Fdlog70ayPshP7YLiAcdYHF80P6OhdrQoAVBIrD7EukbmBSogSzAxuKcC0dljpu2GBksca964WELvg1Ca+5Vg2SYzY2obOjGUV65TsMi6fa0JDTmPbNmzrhtu6YltXD9wt+2Q/EdDaaG0CANSLWCapUNJ1kuHOKEXntw6AxW6qqbWTC4M71vZMGPU9TJDgM0lI2B2/p/bPxGc9+rTv5lnrdGcqTiSVhJbdwdkz1fuwEy8iJbC50dqX5afB18gCR2uVM8EhdrxlP8IqMNnS9D1R/AGwuAOP0syVoGHD12c7yef7+Nam+aMEmLeeQfbPqPvp0BUzirRsUtxH5p6YmJiYmJiYmJiYmJj4aix7X+aCvOosXtzuujOyf3R1Ge+IDX8XtdXaSqRndyb3lrd67A22ozi9dKbz1eXJufsockJOdA554czk29HLEx1Wy7krFF12R0Sphfrh/CH9fXvZVrZ25ZAVcl3ttwoZUu3diviU8qBhGPmc2lo+dfU6ugpCEe+C1f9OT5YaJrJIQOEgilmtLpqJGLrKnc1FTw/3OE39+AMEuiz4y+sznvunA8f5U0JFredfPuFf/vWvKE+LCBin7pZIY0oU1CLBtJdaQeAQN1KweHf1ZPEtXFxDiBdJ+LhcLin2BYELu2uwDvkOQAJTA7DhiNRPIlbGIiIJZByZ1YdZfmQ90fpK41Eg6xwWEd5c6sqpq3uqTYWE7GLJ+15X6x+zkugipm1NAm5fLhfUWgcLIlYxziw2eBA1eLTmaE3Kl8QSmyTsnDwnEacx5z7h2fWGnuYRX32crs9+vhQXRjILhRusPGepUESSAWmAmmHc3XV5l0fkOIO5b3p+n3um4zHkc/mgk+BkLn4j+eASOf1Osyx9jAupPULkOdt5Lv1kgcCEIrum747DcRdfV0s/i86EpXwgBSnL48KE/Wn5msbqdjoc+81XhaNI++cXw15i+dC+cJbUJKsHHJ4bH3Tzx3Lpd0IaxwScxKTJ9hfj8zLleUYOHbvhfiq14XNiSTsW6R/haej8mX1iYmJiYmJiYmJiYuJ7IFxIscRKGJYyIokTO3J9pIkoNtFIegnXn4iLtOItu3w5j6Mh/1gw44N7mYGt5zMVIqWt+zqDy/3XrCEQ7MFagwdShlBgPG7vsdLXqUUGLOD1ACca5dX44H//rG7qjofTvsEX/v4cQFalq8+YfbqNGzpidXlncbXDHeh+bGqHAtxuN7y+vg6r1FlJZF+tbq52VOiw+AGb/oEZ5ani//n93/DU/g3u/+hnBgOVCL/85RfUvxGWpwV1EcsJKhF8Gwgy2ywwPv1ywWWpMbiKkdbFLTH2cVWAsa8UJAFjueByeXKxo+s1NQuMriv6xarCBDgx9RHyX1xFWVDwUsTyYnRTFQIGkzCQbRPrhd6bxIXgNlhNtNZ8nPSm/apLQOvb7Ybb7YY1iRi5P25bBMo2AW7bJBD289MzLsuyEz9MwFCroZP56qwdDfsYNvvz9tZbcbgKhYMYKm62SqGdMCHHm4UbQ8WXjkECIbfiKWDsLCuIRczFcf6l3Tx+rCRUSH1MNQ77OZPZJl6kG8Ld/O+kvRdvUz77In8NIXjmFnD4fdIulPf5/cvmamgsKRrm4w8hK/02lSvPp5fuHg5xlc4yObC79KAHjCIHHiT99YQ1uYDxx99Fdjme8fQ/uZgxDMWTcXkKXxjynRvPuucwL73zPBwjvwznPhAvhsPesOL5iLnh+1oKjfLsxMTExMTExMTExMTE98JycDuCJE0Mi271Te9UzAhCzs4N90mIZW1DehH/QRYRJ2LN/5WovKwk3pjHfk3wHjSUywglqvp5IBD1+J0hxUj8p1yoCwHYpV0ajz7qvQ2VFDqsAjXtwS0ZTsQLJfjc7/0bAsZwLoDWN/SeypMsNxqryMC2kp2x9e7+9uU8tX5RKuF2E9//dp7VV1a7x8p11t+doe5vCI0JXduYsAgZ26PZf2ZI0O4L/vZvfwN6Ay3F41awCgTchSy2uBhVA3OLGyigK8HfYdegO4F6EP60v7VQ0Tzd56dnPD9/cuFj6GM6B3hcDgrS0yx7trU5cyexVUzss8whZDuz973eO9q2Yds23K43ERx6uHoKYUzKahYPjUXwWFcJnN2TCOFCRI6R0cc8wQC3jrXUQWjY/z28dicr6M1llldZ94eYaNddGiRJG76zU/f5kP1T5sgwpIkg0qyWK3G5QrDtnDIdRFiciofv9dF+EGbeGsgueOyJcErlS3PenTLl48Zjxm+kxOR7YmCcYeTy8zxOaWxEO7gwRRbbpaS2zhYnPJ5jObwhmLwffy5r/ij3b57q93rXoI/xceP+dB03H0Lqelnoz27yHx7Ha/MD3PTPtdMRB71O5/r0bOlp6fHE5NZ1wz7N70tmox/JTdleMD4XL36c8k5MTExMTExMTExM/HNhOW6iWD2WrRp8ZWcWL+jOObqV/B+4mX4m3pDfAcmPDWGDweTOSk7JtkPpT1abW/q5KPlFy1cx80hEZKFgIOuMBFE//oOrJSNPlUkUkmosh6MnEeKEMHWxYiduuBgxkIfjC6YQ1E3Jb6sfO4nWTayAlHXrXf7U1VPXWARWBmaW4MXrKuIHd3FR1a348kJrggZ7/A1C7ySfbO1VgF1b/7yQsVDLgud/uWBBASoBpcAsILZtc4GiVAncXZeKulSNpaCiEYs7I7F66REzoZvrp7j25iLJ3DMVknTX5w1bayKgUIxRtzjS48wqoFvwbIhVw3rbIl8VMZzo7YzexGpiaxs2tXDo6gZqXVest1XqqxYQW4pbwd2qIP+5+KEiB6tQAcD7eoyXGCO2nUDgrbs7rHOySNvA9dCdEImRhDYrk316LkIiCKxjvIckTPY4wMoPDmHERAxza3cUP23M98jHxELn3h7Mp3dI4lyDgzXKowG94y9tTg/ay4RSPuQnZOE4L97LIuuhNBDLXz7Z+F2KdOyQWRTaATH3A1b/cCtIRayRrEoE1cftEn2U9UXGd+OFzxPeWxadjSG/7X8L9n2Crc9Y4m+z0e6q7aNJ4Xck97Pc6h7W84Tjlr6xFzW+u+3FiCwqvOcZM/W7B7eMc4EDb7hps6O+m3DxEekyxvGWZ92JiYmJiYmJiYmJiYnvgxMBw1atIhEDSC92BDKRIrtbon0KCqbh3Jz+ufMJ89luBIUS3np6Tomzb3k8IoQYjAbmHLMi7+5CRjGQVwafunNCvLTaqngXFDyv+J6Pz2W1vX4OWQVDFJFQHUqLmUBgCXIIC4Cu1k2Cgxx/gb0wG7FrMQVKIVAtMNdApTNqZyytYasbtm0Myt1UCOFGQCOIryIWPwpGuDrRqP3DTDDAYDb//SoUUe4XPzFYic5KeHp6wuW5opHFm+hobcXtdnWCXoSLgqUv6K1Kz24N63oTa4W+SSDp5M4rVuebGBYxJyRmA1SQKHh9fcXl9ye3snD2NY28WiXYNxGF0MVdLCFuq+QJdgLXXch1VldOm7tn6mYp0SIItsSVSP1PXZG5QJhJY4YIN31nOqXjyeLU+Oa0Gp8YKEwgTvMSxm6ZF9IeuHWCj9XIM0RGTmM2zwlBc+e0RklByqG/zBjDRFjsRVEc5z3/XdKx5G0AIOacE9Eg1/neb6/7Fw/j3Q2DEPMyY4gbMrh2+bOmC1MjvHPYX4gww73Hhf8g+E7vO/vz/onxiDZ9d+1dBNsxw/k55U/B+yWaY3yFf0LcW5vAMZ59WYrGOMvhZFxoSlv+qGa714uki1Ee0mm+j8ceey47DPgfRsH61oLw7nOKFxMTExMTExMTExMTfwwGAcOJRk7unXynfiqJQ3kb3TvQvmZyzsQL3XLy7jMQOpxfdlMOiZg8rlZLwgqCQgTRQUyw45g70I/7s6uPvfDAwBhgeNcenBeppV17EsO8cznTSCVEInt5ZyM+NUFb+Z4EDLluUeaaCdUCcSsEArjHaygDBYzCQFWCuWwFRBqzoqnw06VcIAJTibd2gogUJdqcCnvlyVhJcBKiVKgp9HMQOg+hDVAJ9bJguVQAG9CaWLq0DasKGL13lF6x9ApewkVS6w232xXrtqE1sWCQv2xJY4HX2Qx+PIaJ8a1UCLVUVIu7oUtMsxUDIAG8l0sFgdCbWEKI+yh15aSiSakFVCXYuHST7rEoTJjI7p7c7ZNZlbBYXXA+FmEZZGKnj2/k1bE74WKY2xBnN7iAMUxHpGMt8zX7vspQz3jBVoVwkT6TiBGn8lBWE0OOwyHNnflwOj1i1AXO3EPlecbnRzqorHG982SWamMuAb90/HrWu3uMiUHJEq2UMs6f78jLLtOxJTPh9kVF1fZBXIThfpaKqMcM87QfyXiLkP2RhIxvbz2M1+6Nw96XLo191onuvjvqwbPL98AjASWNIX40Xn4YgvuD4MHegWFm49zP1RpBRQzQuPDjj0OMWEYu9zjP7ovFHPdSUnHDn8l+yOv5bYUKfXYvXvw489bExMTExMTExMTExD8vzi0wBouJoN9sEyUy5/hSg3Ru/sS7lx8aoXp43Xrw/jXscsLViEQrCh1XxtnJwbnrOYls5HitJiInKTuUBIa6CtkXJpSJITtfVUz6ojuQgWRay+AmI0KIqJuOoqSjE7LuzEqrGiwn6XGdCOhdqUOOIurvUqGigsSrEMKFgN7QARSqIKqg4r58VAEpw0p1cWdkK+WbChpaxQJQrSiXCroULc/PDRsWTAymDvSO3sWaYt0k7oi5VSpbwVYr2qVhWRY0NGx9w3q9YdtWrG3Dtm7Y1lWtMNpACme//T39DpdE6gLJrJUsfgSMqJHA0HWpKkqoAMHiQmpbNe5KZ9CloCwFlSqIs+uqFq6v9mS/l9PEDQwCh4lvRoCTun8Kd3WPkUniCLR8dlFOvz7uq/t0duTX4GKH0ph2EeS8HPtqJa1Qim9zFsUs8J7inc6D9/bpxizBnPjA+ioM8ZIOO78i/eHWwzjem74NB/Fl2MkDYS5NZH0WyA3781B/H1jLt5L6QsHrm/Eoj5/9xkY+BB8cYta++eHrpFH/qIGyU+pk8YrOe8SHY63UMhf/HKN5YmJiYmJiYmJiYmLiz8Dy5qpPpvCGkYihs7N6IorIrR3SG6ELBbHKbe9D3rNlFoKfjHAd90Ueifi0fS4gjOfhHsnpb6FhgXEmiBA0PRNXdsTYsKLaUyEva24bsjIXiRnh1gzWZhwpMnG86GMUd+hE6fHLpUFkSdu5qFhAZonhljTJhQMVMIp+J4AauBEqCGURN1PUR3KZkwsecCK1eweVBkZBpQ0MoF4qLr9cRMCoyuJ+MMH4DwdthtZWrCuwtdXjjby8fMbnz79jXVe03jSAd8VluaAuC3rpInTcblhXiS2xbiJg9G0Mag3Y+JDv3cdKjKdBvGSAsxuqDlCX8VqqWuO4GzVGax1ta54HbQVUCgp0Jb0G0Y5zumdl+Uc5932Mh7L7amZ3wUPW6SWx3bg3UTQjW2OM4x0xT5kYat00CY1Ozum8YtPLEJjZ5V+zMMkr8Y8IQizFCTKVA3sBBgcXV3E8XDQc5mCZUIYTfF7zTT7BRUYZlI8Zk/9SDH2OxI1ZuDWL7ffuEW9hDNrsE+xXFDTyJL0h2nXSqXRfOJkXO8exu/ufCHB/Jun5uC3298Cva71zu6Kvwr1k9v0wb/+ezfuetH/WW5vPz4/6uD03mFWXbWe3iuM47Lvj3qUan2PHE6Lr2ZzwaG7P4uXbldrHl5mYmJiYmJiYmJiYmPiZcWqBcYC9RVIQXsO+zGWdkEpu2eDqQLikGSwG9udR95V5Z9YRZ+KFvQgLbxUvwc5lmFXC3mLiTMAw/ozt5VQtFxjhDtmIS4sZQPdf2OmQt7qnakqUdiNuu2d+thgx3GcYMwsvk71UM4Ci5BEXceXEWkZP0lw5QawuZGcHiCQGAxG4FHAp6ERYGLItua9iL6QJXeTxFZglpgHVivVWARRc/vKEp399Rn26aL5xXX9KGPdOjOvtFdffxYritt5wu634/fff8Nuvv+G23iS4diGUUrDUBcuygCuhc8d6EwuMTYN+b1sEtd4LGAN9rf/QSb9lZvSti4jB4n4Kjb2/74NFdAa4M6joqtWbjqdO8rert7itgoRTydYJyKIFXAyz5gKRx9Z4F7Gjdey79AlA4TKSSWxzDXudnaTWvEOwIP0Zok8hEZgsY5lW2McEqMt1SZ0+fzO6m8ESnwNKAZdcxv15SAoEfGo4tIGZzj0kgpOABTsHvo18XxJ17yT3Jij+8txoQdDtcz9vnlnHnSPkoFjhTbt6Mg4rq9+AW8UBcOE5tqg7GXLLpXCNk88nlB/Chd5Z3c8L9b31gI/Aj16+nwI6J8riFDycIGLohOWYC8NIX37wC+sOPtNz7reIDt8viPfExMTExMTExMTExMQ/JpazN0MLPntA8OpynJFDRpo7+afiA7J4QYOIwSpe8KOXvdKE8FM//WAkcpyV6B8FDD/Vg60CxpaWUpwUcwzxPMZ1cVxw8BnPJ+dmIvU9K4UHErmzuNMB0AvQCqNRF/GByQMN107iNcrOw3jlkrxiNVYxpQiZTEDRVchM2UN7tJFUtYAKoywLKhWgdFBdUJYFVFaAitcn8iIQVQ38LC/xFl9h2xq2tWG9rVivKy7/8YTLvz9jeb5Ef/lZYXUvhE4dv77+huv//Ibb681jWry+vODl82es24qtNyXuC5ZSUasKGGBs6+qxJZoGxZZg7qOVkmHPK1k/PhSxMdjiaHQGNxmI1JOAoWKExKsAqIo7svq0oF4uqJcLqBRxLbY1tGtDu63YbitKlTF5DGYcVLmtzAdirN212krpDOMu8dWc5iwRU42QTmQ0kQuXlLd7Y2liNLpssjmN8vzDSPMEBgOIGLfjQMgzka9OjuZOZXepcz85Jfd++0bIjXbSQCBQdumi7bE/zay2CBzepLL1yRuD2ywQ7v3l9HKAbAaGoO3H+VVLaHWg/c7U+028eI8OZvmY27t7YjjCN76XY7gWInT9OKuqh7vG+w7dI0z48EdP6pS//Mz3kx8AadaOXztxejw2usvQ+2ye/lGGCDD0LR7+lec0t9jT+X8fT0NuAiZ0mFjzvgp+q6Dx48w1ExMTExMTExMTExMTX4f3WWAYGEAXHUNWle5WEqtQ4aEdoEF4/eUuJZXdwuD8BYtqkzS4i+uVHm5qOnf03g7pOQFWyAkyUAFRweVySaujEQRUIRDU1Y2+XLqFxZA+nKDCjmQ7K39233MgaC2t3tFbR7sA6yfCtjDWS5eyM+FpAy5bQV0JpZOWi41WfQiLg5HzLbWAwGgpgDM7KQi9ogQqBZUIxBVVRQmiq7i78gzkn6Jk+lIXlFJlh8bB6F1FjOuG23VF/d8X1P+9YPm0gFv/w8muHxFEhI06/uf2G/7+P/8fXv77M9bXKxoa2rZiWzcRJ9BBEOKzlopCBY3kmg2uoqy/Idy37En9veBHu2NM7KNCMBdhXBgo2b1PuIkjueTyu8n4+Mu//Sv+8n/9Cz797S+ony7greP66ys+/+dv+Px/fsX6+irCGuppXy6p7zKdj7P9PDLUMc0tUlpywt/HkYmqppSQzAdFY2uI5YiR4boJZn3Ufa6wNpSA4xxl130REyY+c3n3pD0zAx1uQQOWcomw4mdqxja33iGpktA0HrLfYKLA22PSyhgiRsrnvUjX1P7MRdre5Z5xmTluShYxgBOSjyFxZQYh5n31uwfL067ZQRD3rDksMEofhCIivX8Ukmv81aX5COS2YZybhLx9XUkDunhXnPhpkUl5s8C4J2I4fuhO4w+0I5JWJ4sF8v2V49zcIi58A4eHzImJiYmJiYmJiYmJiYm7WFo/IZE5kT47sixey5IPeweB2Mj1riQO3Noi3vbyCuJxRVoqBKhDVjiraycj/lrv2NqGra2+Uo/V1ziVgkLFV3YLMSYubQoXLFiwJ2Q6xLqhD7lbXZWo9RYJIcM5Nlthm1bdSVm7xhnYEcpMYGK00rEtHWvp2C7A9gy0CrSFPdm1FSyNcd06Lo2wbAW1EdAJ1PNrcrxNm8ZirkpcpGC/Kk42WyDnrmVtFqcgX3qKYMl6lV0cEcsWsb6QNtcl5kWIVmpKkmq6y18vqP96AS3fvqrwHx1O/EIInvIvC/gvBa//7ytef/0dXCEik7lwYgZRk7bvjA4RMDIx7gknZoS8b3AEfEYsWPbRYP3avnchNM3nPwCPIO0CRpfBzK1jeX7C5W/P+OU//gW//Ptf8Mu//xXP//YJT//6CfWpghtj/XzD668veP2v3/H5v37D9e8vuP73C9q6oa1NxMQyrkulYWl1KnGYd6VGHfvUnqInEypwQtZSStvIXDrJwwmoXePlolkuieCyuB5pgOZC+ZyS9RR3C0dGhPGYrf0+cMw7mw5tCPZ6QoWpJKTsrS528Kazemma7gruC3C68lgr4pYqO9H3LBZK3hfJpL4yxPvI30/yvlPSnFaORRQXyI5L15TTtk7IApjtjvvieZ7DFeTDl+gsiPrToY5vXZhdf398VPT7A+z84869A634SvtmOZ77rn5Fh29Dst/pNvPm7eukEA85/HclSMM4vJvh/vHtAd7UHbVv7K3EYOXg2GOWBznJuIZnFpfsYvKhvTLZf1Lh0+YiKxKPG88P9S/32/5xC1rcr7hF5Pki/z4ZGw+TfjBXfQW+x7NWpPlzP8dNTExMTExMTExMTPwxWNq6Ajh7wdEXMCXbB1rISJMwtdDVqARboO/m/3zybqfJG+mf+cH4SqgoTuUU6mB09AJQ6UKGtuakIGvGVApqLVjKAgtATmAQlKCHWjegKFNISuwTBh9ZbC+nJgJIZUSQsRJKaYXHL85ncbJO4XQMNF8CsBXG7bnj86eGz586Wo2V3LqYFQBw1XgYxIxlI/zb5ws+vVQsG0G9dEVAY2SSTwlAxHcRVLoLFCDSleEdrTNaa1jbhq3tVgwXQqkVbdvcWqPoizspycDU0bcGLiF+MNTl19aw8YYNK8rTBfUXAup86dXhhf+fvXeLtSUr677/Y4yqeT6s4z727m66aQQ8vC+ffMZENF54Y2JiohHBRIxcecMNiUBITEhMDIQLbwzileFCA6iJJgaDmHxGX/Vt4yuYIPQrYNPQm+59XMd5rBrj+S7GGFWjataca8512mut/fw6u+epatSoUaOqVj3/5yCNjXjoXOsjOZhg/3uPoacGomYFIuFigwzIzWXn8QwB6eyG4ZiHulqIKL0Pk45looX7ZOdW2GLRaEzeyi5gBZapRn09RvfZddz80eew+eINEGlrzArPc9eiSTT0KMX9V17H6//2HeiDFCZJIWMFCJnZrcivQe688vbWYAzdBJ/x/vabLY5KjgwbKqk4xuS7X5XKpNo8ZkdUKRl2zr2Y7BoFf176sfb7UPjsr5lka4RkYkrQkQWGR7ukv07PW9asZHqqtLP7qBGUDdXzCVNHAf566sVd2052jzGhgd5FGiG/zi3sa3bYyju/otri1yHY+0awH/nGgLK13et+FC7j72iErCbLTG/KXzjRayYlWihYl1nqUIQTI7gJVza2eN4R4JwXFh2TWQOuOEqhoEXzd3b8ZrZe/uI4h/6ELHMoKudydkgIkKGxeMmdqL5IrUZ4XZy3I0HbJKhw46Gq5ShYtrxMYfnlr0751bP0/ZwmcueT0oIzc618zhXXs9dqkS1pI038p/L8FPnxXFZYrOrHMTmbNFSLrhsMwzAMwzAMwzCnR2SMyY10hYez/HP+QIbMeGTf5xZJ+3ATFF0N7J0k8qVDD3AEKVaKD4T5w2T+0GQFAp973xCgtU8vladDUkqCoGxRbSEgSEI6w5+BNd7bNDwUeB4KhF6ErnOZcSV7bvWRINkDa/6AKkNjofeyrni205KQ1gjTmsa4ZjCpEbTVWrLH2oJBJhMICGkEDBsaZIBGSogMrJd8ZiB2cglZ4UBkByFP52R8kXAnqBgXlaG1RpKmSNIUqdb5IXBpUqSrq2CMcV6WyIzrXkDS0LbmgjuGBNj0WIZglDXIi7qAiPMxD4/900bBti4E4lYdtU4DMpawxZ5lQVmYtSVRNtbWeJ4bTUNDeGmLeSPIV8gNiZkSll8ORLXxwxcJV7UIjX4La89uYesdt9Da7kLGAoZkdowL4onwophE784GbpLG8M19HL6xh9HeAMloYlORyYqJQYWXIt7GX/1T6XOFQXZGpFjdMFMwzAcX0ODqGfRgUfvu9zD6pdzf8OOic2jO/s39bhlCY6SfQ6U0ZeH7gvFrjjHaR+mFNUGytrwwi6J4sdgoNyMTrQRVvhHBbStodRXLuRfD/eEtGwbL0SQU/kRunhfMopWbyT8ete++LyUBprTMUcw3RM9ZQBx9htFsRxZsoLRsKAAs1c5q5AEw8xs+jSLtmVZVnmyLQif8InRUdNSCI1A+bsvsS9W1pnCtmF0+uGVVNLaEalLoZPDNUViv6ncAACAASURBVJPriH4VmvVdydL8BX+tud+y6LmCOkKlORlubNGFOezIMoLHIuaLvYvOwKOvmSxcMAzDMAzDMAxzfkRk8gLZOeScxWYNUVT+XDJQ+ZzgWXoMETRdtN/lokVgqCzk5Q9rOJA1sgphAEhnpLfihdYahrQzcEVQAKSyIobyxi4RpB9xXoL5YyFlokSouoQGXJ9Fn1wlgmAkrBd8aR8FBQb64EFQCxd10TKYRgZG5QY2b3CutvcIGAUMmwaGADkhIBV+V7IC3zY1lNtXE+SLJ4I2LmIlOAhEBE3GihdJgiRJkWgdbhZSSkRR5NZ3x12QG4W8WDtpkR9ve4CgDSElAmqA7CiIpgBiABqYyUD2lJHbo6z1o9aqo95tQNVd9XjjhCaXosx7dxL5NB35xMuM2KEBZaF9IRQ4Zg0V4apVtQUAAJpAqUbcbaJ7ex1bb7uJ2//zLUjTBKlO52/aXzNigd6zm1h7yzZ2vn0Pj775A9z/5usY7RwiagBSRoWeFMRP/11QN8L+tqJRRVTtffH3hT+XjYgCrm5IubO5d3pBOM0WqlYlhDN4zTdCinDxlXbhSZifsvGq1KZstFlZACnfayqL0s/NsX98w19+dIoiSGUB+VUNgYVJPGfFeRbY4Ous7Urjb37fWsTM7MtuhaufS6suTyt42c+QXbsWb9hfGQoOE6fA7PBUnMMn9Xr3r1V/Nx218vzQlbx9MV8HyY7/0c0sN6wle/1MrYzKPizbKHDsK1qgQ8z+JorLVX3IhFt/bfD/lr0eVQ3wMheIZfHhhAson4enobwxDMMwDMMwDMOcIpGQ0qsNRUTxAbNg0ARmniwzz2Ofw9xVJyVR8QwoQkNWMaez97IFFQvb+u0KIV10hS247Y0Y5AQHQwTpUsqEdR+kMzFlaXiCKISCsOIf+inwaA/3UwpI8mlECDYlVEmoyR4GbbooIwhGEqY1g0ldY1InpMrMNd4sftQkpJHBsE3QBqgdAk7TyR+kDdnUTdrAzJQpcYKGKzacGo1UW/FimkyRJDYCwy7rqnZIYYufS5EdTzImO9Z2FL3AI7J0VUS26HqaaMTNOurrTYiadCrLwp18CrERMlGzhvUXb4BIYP+NXaTjaUHgAzDfNpI3deYQrGomY4XmZgdbb7+F1rWuTTM2M+nmNgIYO1eaa21svO0mVDNG+8YaDt7YxXhv5GO6gOpayU+eirGuKiwOhEb2q2YcOv6E83Uuwtew2TCdlF++6vUsmOu1fGrbdNUC5npHo/K34jXXLejzDlK4slt+7taPWOCEnHmNo6OugwxzXvjT7yhR6yLia7gVyOuVHCd6jWEYhmEYhmEY5rSJQiP0TCHQKuezOa5yIvgfOVGCyMcqeM9jkQsjoTiRu4LnXuZBfuRi+pE8f3rZu9TpFkU9wr0akBUS4M3s/kensgSpSgDpUnSEViCXOsR52ImgHSLAwEc25F6LNnJEgCKDNCIMOhqjRgojC47ZKLwtORQWRtstm0YGug2YKUHsAUIDJL3gIwFjRYwkMdDGZCKElBI+/7wxZCMudIokTTCdTjGZTLIUUuH4+nVVrCBjBV9UmURx7P2rIcqLg6caOtGQKkK0XoNQAtAVatnTSCB6CQKM1ohaNWy+dBMggfHeCOk4ySd04Nxp5684ugArUDQMh568VZSMFtWe5fZEE0pCxTHa13rY/qFbUI0IOk2rm5+3PWN3rrHWQmOjg/Z2D73bu/j+//42kuEUSJ0YOdP1OdehZQbkFPBbKY9OKF5URQgURIwlToMLb58N0wBVdHbR8ZgnXpTnankOnod4ceZkw1JV2HjBaqWIo8I8ovD7o0VigbNxtD6zyI3LaBxmniLK8/GSzM95ETZHXT/O6V7LMAzDMAzDMAwDANEc82SBucHtR1opbcqlcvqp3HZTjOsg8q8+33nJjZQAbbQrPA1ASFvckgwgbTolGyUgQFKAJEASeZ7x8j/kUoTtq30rKPhOFJ/ufGosq8VQZsw32hXuFjbyRBCgI4KuAeOGxrRukMS2CPnc4TuCUESiCEgahFHHQA0BNQGkFJDKPlhKJVHzhcyltL9JlbkJTiZTaG0AV+8iM7h6b95wn41NP6WNAdIkP17umJYLh/vojzRJEXfraN5sW/EiVm69kmf6iuNwFfH6hIoVGv0W2tf66N5ah9EGk/2RFQN9JMZp2G1XbSM4eYkAkxq0+i1svu0m1l+4BlmzNWdgcKy0MLYWD0HVI7S3+7j9Ey+g+8w6Hn/7TQwfHkBPUit2yIsaipFjfARKReTAKlxqA/0RVIkWVULbPPHisjA/tdXq7cz7XNbaLz405/0iqqysVzGaiWEYhmEYhmEYhmGYMhEQ2CZL6ZrKEQghuedmhUs35WYn65GfL5sVaKVZw5RPmZGlfnL1GoQoLec8mIWQEMIadoUrGCykhFDSfielNXi6lFNZ5EYW/SGQ5UQSPrtScQy8bcga2Gx0gfduFT6zjSFoX0hc5OlzEkWY1g1GDY1pw7hRWuQ/HozkIruOACABXROYtIE4JWCELLrFR2IooSCEhJTKFuKWNpGWMQYiSazgkKWaMoGIkWd9JxiXQpmgQTAijzQJBQy/vsnSV2mYVEOuR4i3G4j6NUglsm0y1UgpIWsKzY02urfWkAwnSEZT6MSlHKOwdoWboXMmUxZFMVOPYRnKoUH5eyEAGUk01trYePE6Ojf6EFEQVuSCmlZyrHZzT8YK9VqEuFNDY72NdDqF0RrjnaEVMbLuuBk6T209pxzeM1uhMCIhHPflBn9uvZELyFFdO8pDt7yvpxVZkdeAeYKDF/TBRuK5jxWjdqTIMRN1kb8nl+rQixiFdoStG7X0KFTcyo/DcnpKEFO1kg6xuqv4Mk4aDHN5mDd7T3DilpvkE4RhGIZhGIZhmAtGFOZCKdjvQ/tllSGqZN+s8pSlLJVQvqT1zhal9Z1XfhCBYYwXKnIjbWYHEsKKEkpZh2wSEMYWKpRSQkUKURRDuULe0tfmkLZ+hqU6bj7bJGxkBzmRg1yECDlPcW839gV7tda26DIRTCygmwKTNmHaMtAKwTZLg3eMh04CrKd7TcCsRVY8GBknJsBGnqism1YI0gAZG8GSmgTjyQST6QTT6QTJdJoV8NauNkZ+OHxBcIKGhg5SZflIGAqW8UXVVS1CrV9Hc6uNzo0OVD3O0gVxTuX5EBF0mqLWqWHrh24CAKaDMUa7h0gniY3CmDkfz348BXKhKm7W0N5ew/oLN9C5sYZap+5EKSpntFmhh16YceKmEKh3m7j1P15Ae72Hh//3DRze28XkYGyFDulbvWBe2XPEiitrDzqF4b5sURUrs1CMzsWJ446DFTazBgsvRx6e4BZ73OO4aLX5e7SCUCcWdW6ZCUgLPh2fKz5rmQvPyf6OrGxm0XcMwzAMwzAMwzBPkCg3eJTcL53QMPc5ppweRZR/DtIKzXhzz4odJvOozusn5AYdF7kRGHuEEyQUAEkiiwxQygoYSkVQyhf89uvJvMZG0GVvqAeJII0SnBHf9sFHFpDJU2IJAJB2GaOtgAJJ0LFA2lA2bVTNGe3LHuEVWsbKj6BSgGIB0wJ0hyCmAjIp1qYwgCuarWFgkOoUU23Fi8lkgiTJxYs0Te3YG2QpvKwwYUUNbTRSSjPByR9bAkDh2CggbtbRutZFa6uNWqdu627oJYs7P0XMmN+tSgZVU2i1uugdjjF8dACAMHiwnx0TX6HErlTd9ux5V/x1cU/yb71IJQComkJjrYX+nU30b2+g3m1CxtIW7wYK3uYLmp39vdRVASBqxKi3m5BSIR2n7pTcQzqawCQpXIkZBLEmQTun5E6+IlToQ2n7NPNN/nXll0dz2aTAZQtjnyS/eqH2yLFbOS0WRAyIopRbrkXl1y3+P5vppRZpZtm5my91o/D+FEQMf/6Gr8fHn+TzO1Zdp6eiY4V1GOYyc4ozuFIDfPJXToZhGIZhGIZhmJBcwJh5YMlMl9k3ZRtH6H1vf6Dge/u/2YoHIXkyIp+6yPg2gzWlN9dSvl0hJJRSLtzAwAi7vIqkFTGUglR52igANhIDrl+ZnVHY1E8EayghK3DA6RG2VQOtbZ0LY7SNaCAr7wgX0EEpwTQETA8wLQHdAowKR+008QNBgHbbuilhdgHsEiICVIpM1PGCBEgjSRJMkinG4zHGkwmSJEGaJEjTFKlOM13KikgGhkwWVZFqDa3TTGzKMuQIu4xJjE1/tNZC75k1XHvpJmq9ekmMYhbiDPJEAFKD9nYPd37yJdz/RhPJ/3kV6ThBOk7svAvrQRROzvC8PYWaEUQwiYaqR+jeXMP6C9ew+cINtK71AWHThZ0JRNCpRq3fxI0ffQ6tjQ4ef/c+9r73APuvP4JQEjLyJ+AJXMhPkxnxxlmHqqb/KaTqucw8NdeEGT2hWoSoXjWcO2L+bdqR2SL9/eyI06IqJvA0lg37U8051LKZo548JbOOudKcztVfZE5KfFYwDMMwDMMwDHOxicyMwQ2FL7IYiCAaIoyqKBR/DlM8BV6mVU178UC4dDEFo6t0IRLGFcQW3nAjMoO8bdpHiygI5yFui1W7qAshCqnwU22gKbEFtw1l6aKyqAPAiRg2nz9pV69DENJUQ6fGFhF3kQY2hRQgYwnZjIG2hO5IUANAjYqObYUBWMJDdpnnU7IRK6QETCQhtPXeT4cppkMDPUxhEifuEMGYFMk0wWQ6xXg0xngyhU5T6FS71FEmG1PjanoY8gW8NbS2/8KUX1Z3MhBKIG7W0dzsoHurj94z62hutSEiiVQnxePFzFKeH2TPragZo9ZpIBlOkQynGDw4wOD+PvQ4gZ4mtt7LzLCWXavFjIy4yJ85f7FRR0IINDbaaG/30H9uE2vPbaF1rYe4FWfnAoAg0mn13a/qkL2sGKhYobZRs5njYgWpBIzWmB5OkAwmWc2bQtdLTT6RmVewJiNX/MoFmbPFy9Ebs8ss+/1FZ5F4cVGuE0fW0ghrRpUjSGbeFL8I7w3Z8Z/ZTtk1OjiX/f02vBeDgjiN5cdwXpTEvLRWpxJVgRM3stq22EDLXBlWvz7Ou6bav28r2ix4Ka28OYZhGIZhGIZhmFMn0jCltBOzDzpFESNPDeW/IzJ2PSEC7SIvVy19keygPeEMpN4IAxEII75+BoTNrV+o4i1gyGVFCjuZGU8VIGQWHZL3EbZGAxno1EYL+ELhPj2Ub19Agoywxlu3+SRNkaTWkG+0yR7shARUL0bjRgeyr4CGgFA4F7IjZgQACd000DUB/WiC9HCE6eMB0oMpSFnBQWuNaZJgOp1iOp0iTVIYYwBDrjg5IJUEXLon42paGDLQZCNPsiLmcCKQIZjUIG7V0N/uYuPF6+jdXkdjveWCY87IO/+Kk81tshEO7es93Om9hMf/fR+PojdwcPcxpvtDqHoMEUt3QoXnyhyv/6W3bk8yH3nRf24TW2+/hc61PhprLYhI2rmDszQ4O4MsGeiUELdrWH9uGypWUPUIj7/1Jh4/GkA1IiASgDnJPp8R3ga9RLSBKF/TmKeSUDjwBsZK0cDe5DKx81JCgKDFcgv5LFIn4uSyC8NcNTLnIDHn3OCsnwzDMAzDMAzDXBAib+wHEHhi5Q8z5A2hgTOqz07hYwzI56kQKHqlQrjC20XLXJ5eyn+mojgCWOOpNggjP0CANgZap0h0gsR79rseCyFgyEAaCaGFs+FSts00TZFOUxdNYSMpjNHQvr5FJry4CAzjPpNAqlOkTrzw/ZU1ifpaE2ojBvUUqCkglPDO62fmHj1j7iGyfrkKICVAHQlKIpgkRqoSTPZHmAzGmByOkfpC3S49VJBLywpQxh1bnYs+/j9jjNt/AxiDqFVD3K6j1m6gvdlF/84GurfW0FhrIapHQBapUpH6hFlM6KRMhKgeWZEo2YCUAo1+E4dvtDA+HGE6nMBMUjsnlZob9TS3fcApgsZFJhnEzTpqzTpqvQaaW12sv3AN/TubiNt1qFrk6p2c7WENryQgglASqhahfa0HEQtb0wYSw90DTPYHUCqCyNJqFXu2yHR55lOztHH2Cc9Fr7M2vFdF41wErKblRYpq/K+FO/IKw/XETfZHbZyEdRBAxT0ta8JFd2XfrHgkQ9+H8P+lqDGGeVqxTgLzHZcYhmEYhmEYhmEuAhGkCxcgKj7DUJCQQsA+4Aifuwn5KwHCSG+FCFeHr2mRt+eM4oHXqP9syBadyIt/E6CtAT373pCrxWDFi2ma5JsVgJACSke2/oV0AobJPVmTSYLJaGrFiDTNUyMZY9NKOVdPARnsjLRRHy51lK8AbkCIajU0rzVQe6YD0ZAQqmByLQzVmSIIIA1hrBlItGqQ9QiyG0M8ijD8rxFGO2Mc7uxCJwYyUravyiUbCfKdaC9YmFyBEbD1Q4hgBYxUw6QazY0Ourc2sPH8Nazd2UDUiqHqCkIARusLZzS8zBARkGq0Ntpo9Jvo3lrH4P4e3vzG95B+/yGSgQGlBqjbYyvEMgpaKFQSSBsYrUHaoLneRfv6GrbefhPrL2xDNWpQtcgumdW8OOsjLApviQgmTRG1YnRbG4jqMZprLdz9P9/GwZuPUW8JKOWuCHmWnUsDG4zYoAzMijqhUDiTCi6LPDzPtEzLk+fYr/rtyXBpo1UY5rTx3kgX7cLBMAzDMAzDMAxTIvIe3uWUS75ARJ7uyf8eltf2PyAzGGYmFidO2ILXgTCBQLwwefSFIZOljzLOE1wYEQgf9p/WGmnqIjDSaW4EkbZYd2QISmkIKXMvV9fGdJpgPJ04ESS1NR1c7QdjCGSKAkbuFSrcMsbm4a8p1DeaaF7voL7VhGpGtqhy8Bx4HsYZyq1aQeYgK0hIKRF1amgIoJduorbWQPNWF8nhBOlYQ09sDQWTWDHCDr0pHCPfnowU4noMWVMQsUTUqFnj8VYbra0u2ts9NNaagHIRNwaB2YoNksehKqMDARBKIooUmusCKlYwMOhc62O6N0IymGI6sUW+9WQKPdEwiS4Y7KwsacUoIV3B+0YEVYsRN2PEjRqieoTmZhfNjS66t9ZQX2vBH0fKcrcJPAlbsxUlBKQSaPRbkFIiGU0glcLgwT6mh5OgZk7owf2E5mFJ1BWFLFdlczSziJk6ExdI7DhORIlPFTVbQ6bgSVB8IZ9SStjJJAApZGFxItiIKiwWEM6L+eee+4tBnOOZQH5MnvSoMMxFYvYcLX8zW0fr4lx/GYZhGIZhGIa5+kSZwRqumLUXL7IIiWLKBc+MAYC8juHaMgZkTCYSVLVFwffk1yHKXmXedEHA8CJGqtN8+0I4gxBgtCvqazeWGeUn0ymmOsnWNy5NlRdMYHw8Bzkhwz+ikasHAYhIQDUk2s/10Xt+AypWEFJ4DSd76pvnMXsmiDD8hQBjPyklEfWbaG60QJqQjhMMHx7g8M09DO8fYPx4gOnBGJOBjSyxxum8v0ICkAKqHqHea6Kx1UZjo4XuzXV0r61B1iRkJJ0ARZnxrJDAiJ9xT0Qh3VAWUURQkUJrvY3WZgdGE9LhFMOdAXbvPsDg3h7GOwOM90aYenEKVDiHhRCQQiKqx6j1m2htdtG9vobujTV0rvWgGhFETYG0ybaZT+VzOqgzFpTgnE4JUS1CtNnFzf/xFnRvbOC//7+vY7x7DyqSs+tWdfmsT01v1PZXASdezK0NXWG3vmqUDf1nLUKc9+Vnlf2pFDtEIGS4SeNFi8LyXr9wQr2AgJQSZAhpeknqDmXBUjT/QHmx9BSOpL8vsXjBMMeE8x8yDMMwDMMwDPOEiLTJIxQMuWLWrjZCntppsWdp7uVJmQhCLmLBCg3aefej0JYXS7J1ApGCiCCFyIIaskgNVwMj1baOBZAbd4W0xh4tpU0hFa5HhCRJs/7oVLv9dXn/bQYr93zmipJnjTuThxJo3+qhfbuHxmYbIhLOpjprEjn/57vqLRJZQYNcDYF6rwmpFJrrbaSjKdJxCj1JXfoge8wIBCElpJSQkYKqKRt10aohasWot+qQkciMapyS4+yoGtks0sl5WhMRRCxR7zWwJrfQ2ugiHU2RjKbQY3dsjXF1TQhSSltPInbHtmmPbb3dQNyuA5Gri1OoQXMRKMR9wYeHyZpCfa2F7XfcRtSoYf/1x5gejgGXRi40fvooMnFKRtFlCQszFwi/EhXviw74T5UeeBpzr1x/qUxV5MQ8EWKmmSXWWZZMtEDw6q6v4dzJl5OQQmYpAIUQUFK5+3XqTg1795w7z8O5JUQW9nXkPC10fPYnmvOBZsLKlj2+iyWH8v4Vlj6Dy9dMJNBTdVY+WWYPJxVeZ+buvOsrTueamv3teZFuk6viC9EAgadExWI8zxmGYRiGYRiGeYJE3lvTRz3YaAkDkM4M/1VG6tyQ4oxzQCZGZNEXxiCZJkh1EggUdv3w4dGYYpoprW29CemiKrLliWzRbe3/OXEFlD2EaaWgpHR9yts0RDb1VOIiMJxIQk6vsVELIuhVYKA3BFWLoOoK7Ts9rL+0DQQpo57Us+vcx8nyg7pLJyIjK2A01zvIEoEZAJpgXF0L44ozy0hCKokojiCUBEnYKBfAeuVrkwlL2Sb5+fb0mDeWga5mz1P3dSRQ6zXQWG9nqdOsMGdgUg1KDUxqv5ORhIwlZBxBRnm0kj0XvIiFLA3NkX06L8rzmgiAgIwVGutNbL/zGdR7LSSHEyTDacmQGRq6CNk5fspnb9mQTVW/hefMvGWy32h2oTntXyZm9jNMczZHuFg5amOJw1u1rScl2oUihhctwte8gwKCBKQTMSCs4KxkBFCKUOgTR+S3p8KvRy1bkAKrXrJ2wnOvWI6nGEVyJFXrFb5anJ6q6ttVju7CuXBZT74rAJXfLPpjiISb2pR/tWiVS8sCJ6O5oiwBMlwwb2ahYHH1Bo9hGIZhGIZhmAtONBgMitENxoCMtoZMMtBksvdA4CEa5OvO/kn7JERE0KmNckiSKZI0zSI6yK8rrQe0dCKBqRAwfIRG6CVLmYhh01PBG+KdC5x04kVheTIuckNnbXtDfWbkIZeTg4S3DtvfXd/Wb3Wx/sIWmtvtSx1GT0QwWuepO/w+SAERKSjILM2NENKOrhsrAYCEsF7/T3AfmDm4yKdMWPIGX2XPCaHc8ZbCiRb+2BYjFC6VbYKsQEcA4maM9nYP137kGdR6Tey8dh/pcApfK8CSp3Wy6t0Ze5a6CJCVvNpX5LjNXKRzeG6EiqMscCwf8eDcoy/SzgbM3EedeAF3/QUF9amENb/7ZaUSkFJACBstFymVCQfklpuNeihsPXi76gAdMf5zPd9PXzRcyMzmVpUwVmmbOS9CPWLRMoB3fkFxup9Jry4Kx5yUPJcZhmEYhmEYhrnARPuHBwDBGfWtMGDTzRgY0tCks0LchSQXJcOLUgpSqcwYmKYpdKIxnU6RprZWRWaEEgLCGV+Ur1sRChjGFddOU+sFLjAjSngxwiW/ynZIZGmnclHECxbG5BEG3jCELJmMtMZ5b9c0Np2WjBVUPUbzRgcb77h2JR7ybFqpooVJSHtMigs6r3wndHjTmJhrnGLOkyojLhlTWigUDIMv/dH09S0u63H081MAUT1Ga1NBxrcRdeqYHI4wNPswE1fIXPgxkCAYEIy7Xp3dzoc2zmU8+49aJnQ4vhCXolMcuqNEjFXaAVC4xp/GaFX17TjFu8vre+ECApDwYqOblya/5xFMVg9DChs5JWTuPCCyyENnpvdTe26QQrHPy8jSy+5mQbQrC/7ifEIXs6Mf1KIp9ANPLtqGOR4zokT4/Zx57v8enBd2cbWiMU4gXvCpwDAMwzAMwzDMBSY6ODzMalbozMhPWYFtA1P0DgUyY4t/ahQECKkzo4sAbKSDNkjTBDrV7gEyMCilCJ4ai9EOPgWVzcFvCoYVch7XPlrEruvL5Nq6GVmqI7hlybZjskgPBA9szqsVBoTQaAQYGKzfXMfmW6+jd6s/6/V3qZ967XGjucatc+0Mc2bMM96K5a2RFxlBQVFsmw4jbtfQvdHH9R+5g73XHuDxt+9BT1ObKsOl37HrnkP/5lnHrsDQAzh1698ykRUr15s4Yp4v095ZG7rtPVZCCuFqD7lIQgojD8neD13oIBnYyEgCjDAw0iDVqWswMODPOUBLx9Edw7h5rqmXjgwGWSL66VLfy59OjgyEFUFg0VFi3pXgmEKtudKD8sQYDocwZYeSC0673T5xPSeGYRiGYRiGOSui4WhYSNtUrkcB2LRMPjWTEM4A6B8GvXHDe3qL0OBi8+8bX2/CLegL6RJs5AWAwjb9P6GtkmDgC4DnNSt8+/m6tt9SyswrFSQKnqsGPhWWsGlznCHIlyC3xiIbEaLqERq9FvrPbWD7nTcQ1SIgWO5qPAgLCKJZEeOqGFeZnKt4TMsBQ+5aEtVjNDc6UFEEFSuMd4cY7RwinUxhi7k4D/HzOpGDdF7287zllmqs0nn9IhodFhmxF/W3qj5G+N3S++oi7cK+VK7r0nzNq8txPuKFyMULqaDUbCpEaQhGCxgvyhtXySjbtyDCEPYOe+L0aJl4sXw7y43XBZivCzWW/Me58+0qXlMvCaF4UcxQFnyi/LOQ+YLhYfPrX7RDuahGUKGzQeiJEKvvDRfmPjve+c534rXXXnvS3ViJb37zm3j729/+pLvBMAzDMAzDMJVE2okLtkaEyaMvfLoK5GJBlurC45+VXJonyKLBJUtFpY0VIdwq3rBEgPu2aCzyBYhhjFvOG2ny6AkiFLybjEuFY4yBIAlXjgNZoe7QEy/7QIAgZFn/jRU1DGl0t3q4/a4X0LnRg4hk1k/fxFVgcX505mpQdYyv0HGvcq7WBCkV6v0W+s9tAQQ8+OZdPPy/PwCEgYyl88yVsyufGRfRTPZ0sNCgfoSQUhY+wrZOxaGbclO511tCUcJvM/9XLfb7XTFZeCGzFOSMuBTcD6/KDZ6BO8Dzfim8UckMawAAIABJREFUHpcnOl3Cv2sLXzAMwzAMwzAMw1wtIp9iyadtIp2nXKJAXFjoheiNLTI3+Nj28nRQBQEgEDDICQqFCAsfraG98mCcUcdvM99OWfiwGaQMAGlrcgfqBWX/c+mTQh9st10VKzTWWujdXkf/zgZqnVrujTnzsHiJWWEfZha9Cvv/tHOVjyHBRmFFMhMxJodjjHcGmByOkUxSSJF73J/5aV12+Z1DuP3zNkPN295SY3KMAZyJiJgJKQneHje6JEyMvzAqQCzd93l9OU4Pi9OCbExQkC5KiFlR36ZBdJGEXtAI709EQXTRk2GlCJlspTPpCY48k66yrph5i2QfrgxZUBDB/kE5b/fO9fiWIyBOb8yziJOq8JErO4EZhmEYhmEYhmFyoixFhDPgG3JRE4ZAwqZfWlSoNIuWECgYTrJ0VD5vN+ADHhB6xdnC2WTrTrjcKMZ3yAA+5MILHBYfMj/br1BAyQt6u7VIIPS6tpEXJtt/YzQavSZuves59G6tI2oo8MMhw1wyhIva0oCMJFqbHWy+dB1RM8aD/7yLh6+8AVGLIGJ1fn26CvVGzpwjs9ofu9W5LRLmRmGcZ4FnX/9JShsBaQKjLDlhIryn2lSI7ncbPuDeI7sXXwoyI+xpE9775xxHV2Mk2/6q/bjo4ocIi9hfNWj5EKg5y5yq1EDlN6c85osCq0T5w0WelAzDMAzDMAzDMMcjSpIki8DQRruUT87bEwQSgaGkLBRkURPBAxN5R1ATFB4NHqPJJ2zydTTyWhRU2AZlokoYmWHzhSMTJ4LNBqtRZr6Ydez1hh7Kndc0IJVCY9NGXnRvraG50bZFycvOuVfRFlDmadjHp42rekyPSA8ihICMJJobHQglMdkdYfx4gHScuMLeYSRGaM08fSPQKodgQWKjS3MsFwnf8znNcc+NeXOjAk44lmWf6+Os76MmIKzYb0i4wrqUiRdZZIZPGTUj2gcRihd0fhxvPhxjO+V7/IJ6AsVzH9nYLSMAXWiR6BQuY+d1vI7NUcN/xCl/agFAPgDqDIepqq8X9KgwDMMwDMMwDMOcCdFoNMpSUpCPmPBFsrPUTXkqi/BVZNEblBliPD7lkxUy7Heho6M12AjMyyDixQsnozgjjigUE5fSFuMmQ/BxG8aLIVlBw0XZBQRAAiYlRPUIW2+9gbXnN9Fc60DVIlvbg2GYywsRTKoRNWvoNOtIDicQIDz69j3s392BjBREIRCDz/knxxmNvRfJq34qbPd4ZszT6LWPPgQAcg4EWeooILvHlsULX6fD18cQLvriIhZ1ZxiGYRiGYRiGYRiGOQ7ReDxy0elOqDBhoWwrIHhCIwtg7SiFKIiykciJCYKsmOBTkotQWCD/XWHFity++TpSWCFDRdJGi7h0UwbGesP66JBSeopyygCjrcWnudFC90Yf/TsbaF3rQZ1nahmGYU4dX483S6LiBM/2dg8yUkgnKZJRAj2xkRjCR4NdMGYjyC4oCzpWWQC7PNalj+Vi2cciC+ablRiK27cpmAg01+u8LN4vbu94fc3uwQIFUSIP8wjuXlldJyo0YkWMvC++QHVlnyGK+3NhJ9cJOa3IlFNz2WeYFQnCLqj8HcMwDMMwDMMwzFNANJlOECoI1vgvIIyrD7Eof/TMV/l3xSKtwmWLEnOXt18g2x4h9yb1hUp9G0JISCkhlYKAjRQB4MpZ5O8X5rYmgLSBiCTa1zpYf3EL7Zs91HsNkDEcfcEwVwgbWWbQ6LfQWOtgvDPAeHeIwYN9pJMphJBHN8LMctZGtJNehpfJ6+LvLy6aYZ4YcV6pdGz6xrw206y6E3xXSiXloxOX25C7v4btzjgOXHa4JgBzRQhzRy1b/4NhGIZhGIZhGOaKEMG4CAYfHgHYdBWCbOQE5YJCSPFZKhdAys9Y/svKZ62Sscj2wwkfXshwRhzhipRK4SIwgva9SJLnIqZSfc7QIxVWoABQ7zXQ3Ghj7blNdG+vI2rEbOtgmCvBrLt0VuNCAL3bmxAkcP+V15FOpqDUgHSWg+eJM+8ydJHKHOTX91nhunL5JQ3rM9EPq3bsGNuoEt/D11MXL0oHMoukoPL+5jkW5wor5REqLDu/UsOiuXRh6x6U3c8XRf5kqzgRg8rLLydulVu8COcec1Wh0ns358LIsLLfjxDB8gzDMAzDMAzDMFeTqPjM48WDma9ywkgN5P6NovR7IU936cG/uMkgbUfuclroQNgf4aM5YLWOsrGHAsFFIC9sGhb9tg+BhEa/gd4z6+jd2UDneh8mNZm4wTDMJaZg38yvPIasUNG5sYZmt4nRzgEO33yMZJjApAYQF7o071yO2+eTm7xEwbhWJQQ8SU5aiPhMxIvCBpB7U1dtJki56D4WDnZ2tytkghLBOotmRsUGybdgPzz5IziPZYy2/uzP/ho45pYu4xWBubyU/w4WlW8Zhnn6+MQnPoFXXnllpXWUUviTP/mTM+oRwzAMwzDM+REBxWci4R/0A8N/wYCz0JhDxYwXhdwUlfIIIIQt9O3bnskqJeYbkAgQRBAgZ6QgCEEgMs6mU7L0OOdqFSuoeoTe7U1svHgd9U4TRuu8dkaxA8xpcJz84ausU162kNv9LA9ieG5UpUQrelfP9eHleXY2UPY/AHkshtEaUALrL90AIolHr/wAh2/uBkbOsgl38QFa6fAd9xpTcT6cVwanudu54PM2uwzQ0eb4RemjigsGb0uC/lGIcCX3hQj6WHwzP+Vi8Z4srCMAhcLFKXBh1YuT7WP2Nw6CqCy+7zNPlJmLwILveXIyzNPK3/3d3+Gf/umfVlqHBQyGYRiGYa4KkX8jSOTens7YUxQ2yo9WVGFwKX2i4ucZn1BhDS9UtXBhQRSf43xqK2/Ece+9iJGvU/K8dC9xp47WVgfd2+voPbMBMi59DHM+zBvqCqfaeY7JT5pKE6M3KlNJy7iIO3DlmW/ksWnkNKCA3jObiJo1TPdHmByMkI4mMHom18zpcMrzwDvuH5fTnpblaIcZMYDyiLiFqX9C0fw0dnCZTEFEs2L9QoK0RCt2qRzhSLOhhMjSMi7RvhdFqDC4Jxu4ixt7IYJ/qxwrR2GM5kSgFH7Kx5JKE4ojM5jTZd58Xk5EZxiGYRiGYRiGucpEeXqn3Aux/JiUFTfFvFgKKgZmlCPgg69DD3RBVJ2yhQKDQcmL3aeZ8qk9jLH/8lQfAkKEW3JtCQJpgkkNOtf7uPZjz6C52bHrXVhjzVOOy5eS2UGle2POOK3LSrgZJoSdy+4kEU5oWxyv5Ftgzh0vNgkgbtaw/sJ1GEN49MoPoA/HEKos4TLnxVmd26Hn/XHJ7jFZi6dApq8X+5YnS1yOVZdfokHLWV5qz/0UC3eqLLCJJfpzUe47zNXjorprMMfhJ3/yJ/H8888fa91vfOMbePDgwcrrKaXwnve851jbBIBWq3XsdRmGYRiGYRjmrIkqn9cXPEOJYBn7uFVaeJl1C++paHgpBWLMGLOoLF6YQp5y4VuULhM4iSyllKwpRG1b92LjpetuPTPbOebkUOmttw3Njb4IDEtO1LKBNeQiZExu8I8kpBRHpIY6rwNq+6oTDZ2abD4pISGUhJASQopgelcMDJB7BvM8PF2OyH1EAKJGjN6zmyAiDO7vQ6cGOkmDY3JGB2VRFIJ7paovz5JVjdY+Wq9ijGav3bBjKgTmGe1PlQVNV2UMOkpQnCn8fcIDErYXivXHaruUUmxuFEy4yjJFwVeIYjmqbws5qv1ZL4djdCOPEJo7vsuoynyNZk6d5UI27TlbnoAselxEPv/5zx973fe97334whe+sPJ6nU4Hf//3f3/s7TIMwzAMwzDMRSbKjSiB4bhiQVH+JGbTKPgoDcz84htYwiwTeEYTZjtDPvrCEAyK4oVdT0B5T3grYcAYAdIG7Rs9bLztOjq3+rbuxoXx4n9amDO7/DF3ooTP5K6nGnqSYjoYY3I4tkZlIdDa6qDebUBE0j7QP5HDGBgIhcRobx+Dx4fQowQgoN5uoN5totZtIGrGVswQsKnanDCTRSqxQeyJQP4CIwXiVg2trS7W3rINISX2X38Mo7Wdk8wCihN4qZij8wo98pebTCOc37dVTsOznxHOSDmj3Zf7T8X7qvLfLjDQn0a/lmZOKGYBuWIbfM9mmFlOQ2lkGIZhGIZhGIa5uLgaGKEn6Ow7ALkHffgFkHkpFxNruCVEeY0F3qBwxi9CliqonOwhq/FNftk8GoNMngpKOAFDuFRUSkmoWKJ9vYfNd9xEVI9YvHhiuBgZIbODaQzBJCn0NEU6TWGSFJQY6HGKdJRicjjCZDCGnqSAACa7QzQ3O2htdxG3ai7axrd9tuS2VzvHpoMJksMJdr7/EAdv7kKPE8AAjW4T9V4T9W4DUasGWY9s8fhYQcURonoMqRSEklmLBFPtGs6cLQKQkUJjrYWNF64DBIx2BkgGY5ChPGqgelUGS6Z9Cm3RJXHhzDjFNEjZ/eyUbx1hc0cJDwWBKLgnk1gm/dFJOU6KqopoDgj4+lRLtzeTo7LcL4Z52qgQNBmGYRiGYRiGYa4oUfHZv2xRKrvKrpCT4zj4jC2uG+QMXkXxQtgaCNKnGQrqX/iFBACSVtQwhKhbQ329gdaNLhprLWuU1GSdP9n2cYaU502eJkUpCSEVDBno6RTj3TEO7+3i4M0djB8fYnowhp4a6ImxRW4FQRjbxMNX3kBzo43b/++L6N3ZQNSMICN5rodSSAGpBA7u7uDBN36AwYM9jHYGLiJD4EDIzOYtlIRqKNS6DTQ32mhv99G7tY7GWhu1RtOlOiNoDRif0ow5V0gTVC1G/84WKCUcvrGDwQPCZH/MQTIL8ZECYQq4J9idS8is+HrUABYlj/zdJRl4WlUICZdfJqqDYZ4WOPKCYRiGYRiGYZing8gXHfbZVAqRELBf2iwVRYNDweM2qz9RbUyZ8VwN2i6/O9qTl0BkDb7GGCdSmCwlj1CiuDNKoN5vov/cFtrbPefxbtdjzg4flyOEgJQy+zYdJ0gnKUbj1KaImqaYDqaY7I8xeLiPwcM9TPaGSIdT6JRgUoJQAtKXmzeAnqYwicbB3R3ErRraN3tQsco3fA6H1mgDSoHR40Psvf4Q08MxktEUUio73514ZrS2YxArTA/GSA7HSAZTJIMpGr0Wap0GoloEVVNQjQiqHkHVIjtPRZBqijl1siuUu14IKaHiCM2NNtae3wYJYHI4AbSZG4HBAHk0HvI566HScuTs11lY3fLNn6R3VT77y55WQogzjtgTFdOrquB4cHEr6B3CZ3Q8B+ZtaPYg5bf5wBGCwr8FgnXmHuPyDxX7zzBPNfwHAsMwDMMwDMMwV5+KIt4EsYrNtLKwqU9nUdEKBQID8pQYoZEoM9oWcpcXU0gYQ3bTzlDs15VCQki7GSEFZCTR3u7h2jvuoN6vw6Tp+aQueeqxR1YIAaVU5qg9HQwweLCP/dcfYXB/H9P9CfQoBWlr7DfaiVFCQkWAUsJGYEjKjHQyViBN2PveQ0TNGK2tDmRbwujzi17QUw09mGK0M8Dw8YGd8rF09TskIAyEBEQUZXU90nEKPTnE6PEQu68+gJQRVKTQWG+hudFC984mOjfX0FyPEMXKmiu1Odf9epohImidIu7Vce1H7wAA9l57CK2fcMcuMgUhonjdrs5wQvktozIzVzkV0ukY57yIXqiXtOL6py1izBbRLkY8lrc5I8IIXy3IrhPu49kKLuVcYCukgSK+8TIMwzAMwzAMwzAMsxpRHj2RY21QvqYElcSIwAMyM1blVqzcfpWnEqFwnVIqCAq2VtxOaMgqGkty8wlZe4gMF3G9NgZxo4bGehuNzTbiVgwRSZDRQV+YSuY4UIuZN1UIW/jYGJhUYzKYYDCY2siLUYLho0OMdgcYPNp3kRYpaGqK7QoByCCKp5RdxRb7JugkhUlTEIzf8kyO+LPEzjOC0QZSScisrIef76JopCTAGAJ0Cj0BgARCSug0RTKaIJ2mGO8OUO+3bO2MZoy4VUfUrEFFrl6GT5nmjMWz6Wcq4Lm+JHZMVSShek00Nztob/cw3h0hHSbZNWp2zKtT+mTfCH9tpNLiC4zMlT8Vxd+TIlyePirtir8MU77gYsp9Wrhb+Vk6a8B3m6Pi8qeyv87jf1GCxOJhzJe1u+d6ntVuOI3+uK3Pu7DOOAfA1rqoagMoOgBkTaw+dvOnJ1X8UPVd+efyeJ1k/ErRnPO3vLiNqvNoqW7ZcwZCLBXtN8+Pw5NdUwSVD3fw95I7viL/m6p8Vhx3RGmJ68nKcyg45lX9sj+f7Jw+LUHxNG6NJCmfCwTkIWYzV5hFrcx5X7XMWYqTDMMwDMMwDMMwF5Oo8lsRPigteEqv9JSdTXxRXsc+3x31wOaNhSJ4Wi+ZnwiAgjU2GQDGtk1kvfnr9QjtW300t1ogSVxf4ByQUkAqBUNAmkwxfLCHR/99H6MHA4zuHSCdJkjTNJ9jJGzarxBfBMWJW8WIIHIRGbDryWA98ilqZkW500ZIAVVTkEq6mAuRGYQzQa9soBXeaFucx5PBGJPBGIN7+5CRhGxFaG5Y43n/ziZ6z2xCdSOoKILRGlQZEjDP7Z1ZFSElZKTQXO9g7bkt7NFjHAz2nPmbAOlS/rjLSdHILmYM80SUiRh2+RWOUXb5dfOG4DZ8miKG2xAVZ2bhsrvwZFrQl6r1hJjbXKWp+6Re+wUjbd7Warsk3PhUpXY6LuExXaYzs+NGJPLxEad0fyvf/isVnqoVqjiLq3BZ6FlxdQrP1+Mcy0C8oJPtYShtVtWIJ+Fvlf5+QihfQU42wkdH66wkFHih8KhVTqH+2ElFjFOZmSJ4JbhLc7lPR4meq+wD398ZhmEYhmEYhnk6KQoYJRtoIWCixIke/mi1p/5iP4KnfPdwX3Zm9Z9VXaGx1kLcbrgojWBh9ko/PqXncSEFyBCScYLpwRiTvREme0NM9ocY740wenyIZGijMIzWTp8Q8ydYweAYihG5F3vUiLH2/Bb6z2xA1aK8pomgWXvbKR5r35RSEqpZQ2uri/6dTUz2hpgOJjatiyvknU/Z6g4Uas0QYIhAqYYZEkbmEOkwwWR3hP3vP0ZjrYXGWhuNXhP1fguqHkHWFEibam9i5miOsGBH9QitjS6Gbx6AUm2FC+kMxjNG0ONMsqPXmb/EySd1frk/bv+zBkrteo/xit+qbM9V95s5osPxO3f8EtfFcTpJS0dtYd4vVVEXFR9PeBhntnxSK3l4Yz5JOwVOxWf+WIscddyrf128Tp5Cs2Kz2dQt/2F2mf54mRuDsVorc2qwzYviOi/sZWqZ/aHKvp5tqjeGYRiGYRiGYZirQ3UExhlzrIfORU6+pegMAYIQgKpFqHWaiJs1ly/8yKaYErNGxfAXFyUjJcgYJKMpDu/tYve7D3FwdwejxwNQauzYSwGhJISUEFT2Ya4+InlasVzA8B+iZoy157bQu70JFStXNyPr0pkfYxEpKKXQ2uygf2cTO6nGeH8EqZQ1cge9PrKtcqopDSQHE0x2Rzi8uwMIoLnZQefGGtaf38Z6HEO57WsiLkh/yhAIhgiyptDstxDVIitgRBKQCsJ4kcouW/bKrzJKFWovzBO0Lr0xa/UrrCgZubPArNNK1VRs/pJxZOhLYDy97HPnSXES4Y7HnGEYhmEYhmEYhmHOgyciYJyEgpHvKAddYaMDIC+l9eri4cUEaSMM9FQjGU4xenSI4YMDDB8dYPT4EJODMZLhBEJQPsPKuWnC6IOF23RpdwiAIQil0NnsoPvMhotEUPDewqds8lwIGQNDQGOjjc133MTkYIzhg8PMcXmpfhRyhYRu6PZVqLy4SzKa4uCNHUyHY+z94BFam110rvXR2u6h3m1AxCpLa5Qb0zncaCXKDs5CAEqCXKqSUMxbynRJxffLpB4qixii4t2FoSQcHz+1z1ntGxuYmUWcUzQHwzDMU0Kapnj55Zfxj//4j/j3f/93fOc738Hrr7+Ow8NDjMdjNBoNtFot3LhxA88++yx++Id/GO9+97vxMz/zM7hx48aT7j7DMAzDMAxzgbl0AkZG6Gk/14bg80Yv4cnKLInLO02EdDTFeGeAve8+xO6rDzB6fIjpYGJT7SjYf361qmMkyCX59svM5KLJxQ73sxIS7a0uejfXUOs2IGMFSk226kyQyFlBgDEG9V4T9fUWdr/zAHG9Bp2mMMYgy2e2crs+qsWmLCGX3z4dJ0iGEwwf7wNKoHNtDePdIbbg0lm1a5CRyobs9PL0P4WEGml46RCilEaG8oXLWpFLV0fZ0VhdvLg8uEI05ZRBK8z/szlVL+t4MmfPac04nmMMwzD/8R//gT/8wz/EX/zFX+Dhw4dzlxsOhxgOh3j48CG+/vWv40tf+hIAG5n6Ez/xE/i1X/s1/MZv/Ab6/f6x+kFE+MAHPoCvfvWrx1pfCIHPfOYz+Omf/umV1vuDP/gDfPaznz3WNgHgAx/4AD7ykY9knz/zmc/gM5/5zMxyr7766spta63xIz/yIwuX+eQnP4lf+IVfWLlthmEYhmGY8+TyChgL8CYFkxqk4wR6miKqOU995lgI6YzpBji8t4fBvV37+vAA6eEUyWACow1EpDLNiEDzRzwz+AbRAuXcT5kx2L2RgGwqdG+vo//slk3to5+gqV649E8QaG310H9hGwd3H2O8MwTU0anSROWHOZnUhXTFoyWEEEgOJth77RFGjwdorLXQvbWG9nYfnRt9RI04T2vE9rXVcSm9SBukgylMYgBprx8EylKgZREZR47xeSQ1YxiGYRiGOT++9rWv4WMf+xi+/OUvn6gdIsLLL7+Ml19+GR//+MfxoQ99CB/96Eextra2UjtCCHziE5/Aj//4j2Nvb+9YfXn/+9+Pr371q9je3l5q+Zdffhkf/vCHkSTJsbb3Yz/2Y/jQhz5U+O7+/fv4z//8z2O1V8VRbe3u7p7athiGYRiGYc4Keew16Xz/Bb7OhS5UIqzns0k1ksEYepzMpjs5aptXnMKuhuNc3nc3ZCY1SIZTjHdHOLi7i8ffuoeHr7yBR//1Bg7v7SIZTmEMQbo6FxACICd6UNlzvaonwfaqAmZIQMYScbuG5lYXra0uVKRs/QdnTRZPIOBGuP+a6210n1lH3KzlhbWPnEtBZ0XpX2knhBCQUkIKBQGJdJRi8PAAj779Ju7/5/fx6L/exN73HmL08ADTwwl0okEm2FRwfH0ETVbU/imY76siIGCmBpP9MdKJdnPajl1huDKNyEZZEFG2TP69++DGnGb+BefdOV6H/PaXWxgn6o/AnFO7dO2hMFqFUBwnM+dfxZgf1dlss4vuOaVtgzB328Vji5l/lX0obyP4r3KeGAIZ4/75PpWXMaV/C8Zt0XiafL/C9quO1ZES8qJBRrhvVf03FedLccyXOMqlf6Xfwv2c14c5/cJxxrZwzP0Q+XMxn/v5XpTGOLiWZNfwI6geNz9XF59fhXMhW6fUJpPjokYLx00Uz5fstywl4RLnEcNcIIbDIT70oQ/h3e9+94nFizKDwQCf/OQn8dJLL+ELX/jCyuu/+OKL+NznPnfs7d+9exe//uu/vtS1bWdnB7/6q796bPGi1+vhz//8z9FsNo+1PsMwDMMwzNPECSMwzsNSHBodTLC9o7ebjKY4eHMH9W4D7c0epFQwOl2wKZ/65Ol+kHR6gI0uUBLJcIrhw33sfe8R9r+3g8nBCMl4Aj1JEcVxYHQvMl+0APIn90WplnIDlyCg3m+jda2HWrMGKQUMiSMMWGePN/zUew30bq5h5zv3QOcxfwQgpcgKqB/e28Vkb4S9Vx+g+8wG+s9to32ti3q/ZSMGjDOPcBDSDOUi3ICdVuP9MXZee4jR7gCQlAsR0CAhipeJheM6a5SsXMeJfcWFz2Eu0fzuC7hT7AznjZhp3++zO78NwTjDcv6TvbrYGkcSQpU6KJYz7BY2h1I/nKE4Nz5bAQNwicGEsFFhStp+FNoTS9xHqjYc7rszbHohwhu4aWbiuPnrjaLFdsXM8vO3mn/Io+eyMRbBtXrp+VA9Bvk5RzaKTufG8LzWjJuYUkJKaaPQZsKejuoIzWyvsn8mEKdsIaEsw+F87T0XdBb1wl9bhNP1Ia3fiHDzp8qvIsucWJ5Dpd2ePXcqOxC8EUG7lO+GNtkxAJG9d4ep8yRcashwAhR6+9SSz0ZypyaVjulR1wFxLpd5hjktvv71r+O9730vvvnNb57pdh4+fIj3ve99+Ju/+Rv80R/9Eer1+tLr/uIv/iJ++7d/G5/+9KePte0vf/nL+NSnPoWPfexjC5f7zd/8Tbz22mvH2gYA/PEf/zFeeumlY6/PMAzDMAzzNHEJBAyg+HTnHpgXWwwgBKDHCYYP9jHa7CIdJ5CRtSD4FDALt+Qd557WZ3MBmFRjujfC6NEhDl5/jN1XH2D3eza3rVA2pZFUas7qcwbOPe3blyUH14kU9V4T7a0eokact/WE8R5acasOCIFapw5Zi44Qbxyr9L9kWPUGJiEkiAjTgwkm+2MMDDA9nEKPNUxiIweiWgSlZMHuyWYnzOoKwkVepDZ11PDRAQ7f2MV0MC7oCiQot49XWoBRNPZmhudwgYqOFL70ZrGzP0pHXEqXEDHKF0tvND5OB5xR21ijatSIETcjyDiCilVwyxEgTTCJRjJKkIymIGMgpchttMtuv7wnwhrUhRBQ9QiqESOq16xQEXRZa4000UhGExvlV9iZVSySfiKKPMpHE2QsoeoxokaMqBZZISEzXOd7RyIXeu34hN76oShWRGTbnhVEQIDRGqQNdGqycTapi0aat2rlruX98f3OhBYJ1LsNRI06ZCQgIhn0ywpXeqqRjhKko6mNKssOwkrW+5n9y04xQ5CRQr1Vh2rEUDUXMejF9XlCVJUeMnfXCUQGxhiY1P2bptBpCq11HhkhnahRUpsr97JGcfTPAAAgAElEQVQsTMztRzgfAhEDVpySUqLWayGq1yCUyK9drksm1UinCdLxFNPRBMKLWUdv+soTXqW9iLHSeBQmSS6kL/X3A8M8Ab7yla/gl3/5l3FwcHBu2/zc5z6Hb33rW/jSl760Um2M3/u938PLL7+Mf/iHfzjWdn/nd34H73nPe/Ce97yn8vff//3fx1/91V8dq20A+PCHP4xf+qVfOvb6DMMwDMMwTxtXsgYGYA27JjGY7IwxejTAaPcQIgLiVgwyApWe+09R5MUij2sIK1BM9kd4+M27OPzBHob3DzAZjK1wAVhDy3n1lAQgBeqtBpr9NmSkLlzqChlJRPUYtVYD9XYdeqJBqTl6xZPi03NJN30lMNodIvn6XaSTFCBC50Yfzc0ORObFzcYRIDTseaORhFIR0uEIB3d3MHhzF9P9EbTWdr57o99pDOE8i6R17T9h4+fJPE94Kr1WIUJbar60AIzRMGON9lYH/Wc30L7RR3u7CxEJJ0ILJKMEo4eH2PnuA+x85z4o1UCkCsdqqeMUOvRnhnMNFSnU11vo3V5H79YWGv0WVOTFKYHx3hCDx4d49F9vYv/7j3IBR1AgfC/oQPiT8dsWIEPQSYp6u4HWjTbWbm+id2MDUTOGjMsF00PmjfWcPoTGU2EHIXTOn44mmI4mGD44xPDhAfZef4h0fwIpIwhIZ+Q/ejf9ZoRbkIhgyEBGAjJW6N3ZwOZbbqDeb6DWrdteuD4kwykG9w+x89372P3uPZhUQ0oVbG/xyegNwQU7sX81Iku7VW83sfnSDfTvbKCx3oSMVC4SLfq7YAkBQwh7TKejMabDCSZ7I0z2Rhg+PsDw8SHS/QFMqgESVsTJVLpQiROlnUBp3i5DIKoaABpQTYm4E2PzrbfQf2Yb9XYdUT0CCdioJ00YPT7A/hs72Hv9IUb/PXBiooQgn4H04tyHGYY5O/7yL/8S733ve4+dLukk/PM//zN+/ud/Hn/7t3+LTqez1DpRFOHzn/883vWud+HevXsrbzNNU7z//e/H1772NWxubhZ++9d//Vd89KMfXblNz0/91E/hU5/61LHXZxiGYRiGeRq5sALGiR+JBWw6iMRg+OAQj779JpLxBro3+ojqMVQtBpGZMYKvbBO4CpSdhgnQ0xTjvSF2X3uIwzf2kAym9jeJUwxLqXCBn9MtIYBau45GvwUZyScvXoSdIyv4qMj2sd5pYkIjpPpsBIxy/JA3d3l7m54kSA7GiO7GkHUJ1YjR2u6ByBSij552KUMIkQk6QghQShjuH+LwzT08+tabOPjBDnSSOkdsqxAtK15k3vCrTFMBzJ4Tq1jiz5lChElI4Jq+MLyjLF1YYZkMod5poHGzif5zm9h4YRutaz00tzq5hzgB6TjBaKuNqBUhakY4fGMPo8eHbuyPGK/QM973k5ClaWqutdHa7KB3ZwP9Z7fQvb6Oeq9pDcwCgBCY7I/Q3BlAKoVau47B/X2M90bIaz6IFQ6bEy+IoGJlU9I9s4b+85tYe2YDne21XMAg2+uCl/aK2sXMOplDPeUCxniKZDzFaGuA0aNDNNaaOLy3h9HuEOlwCkqdi74IGqjYfp7Nyw6yjCSiOEJzq4vuzT76z2xh/bkt1LsN1Dp1H4MDQUAyTNDaOETcilFrxzi8t4fx4yG0NtVOCBUdKN4rnFBDBCEF4kYd7a0O1u5sYv2Fa+jeWkNjrQmpVNGrfs6+LYRELmAQYTqaIBlOMD0YY7I/wnDnEKPdASb7IyTDCdJJimQwwnQ4hk4IJjU26kcUVb7C9X+VKebWNoYQ1WLEtRi92310bvSx/pYb6N7aQL1dh6pH9hgYgtEG490emuttNLpNxPUahjuHGO4cWhFLeFmKOTZlMe6p+wOUuQx85StfOVGth9PgX/7lX/DBD34QX/ziF5de5+bNm/j85z+Pn/u5n7POKCvy+uuv4wMf+AD++q//Orve7e7unmgsrl27hi9+8YuIogv7CM4wDMMwDHMhuZp/PQn3kC8JggTGO4eYfn2MZDCFFBLtrR7qrSZSrUE6RSE/jFjd7nhlcIYoow2mgwnGu0OMdoeYDiezOeZPg/Igz3PiBgAhEDVriDsNCBeBcdEQUiJu1BC3a0hGUwBn/6AXmEodBogIUkhMhmPsvvYA3ZtrEELaBBUkMo9iNjpZ46IQAkJGmAxH2H3tAXa/+xCPv3UPk8MJyKUNKmR1WULAo+DIiCqj1EIhpJSG5OJN9QoC0cKzkvHevRoCpYTO833cfNdzaF/ronOtBxEpiFgWnNGjZozOzS7qa02svfUa3vi3V/H6v3wLlGoIUZ3arnKzIj+2JtWQQmDtzia2fugWurfX0NrsWo98Ke1EcCvXOnWoeoxGv4XNl67jtf/1X5gc3gVpLKUOzmR6ctEAcauOzbfewOZbr2PtLZuotes2/aF00ScijCyghdsSR1m4K/QxP19rzRqieox6u4He7XVsve0m9r7/CHe/+ir2vvcQWqe5ALh4TwPHf0KtEaG51sK1d97G7f/nLZC1CKquIKWCgLQp2uBE61YMdbOP5noTWy9dww/+43u4+2+vwgymIK3n73hh9+bc0RXQ3Gjh2Z94K7Z/6BZkXUHWFGSUz53Z6jjhXuHIa4EgYf+8IIG4Zcez0W/lqbm0jdQb7QwweHyAx995E4+/cw8mmcBMNUTN1jny0RfkavGscu0O956MFUbijRra233c/J8v4NrbbkPWJWQsIZSyqcqIrFgoBVrrHTS6DfSur2P7rbfx2suv4ODNHchYurRqpUIezOoUxGAeS+Zi8e1vfxu/8iu/gul0eqJ2ms0mkiRBmi6oRXgEf/Znf4bPfvaz+K3f+q2l1/nZn/1Z/O7v/i4+/vGPH2ubX/rSl/DpT38aH/nIRwAAH/zgB/Hd7373WG1JKfGnf/qnuHXr1rHWZxiGYRiGeZq5tAJGhfNo0ajtrUMC0FONZJLg8O4OpBAYPTrEeLePWq+BuFu33pbCef+W2nwqCIdNAiCCmaZIJwnMNAVpA6miwrJnYVNdpF9ACJu6ohYXC+ZeIEgAqhEjbtUh5ACGjDVfn3YhlcBburBxBBm0pQAZg2Q8hU51lvaGsfhjoicpkuEU470xBvf2sPvaAwzu72OyP3b5/kPj7Irjl1kNVzxbLpt4URqWmS5XDVtopwuuKSKSiGsS7es9bL503Rp9m/FMyjgBAEpAxjFUvYZGv43hnT0c3t3E4OEhJgfjLFKiGpp9KwSiOEbciNC7vYH1F7ZR77UQNWPAmKxms0cqBaUixK0a6r0m1p7dxPTAzqPJYFKcOwumTib9CCuC1jtNbL5wHevPb6Ox3oJUAsZUR3MdKVAchSh/FNn3QkkoBahaZEWTnoRUCnqaQimJnVcfYDqYzL2+FQ6vE2iMMaj3m9h82w1svHgNnet9GFcbIo+AyVcWSiKOJOJmjMZ6G+u7Q4weD7D3/ccY3N+HkDR3+4U/B/Ke2ELsQqCz3cfGC9fQv7OJ9rVuVouiHPx0ouu33w8BKw4o/wHOWcIKUvVeC82NNuJmDc21NoYPDjG4f4DR3gDJMKw74UOFsNJxz/ZfCqhYondzHdd++A7W37KN1nYXxqTBHKOg3wKyphDVI8SNOv5/9t48WLKrPuz/nHOX3t4+i0bLSIOwNBjbwiBku2yFVMplKmBsbJABg10q2yEQsMsh5g8SKFeBE6gUSRUqUjZlylgux4Bi5ARECMZ2yjFrrJ8RGMImtM2MZn1bv9773nvO74+7dvftft39+r3XM3M+Uz3vve571nvuuX2/a2l1gZ1Lm1TPb9CptkJFvdFfTM+QeRv0DDYTbDgcut0ur3rVq6hWqxOXveGGG3jzm9/My1/+cu666y6KxSIAFy5c4POf/zwf/vCH+eu//uuJ6/23//bf8upXv5pjx46NXeYd73gHX/7yl3nkkUcmbg/gne98J/feey+PPvoo//2///ep6gB4z3vew0//9E9PXd5gMBgMBoPheuaqVWCMJiNpihKFSiStzTqtzRruUpHSkQrHfvBm1u64MUwc6kjwxOTCxmsIAakwUYXW0OH7AhlbkhKHLznIfonEUl5mE4jOEbHYyyrYYZJxGSYZzibe3a+Wk98SSSGgIuGTLZFSpocb7wsgFBSjwG/51M5vc+Vbz7L99DrdRhcdqDC5bZTvRWcl7JO1Mn3/pi55lZB1PxAijMmvNFbZxl0sUD5WYeHYEoEKhgrv0UTJtsM9YuHIMid+6FYufftZWtsthAUyJyFy2i6J9kAg0AjchQKl1TKLN61SOb6cWKwPeJhEYYGEDpWFliNZue0oKM2zjTbtahNh25HXRP7CSfdTklBDlm1RWiyzdusxKsn4exU3ubeofVwwWik0oZy6vFzmlhfdjuPaNC7t0K21M14peYUzvhdaoQNFaW2BG15wkoXjyxmlwZA50jqMUiUEwhIs3LDMjXedJOh67JzfRNo2lhzVARi4cBUIS7By8ijHnnczzlIBP/DDe14Pe5/U7LhiIbSKNGEi9qoQ4BRd3JLLwrFlbvihk+yc22Tz8Ytc+PrTbG7XsFwXYVskOY/Ihu8aH2lJpG2xduo4t/34HSDA8zujxxAploQU2CWblVuPcnPrdi59/RzV2gY4k/XBsAtC9+xN18HdwDDHvPe97+Ub3/jGxOV++Zd/mQ996EMsLS0NfHbjjTfymte8hte85jV84hOf4P7776fZbI5d9/b2Nv/+3/97HnjggbHLCCH4kz/5E+6++26eeuqpscvF+L7Pfffdx8bGxsRlY17+8pdP7QViMBgMBoPBYNizAmO/hf065/fBh7m8cELxQ376/Bd5WCiN3/JobTbZevIKfjdg4cYVykcXsR0bISVaZwRmea4e1yiRfSpChOEuCgtFnLKL3+hEQsRUiSEmzGQ8YD29yzE9B2esWGeTQXmG9JhYZ8aow/AhApEmg50Anfm/Z+kPUbBppVNBUyT4KCyVWLhlheJqGaUVGkVisH296ulEKHDu1tu0t5vsnN2k+swG9UtVvFaovAD65lmk8zbM9DkWRutU6Zf9bJKQZ4mC7lo5R2OMQ0fyWLdcYOnmVUqrFZJdeGQMn3huFc5igYWTa2yf30w/3q3tuA6l0AG4iwUqNy7jVNzQi0kPybUQhXrRkSBZA4XFYngfKTmR4oKhyoueLmiNVqH3ibtYwF0pYhUskNF1HftVHcJ6yK5bjUZIie04uKUibqmI5TRRSoWf9eVqAIgTYGgN0rawSy6FpSJupYDlWKT5QuLje35ke4JW4JRcKseXKCyWQuWsyBgdDNlmB94WoQKjtFqhcmwRu+Ak4+xVOPQ0P1jhMH3Hrgqm1MAiVV6FBwkpsFybytFFUOH8O+UiOxe28BodkFEOlt0Wth78qTVYRYvCcgl3oYhdcAj8AK2C/LJ51WpNcaXMysmjbDx+mSAIkJaINIWj78zDg3FNy+FvkP3TvGeya2jUejIYDoAzZ87wvve9b+Jyr33ta/mzP/uzsYyN7rvvPpaWlnjZy1423Fghh4985CO8+93vZmVlZewyq6ur/Pmf/zk/9VM/RaczWnGbx4ULFyYuE3Pbbbfxp3/6p2MbYP3wD/8wr33tawfe/5u/+RvW19cnalsIwWte85qRx5w6dWqiOg0Gg8FgMBgOg+kVGPstBdWjHw1HRlnJCowzUh9hSYQlUb6mU23TqV6kem6LG+46ie3YWGsLSEeQfZ4/DKHRgdMfRsQSFBdKeCsVCosFujtNvJoXzrMVxYFPLPkPUpmgIzvp+UNo0FqkoW4iq+4wA/TkPRYZCcZIwU/UJkonSVeJShbXQi+j8rFFAuXnxVS57pAytECuVbfYfPISV74Zel5IywrjucfhybKhZESkVBAke0uP3HXIXhg/qMZ5AlKG7GlCDCgv5jHXy25MJHfLbvNR8u7V245SOrJIoFTiBTaqZh39c5ZcnKUCxe+VkbHSeqhUO1OXAO0rlBdQWC6xdMsaTrmQXMeDZXq7FF/zhcUi6tgidtmN8geMvvRFco5BBGC5kuJqieJqCeFkTbDThZZdkwdCZgAa0BKktLBdF7dcxHYduq12+KGVWbs5WgO74FJcKVJaKWEXHKSUicJw6DxllVRaY5cc7JJDcbmE7VjR/hc3mtf1aA/OdkqAtCXltQoLx5fCPBSxN0OmmoFE6ROQt1LTAH8MzI/WvSHKiisVSssLVI4tUzm+yvf/6ut0dppIYaWTFdcxbPLiexDRGlUay7UpHangVFykkCgxflLbON9KaaWC5djYZQc/8HCUk9gVjGVeMLMvVfOxN+rkPj1DjPLCMAe8733vmzjvxZEjR/iDP/iDiTylX/rSl/Iv/+W/5EMf+tDYZer1Oh/72Mf4V//qX03Uv7vvvpsHHnhgohwae8V1Xf78z/+ctbW1scvcd9993HfffQPv33vvvRMrMKSUfPzjH5+ojMFgMBgMBsM8slvshTHQ+/SCnqc3TSSI6P1SvLuAL0eIFcuUBQSeR/XMOpe/dY76xW28Rie0nO8XEsS96e/iNUQsiItDejiLRY790Elu+NFTrN55gsWTq5SOVigsFLBsBwgTfqtAhRbMsRl1LImZ0VzFwmKlQAU6DMExJ8TDDS30NcoLCLp+9F7W3WHUOt+lDZFZe5GUSEcKCyyJXXQprlXCRLvPu5EbX3SKkz91J8d+8GYqR5awC04oCJ6faTs4omkWhMlwO7UW1XPrbHzvPBvfPU9np4VlW2GS+jGet2NheVhp9Boysb15GzLHTyQTFeTuYdcc4dxIV+JUCqEHgo5DN2WOyRNUx/+kjBRRVpR0e6wTSmydrjVYrpMmze73DhhSNj5EWBLpWNgFB7vgRB4ceug9KlXHRp4cQuAUXeyiE+pzYu+LQ7zfDKw8HSbi1hZJ0uu8MuGhmWtFaaRtUVgs41ZKoafjaNVsfn9EGNrNKtjYJQdhyczcjLe3WnaY00FYu3/9GahpzMtwoFz/G7t0VSuF0gqraFM5vsQNP3KSY8+7GbvohB4vQo+xLKJ7hQ7vmUopLMeitFwO69FqLAVpT1e1RgqJYztYdqj07Vf87F6hmPir4LBvhz1r7FBeaV8MhmuJarXKgw8+OHG5N7zhDayurk5c7m1ve9vEZf7iL/5i4jIAb3rTm/jVX/3VqcpOwwc+8AHuueeeA2vPYDAYDAaD4VpljnNg5D0SxtaWwz5PDkt+yZM3CBFbsmqCrs/2mXXa202coou0LSpHF5HSJhBBFH/7+ntAVWjcxSLHf+hW2jc32D6zTmuzRqfWorvTwat5dFsd/E43FfTFAnbosbKMf+u1SB+XUDqidRiLPvBVmK/k0ElG1fOu8nyCThhPPQ55NXy0mfAno1rJWCGLWKQYhUGxSy5uuUBppUxppUz52AKlI4sUV8u45QKO644tqLqmyBmuQNDeblA9u876d8+z9eRlbMdF2nZ4eCwsG/D+EgPzN3BWYyH2sDU+yhI/a+U+cJ5E38/ZnceprkfN2ELc0YdFbccKoEi5YzkWbqWA7drptTFme1JIRKS8kLYVWtbHISn6rUGTkEHZ9wS261CoFJG2lSZ0HtF+KkAlDP9jWzgFB9u18dveWCJ6jQgdTWQYIskpuWihUVr1dvAQdVjJdi40GhV6YrgW0haj5deJh0qoOCguV3ArRWSsXJpwTEJKpC2xXRu75ILyUJ6Xf7Du+ZGMQ7oSu2Aj7SifUiYMVb/VcLIT5Og3B+5wfXtA7qTk9ClbRkCS7BzALlgs3LDIiRfchlNxaW7X8DodenxD9OAqE7E7BBlBf6CwbIvy8gJ2wUHpIH99Djsn8fgsiW072LaNtCyEGPN+nL1l7q59CYukg4zK5nuyHdbdbcxhGAxXHQ899BDtdnvici95yUumau/OO+/kjjvu4PHHHx+7zP/5P/+HVqtFqVSauL0PfehDPPbYY3zzm9+cuOwkvP71r5/YS8RgMBgMBoPBkM8cKzD2n1hkIIUk6PhsfO8ifttD3nWS4ko5CUGxmxDrWkZrjV12Wb71KAsnliMPgwDlKdrbDVqbdRrrO7Q2avhtH+0rkDKKttOn1Oiteew+CA0ojd/q0qm1sZwywrXnSyivNVopvKZHt95B+cHua2YgtFAfiS4oDhMFyFDIWz6+SOXGZSrHVygulXFcG8u1kQULq2BjRflcrkvlRRYRCpb9loff7LL5/Utc+uZZurU2luuCkH1C7OiFnuk1L8SgEuRqIhSssr/7oBAIO7Sul86gZf8wsrMqY28MkSNxHtk2iVJQWNZEZVN9uYjC64mM99WY5UVYTrp2mBsCdlVuHgYCgUQifE3Q9EJlbb9uKLekRlgiPbdTrqNY4C4dC6foELSDjFItX5XSIwPXoBWh99qE0ztcuTgcoUVGSTCo3B+nTSkllbUlgps8Vk8eQyBobTVC5ZwQ+coVrdNrVkYqDK1ACmTBDj0nJpS+Z9SsKBTStXDKLtpXocI+8pDddVYmmfdIsZevQJqHL2Xz0AeDYfZ88pOfnKrcRz7yER555JGpyrZarYmO9zyPxx57jJ/8yZ+cuK1yuczDDz/Mi1/8Ymq12sTlx+H5z38+f/iHf7gvdRsMBoPBYDBcj1w1CoxZi3JCgWL6u/ICaue30FpTuWkZ6UgK5WIaZCvyxLiWn1cHYtZGglzLtXFKLkLKyHsl/Ndeb9C8soNVtEFAt9rCa3RQQV9wkMSNZfKzGCpCwr74rS5evY1aKu5hlLMlUXJpUIHCa3bpNkIFRtaqf3J6zGdDoawlsEsOhaUCSyfXWLn9OEs3rVFcrkSRqjSBCkLhXJQ7QKvZCuKvNsLTI/CaHeoXq1TPbrD99Dp2wcFynVAplLXGz3esmbzdHK+KSWJCz4tp74Dfx8TzM+YgROSEEeUokZbcXemT450QKy+EFIhAjD+FApBR++O0nWkTLWL9A3GSeDHGzSIx1M/oeeMQWMnwxMFmGdoVLUKhvK/x2x5BN8qhsNvajpRDVqygmTQvUMYRSaMRlsRyHaTsRtftOMmtdVrVmPOa1TXoHAVJvN5yLwuR9U7Rues1t82MZ1SowBAUFopUji6xfMtR/LZHp9rC9xWjnB96lm9UlxDhORCWDO8NYyD6ftdotNBIN1SWB8oP4ztmQ28mo85oPUV2GsaZ/V4vKT0wKDGPOj6D4aonCAK+8IUvTFX2M5/5zIx7M5qvfvWrUykwIPT6+KM/+qNdE1xPw8LCAg8//DCVSmXmdRsMBoPBYDBcr1w1CgxgZibAA4lyCWNEax3Q3K5x8evP0K11uPFHTmE5Nirw99zm1U0U0iKy+IyFOnbJpnzDEoXlMqu3Hqd+pcrO2Q2qZ9bp1ttIOyOo2k0W2CfQSoRCUXtCgNfo0N5qUDm2GMZQV2purJRVoPA7Ae1ai1a1yZi2qLug0QqUr3EqLsWVEkeeewNHfuAEsmxjlV0s10L5fnx4aGVrSNAACnbOb3Ph/3uSxuUdHNcGKaL103ee5kpaPD8MCjFzLmmRPXKS61KkHluME9t/sNlYKKpjzxnJROcyVkBIKZDSmkzZRFhYCImUFtKxkI5EdMatIx2xkAKknGj8B0UkVicIAryuR7fVxe/60J9rJHVJiZQzAq0VUlo4RRfLdSaf3/6+RIouZKQYH1mdBpHmIpF2mIR8nBwYYXGR0WSkq00KgW2FybAHz5eOvlcohIqShCdhkIb5iuSjAAIfu+Rw9AdvQmnF9tkN/Fh5NLQindx/tQ6V2UIK3IKLtGQSpmpSkpFIgbQlSqZeqrldGctDx2AwzAvf//732dnZOexujMXTTz+9p/K/9Eu/xG//9m/zwAMPzKhHIR/+8Id53vOeN9M6DQaDwWAwGK53ri4FxgzoEZz0hVPWgNfqUju3hVsq4v1AF5nNtzAj6+yrhqwsMomznYofpGPhujb2so1aUzgLBYQkyosBfpILIhQADVhxx83kKC+Sn0IkArJuvUN7q4nygskteGdO2r4Q4Hd9OvUW3UYHr+VhO6kl9TSKjFjwHoaVsSiuVlg6ucrq7cdZu+MEgQoIVBCel0DlCoUOfYoOk0hIGPgKv92lcanK9jPrKC+I8hsQeamINASO7qtgVPUznNzpcsPMru3d2w3FrakRfCiwzNVNit4yw8jdC/bg4Sb6/0jqGrPCaKuJvTcm037E6srwn5QyDEW1SxXJ0GPPgViLMmfXbRyOS2jwOx6dWpv6epVuq4NSCillKijvu6cmUu1owUjbThM/T0LGDF9HfRJCDlj893Y8rxqNkBLLnlBJpQUio8QQQqK6ikZtB9VVfWsmVaJpS4d5QmyJ4zqZ3Co6Oe3D+pqtTuswZNPiiRWaG3WccgGv2U1zvOQhYnVgOnaEwLKisU/xfSY2IQnnUUT5LzIeFnF/k3nIKH+mtT8ZWmYSNZDBYBiXSfJQHDZnz57dcx3vf//7+fu//3u+/OUvz6BH8Ju/+Zu87nWvm0ldBoPBYDAYDIaU606BAWloBi1UlKdBhJa3IgyFhK/w6i0a65touYy9UERmPA+uV3LFBRqUCkBCcbXMmnOC8uoSG9+/xPmvP0nQCbDtaJkJBXrQ6jUObZHzQY+HRbvaRNgCr3PT3Aj5hJRIKelsV6md38JvdJFCkpiAT9nP2POisFigdKTM0Ttu5Pjzb8EuOfheFxVJBSNVz6yGc+0gQFgW3k6brSevUL+4E4U2MyKv4QxInzPvjSksnEpI2avFEEJMrvTTILSOcmBEuoBEcD5OBanbhhZhiJzx5aP5B427zrSIdSA6ejEXl3SoJBBh6EAEWmmam9uc+4cnWH/8Ap1mC8ZKVZJ6P5Cd171ciDrz2nWuek9kUjSR4e+iaBO675QIXKvI9pUNHv/br7N9bh3HDXMOhR+HJ9SpuBQWCyzcuMLCDSus3XwMt+DiB93QU64/eciIXmjC7yiWZVMoFykdqeC1O3Sr7bTNvN5nL9/4sJDnRnAAACAASURBVBnoyMLlKpK8F2mszZw+ZDtgMBjmngsXLhx2F8amWq3uuQ7Hcfhv/+2/8cIXvpD19fU91fXjP/7j/Of//J/33CeDwWAwGAwGwyBXoQJjb2GkEuWFjjM5xMIJGVlRAkrj1TtUz64jHIulhVL45K/Vntq+Fhg0ataJjsEqWBSdCm65hNKadq1B48oOne0maJ0K6SKrXJ2VbURvC9L3RfYzCGN/77RobdYprVVwymEojB4L8n6L7n0itmyNm2hu1tl+Zh2v2cnEwN+lHzlysyTMiZRYJZvK8SVWbz/CynOOUjm+hAr80PPCkEuPuExruvU222fWaW7USELQ9Zq+9xY8BKveoR4QsYn2PnYnPzHxsAZjT5GePwfrzJrLDz9saAuJIffYOSiybU9LjgfBvqN7NjzRvx5139wN0fFmP9Zao32FUhodaPaidddBWE/Q8Qk6Pl6rS/XsBhuPX6JxcSfxhNillt7fRzhM7BuxJ0gmhNPYi7Kvv5owdJQUkqDls31mgyvfexanFN+LiLx5BE7ZxV0o0Nxu0Fiv4+10WLxhhfLRClbJibwah09E3lQJBE7BoXx0gfZ2g852mPRWDCTDSH0leuegxz1pMkYU0aS39txySR6syZsd2uachI+cKX2XS/watU7GYZYeg4Zrn3q9fthdGJtJE38P48SJE5w+fXrPCoyf+ImfwHXdmfTJYDAYDAaDwdDL1aXAyAi/Z0GYLiB8mNeCJASIRtFtdNl+epPC0hKrJ0PPjAA1L8axc0XycB1Fs5CuZPHGZSz7uax/7zzn/+FJVKCxpEUoxIo9MUSmjoy4JZZz9RiEa1Aa1Q5oXKjilAqs3HYEq2KjVbBPArHdpW1a69AyeaMexiVvdZG2HH+N9FetQSuNVbQoLpdYPrnGsdM3U1gpE/h+RpBhVuEotAKlFN1Gh9rFbTq1FsisF1XWJDn+u9/bYE4EZEJnBJD7ie77Gf8+oRfGQB3jF9FEgskp8h2FZyz1stFjbNaDI9KJxf0kQ0hF0Vk7/UnIU6ZNhgB0oAnaPn4nIOgGSV6R4S2K5JbaP+Zuq02n2aF5ZYfmRp3mRo32Vgu/3kUomSpHdh2oTiz/Z3pd7TU0UVoR4/QpvTo0mgCExpI2tu1i2VFi7MQgQqC9gG61TbfZofrsJlvPXGLl5FFuv/eHWFooEWi/VwO1S+taK5QKEI6kvLpArbyNiowqRN8Q0qkW4XrOTrvYu6IuTk+uRWwGolJvjNjDQ4s+wfvs7l3XpPIiJutdZDAcAtPmxzkMPM+bST1ve9vb+OIXv7jneh544AF+7Md+jNe//vUz6JXBYDAYDAaDIcvcKjD648L3hLoedAMYi+xDr9Y6FDJkEoDGFrBCCJQX0Nps0K21UX7QE+f7umPSYUtwKi6L7irdZofa+W3aO026jU4cXaOXIee0Ry4VO8f4itqFbYQlKa2UsQp2z+czI9akDKk4Dq3SqbZoV1s0rtToVFtJHP2kS6OEEDmWlbEA110osHL7UZZvO0JxrYx0LHSgonr7+iRmP/yrhn7r+2h9BX6AV+/Q2qzTqTbx217oYSVimaHOnKeBzSX8a1/W1LSF9iylnbC9/vdEzjGZdT6k5LTNTzRa0fcz77P+t+P7C7H4etbSwkiMPeY9Ixb4TnuOBSCkwO941C9XaVyp09wKPd+QuygwyBMta7y2h9fp0tpq0t1p0dlpEnT80Nq/LxZR3inIa1VM4ZUzovNDWhlWpL/V3QOV9d9/UjQCjZSE+U6sMO8JWqRRsrRG+QHKU+hml26zg/I1KzdfAgGllTLSkSPW3uCVpVFIW1JYKuGU3PA85Moa47Ork41soLYpT0KvriSUtA9WlbY5oGyY5lLrv+WNlbvnaue6vasbDplKpTJ1WcsaK67gzFhYWNhzHX/0R3/Ef/kv/2UGvQn5jd/4De68805e/OIXz6xOg8FgMBgMBsMcKzAOilwBkxBopfC7Pn6nS+B1wXajY6/1h+YZoDVIgVWyKR9d4MhzT7B9Zp3OzqXQCl5mpF75Uq7cN4UUKK1pXKmhgeXbjuIulZCuRFhydPHpB5P7rhQSadm0NptsfP8SzfU62ldhP/KieUzQnkbjLBVZ/YHjLN68Ahajk7VexwwK/6J14gc0Nmq0NusEbS9U/vQk/Q1Lx2UG3zMMYsyC9wORiPX3VAlCCPx2l+r5DTa+f5mtZzZBqXS/HUHvlRD+FXqXhR5hOgChwwTYOqOZ3U0FkKcavGbEsoIk1VHoeZFzX4uTjQPa07S3W1x5/DwaxYkfvpVCoYhWE7n6IC2JU3KxXDv5TpI4aXHw8zuyvWEGJ+MOeUTl17wSw3zdNBwSR44cmarcqVOneOqpp2bcm/3li1/8Im95y1tmWme73eYXf/EXefTRRzlx4sRM6zYYDAaDwWC4npl7BUa/J8Ze64oTRluWRRiDmjRZaUa4qRFIJEHLo7VRp7i2gLNQyFjmG0aiQ4lKYbHE6u3H8Tpdts9dibwOwpAXPeF8slOaCWmiVf8hmsALaG83Wf/ueVQQsHLqGO6CneY1ibxrZivRiaQJIsxPEXQCWtstds5tsvXEFbrVdk4c8nFqjUNxAEpjFxyKK2WWT65RXq1gF51kLgcGdM1IAqcnFhrqeMVEAkPtKdqbddrbTQh0dKTIFuz95RDnclTTc7vTiHzvoVx3jP2a27mdnPHoXbuxz8a409XnmSgkWoQK3sAL8FsefqOD3kWBkZzDJD+Bjre5TEcjj4vDv1TmE5FOWGLjkAjtdbrHA0HXo3puA7vocPQHbsRdLI5Rf6pW0oSeNnbBxXJCBYYWg9dheo6iG23m/O0H8brNt0cQ8bJKBjFOV8bxS5mH3A7XtBLFcF1yxx13TFXu8uXLM+7J/nL27Fle/epX0+12Z173uXPneNWrXsXf/u3fHnpODLNHGQwGg8FguFaYXOJ6SCQKhswDa1bxMO5LSollWcnLzvwuZWqhLYTEshyCVkDzcg2/0UHOwcPyVYMGHYShkJZvP0rlhiXsot3jKZFXpufPTCzoOOxPLKj2Gh0uf+MMG985T9D2EZYVnuN9GYxIJDRCSizbxm97VJ/dZPuZDapPb9Ctd7AsiRQSoUVP7lyx2yvud6BxiwXWnnMDa6eO4VaK4ZrToYV2b0ij8UWd1zLptR0KWeP5VL6ivdmku9NOhH7haRTIWJkRHz/itdd+JYxYD1cD8dabjktHgtEoDn70expSJv5n2I2ZzVG4+BFSIK34Pmdj2fkvaVtY8cuSWLbM/B3dEy0LaQmEReRtIBBaIKOX2E0YfT3LTUR63SAE0haowKd2fovas5v47S5M4n0BEHnB2K4T5t2Q0fUo6XlpkY1IFn2nQez7njNYd7rTCRVuEZLBPXDcl8x8BZy2jlm+DIZrkdOnT+M4zsTlms0m3/nOd/ahR7On1WrxC7/wC1y6dGnf2vjyl7/Mm9/85n2rf1yUUrTb7cPuhsFgMBgMBsOemV8FRiyx7X/t8dExVmI4joPjujiui+u6OI6DbdmhQkOGig3HthGBxq93CDp+VH4mo7tm0VonnhBE3hBEOR2Wbj6CWymg/CAKm9Ev/QqP1SqtQ5PWFZPqsQSN9TrnH3uGje9cwKt3khAbjBE2ZYJRASClheoE1C5W2XzyCuvfPE9rvY60Q4+SOPfwuMZOibg3KqSURjqS8tEFSmsLSFv2zGXak2zp6xhN7/zoMN670ALta7q1Dl6zC+hejx9Bevwe2s59XeOk16NhpsSOEHusprd8r1JpksUa3m4z5ZONLVJa7VpDf78iIXakDN5Duo85ZPRAkp07ujeqQIVeMm2PwAuGTmT/HCe6iJzmRP+pHqN7875l9Yf7E5m1K3rWsTrE12xnsOcUzvPJMVzTlEolfuzHfmyqsn/xF38xkz5Uq9WZ1DOMX//1X+erX/3qvrYB8Md//Mc88MAD+97Obpw5c+awu2AwGAwGg8GwZ+ZXgQFENnc9L8FguKdhJML06AUk4aNs28ZxHNxIgeG6Lo5tJ0oM27JwHBsRKLxaG9XxDlZ5kUjDGfrqEZjPoSA16YbSFBZKrNx6FLdSQAcKpTNCMJEp0P/KCNB0EkoJEAKNoLnR4MJXn+byN5+lvdGIFE19JprjdDRXWpQZh9agoFvvsn1mk43vXeLK/ztPa72GZcvQwj+pR5Akc931XKRhrzQaq2BTPrZA+UgFYUuUji3dSbwGEl3efpJdXKNkoPOw5lK5KiIKOaZ9RbceKTAyVrvxfEN8iven41mX/WvBaje7hybrb8h539Uif5/XTK+CZbeGDnPxZtuOlfSZdTJSZz98VcXrWovhl2oiVB/WnVQtSLIF6UxL/f0ZYz/IOANcvRdCPyPGMbD3R7J2FQQEno/f9lBdn0kUqb3nLrN2Mgqi7L9x+jkp+3W19HvLhu+RiX4VKi5i5UWsh+69Vx3sS8w8NEv2DGe+G+zhBb37t8EwDi972cumKvef/tN/Yn19fU9tv+1tb+PkyZO8853vZHNzc0915fG+972Pj3/84zOvdxi/8zu/w1//9V/PpK5icYywgzl84QtfmEn7BoPBYDAYDIfJnCsw9o/wIZkwvHcchiF+aI6EAZownrjX6BJ4qjd+wWEz98+ioSU8URJYu+RQuWEJp1II5zs5ioGx9Ivlwkd6nfyRiGq0AKVRfkD9whbnHn2Cy984R+NCFa/eCZOnShnlpphEhBwJUWRYHq3p7LRY/84FLn39DFe++Sz1C9vRof3xOiYUU8fCdwSFSgF3sYi0ratY4HA410eaQFejlCIIfHQQoOMkKjnX7b7Mbl6l87JnzJTd1vloxYDo+W323lLTcuBKJj3Y46lHkBTMjGIPikcBgyGi8srtVne/JoX0HjsJwwXz6b3hwBWqu7mSJPOXWfFRLi6/6xH4wdCu5t1Rdp21YQfMfD4O49407B7b//5Bvvqme0Z7fRKWb/6/6BmuUX7lV35lqhwzW1tbvPrVr6Zer09c1vd9fuM3foMPfOAD1Go13vve93Lq1KmZKjI+/elP8653vWsmdY1LEAS89rWv5YknnthzXUtLS1OV++AHP4jv+z3vaa357ne/u+c+GQwGg8FgMBwUc67AGJSI9FuW5ZYa8XmcxFtEscKFFMgkvwbRA2hk7yeihKgND+WrJI7+QZI7Cg2peW7IQLcO6dk3mdvwr9BxQWnsokvl+BLuQiigF0IkzhWJOCBrgZkVwMVSvkjQnwjEiAwhA0XjcpWL/3iWy988x/ZTGzQu1ejW2gQtDx2odM4QZKNwpx49UftIBKAVKD/Aa3bpbDWpPbvNlW89y+VvPsvm9y7SuFwDQajgQEblZFI+28ehcxX3SYEQksJiieJSKQwdlfESyArtplCRzIgxWj3o9RbrLOJZEqGiSwUBgR+ggjhUmeg5fmxiq/O8F8PEWfTMQyIAmFMdxqQCCo0A0StEzp2DMRZqb4aMyRdPT4i57HYRtz9G2cFOHcKJyrjQ6XH37aHKguhn3mLcra7d6u95jWOhHnck3AuT+3LmGhqbUetJDB52cOQ0Lvo/71M/RAnTfd8nCILwk1msu2Hy/SFLYC9zdfBfLcLeah2+cuf1EJUYMx9qdJHM5N9VaQhhOGxuu+02fvZnf3aqsn/3d3/Hvffey2OPPTZ2mSeeeIKXvOQlfOQjH+l5f5aKjG9/+9u84Q1vQCk1VfmFhYWp9+rNzU1+/ud/nlqtNlX5mJWVlanKfe1rX+Pnfu7n+Ku/+iu+9KUv8cADD/CCF7yA+++/f0/9MRgMBoPBYDhI7MPuwO4MSGRmQpLUmzhxd6jY0GhUHNtYgQ40yleRIPQwiNNWk/yfPODmHj1HstJM0oE4OaxbKmCXHFQ3QPu7zOkIaaSGXgtFKZFAY72B/9VncBcLFFdLLN64yuLNKxRWyjhlN0zUjsgVXobKB4tup0Wn0aK1Xqd+oUr9fJXWlQZeu4vf8SFWeu1Z/yfigSCEoLBYprBcQUoZKV3mSfDQ25dYdyaSWEL5xx0EmnCpaREJSQOF8gJUoEJN1Ejm5mq57sheg7NbNXusSR9Oho9oG5iaXnVSRokwZqn+eYtvMXuZi3CXvcavryms5OM8IPud5D6ue57uIjMhcbi55kY2iIAx4lCOJlEkGgyT8bu/+7t8+tOfnqrs17/+de655x7uu+8+3vjGN3LvvfdSKBR6jmm1Wvzf//t/+djHPsaDDz5It9sdWl+syPjgBz/Ib/3Wb/E7v/M7rK2tjd2fra0tXvnKV7KzszPVeIQQfOxjH+MrX/kK/+E//Iep6vjWt77Fr/zKr/A//sf/mFoRcscdd0xVDuCzn/0sn/3sZ3veE0Jw8eJFTpw4MXW9BoPBYDAYDAfF/Cow+oX0OQ9h01iWZT04hE4F2VqHoWe01iitos+BwA/d+YNgoP39pVf8EComdGIhHH/51YT9nCtZN/R1XyOkhbQs7IKDU3DwfI2v/USYM6qi/I/TAYvYIlMKvEaXznYLu2jjXHbwWz4qUBQbHdyFArZlhSGlZDp/SXeVBCXotJq0a3Ual2rUzm1TO7tFp9pGFmyEJVNvHfa4ImKppQYhJE6liFspICwxB1aT6cjylGIiuw61QKPSYlmZ3j5eMvEUCd0nDE8svfXI62J/xYeGXPo9JWJmvdzHrC/P8WAemKofkbBzsOwwZXfWgjxVNYTeEXtXP8x0LnWqqJwvJlVekCgvDBPQpyNPVvm1Po2zGN/cXTOGq4V77rmH17/+9Xz0ox+dqnwQBDz00EM89NBDFAoFbr31VlZWVuh2u6yvr3PhwoWJvSGyiozf/u3f5j3vec+uyoAgCHjd617H448/PtU4AN75znfyile8gpe//OU8+uijfO5zn5uqnk996lO8613vmloJctddd01Vbhhaax555BHe+MY3zrReg8FgMBgMhv1gfhUY+0AsFFZK4ft+T8girTS+7xEEAYGKPC60hgAIwAs8IJXPHhQD3gJCIKXEEhINKFSoeAkUgjCY0Vw+rwoNEqRrY5eK+O0ArT2EmNZnRKQhmBIptUZIsFwLgKAdUD2zReNyPdRvWALLsZG2heWGSgwdS8aUIOh4BF0PpcI5Vb4m6PgopbCKVibMGIliZjYSBgFSYpccnIoLjkySJB+2fCZ3LQmSvDEiCoeiIpGcEIQeS4eggEkUGlIirTj/SeYcG65x9n6Se0JjiXlQJI6HFr33pvF6nd2/JOPHrxoPMa/3osPkkDb0NOfIYd9R9kDipjQHN0aD4TrhAx/4AJ/73Of2nJi70+nsSYHQT61Wo1qtjuXJ8Pa3v31qhQPAS1/6Ut797ncDIKXkox/9KC960Ys4c+bMVPW9973v5a677uK1r33txGXvuecepJRTh8HK45Of/KRRYBgMBoPBYLgquIoUGNObdPcLoYbFnQ78UFitYi8NFSbuFlJE4T30gVmX9xLF91cav9lFx+GXhAYbLNfBLjo9JQ68i8MQmZ+CUHng2GE+kT3I/wWgI+F5tpnYswINKlAEtTadagvlh/kQpCOjPsjQ04HISl8Jgk4Xv+uF60JKpGVFQnCNsGVGaTGuBW0cziV/TnpCxohQ8SJdK5ybA1eV9ZPXdigl1Urjd7oEbQ+v2Q1DNhH23ynaOMUCtmujDrD/yWwJkFZ87qLrxkhRD4fsvA+9XKY/OXmCC4EApVGBSl7DO6CT7VwHGpROk77POyO2H600WkUehUqP3qvi/VOIZDtO9MK7N3XAXL0XcuoxuT+zOWpmkjQ8Y+RlmkuS7wmzcHs0GAyTcOzYMT760Y/yz//5P5+p0HyvnD59mv/4H//jrsc9+OCDfOADH5i6ndtuu42PfvSjSJmGjD1y5Aif+MQn+Cf/5J/Q6XSmqvfXf/3XufPOO3nhC184Ubnjx49z77338nd/93dTtZvH3/zN39BoNKhUKjOr02AwGAwGg2E/mPMk3v3oqWJOJ6UjxUQQBHieR7fb7Xn5vh96X+gw+aESGulauJUSlns4up5EiCQstK9pXKxy5dvPcvYr3+Ps3z/OxW+coX5pOxTYijA5dThDs7Wm3TtRyCEpEHYUA3wvXRTpSEcNVcjI88K1sAo20pYgokTPXvhSfkCgfLDALtpYBRvLsZJ8GT1D6NErDJOkZKxdRwqNdBTyJcy5Iqywr0JLhJqfcCnx9AoBwpIEXkD94jYXv36GJz77j3zvkcf4/qe/xjN/+20ufO1pmpt1pONECc4PgEi4FSZil1i2HSrJLAuknK/LwDCE2XhPCAQ60ARdH9X1CUa+AlTXR3VDzyuv043ypkRr/irTfIVh8KIcML6P8nyCrjd0/OHYuyjPQ/k+OlBhkmQId6R4C5uyP9dFjoKrhPg+1h+Z86oh+SKke26vBoNh//mZn/kZ3v/+9x92NxKWl5d56KGHKJVKI4/7yle+wpvf/Oap2ykUCnziE5/gyJEjA5/dc889PPDAA1PX3Ww2eeUrX8nly5cnLvu6171u6nbzaLfb/OVf/uVM6zQYDAaDwWDYD0KpfN8DbRK2SPRZvOmcg/uYtcwnMbLt6cNkT65pPO/Y4r43Xn7KoLW85TgUFkrYrh0de7BP/3EYk26tTWurQfWZDeoXt+lUmyDBLrsQgGU7FJZKFBZLveOYl4f8OKdIbviHYdqH4R9lD+nXMSSis9hDQ4TzGH+W/Miee6HTcGKxa0R2vfesv2y/x/SUyHqb9NWR6EW0jpLF963Jng4cDmH/AF/RrbXZObvJ9lNXqD27RdANQEC30abbaFNaXqS0vIB0Qy+XHoZ4o0zVp2w0EZG+JyLvGcsNw4UpfzAhelJmt1M38fTH66uv8p7TqnuqHov4BPSEL9vbbpTu82MK64UgL8fCQMmeSya+lobVP3gSxpZN9lUZK6jdxSJLN68SeEGUuyjTj4EqQg8sHWh0oCkul5C2ZIyoFCM7JXr/HH6o6Pt7ZG/zj42P1zr0cCsslakcXybwFCi9a4gNrTTa1wRegN/1UZFSV1hE53yXcQwhLJZz3cHeE5ZH+3eqqB9Voab3ApzAEGIcLyKRqTdzYM/Y9zLg/EaJVXZJO7nDyfNSmt3dRPS7EcZEU5xsg3m3tP5O7db/eNPW8Xe5YZUdFrG73xC3vyHX+tA6Rp5Xg+Fg+Tf/5t+wvb3N7/3e7x1qP4rFIp/61Kd4wQteMPK4Z599lle96lVTe0gAfPCDH+TFL37x0M/f9KY38ZWvfIUHH3xwqvrPnj3Lq1/9av73//7fOI6ze4GI+++/n3e/+91cunRpqnbz+OQnP8mrXvWqmdVnMBgMBoPBsB/kuhWEwl6RPnBJQsGV0qkQP/ehqk9gFz+EHboQPR7PsH5D0k9BZH4alnIKDuWVCnbRjQQmB4gQSEuivID6xS22n15n8/GLtLebqWZnvYlX82itNzn2/Jso/GCpT6hwuMQipljolITmgkg4tktfc4X+OcfkCcZ75FrZcy/o/y0NrzGGFLHHjDVvTYnx13x8nWiRCA51LKieE8lFvBdopQk6Ac0rdTYfv0T17Aao9PruVtt0t1tUjizjLJRYOL5Icbl0AGEHUqFxnIBYWhKn5GAXbIK2F8r+BalZeaQ4GjXFyXYx1eUUS+9yu5rT2BhV9glKs+LSYTLBUcRJmnVu4uf8Eru/M6wHQy7kaIJFtDeEcuXpRKxh6D+fyo1L3HT3c8IQSruEhEoueSUQCpZuXcNy7b17D40rI9fh3iP0lMssqSfcV52Sy+LNazjlAsu3HAVGb7FaQ9Dx8Godmpv1UDm+0ybwAsBC2H1K4sz1k6zHUddQdEBPQnARr70p97dss1mloMirMdPP+PjZSe/7rskBTXpPfxMl6x460LukBOGXsz4lUf9p0X1lptfO9XUmvG+JxFUno7KMlZg6luXHgv3sSJKjh6+heFp7NjmR1j2WlcP+30dTvXL0W78Ce9iUD1XazHShzqgew/XOe97zHsrlMv/u3/27Q/FQXFpa4qGHHuIlL3nJyOPa7Ta/+Iu/yIULF6Zu69d+7dfGygvx+7//+3zta1/ja1/72lTtfOELX+Ctb30rf/iHfzh2mXK5zLve9S5+67d+a6o28/if//N/EgQBlmXNrE6DwWAwGAyGWWOnXgnxc21k2SdIkhaLKK+AEgFK68TyUetIKCH6HkJ19uFriDXaruRbfopphFtx97RIu9ovrM77M9A4pQKV48u4lUKa2HsfnweTh4LIvDzo+qHF+7ktqmc36NQ7qECHoZgIhcqd7RZ+22PhppUkD+u8PLL2yo40ylcEHT+MOZ91r8kRMmR1D0OFLll5QXJw9HYidIve65GhDhef9bTU36/+buTIGcSwY/vb0WHfYqWO1+zSbXQorgajCx4EIhUShb+ACgLa202a6zW69Q6qGyCllbm+FNrX1M5vIyyJ7Z6kuBy5+O/Xs24kExM6FeprHeYsKS6XaW816VY7aBX0Kcz6BG4j2HPXc7w/eoWQ7LqvxMnIY4+JfpnsnruYt66z7Wd/6dlDh3cgXTrZC69vzgXRNTBYfuIk2lqjFRSWSqzcdjSdr5FlorYiYay74CIdK7nnTUP2DjW6FhF3u1fZIXp+jEV8DUjHorRSxi25BMeiXE+7SE+VH+C3Pbr1Nu3qURoXazSv1Ghu1mjvtECSeHGkCon4r2g95rSRe+qG6LHGJ7PosvLh/gnP1j+4/CYiX0SeOctC99yeRH6BFA1xuLPp6VcO7qIIiD+a0RcDESkdUwX88AZF9g2dORm7roVMZ0cpOEYpMUSmzX36UtTzXSMeY9aLYtTl17fxhiUir6ls3/fUv3n5Nmi4FnjHO97B7bffzr/4F/+CWq12YO3ecccdfOpTn+J5z3verse+8Y1v5NFHH526rRe96EX8/u///ljHlkolHn74Ye6++262t7enau/DH/4wL3jBC3jrW986dpk3velN/PEf/zFf/epXp2qztoiMswAAIABJREFUn42NDb7whS/wT//pP51JfQaDwWAwGAz7QY8HRqzEiJUXYRJckSgyQsWFSi0fdSwT2O1Bc5oHKDFE4LDXh7H0qXmkDEUDCpySS+X4Ek6lGCkw9tj8uETd9Doe7WqT2oUt6herCCGQthUdIpAC/K5Pp9bEa3QipdLBW0aNIhGvCFBeEFrEK927tnarYNSQxjDCzC+0G5PP40SrM6Ns0krhN7t4jTApdrr45+BcRidQ+QHt7QatrQbKC8KcK4lwSCOERNjQvLSD12izfOsa+ra1A+liVvarUUhbUlgu4y4UEUhQKkzwnpygMF9MkkdlCLM+AyLqbLoL5SvvespE18h++X8d/grLCl+nUFBnURq3XMAtF8auSiSC1oySaC/E1/VY1fRkJ9jT0DVhAvvCQgEWihnjguES1Hh960SRpNl5epPqMxtc+sYZ2lsNQI7IljWOiib7R2a0U05z7DHUo+/pH+YoZeB0zebUIgb/3M0ZgPiMz6IXYw5433aOpHp6r+H0/eTc6GHfSuL73Ij+76IM2rVvwy7EIUrbHpXLpN4b/cqLkYsif8xpWLC+vcFgmBNe85rXcPfdd/Orv/qrfPnLX97XtizL4q1vfSu/93u/x9LS0q7Hv//97+e//tf/OnV7q6urPPzwwxSLxbHL3H777fzpn/4pP//zPz+1Z8q//tf/muc///n8s3/2z8Y63nEcPvGJT/CSl7yEc+fOTdVmP5/61KeMAsNgMBgMBsNcI+NH26xIIHKySJ674tj8Kgoh1f+KSX8VM3ildWZfKZPXmQjMMzWkDQl6Ey6Hv9gFl9JyBafgROOFHunJtOi+V9+oZBTGq35hm83HL9OpttGKgYfs1KBREHR8uvUw/MdYioEDIZxXFSh8z8NvhUJ65eeEdhm6FNJz1/9KikaKt2xFSRXJsb1rS/T96+1EXO/odvP6mygBRwxJ9JfV0K116O600YFGJiFsZiXsmpJYripE6O1Ta9GpNsMY+fF4Ic2tKgTKV3iNLtWn11n/zgW69U6YC2MfhjFwCUXeLNKWlI8uUlpdiPJw6N5+6kgZqfXw89M7BWORV35fmGDrEyNeIwe928TsenD/x3kdmOEcZN7PKt93eyUXqxw2vtl1cZwpH2sBDasomWdShZ0c9hJoGRoqSCGQUmJZFpVjSxy98yZWbz/O0q1rOJVCcu/pUfDE98sx7oUHsYMNntvBtT5dP8YrFU99+PtePSzGJ20r7+JnX9b0AFlvjL5FKTI/R/R0yLtDXgPrbsQF0ZPTKlMiCn/V+5KZ74ICofo+z/kXt7frNpk7l3nfQfL2hj38y/veYjDskec+97l88Ytf5MEHH+SWW26Zef1SSl7xilfw2GOP8cADD4ylvPhf/+t/8Y53vGNPbf7Zn/0Zp06dmrjsK17xCt75zndO3bbv+/zSL/0STz311NhlnvOc5/D5z3+eu+++e+p2szz22GMzqcdgMBgMBoNhv5CQKi9CeV6+kiKMJx4pLGLviyGKjGsGIbBdm0KlhO2On2Btj41GD50SNDQu7LD15GW8ehcZxvLoOTqWGUgZ5sroNjqhcmCuHlYFOgjDYXmtLl7Tm8M+HgKxjEkK0Bqv3sGrdcLEu3LwXB8GsZxICAkKuo0OnXobHShim/WQVASjtSbwFNUzm2x85zxevY205IEJUHSgELakdGSB4moZGSkwskqMdHSGa5HkvqTGePXd464V0jlQI18q6H0Vl8usPec4K885liowhjofHv4eZThkxvY2moxJlH75r8w/nXk/R5ifHi8R9CoxzG3CYMhHCMH999/Pk08+yYMPPshP/MRP7LnO2267jbe//e08/vjjPPLII/zIj/zIWOW++93v8su//Mt7yrn2u7/7u7zsZS+buvy73/1uXvrSl05dfmNjg1e+8pXU6/Wxy5w6dYovfelLvP/97+fIkSNTtfujP/qj/Mmf/Amf+9znpipvMBgMBoPBcFDYWQVEYv0ZoTVh2CR6lRWxJWZ6XPr7fgspdRJHeUZPlX3VhJ4eYTLU4soChbUSgQiQWjLDVge7kAkrgyC0dN9p09yoh94XgRoI45H0RUqEA91ml8bFKpZtUVgqogPR77ZyYGTDewgB3VaH1naDTr1N4EeJ4iwj/IoVUBpNt9mmU28SeMFcyUxi61jlazrbTdqbDQh0aOGd9dSKFrGQgNa0NhsIIWifblO+IcjkIBH7I/FKu4K0JG6lSHGlQvn4IgjwWl50nYm+ow2G65DYm6/PoSJQAQRhLpGlm45Qf3YHFajQKzDODaIzFRiue+JgSfOLTL4L9QTUSm7AI+4J0WdxmDWYC9sCwzXMxz/+cT7+8Y8fdjfGxnEc7r//fu6//36efvppPvOZz/D5z3+exx57jKeffppOp5Nbbm1tjdOnT3P69GnuvvtufuZnfobTp09P1YfTp09PnYNiVkgp+cu//MsDb9d1Xd7+9rfzm7/5mzz88MN85jOf4Utf+hJnzpwZUOhIKbn99tu56667+Mmf/El+4Rd+gec+97kH3meDwWAwGAyGaejNgcFQM8sBC9X4p4jd+Wf8QDfcEjZUI8w8PEPm+VVrjV1yWb51jeJaBSXCL4CDltt76EMsB+oPayAAKelUW9TOb9PabOA1OwOJZbMP4UIKhGPhNbo0LuxQWq0k0SMOTsTU31IcS0MghcRvdqld3KZbD/N0aPTwsOrXCzpdR1or/I5Ht9Gm2+oQdP00xE92uWWneR+EKD3rKnOVKU/hNT061TZevY3lOqGXSBxSJqOAE9GW4NXaNANNc71O5cQybsUN87doUiXGHveOYUWFFFiuTXGlxOKNK/gdn27bAxUrWXtD5vVUNIPEqfnM7mrs3x9HKY6H7VbGEcUgBgS3YZ6rQIG7UGDh2BKW64QKdDsbYyvWfsyRJHfILehAlne/IUQ21NZ1TKjnOux56LPK6Uf3HZNTNtlvo3tHaPATf8c57PEZDPPDqVOneMtb3sJb3vIWAJRSrK+vU6/X6XQ6lEolFhYWWFxcpFAoHHJvry2KxSJveMMbeMMb3gBAu93mypUrNBoNHMdhaWmJlZUVHOegIgoYDAaDwWAwzBY7781BC7VegdnhhtmYsbYkpyqBwK0UWb31GOW1xfBBVaUCnlnS37yQEmnZtNYbbD9xJQy/Y8fhdwaFTWGh8P1uo039cpXFW1dQgZoDwQFIEY7Ha3rUzm3it7pYtkTIMFHlQcUJn1cEJF4BWlooT9G4XKWwVKC4uoDt2IcqCBNSoAJNe6tO/dI2QddHWlaiBMjmyolKEK9T6dggoHrmCnbJZu0HbqC4UkGzv2sztrNVKsAuuyyfOka71qb+7HbYrD246magkhy7bwNt7LKlpcKz/emT4fpF97hfZLWQpLkyhAadBpoU+6bgu8o5/Nvt4TJJ8odDIza4mfJk6Zx4WfPvfmIwHBpSSo4fP87x48cPuyvXHcVikZMnTx52NwwGg8FgMBhmxoACIxYq9ysphuW6iIWrV4MgerDvkJi1a41WICyBXS5QPrLAwo0rOAsFlA4Y4ZwyZWfy31a+Rnk+zSt1ds5uooIgiuHfLx1Q2SBeIAReq4sKFK3NBp2dFk7RRToW/QHMswaHezpr/aGDMh8lqhalUYGiU21Te3Ybv+2HVviGhFCJIZDSwu/47JzbwC7ZOOUCtmPvjydNjgxm4M0oppz2feoXt6k+cwW/2Y08L+gJH9W7kKKkoXa4NnbObSIdi4UblikulWc9klw0gFLYRYfFW1epX6qyXS4QtD20Uj1J7nV/uczOMFsONqTbOEqZa1nmOtbYsrqhYRfa/N/aRg92SP+zzhe5Cr3MLSe1pBe98toey/QDJKfP8fvAwIepEnhyQ4SxFMgZPRBaZ8Ll9R94VUj5pyPHxmK+RirS/6fZh+OFn+fpoyNlYHxbuZY3VoPBYDAYDAaDwWA4BCTkJzWE4UqLa4Y46Hf0U3kB0rFZPnWUhVtWELbYX+v3nASRnUaL6vkNWtU6XsdDBVlxUa/EuD9JJYFGdQNaV2rsnNkgaHtYUqbKpX23nB1UtPjtLo31Kq3NGt1aB+UF+9yHq4/wrIooT0OX7Wc2qJ7ZxGt2D+ba61sX4RlMJZehUmWTre9foltvhyHLREbom+tSEMXdDTSdnRatzTqdagu/7YXeTPss8BTRpSJtiVspsHDDEqu3H6O4UkYFGpXEQ4+Fs+FA9mO6RdSf6USXGi32Fo7mGt29Z0d8D4jP0FXqYSC0CL0jsq9R5HhPxcpUNGhFIujtXbuHK4SPQ9vF19Tg58MK6r1ejPnoPgOOnroFPZtlck6uzjV27SAzr3HPRZ/GPnt9DVtT17C+ymAwGAwGg8FgMBgOEluI1NtimBIj7/dhaK33PZE3pJ4ie65Hg9ChQNUpuZSPLLD6nKMs3LgMdpibAOgVSMyUyHwvUiJ1d9psPXmJ1lYjTJxqydTifaBcPIjMT1/TvFLHLm5QXCzjlAoIGYUoAvYtgXJSZTyecD11am22n7xC/WKVoBOE4bB6EsFy/T7gx8kiMjIt5Sm6zQ71CzvsnN1CWhallTLCkhkh9j74ZET9iG1UQw8FQbvapHZ+m/r5LZobdSzXCROwR4VEosXIqY9wPwg6Pt2dNo1LVYorZcpHF7EK9myVM33yw8QCWAqkJSkfX+LI6RNopenstFPlbLasTn/MdoZjwfCgGmLsdmZwjfS3c7UrNnLzBw0cNE5F5Lgg7NM+OStyuzZpLJusMjx6R4tIQWnhtzza202UF4R7thAzuedORZ/cP6tgzTtst3rGlitPItvO3PsgVCj1di/+4Hq94c0L4fzrgXd2K9G75+j+G0f24DneOgwGg8FgMBgMBoPhaiQ3B8b8M6mgJofIElMrhfIVtuuweHKV5VNHWTl5DHepGBrZjVDu7JmMgExKiW25eNUOm9+9SLfeRUq5a3LerN5AyNACt73dQilF+egidsmhsFLGcm0IElv/2Q6DfkFN+J4UkvZmg4tfO0On1opyJ2Q6b0gR4XmRUuI6Ll61y6V/OIvqBJx44a04ZWv/vDFEKshM3pICYUm2nrzCpa+fpbnRwHIdZK4ybQRSYLs2ygvYfmYdu+RSWC5hlxxi3eD+EHlYaMCH0moFd7GI1/Jordfp1tt4TQ8he4+/1mRP5nIb404hoWeGrkL5so48dXol/eOpG3p0kBqksLAth9Zmg42nr9BtdFKl5fW8kAyGrOorx4bEYDAYDAaDwWAwGAz7w0gFhu4Vke9iOJjx1BiwO5xCIrSrJHFyJUaPMiCywBZaYxUdSqsVlm89wtLNq7gLBSzHIgiCyDliP5QXvX+qQOF5Xbr1Dp2tVtg3KXKHOGDHnciswklTvsKrd6id30I6khVLUlguR8LaGYpo+6tKnuslXrNDa6NB9dxm6E3iB4PjmYEe6qpGDwpNBYCUqK5Pc73GzrktykcWqJxYorBaSrRWfcEsJkfk/RldqZHnTLfepnZuk8bFbQIvyOS+yITLGLaUkuTAIKREeYrmep3GlR28Zgen7E7T6/HJmMxqQDoWVtFh8aYVvOedoPrMBrUL2+hAoZXuyYkhkv/nVywV70lZ56vkswPvzXwy0TxMFItoXkkt/NOwkGJovP/sfU3E3mAavEaHdqtB7fwWtWe38FpdsPLvRQbD9UXWhSe8YOb3LmEwGAwGg8FgMBgM1w6JAiMbSiolE8t+GAMhZKYOkjKk8v2TmmgvDN1UOlph5dQR1n7gBsrHFhEiVCjEwtx9R4gwzE6jQ6feJOj6CEuC3L3tnhmKk0jqsP/bT63jNboUKiXsgoO94ITDCWab2yOJniEEMor53dpocPYrT9C4VEUFauBUGlnYCIRGK03g+9TPb3FRKY7+8M3ccOxWCEKvof2SmogoZFnjYpX171xg55kNvHqnN+8FRHLRcc+iIOgG+O0mrY067WoTd7GIXXA5KCWB1goCzeLJVSrHl0B+j1a1jt/ww7wsVnSp97gszLcSw7B/XG2eK7GRgBACpAzDBib3ruHXaTLOJOwR7JzbYvvJK2w+fpHa2a3wIIvMZMwmfKPBcHWR1XAPJJAxGAwGg8FgMBgMBsM+YutEQaEz8jrd+96upHkvBo099XRKgF2fDUfVmadEIelcHLqmsFSiuFJi8ZZVlm5dw10qIm0ZCtx3bWNGCIGUkla9zdZTl2ms15J2x5u2+KSF7gw6U9Dv+LQ3G2w9cRm/7VG5eZnCYqjMEGTye+yp/6n1rvIUnXqH1maD7afXaV7aoVvvRGMRPeLg6935YgCd86cQeK0u9Ys72GUXq2BRXClTXKkgbYllSZTa+zkUUqAVaF/R3GzQqTbZevIyO2c36dTbxGcrq7wYN9Fxz5aiFF6jQ+PyDu5CkcUTBRAzWof95PRPa43lWFiuzcqpYyhfUb9QpXG5RtDyEkUG4uoWz84ytcw8z8Ow/VErPXa4tf4zneR0meeBZ0jOtQjHEngBfqdN0PUJukH42S73Sq00KlD4HR+/1WXn3Ca181u0NupoX4dmDpIwQXgmf8DVfZUYrldy94Zdvmzp7KY68OXFKDIMBoPBYDAYDAaDYb+xez0sskoMmN8Hs+kEJ1oDShMEoVJl+YYljpw+ztIta5SPLaK1zigvDgZBmPegvd3g8jfP0qm2Q++LsZUXfWcpm3xWQ7fe4fI/nqV+scox71ZWbpM4x1yEhCCYzRikZUGg8ToetfNVLv3jGWrnNgnaQZhsPQldle2tCLs6my5cI4isLgohQ8+FoFVno9ulfnGLYz90C8d/5CSFSgHbtfHx96bEEAJpWSilCDo+1afXufLtczQu1WhvNcESCEdmPBIyRfX4OZLj8fhNj50zWxQWyyzduAYS9IzW4ThoNGjN2h0nWLppjUv/7yx861nq56r4TQ8hIuv1jPJ2XndBQz5aK3Sg+tZs/k6jI2+CSLWNsARSDub0mXc0oQw26Ho0ruzQ3GjQ2W7lXrcJIvSwDLwAv+nR2mrSuLxDt9HCa3eRwsKybZQMCA0dJNkvCCZsmeG6wyx6g8FgMBgMBoPBYDgUbJ31sug33R0hxRmWFyKNzZ7JiTGONWwcqlvHAf51ZEUt0tgusbV/YgKX32Edm3zr0Bo36hiWa2EXbIpHFikfW2bhhkUWTiziLhbDNvYrSfIItNL4HQ+v3qW92SLo+FGuAT04vOTP3jd05qDB06JRgaZdbbLxnQt0thpUji9SOlKhsFLCKjhYjp2ZwngOB+cizRMgEAiU7xN0fVq1Oq2tBvVLNWoXtmlc3iHoBJAkDE+FiFdbaJYDJw1jn14TgN/2UVtNNr53kW6tTfnIIpWjYV4Mu+IgbSvNMRIvgjxhS6zbUgrlB/htn26tTXuzQePCDvWL2zQu1fDb3dDqGhAMSdwtGH7N6J4fkaZO4Hc8Gpd2qBxfwu90ke7INDz7hwSr7LB86ihWwaZ54w71izs0r+zgNbsoz+/ZO8JwbuNLr7QOFSU62s+SqCPZ80P8ZlRfrmXwkPrHGOJe2G/Z3MD+Mk2D/VVIgZCS7aeusPH9iyhfw5jKsdBrQ3HkzhMs33oEq2Aj7QkT1h8m0RoNvIDWRp3qMxvUz1fD/C5y2DhCbakKwrxJfsvDa3QI/CDUbQgNQkf68Hjvjq+J/R/SfqL3EkQx+Q4SvcRwh7T+XSP8ajOL8ENRHTq7b4TnS2c8ZYjPY2+p/WHCitN7g+5/Z4I6RpQZWKp7X7TZvHDZens8DTN7m04UiDo9fpxu6Jx7uMFgMBgMBoPBYDBcx9g6evIK5WpxGKjhT009iosR0uhJDNXiZzwVCf1Q0YtIeCpkJNDtF/wNqzlVXiQeFULgOC7ucpG1O45z9Advxi7YWK6F/v/ZO/OoSar67n/vrarenv2ZfYYZBgUEEZVFo74sHgGFGOEETMIQMYKeo0c9iQqCGtwOejAqkcQEQQUBGUU5QY24MAF1CCRnRAVHguMwwCzM8uxPP73Wdu/7x629q7fq7meZ536G4unuqrvUrapbVb+VMZFXYJ4hIOCMwTItmBUDZtEA5xxqVhGOMY12MVKP8wH+B1dq6oSTKhuYefYIymN5ZFf1Y+T41RhRVyOjqKDpgMcH98OvBM8DQogQkFMCkemCgJkWrIqJwpFZzL4wgdl9U6hMlUCpAkJEDg8S7RpErpUlLv/qCnWTw5OI7opScJvBLNuYfX4c+b0TGFg/iuFjV2PFy1ajLz0IqIoQVAZj3we8OULXKhfXGrcYjEIF+X1TmNs3hZk947CqlthOgRDgcqGsijtgjeYJ34mL+6chIULhNVlAdboIyzChKgRU6YGguMkJxjkDKDCwYRh9awZRnSpidu8kxp6yYFsWbIODuXMQBSgnvpCS11ZfY+jOnNA8zFdgEAAIeFe5AmFCiBBvceH5EVL6oFYImnCXFy+t7mCzaiiFoqjI75/B3u1/gq3bIs9RHJFzl9k2GGMglKBv9aA49xezAoOEvxCIpN22aaM6XUbh4Cxmnh8HtzloHQVG6B7tJv125nmqKt5WTu1L+AQLn2LcNQ5IaLDg31Z5+McaQwOfrsmhPfl5nLLTV6jUaFRi+tdtePiRo3WiY9lyGbR3DGsm6SRwuLnGQnUFldTRCZwHDG6aNE8idUokEolEIpFIJBKJRKC6igJfdBb0YmgCr/ulsUzKe0dzrJQZB7M41JQGNaNByWhQUooQ8Nsctm4LLwXdBLMYCBcCGUJ45EU9YB2nUKhZDVpOQ2akD9mRfqSHc0gPZZEezoJqRFgrMtZyvPSuQwGzaKB4OI/KVBFEJY5RpUh63VGM8aC0FQAIQFUFzGLQZ6qY3jWGwoszIqfCkDMug1koaZEjgKoimTMhBIxxMMuGbTLYlg0jX4ExV0FlRuRLMIo69LkqrIoFqlDn3T76sr6EpV8LQCAKmO/1wgEnvhGq+TJmnh9DZUrkk9D6UlD70kgPZqH1paBkFFBVAVUUcJuBWSJUjK2b0PNVcQyLOsyyDqNQhVHSwWweShzPWUKBVGhHSEjgxhkHtxn0QhWFQ7PIrRpAdkW/o7iMlu2g3RYRCk4OtS+NoU0rkO7LoDpTRnlaeBWVJwswilWYFd1R5jjlQJycABDzCMQMKqyxOdSMBi2bhpJRQVMKuMVEjphCFVbVdOYd4TXgayo4GJivNFpAQgq0pUDgnuIJ4BnAOIv6DnjbuX85AEoBWKSBt8LSgEAocigVikFOuFAm19lWfHBGiPjnIVCrwF7SOA8knu18jUHEPMBFEK7umkq0qNrkpLVN5w3qaDxcZVKyY0Hc59aW9q39NmqfDX2lhKdqDhoM1D52xVQaWOq1W/M8vcSvP4lEIpFIJBKJRCLpENUP9cJjI8+0TSBcgRflILQ+8pEARBFWn6lcGulcFqnhDNRcCoRQMMOGWdJRnauAg8M2OQgTIZaIWwunvvUhxPskTSlID2SQGc1h8JhRDB4zisxIP7SBDJhlgdnC0nzBlBeOAMWqmigemoU+W/YUGMJSr7MX1oDoCQATSV5VCmZxGHM6qtMlWJaJ9FAG2dF+9K8ZQt/aIaT6UtD6U1DTIiwRJRTMYjB0E2bFhFmxUDoyi/KRWZQm5lCdLQFEWP9TxREAdt79Dvb3KMK9NoNyRSLi8xtFHUa+isL+aVBKoQ2kkR7OIbdahJXSBjSomRRUTQMzbdi6BatqwijoKI3lUR4vwJirwjZtIURXKIjqeNm0Kg9qgO/NFdwd4d3D3GTeY3NQsxpyqwecPBjzexR5oINqVkOqL42h9StglKoojucxd2hG5P+YILCZDcoAuJ4YjmaJcAIeDLVDxXSUHswiO9KP1GAaak6FbTJYFQtkrAB9tgzbtMBsO2JNzd2D3HQo5uvyWtSKjDivoKACw4aYU+vC4YohhZ8BDYTJW6J4XhRiPmYQOW7a46icTWNMLOYXAtdLoYsKSuLPq2HPveDE6ytJ58ERI4agZB8I9iXsrZBMwdD+WCY5vwP74CXvCj/rej+79wf35hd9rnanefjPn3GhVyUSiUQikUgkEolE4qPWhoTiLSfnbQ73hH01Am0GYe1NKYY2rsDoCeuQSqeRSqUAjQKKY3XGObglPDSYaYuEo6YNZtlgpu28vBJhmE4BqqlQNJF8lKYUkBSBltOg5dIgKsAsC5yxRSCjEiE7rIqJuYOzKE8XPa8L0mVryVAmCioEHYQQqKoGzoBqQQiyy5MFEIWCKI4QjDiCFiZCDjHGRLLnigm7asA2LVBNcbaFk+M16hUj6QQCMaSMRwQgEMoGEfqFgpsMxlwFzLBRHp8DURVQZz244/nAhCeGWdFhG6YIE0WpL3RBrf6xa7jyHyKSvpsVE3MHZpAeygkLccLAImq3eYWL8bG4GJf0SBbDGRW5NYNgVRtc57BsU+R9MS0wk4V6SRXqzDsKFE0F1RTQtCKUQoqQZXHGwSocZqGKylwR+YNTmN03BatkgFIF3El84orypCirMcHxId7f2vtZs9I1Y73kB95Rysh52GPJH9J6BOdrz6NBqC+8uXy+JxMSvo+06I67OAnOIdyfX3jg/96n5lNNZPsGyhf3xr/oPGckEolEIpFIJBKJZGEIKzDc1zLPCDXBy2XAoyNUZ/RVjQBqNoX0UB9GjluDNa88FpqqQqEqbGaDcSZe8AgBocITgIDAtmyYpglmiATSIQWGQqBkVCiaCoVq4ARgMMFtJhLycoDbLWZ17TVchNIxKwbKkwXoxaoQdgo7YC8FBpDwFT8y2J79IBdKJdEWBeccZsWEVTZQYUJJwW0GxsS2xH1pp0QoKKgIe0OJ8HqhqhvTPyo4hHzx7gauEWdIqUhcQ2tQuKHWGKyyOI5CWO4YgDL3GiKO4gqOxwAAClBSG16EhBoPZWVoGxIU9TjXM6jjeXQkj8FjRgHb0dDMsyyLRHaLu2FBFEDrS0EbyKBPGYIKDRQqLFuHZRmwqxaYaXsZiSbUAAAgAElEQVThaAiEUkZJq1A1FaqmgUOEg+JM5FaAo0xSiQa7aqE0Mwc1p8HWLZE0vWI6il4/3I9UYrRO+Npwc+80idOCoOKjRhy5hAjcKWrcHTu4dpd62KggS+2QNsGdT2vygbuSdB6850fXd5uARwkJPw6QoLsBxNzme70uzPlVe17Xnyeiz6zhh5twPbzmQ8zXVsY/dNBq25FIJJJu8OUvfxk//OEP2y73ne98B5s2bepBjyQSiUQikUgao9b+1KnFl6P64O6rV5z/PAFUiv51I1j7ys3IjPSBACIGv6O84E6kaM45COdggXqIAtAUBVVUuAIr7jZNIASG3BRyWVeSu8gEGNzmsAwTZkkHs2whaOZha/hOqFeFmyzYFU0DbtoD4lnkc4WCBgTngDhkDFzEVA9JKYSXjB82oc0OdYFFdmi7R0QR5IkjA3JJ7gk4ggokDqI4G3IKgkDSdMf7xhePBJJs+4am8HOY1B/dtsadQIRbcj4zy4ZRqMAoVGGVDBHqJ5A0eb7FNr5lbUCMzRg4OCwOENiO5xYF1RQQlfpKXu4kPqZivrEsw9EJcUfIyAEnWpRFTHCFIzWYwfBxq6Bm05jcdRgTzxwC000v4XK3zumj9troKUtUYFhzsOXRP5rhTebnIKSXoTKJvxDCQ/eSpXMONstM0nxOaLhFzYNdo3GZf2W+RCJZfuzZswePP/542+XK5XIPeiORSCQSiUTSHDU+UWeSl05HABiwrgvFYQYRFs6cQM1qSI/kMHDMCoy8ZI2I1W3b4Jw5oXJYWJAYffkmIkEpUdy35miPORi3EZDSLiyRcAqEENiWBX22DH2uIrweAD/8fU08gs7eZglidDgcfm4FN+47B7yErm5fA0Uo515SXCE7cY8w8ZQX8r27O9TkpifiRy8ndlDhEHElIMEDF03m7m7j1hO01PUr8MOmN7t2mpymfl+5vwkBuM1g60KJUZkuIjWQgTaQ9iyLQxX2/KSqb0nLbQ4GG4DtJe2matBLInygGJiQhUWPn+PdYRMAVHiK5VYNIj2Qg6VbKI7nUZkqgpu2V3NSonHV69W20NNiM5rlJ+rGaRF7i1gKk1icUtP5PaLOrF8Fr50T2m3fL9xG2WWCuKd7as7alUkgCLuOETH/1/eS46GPzlTUZVzLhbASPdK634FFQ+Dqb3b+Rh9B3WceEjACiN+0DtF7TviYRo0XJJIglUoFdpve5KqqIpPJ9KhHEolkISkUCnj88cfx4osvghCCjRs34qyzzkIul1vorkkkEolE0lViPDB6QEDKwhlDKpfBiuPWYmDNMABfkCJeg5tZwoVZku93BLBNG5XpEqqzZRHSicYIkrtEjTEkqbO+KSLsVL1SCyLDWoqS2Xap4xQlQr3VcXvxfmLhr059oR8DTjRtdksIw5oU9IQzQb2K45lglKooHJnFAB1GaigLuMq8NvvSGcGAdwGFbrQf0ZhTke0bEpJwcTAukk2ruRT6Vg9g5KUrQShQ2D8rcpvQujU1JE7ov1xly63o3xCzTcxhXjoEZcgtjUB799vaxpbyYM0jnIAT4ikbuKdNQHtD6CqXKSIXdm1FUaW0txlqNu2MYNP16g3FQFxMM9I8nf/uTc0t0micJJIGzMzMYP369ahWq22V27BhA/bt2wdFUXrUM4lEMt8cOHAAN9xwA+677z4YhhFal8lkcOWVV+Kzn/0s1q1bt0A9lEgkEomkuwTEZMGXStL+i3UsvsQyKBbUcmkMbhhFbrQfIE6IFdcLwAtFEF2Iv8Cts8UXYRK3kDZNThPQYAxt3UR5qoDqbFlsQ4kYi5hud9qFIO4wx45BdOxjeuKNf8CyNLaf0QMoSQ4J/HWH07uugso/n1Yu3UDkcl9JQgJH1BOwkToLWj9BY65BQgiMko65Q9MwijooVQL5C+aBSH+83CA1GzT+Ka6uuHXepcC5pzGiKkVmOIfhTSuRHekX3lhJtElozWOhUTePTmIn//jN4r8sHdw5gbRx845OGoGFxyzeOsAVwy9JFUa37q0tzbPCrQxePiDXQ0qhIJQ6CuQ2RtHxDOOM+8elXtsgtfvZgwMWqzb1XDy73153CHqn1LvHuftRZ1JvYeca3TIkkna5++6721ZeAMDBgwfx4IMP9qBHEolkIXj44Yfxyle+Evfcc0+N8gIAqtUqvvGNb+BVr3oVHnvssQXooUQikUgk3YfWl7jVmPglQry2+y+BVKXQsilkR/qR6lum7swMsHQL5ZkiKnMlL9Fyz19pj8Y3Zh6zLBei8pWY723JboPrWhjLVoc6RoQGQiiIosAsGygcnIFZMKBQVSSHj/blaIVzMGZDzWgYWDOM7FAWYDY6swoOsxyGsbss1RFz7Prd0D0EYKSxf5BzR3ZyL1H49/wWtHSOwk8o/ZbTpOvij1HjvXfGx1EqEVczRACqqSCa0voty437xziYaYNZrMVwUPOkriQIK8CDSoxFrTZtdsOcxz7zxTg+ksXE17/+9cRlb7/99i72RCKRLBSPPfYY3vrWt2J2drbpthMTE3jLW96C3/3ud/PQM4lEIpFIegslcP/VI/6Frr7MOCzkAPyNCKFQUirUjAYlpYKojeKktCB1XazvwwG8kXX7yjhs04ZVNWGWDVgVE6EQzN4+dWcHvVpIWLYQf1TdM6HZyzuJ/xfxklkih2hR0ta4tSJ/qXtAnCPvHLfw+hZ60eJBrjm3CEAIATNtmHM6LN0C58yZSyIn6TwRPZ8bjlsHJ7g31gDAORSVItWfgZrWRGgtRxAtwsx0PgCdDOP8HYLetFLryYcaD5+lPj/5ImLi6RL8farjShGSfJNQLa53RS21cYKWytg1e8pJWqvIGdWoXseAAwRwr2kCUIVCTatQNCWUq6YhbqhNxmEbNrjF/OMYrcK738ec8L0i5r4SYlHrudq+cTavMTjPN9mmN+en5Ghj+/bt+OMf/5i4/EMPPYS9e/d2sUcSiWS+KRQKePvb3x7rdVGPcrmMSy+9NJH3lkQikUgki4k6HhidW1VyBENOiPqpIrwv1GxKhE9YhqGFGOewDQtW2YBVMsB0Gy0lEpDURY7cEoUA3GQwSyYsw4TNbDDePe+DJQEHqKpAy6VAU0p4hUTSMkJ5ERSDkmbnUL3wUbFubRFzBd+pQJ6qTW9ATv4LACAchFIoqgItk4KaUtvWL3Cbw9aFB4Y/+NJUQCI52unUg4Ixhm984xtd6o1EIlkIvvKVr2BsbKztcvv27cPXvva1HvRIIpFIJJL5gwpjTO4pHDzDTDfOtZNsN7pEY+NzZ/Fi44u3df87E9ZmakaDkha5w6Nxn486i30CL6SH8EAhAOcwSzqMgg5mBhIsB3Z8ucuDknBUnTc9JSB1JEEppLPWvb5bgAAA4/4SN080qYBx4ZFklg1U5ypghp1wv5YwBCCUQEmp0LJpUFVxf5ZIWsbLS+HczN3cC83/MWfxQ1AFKgS3uQhXxAJhqojrK7U0Lcfd/B2d4N1zohUFp1TnGYBzMYaUKhjcMIKhjSug5VLtGXEQgFIKzjiMsvBaq21YIpEcjUxOTuKBBx7ouJ4777wTpml2oUcSiWQh+O53v7sgZSUSiUQiWQz4MZyi1pjNhBJB74qaZIciprYfW5sIYSYlULMpqBlteeZ1pkKZYxarMOYqjgUlgTuYvjBImrVKekwnIZDgC+5IUA+SwHmCMwZm2zBKVVSnS7B1a8kKRZNCCAEooKRUpAeyUFMpMW8S0rEnnGS5wMEId/JeBEM9NVhIeHHDlrmqCc45GGNgNgO3hJLSC3EmNlk6muNQPz3rjM7r9TxW4hp0Q3GJyZFZDFRVMPqS1Rg9YQ20vgxAG4XRjNZIQCkBtxn0QhWWbsKNEiWfFySSo5tvfetb0HW943qOHDmCH/3oR13okUQimW/y+Tx27dqVuPxvf/tbWFbU+EEikUgkkqWDGpI+hJQYHUolYsJTUEKgpjQoKQ3cVWosIwgIuM2hl3TopSrAOajiZAZwHTU8AZQ//t66mvGKxjFHjTCpVlG0FKRNkm4Sf53xwKngKs1ChdytvM1Dv8V6WTjeVsF8GqFKws0HMQtVlMZmoeU0pAYz4Gz5zA3cUV4qKQVafxq8CNiGDfcYLeRIBAPU9LqlhVLbLlje3KieIfA7iRuMRv3kAGyOdC6DFS9Zi9xwP1a+bJ3IF92itYCrlBCyfiLCupkWjKIOo1hFZboEfa4K27DAGQOnjoLN6WdsO80OaKsnGI/8baEdHv2S5OSKucX6XXH+Me7MVyygcPQ9T8WUSJAb6cPAmmGMvmQNBjeMQkkp4G2FzCMgoGAGQ3W2BLOsA4Q2CRUWP3Bdu854nefHo5RlsIuSRQjnvKuhn26//Xa8/e1v71p9EolkfhgfH++oPGMMk5OTWLt2bZd6JJFIJBLJ/KIS7/U3IMT03tK6KNnhAKEUalqDmlIj7RzlECfUBiFgNodRrEIvVsA5B1HiLDB9QbEnH6qRc/CwQCW4sSNUIMQVCzqdiJTzPi5Ld5ijh64oAgOaCj+MXFiJIRKOBgsRcX55AlceLy9zQtSJsmGJPAEBJxxGoYLioVn0rR0CFBFqzZXtLYfTkwOgmorUQNpRXhjO70f7JJl8//zzvjaHU1R31vopNM9qlKRNRXfQuf5SuQyyL80BJ6xx/Cvb04BxIvwnCQCjqsOs6ChNFlAaL2BmzzjmDsxAnynD1M1Q8w3Hl5PAEYE3xDFq09b7GfdDs12tSV7eXov+vgauSuLMiwiZHIj/EwIowgdVSakYXDuMkePWYMXxa9C3egjMtsEZC9VeTwnEwR2HLApbt1CZKsAs62FFcXy30bVzuqYKUn/9MpizJZL54pFHHsGzzz7b1fr27NmD448/vmt1SiSS3tPX19dxHf39/V3oiUQikUgkC4Pqf+yd0MYTVFACmlZBU6oIWXHUC+d8ODiYzWAbJoySDrOkg1l2YL3zf+4IdQFwMCFRsiHCTXGAphQoKQVKWgHVFFCFCkt4xsFMBmbasHRLhOIhFIQSEA2eZa1E4hFSVJDQCs44uC00E0QBtL4MUv0ZpPsySOXSoGkFRKWAQsFMG3bVhFk2YJSqMPM6zJIB27TAOANVFBBKa6SdHByg4py3DBtGwQQzmXP+dyLeXHoQTqBoClL9GZgF3RGSRqSyPQgntbCjzGM+wcnDED0nm9HI3aeFbrheBHEdarn97sVTarn58GULDgabcc/6P0l/gsJ5JaUht2IAqb4sBleOorBpFhO7X0T+4DT0YhXcYg0VjEHROY/I0Ts+7wLDTaK/1/SD+7L8SNlWeuIbCTjfOQNNUeRG+jGwZhiqpoFQAka4uOdSglQ2jVRfBrlVA8iO9CM31I/MUA5aXxrMtsTx4QjnHWkAs20Ylg69XEV1rgKzYsIf4WYhPwPruzaP8OX9ULGc910yr3SavDsK5xxf//rX8cUvfrGr9Uokkt6yZs0aDAwMoFAoJC4vFRgSiUQiWcqozTfpEgQgCoWSUqGklOX18uc5toh44rZuwtatUJgc7sSxJpyAcAJORDgKRVNBUxTchsghktGg5jQoWRVqWoWiKUJ5YTMww4ZdtWGUdFgVA+AU4ByMMhHywxbWnvVCitQkVV8Opu/LFR4R+DreE24aG6opoBkFVAGUNEVqOCeEcMN9yAzmoOQ00JQKoiqwDAtWSYc+V0FltgQ9U4I+U4Gp67BMJ9Yqg1CIcB4SHnKnTca4p6RbXhkw4B0CRVOR6kujqikgPCwcbEXk2O71Smo+tNhQr/Bi+UeD6LVeQUfdb99dI9ByRB7eQkdCButxuo92+xFUDHp9aH9EfAUGAdUUpDQF6QEKbW0a2eF+2LYByzBgVU1YJmuhn/6AiLBztX1uWgPx/3KnXGjPSOBvRKkRNwKu8URTI4oYTxeR9twGTSnoXz0EMA5FVQFXgaEQEIUiM5BFdqgPQ5tWILd6EKlUGoRS2JYBZjMQNwVZk5NdeFISMMuGWa6iki/DKBqwDUsoS5qOoeMR4vzlUU1SR3A/ZGBNi0e5CnpZ3aQkC8XY2FhPclbcdddd+NznPodUKtX1uiUSSW9QFAUXXXQRvv/97ycq/9a3vrXLPZJIJBKJZH6ZFwWGEIoSUEWEkFJSqniZXm45MAgFVSgUVQVVFXArnHCVA0LRYHEQlUBJqRjYMIL+VUPIjPQh1ZcCUYXlO1HgCC8CcaSYWJgllBlGSYc+V8bckRlUporQZyvgjIMsNwWSJB5XEghhUcwYEzlZVAXDx67E0KaVyA7lkO7PABoB0ZxzV1NAFQIoVCSmtzm4ZcO2GJhhgRkMzGSwdBN6sYKZ5ydQODQDfa4MZtmgRBFCL4g8ONwCtKyG7KosaEYBA1tW3lmAuPqVlIL0YBY0pcCyLBCF1gkxd7QQ8f9w7hOtezJENS9J1R5BEbujPmuzGh78xxGKpta4cXd7v8Bimpp9pTaDZRtQ+1WsOWUTwAkqU2URxgj1zlEe+ssdJSW3WcIjFam3RYWPm6ecOcpSzri49ybOs8PBbAupwTTWnrYZdtX0QjlxuKcwAVUpFE2BmkuDUgLGbBAeMCLgjb1Qg0GrVEVBtaBjes8YZvdNwDYt0VCrSku3f5zDNhlYxzmGgtpVkQuJtJGQXCKRtMYdd9wB0zS7Xu/ExAT+4z/+A1u2bOl63RKJpHd89KMfxf333992+GBFUXDttdf2qFcSiUQikcwPvVdgEDgaDBFaQUmpUDR1cUlp5glCAKqKMDGp/gyqZgncsWDljmUnVRSoTjJfrT+NoU0rMLh+BH2rB5EayIAQIoQwnHk2pMT9R9xPFNxmqBYrqEwXQDUCNaWAKgrMsi6ESIH8BsvwUEhcnPOOUAItrfnn3eaVWHHiWuSGB5Dpz8LmFmxui7wUPChAJN5HV/BLqQKAigTAhYr/u0ZhFKrgthOiiovzXVEpMiN96Fs7CDWrddlCeInAubB270+DqBTMZqB0GXmiOFoE7ipiWzj8cbJbEvg/4Jzbkd/8dcEv3BPEJs0pIzwfmJ/UudV6HH2NUGAnb7/r8IByhzteB2kF/WuHUZ4sQsulQJwQhvW9fwKeRJyDWTaYzTq+vDnnMXmhYtr2PhJ/uvK8wdwfWoAElG0cYJxBSasYXD9SNzwjD55P3Llnh7xPWgxdBTG32hUT+f1TKByZBbNsf85tYTDdfefOMeBMHLNG51o9j8xgSDBxvjZtXiKRJIAx1tXk3VFuv/12qcCQSJYYZ555Jv7hH/4Bt9xyS1vlPvGJT+Dkk0/uUa8kEolEIpkfeq/A4L59K6GAoimg2jK01COOlXVaxcAxo7CqIheGVaj41pg2R3ZNHwY3rcDAMUJpke5LQ0unQDVFCF08O1/ADd/g/kKCjQFQMgpyqweQHshg+LhVKM2Ukd87gdnnxmGWDRHNp1kSUMnRCxGCONuwkBnOYmjTSgxuXoHBY0aQHswi1ZcBVShMy/S9IoKRjUKfOTgRIdAYswEIAZuWS2Hly9ehb90QSuNzyO+fwtyBKej5CpjJkBnKILdyACuOX4OVL9sAJaMCnnBxGcFFiD01mwJVFMAGoCx0p+YR7pjIh4T/7YfECoYfa+sUchQnzGawLBtqEgt1zsA4B2O2CNfXhv6CO95ItmXDDuRGWniCInJXgM1BNIrUQAZaLgWzrNcv7dzbCCHgDLANG8xk7sq2vTBd1SZnzBPEe147sadMrbcGYwy2bQvFi7cySagtDs5sT5NW23Sn85gz9hwwDRPluTLyB6dQGp9zlEZoud+eroZxYTThjRsSecIyCG8a5tbl1MPJMlK6SiQ9Ztu2bdi7d2/P6t++fTt27dqFk046qWdtSCSS7vPlL38Zhw4dajmU1FVXXYXPfvazPe6VRCKRSCS9Rw1Z2fXszZMAhIJQ6oSfWS4hjMKW6pwDVKXIjfaBbVoBy7CQGsyCOwIrSin61w1j8NgV6FsziOxonyeQ8y1zI8KGgDU895oUkmWiEKiqBi2bQmogC20oB0WlUBSK0pE5VGZKsE0bjDHhvSFzXiwbhPCNg6oUmeEB9K8bxuiJazF4zAj61gyCOkm3uSuQDYX6CYQt82oTBsVCX+mEiCEEVBPeFWpf2hN4pvpS0OcqsA0bmeEcciv7MbhxFOmhHLhtg3NXwDlvw7EwBA3EIY6FmkuBqnRxWeLPI65gmtmsuaI7birk4rxmzHHjaEM2zbmTS8iyPa+ChscgZE3vWNw7uYZaOX5B+TFxPjDb9nLBJBHwd5eg/T8JqC9Engc1k4KiqTChN/HCcEoyDtswwUwr8bntKqm4JXJJcZvVnSZ4sFDgBzGnMe/0aLVdV93h5uMIKm9D7dV0AHUaihf2B4tRQmHpJmYPTmPyuTGUp4qwqgZAaVv3a2/cGIdlOOPmhv9rsY7gMSNc5OSwdSugTInfB4lEkozbbrut523cfvvt+MpXvtLzdiQSSfdQFAX33XcfXvWqV+Hzn/88yuVy7HYDAwP47Gc/iw9/+MPz3EOJRCKRSHqD44HRw9dNx8CfKBRUVUAVKl6clyOOkEfNpTCwfgTpwRz0uQr0QgVUo9D60kj3Z0QcfI0Ka8lIlOyWj1RAWMMdSY2W1TB87AoMrBnC1K4jmHjmIKr5MqyKBdqmQESyxOEczLSR6k9j5cvWY2jzCgxuHIWaEwkdOWMg3Jf+1hcS8sDnsLxY5APgIDZAFYLMUBZqVkP/+iEw0wa3ASWlQMmIsHLMssIC22V2OlKVQstoIAoVSkW+fDzVvEPNhBcCt5vGB4qBg4GBcQZmszZl/8IFgzO3bAKLfEeBwSzelgdGKDSRJdoHkvoFdA9ffSG++fpzcR/z7hnNPA2cVA2cibw4lml2pJwjIMJTxrASnSfc9fZxvHwWepzrQUCgUAVGpYKxnfsx8ewRGGUTIAnmBUJACHUUGIaYXygBZ+0ryUSQSog8R4YpzgOZ/0Ii6SqHDh3Cgw8+2PN27r77btx0003IZDI9b0sikXQPQgg+8YlP4Oqrr8bWrVvxyCOP4ODBgyCEYOPGjTj//PNxxRVXYNWqVQvdVYlEIpFIuoYa9+rOYz61RTC0AYFn3U9VurwVGIDIBaBQkD4NSkZDaiCDTCUnkntnNSiaAkVV4mOxtxsbJRhSgwghspJLIdWXwdDmFeDgmN03CfvgtAjbYzsmqaTdhuKbrgtxA763tLXwBGi0TXCc2hIaRmN8B6oibZjnLlbixpcDjAtr/9yaHAbXj2Dk+NXoXzuE1GAWhELkqHA2Dtjctthk7bi5dVCNIqWloPWlvOEnBAAlXt6ApT7knUAVBVo25SkwlKhcOKohaoHWNo1UGleo7oHp9IgFy3PYhgU9X0Z6MAOtP91WGB7GRHJi2/S9GNzUBXECcxL9QgFLN2EUKsgMZR0Br5/s2bOXj9llNycDM23YhgnLMBFM0twwsA5xZNKcwyzpMEuG0/cYD5B6Y9GLCyfmXBPCa2G5b+sWmGnDtVJw++orwgOdIhy2aaE6W4JRrALOtd7s0HozkLshE7k4bN2CVTHBLAbiCPS9ofIePUj4WYY7x8ECmMEcZZEThsr1HkMkNcUCTEju+DHdxuxEHjPPj2Pm+QmUxwtgdiNPl9jKvL9EobAtG5WZMqyqBYUocJPOB+GRsjWeJo6S0TYtkYsjRZ1NY/oVPBeSR6yK9/AIPt/E4G1Tv9YWnql4aLO49SFPH6dPPHIOhaqPOjAiZtuI+5A/jwQqaNCnaJ3dJ+p1RJzrZjGqApcm3/zmN2HbvQ8nODMzg/vvvx9XXnllz9uSSCTdZ+3atbjmmmtwzTXXLHRXJBKJRCLpOXVyYHRgHemEo/YEGoQDlIIqQoFBFCIEQ8swPIqL98pLRZ4KJZ3zFD0gcMKf9AbGOcBtZFf3Iz2aBaccldkS7LIJ2zJrJQVBkpqqxlXJAm/qHL6ksV0SxxmPLyR0KwssweomQTNqR2nAbQaaUTG0eQVGXrIKgxtGoPWnwcBDiXG7fYXymLwCnEMozpYxHCJ3CFEo1IwGSil4PQN+HpR+tTZurWzlC3ujW8/P+e+K5yxdCLmzK/pBiAIOBvB682G4r8wWeYQsw4JtOknQWzUMd0LuWRUD1Zky+tcMAZS0kCg62D6Dbdgwq4awTFfcsHzNx5A490ujoMMs6CAMwqo9ci/oXLXcOjVKF05AOAXlCmBxmGUdlm42CbXliFypCB9VmSrCyFcTetgAjHGA2bB0C1bF9dgK9DPYDS5CHQlltCtZFt4bzGCwTGteBIRtQcXzEWGAWTUx9sxBHH5qHyqTBXCDARQx9+jmZwOhBESlsE0blWlxv1eggMFu4xQXZwRjDKZlijwi7tzdUKniK/4SnbedTEFNHyvqKyX9TVrotfvMG7zPNqq3kRIjUCi6nkeVKcH7e5O+xdP685M/9wTNG9zrjzo5sFqrS9IY27bxzW9+c97au+2226QCQyKRSCQSiUSy6FF7IZ/yDNEcqzwCImI2ewKd5ULjfSUBbwd3XFpS7LQgK6iLUz9VKRRNwdAxo2CGjfzeSRRenHHqqNOHTl5OgzKmoOLCW9foRbp+w9HdbensarLRUfkOHpA1qFkN2ZX9GD52JQaPWQElpznC2sgxaDaYXb6Ul9XUAETkro5lNRWKDG+erHETaPQ9eTd4U0lY7e+dHK+wIN4zmYdRrCJ/YAqZkX6MHkfAGkZicu40RAh9jXwJ5cki9JlyQLBdR4IZmW8JpaCcQp+tYO7FGQweMxpuo9G+UALCCSozRRQPz8IqG0LxQUnDQQrVSgk4YzDmKtDzZTAvt0Nt+/M9P7ntUQowzlAtV1AplmFUdDAnf1P9+7p/XtmGjapVRTVfgVEyoGY0EJXGehsSb++Jd21QSqEXqqjmy6jOlmEbljhuCnW2rdODqIee4ylTniigcGgW2dE+xyss5h4U0I905WqLGse7zwBEPCdZhgmzrKM4lsfsvilM7h5DaTOJNMkAACAASURBVKIg8oYk6oUvbqYgsHUL5akSqnNVmFUTDKz+GR7tq6NcKU8WkX9xGkZJB1GVJvmzAsqLQL6utqlxweBOPY1cMJq110ZH6pwD3jGpMXwIlm29ndg0ZxHvi9azloR7WZf6F05ku6hixf3ua2N4Vy4SyU9/+lMcOHBg3tr7n//5Hzz99NN4xSteMW9tSiQSiUQikUgk7dKDwMXCGosE/gEUxEniDZksOpbYkFG9hHHA5hhYP4J1Zx6H/nXDQkARtfDrKa6pYlIvCklDxKUHEAIOx0qaAKmBNPrXDGF44yoMrBkGFAKb2ZAHYYEhAKXUW47ueTJsFizmHQK9UMHM3klUJgtQFArqTkikpojz1bFYVwn0fBnTu8dQmSyI076V1h2raUIpFEVFdaaMuX1TIsyRJ/907mOx7QOUKqBUQWWyiLn9U7BKJkjNPbDJaBACbjPoc2Xoc2Uwi8EPqOY3zN3Qdo7Qu7F1deeEZgSFwOYWysUiSnMFmLrRxINBzO/E8TBiBoNeqKKSr0AvVGEZdt3zXOi4ieP1IZQXiqrCLBqY2z+NylQRtmmBt+qtyH3zf845mMVQOpRHft8UzIoBoiigIjhW98Yz5pytWeWG16QUVKWwqxZKY3N48bcvYPcjf8DUc0dgVQwwzsEVEhIQu6dA8/764mZWtVCZKqGadxVQLHwexV5uzpVGCKiqoDwxh4lnDkIvVEE1VWi2FiHu2EaXpURtCLmoUgE9u/553SV4dgRmWm9OCiySRCRN3t3f3w9FUea1TYlEIpFIJBKJZL6oE0Kqc4KvWZQIi383hNQ8SsgldfCODyVQUgpyKwcweMwoytMlGIWKyFMSawXeTXwLahl7oAfUeFNwKJqKoU0rMfqydVBzmpMs2tugflFJzxHCISGgJs0k8EucaJg2Di5kYUzkViiOzWHs/15EbkU/siv7HUEeD7r2OcJmAmbZsMomSmNzmNk7gUq+LISqxM+BEG85HVlHhGDbLOuY2z+F9GAWfSsHkRrIwEtIH7mxUUJgVU0YRR35A9OY3T8Fo2qAKDGN1g5CqAccBGAM1dkKxnYewGjFwMD6YdCUyFXgxaQJ1UvCO9MN/AGB69vAOYdZNlA6ksfU7iOY3jMGu2yCNozRRUI5hYhCQKGiMlPCoaf2YXXVwOqXrxfjqNDIQfF3koKK3CizFczun8T4rkMoTxehaC3k0wpqJFyFFBU5Gwrjeag54Y2WGepznk8UEB4UHHdhcINddDwtxMUu8oJYVQuVmSLKU0UUx/IoHJlFYSwP2I4Ki7qKCx5ShvF6J3cNwo2EO2EaORhmD0xi7//8CStPWovhY1d6Z6BrTxDcbzcMmlkxYIwXMLN3EtPPj0MvVEAoB3HDcy0Ay+apIW5HCY9dFVWYRo9n4i5w7s25kt6xf/9+/PznP09U9qqrrsILL7yQKPn3vffeiy9+8YvI5XKJ2pZIJBKJRCKRSHpNTxQYNYk3nQSSVFVAFCpfgOaRmtAcxBNVgEPI+RRNQd/qQZhF3YlBXwYlpMeHKVi5KwBp/yW79uVdEsIVXjgDo6RUDB27EitOWgcAYI4FNfEuWam6WDi4EFQSON5Qy2D8g0J8wsFsBtgMxcOzOPL7/Vh18gZkhvoczwM3RkkgNAwHrLKJ6mwZcwdnMbt3ApxxKCnNU9q13BEC2JYFm1mYfWESVFVBX65AzWq+5Xvg1kYgUvlUZ8sojs9hdt8UZvdNg6gQivpg83Ws8MUmgZWcQJ+t4MhT+8AZR2Ywi9RgxhNguwWDoksS8C7oCu45yDm4M962xVCZKmHm+Ukc/u0+FA/PQlEpKA0oiZxcGDwgWefOwBHCQBQCRVFRmSnj8JP7QBWCoWNGnTxQqnMIvMbBnEmJMAJjrori+BymnxvDxK5D4KYNRW2iJXL1747ixz2GlIr7YnEsD8ZsDB4zir6VA9D6M1BSqhhPACL/ShfGM6TjccaUA8yyYRRFSKyp58YwtecI5g5OozgxBzWlQlE1MSbUK+rpY4J+J6TRvZP7d1b/HOaYPTCB4ngeak5F/7ohR6lDvUTMvsLMvScQVGfKyB+YwvTzE5h+fhxqWoWiJbP27jbtHKaACqj7HekldW/N0RXR/ereforTIxxAKpblcO/qEd/4xjcS58G7+uqrEysw8vk87rvvPlx99dWJ2pZIJBKJRCKRSHpNDxQYvuWiF32Z88A3yWLAsyRmADiQHsygb+0QCodnQVUaMuztzatowMI1ImRPVI0kHgIvrraSTSE9nIWWTUFRFHDbrolIIVk4PIcn4rhgLDcZkKO4ARQYJR35/ZOwdROlsTwyI1mkhrKOZwoBOIFtWDAKZVSmSqhMllE4MiMUB9R1X2nt5OYBy3ahYAcqMyVMP3sERqGC3JohpIdz0HKao5gQYY2Mkg69UEZ5vIDyZBGliTkQ6ngttHVdCQExIRycAozZ0AtVTO05AqNcQW7lIDLDOShpFUQjoakzVEXoQ1RB3A5CeW3pJizdRHVOhHzS8xVUJkswS1VQlbYU754QVwkS6DdjYBbHzHMTeLa6E9nRHLIr+qFlU0JZJAqCA7B1C9XpEiqTBVQmiyiOzwEMCZR7QS8B8ZcqCqyyhSNP7UdpbA7ZlQPIjuSQGsiCUv/8qTdHkpoP9ZvlHLBtC7ZhwSwaMMsG9GIVZlmHWbWg58uozpVhmbZQorhhmQiE5sHpBCdB4XFrY8Aj2xFCwCwOi1k48qSz7ysGxDme1RzFkHDF4AzQCxVUpksoTxdRnCiiPJGHmlZBFVeBtLATVWe3sMV9A3SVgmEvnpqt5rFHkl5iWRbuuOOORGVPO+00vPrVr8Ypp5yC1atXY3x8vO06brvtNqnAkEgkEolEIpEsWhIrMKLqCFcAxP0YBPCE05FkivJ1a/HAmTg2qf4M+tYAmeEc1GzKiS8e8aQJmB97h7QrlnaLW4iwZHEPDRNSNK0/jczKfmi5FBRCEJf1IhR+orcaLEnN+BLPwloI8nuuRVwccPjJjKkIkWRWDBiFCipTRczuncTAhmH0rR0EFJHzAozALOsoTeRRHi+gMlkGCKCkFRH4qEag37B5AI4ollKAcOj5CvR8BYXDs8iM9qF/3QjSwxlQjYCAgnCK8nQBhfEZVCZKMPIVKJoKqtDWD1XIQ8Cxc6cA4xxGSYfxwgRm9k2gf+0QBteNQutLQckoosPM9UaBIyDv7hxKAOjlKvRyBcUjeVSmSrArFsCExx5RSKjNRvcBEoi7RSCE4mAccwemMPvcOPrWDGJg3TDSIzmkBzPB4EUwijrmDk6hMlGAka8Kb05NdY6xe+K0sDNO23DCWrnH2qqamNx1GDMvTKBv7SD61wwht3LA8SzgoXtdbbUtPNC4t0zOYRoGzLKOyrRIQl6ZKcLSLTAOUNdDlYowf6JMoI1G8wCv83twAALeGwABtzhsbmHimUOYeOYQBo5dgf51w0gPZpDKaqJxxmEzjtLEHPL7p0QC9aIOVVWgair8XAhxz4O1n/zutjqpucqaesrIwG+tXHQ8+oVHf/Srcz1VXcVRPQeXmHY7uRKj1xHn3P+tjuIiboTr/tSt+0ij+YaQrs9Hy4X//M//xOHDhxOVffe73w0A0DQNV155JW6++ea263jiiSfw5JNP4rTTTkvUh14zMzODH/7wh9i2bRt27tyJF198EeVyGYODgxgZGcHq1atx2mmn4YwzzsBZZ52FE088cUH6+Yc//AG/+MUvsGPHDuzevRsHDhxAoVCAYRjIZrMYHh7G5s2bcfLJJ+P1r3893vKWt2D9+vUL0ldJ58zMzGDHjh146qmn8NRTT2HPnj3I5/PI5/OYm5sDIQSDg4MYGhrCqlWrcMopp+AVr3gFXvnKV+K1r33tkg3btnPnTvzud7/DU089hZ07d2J8fNzb51KphP7+fgwNDWFoaAjHHXccTj31VJx66qk4/fTTccIJJyx09yUSiUSyRCEf+NzHa940Wnn5cGO1+1EtAjE2SODFkANUU5EazGJk82qse/Vm9K0ehGWZ3dkDSUPqhZCKRh2gVIFVNTH+9IuY+NMhVMYLMMsmEDAE526y7cDhrSe4CrUaMdLkHIANEX/FPV+S5sCIFOvG+zkLitDq7V+da8Sxz+5CL8LtJBEIeCUsBkIpRk5YhdGT1mH42JXoWzMoEuC2Uu3RLDxfSCLXICFCOL/n53/Acw/9AURTQFXFWx+k2yljmnnIdTuclXc+R8bA+80RoLr5EdRsCmpG8+cSTsBtG5ZuwtYt2FVbjKESVPw4E1Xc9OKF0vP7QeDnGhCeaU7ehpQKLZMCTSmBcD4EliE8FOyqBWZaQvlBA0LFoAA6dvjCc3HUUp47/VczKWjZlFCO0Pj5W4RuimsjCSLxtm3bsG0bVsUEMyxw2xlLR8kU2pOIwNftmHcY4lrhHNzmUDMa1KwmFEBqOCQRsxnMig5bF2MMkLByj7RyboYF1TzwVfRBzI9KRoWa0aCkNXEt1u96YA+DurL6NyPOOTjjYLYN27BhGxZs0wZjTCgqqFDekUhfiZPIHN6vIU1EoK1gOQKROSS86zxUzKnNaV/tS4tzTFNAlaCXD4elmzBKOphlwzaZE97Oj4vldyeYdj7s7eKOgdjKvepaO24EAA1lK+eBuhAOr9aM0FxQv0z8+RxTXWA3/SHjfoyvTh1UeBLPZV7nG2+pLyFjpAb9Cp1KrhLNHQdn/O6/9Vst9lkCAG95y1uwbdu2tsul02kcPnwYIyMjAIBnnnkGp5xySqI+vPe97+1aQu+LL74Yzz//fFtlVq5ciV/96leh3w4cOIAbb7wR9957LyqVSst1vfrVr8aWLVtw5ZVXYt26dW31o10KhQJuu+023Hnnndi1a1dbZQkhOOuss/DBD34Ql112WSgR+w033IAf/vCHbfdn27ZtTZUi73vf+3D77be3Xfcf//hHnHTSSQCAL3zhC7j33nvbrsPlzjvvxGtf+9rE5aN8/vOfx3e/+93E5e+55x6cfvrpTbfTdR0//vGPce+99+JnP/sZDMNI1J6mafizP/sz/Pmf/zne/va3tyTYn5qawhvf+Ma227rwwgvxpS99KUk3PXbv3o17770XW7dubfvaDrJ+/Xq8+c1vxiWXXIKLLroI6XS6o35JJBKJZPkQq8Bw4cE3/eg6IGBBSiJrwlJuRVORGuzDyHFCgZFbNQDLsurWLZkHogoMRQEzbcy8MI6pZ8cwu2cC+lwFUIJWoNwX/nSgwAAHYAeFjHCUGG10P04Ais5kBS5HjQLDLWtzKArFmtOPxapXbUR2NIf0QMbZKFIoKESO/ibpLtFzlxJQSrHnoT9gz7Y/CIH10a7AqO2IaC+wXUhxGupUpF+Bz4QHlRO8rgKjUR+CgtJg215+gJr2w5uSNhUY/g3VF56Gxqne7ZiIedkbp04hTlzB0NDWGa9gQnXUUWDUw3mICI1xvWNMoieGI9gnjQ9lXMU8cHxdCbjYos551gKx53MkyXJ0DIkTJitsxR++wYpnK+qsiSop6hiNBBQYbn4UHjmersKBRU+Z6JxEgsoGHvh/ZF/A4Urt/XtgeDs3kKg/VK0oMFhMfczrqDd+rUxPzhCRFu6lbSkwIpdxaL7qVIER12aLzwK1M3rzZ6zQs0u9bcVkE6pWKjA65/nnn8fxxx+f6Fnv8ssvrxHavu51r8OOHTvarqu/vx+HDh3CwMBA22WjnHTSSfjTn/7UVpk1a9bgyJEjAADbtnHzzTfj05/+NKrVauJ+ZLNZvP/978fHPvYxrFy5MnE9cTDG8NWvfhU33ngjpqamOq7v5S9/OW655RZccMEFAIB3vetduPvuu9uu54UXXsDmzZsbbtMNBcbhw4dxwgknoFQqtV0PAJx77rk1Cquk5PN5bNy4EYVCIVH5s88+G48++mjT7R544AFcc8012Lt3b6J2GnHuuefi+uuvx0UXXVR3m7GxMaxdu7btuuPmiVYZGxvDJz7xCdx1112Jc/TUY3R0FO9973vx4Q9/GKtWrepq3RKJRCI5+qAN17qW8TELIeJlJ15YG36r446g2hMSuCskiwZXwJHqyyA7lAOhRCR47vlhktLxeYESaH0Z5EYHoKZTC90bSRyOrJIoFIqi+HHwlzmuZwpRIovrCRAnJKz3e5K23fapKzAnDZOst99s/RJe+w3HwO9fzfokC6Xe/rlLUsKC57gFLewfYsu1N8gx5d3P7p96fWg6XnV+J1QcmzqL2PdaVUC984GE1ifY9ZgVNf2i4QWEBrwtmrTdUteSnktL8LlxUTzauM/saK8/7W7rph2SJObrX/964tBbcXkrkuayKBaL+M53vpOobDeZmJjA+eefj+uvv74j5QUAVCoV3HzzzXjJS16SSGBfj7179+INb3gDPvShD3VFeQEI75k3v/nN+MAHPtDxfs8H69atw3XXXZe4/Pbt2/GTn/ykK3352te+llh5AQjvjUYcOXIE559/Pi677LKeKC8AMR5ve9vb8MILL/Sk/iTccccdOPHEE3HnnXd2XXkBANPT07jppptw/fXXd71uiUQikRx90Hov1oDzzkPqLJ4dXlTAEfemFGMJJlkUePaeTqgCNaNB68uIPLk2C1vadZ3kUsb64jBJFEKEUFzNqEhlU1DUgGC83gDKQe09sTJVZ4Kl1LNiXc7UE/7GL/7weQJpxJ/CDcXqpP7QN2w/+s+VTjc8jDE31zb23d3P9seq2Ti2uG1k9+KfI+r8S9K3UNlWL5FGygvSUMnQzjFo9ntwfOqdV416X7MfrczRwWshZhxjlRjR/Qj2L+ZaIzF98P1I/H/u737BmEPTcJ+6o8RwnFLC/iwx+9x6hTG/Jbx/8ug/XrvUtF2zuFZDQY+L+DGLO2fdMnHXel0Ck2fb4yeBaZr41reSeats2rQJ5513Xs3vl19+eeLY+t0U8idhz549eO1rX9s1y3yXQqGA973vfbjqqqtg23ZHdW3fvh1nnnlmIi+XVrj11ltx/vnnY3p6uif1d5Nrr70WGzZsSFz+Yx/7WMfHQ9d1/Mu//Evi8hdeeCHOPvvsuut3796NM888E4888kjiNlrliiuuwHHHHdfzdlrhuuuuw3ve8x7Mzc31tB1FUfDxj3+8p21IJBKJ5Ohgnkx8SeT/8gVnsSESdnMoaQ1aNgUQAs7CQQik2mlpQigBVSkUlYJofpx+ySKFQFq0SiTLgKMx13GtTJ2DgaP7dpsJiFGSLL5D4KovWEwYqNBm9Zce4Q1bUElBANDm4akk9XnggQcwPj6eqOy73vUu0BhvzcHBQVx22WWJ6nzyySfx61//OlHZTsnn8zjrrLN6ZuEOAHfddRfe8573JC7/8MMP46KLLuqa10U9Hn/8cfz4xz/uaRvdIJfLNfVeaMTTTz+dKExWkLvvvtsLPdYuhJCG/Z+YmMB5552HgwcPJu1eyyiKgk9+8pM9b6cVbrrppo5zZrTKFVdcIRN7SyQSiaQleqvA8KwQ4VnvBVb2tGlJE+pYCBKVOMlqAxLUuJf91s1fJQsNIX6SWPew9ljQIekAeVlJJEueet4My4lu3GYayel5O0PqPMe45YJ9i/N0SBrOJxl+b9z/uzno6vXN82xxPC6IuziVuBFfG+pAmuxvrGOMey5T6iy+Zw51Qo9RSmOF6pJ4kno8EEJw1VVX1V3/7ne/O2mXFswLo1qtYmxsrOft3HXXXbj11lvbLvfEE0/g4osvbiuZ+HLgne98J84444zE5T/1qU8lHlPGGL785S8nbvvSSy9tmLj76quvxosvvpi4/nZYLIL8X//61/OmSFlMShuJRCKRLH5694ZREzJh+b28L0UIJYBCQBUqkgjHHDcp9166dC3Rr0QikUg64qjUaXR7nwi8nO3RpRs0VJA0WbqPr7QAmnjoNOgEAbwE7kmGqdETeziUWCAkmfNZIRQKIVKB0SK7d+/GL3/5y0Rl3/SmNzVM1HzOOefgpS99aaK677vvPuTz+URllwrXX389Dhw40PL2hw4dwiWXXCKVFzEQQnDzzTcnLn/w4MHEIaB+8IMf4Nlnn01UllKKG2+8se76Rx55BA8++GCiuttFURTccMMN89JWM6655pqOw3q1ymJR2kgkEolkadDhG4b/KlcbQxsBxQUJhWyWLB6Cudld80TCGwYuSEbgrV9YQAZjYydvbZ4iJ7TMPEZzSIZUKEokC2BhHdsLLMIZosss2plwUdDKvcI7V4O5DVooGcxB0Tvq9CF0i0l4v4ktxsPrunQr60SJ4XpzdFRpo54FjQ4C20dTiSCoTHAVChBGKK7jZb3hqvEQIqRmiIPP+DT0mYISxflLxe+Ugh6V2rnu04mnQ7NE3c08NBpRLpfx7W9/O1HZpUKxWMRNN93U8vZXX301Dh8+3MMeLW3OPfdcXHLJJYnLf+ELX0gUluuLX/xi4jbf8Y534OSTT667/l//9V8T190uW7ZswYknnjhv7dXjySefxGOPPTYvbS0mpY1EIpFIlgZdUmAEiL4hSVnp0oFDvCwz8dLMeeT4Jo0aFfeiLkwEY1ZIeobMrSCRSOYdOb/XpY2h4bE30rpqj5hte8USbicuRlI3lSOdunHUPoIFFj+xumeYEMmsHvSMCCodmi5eg+H66y/UWcLtSRqj63ri2P/Dw8O49NJLm25XL0dGKyx0Mu/54Fvf+lZLniZ33nknHnrooXno0dLmS1/6EjRNS1Q2n8/jc5/7XFtlfvnLXybO16JpGj7zmc/UXV+pVLBt27ZEdW/evBkXXXQRLr/8cvzlX/4lzjnnHGzcuLHu9ospjNKPfvSjROWy2Sxe//rX47LLLsPll1+OCy+8EKeffjoymUzdMotFaSORSCSSpYMa+6sbRLcR7rtJjf7CMefnwY38TXnom2SxwRkHZwycsbrxC4ibQyF5K05FndThdqbTvhy9eJcx5+AWA7c4YDmKKSIHbbGyrMU+0Z3vyml69E8ShJBF4FESJaqxltTQ1rDE2s6jkQeGG0Io6oXRq3wcos1u1l2nLh750maTwWul4Vjw+l2oR71rMa6aZles6y0p/hC/Hv9/vlFJcD2CvzHXuRb+pvE7FVJehOqI2dZ5ovcUKdG2JU25//77EyeC3rJlS0PBoMuGDRvw5je/GT//+c/bbuPpp5/G448/jv/3//5fki52nbVr12Lz5s1QVRX79+/H/v37O66zWq3ipz/9KbZs2VJ3m1KpJK3EW+SEE07A+9///sThoG699Vb8/d//PY477riWtu/E++I973lPw3Z27tyJarXaVp2UUmzduhWXX3557PojR45g+/btuO+++/Czn/0Muq4DWFyC/B07drRd5g1veAN+8pOfYHh4uGadaZr4/e9/jx/96Ef43ve+54X7WkxKG4lEIpEsHWLNcnyX8VaqqPMKFgxS7FUoLe4XHSFLQw7OOJgt/tamS+jMLJEE/kYMBduvMphEfAHemhtZI/aqnQSlxcI4mM0cJQYTHjaSxccySvTbzAI4FB+l48ZiQqQcJRyt+9VLFsN4hXIHgLR5C2zFTUD8zh0xc28JtMCDd/lOcV0GGy09PpZteGg0uhabFw//4s+DcHbTOV8o8ZNm08B3Wvs99FvEO6PR3NvMUyNu55I/FS5vehk+Kum2URbaC4MQgi1btuC3v/0tDh8+jP/93//Ff//3f2Pfvn34v//7P1xxxRUdt/GrX/2q4frbb7+9q6GjRkZGcOqpp+Lss8/GKaecgoGBga7VvRj41Kc+hZGRkURlDcNoWVm0c+fORIo5QHgLNGvnueeea7veU089ta7yAhBKuL/5m7/BD37wAxw8eBCf/vSnsXr16kUlyE+y3x/96EdjlReA8HQ588wzceONN2L37t14+OGHcd555+GKK65YNEobiUQikSwdGvoVJzHodOM1e9kNOPzEwdx/qZYsMBwAI56FvvtyalVM6IUqOOPi5Xeh+ynpGM4BbjPoxSoq0yVYurnQXZLEwYWiSSz2IrSol0gkSagfPcj9xJyl/jXf+F4cMRZpYijCgyEiOYtd6uWJ8Z7xOEcwNQfnbVojhAtH6m20MHDYzsLEs2Zb5Rf7vBpQa7hKrojiIOjx4Ge6SK6U8LYJ9IB47YePargvUnWRlGeeeSZxnPlTTz0VZ555ZsvbX3LJJVixYkWitu6//35MT08nKtspo6OjeOihh/Cd73wHp59+es36l7/85di6dSvuuusuKIqSuJ2nn3667jrGGP7t3/4tcd0umUwGH/rQh/Cb3/wGU1NT2LlzJx599FE8/fTTmJ6exvbt23HFFVcsCgV7p4yOjnYkkP/ud7+L3/3ud023+6d/+qfEbXzgAx/A+vXrG24zNzfXdr2teEW5rFixAp/5zGfw4osvLipBfq/3+7zzzsPDDz+MO+64o+12JBKJRCLpMAdGczgc5QXjMmrNYsSN5kSFGZ1RNqDPVcE4B1ECXg6SJQuHeAkzChWUJwuwqqY8rouAGn80T564VARtEomkPrUqCw4OTjzzDn+7aL4pjxivpLq46wIhAuvIlz0TE8+4JG6JVyzU7mO0/Uivaiz3I/tes9/NkkYwR3HhLF1TViTzDm7mBdVIxB9dRwBQQsIJsRstjvKCksBCqbe0qsTw1oV3rIl6on3fIYnPbbfdlrjsu9/97ra2T6VSeMc73pGorWq1mjhPRyeMjo7isccewwUXXNB027/7u79rKxl3lEbeFY8++iheeOGFxHUDwOte9zrs3r0bX/nKV3DGGWfUzBeqquKcc87B1q1bsWPHDmzatKmj9hYDH/zgB3H88ccnKss5x3XXXddwm3379uH73/9+ovoHBgbwsY99rOl27QjlXXbs2IGtW7e2VSZpzpBekWS/P/WpT2F2dratMottvyUSiUSyNOihAsO1KvRfghljUii3mCDci/TFAXCbQy9WUJktgTMGolD5brrUcWVeHKjOllE8MgurYoqkCnpTUgAAIABJREFUjlKJsWipK8+USCRLCDdPgKNUCC5xmZxJ8Hs7j2cxEwZpNok0nmDavzu0qSxwlCzhkErNlBmsZl1XPSycjtQoCCLeDTT4O2pD4gkFAhFLPaWBq2RQiLdQhUJRVKjOoigqFEUBpRSKQkGdhbj1Uup4yjZWbCUJc8k5B3OWuPHtZrCw5UalUsG3v/3tRGWTKiOWWhip+++/HyeffHLL21977bWxXhqtUCgU6q77wQ9+kKhOl4svvhjbt29vmMA5yGte8xo88cQTbe37YkTTtI48JB555JGGSdNvvvlmWJaVqO6PfOQjLXkkJfVaesc73oG3ve1t+K//+i8wxpoXWGQk2e8nnngCL3vZy/CFL3wBR44c6UGvJBKJRCIRxCfx7gYEIjQRXGM+Bs7dEAlSMrfghE3twBmHZZiozpZRmSmCWTYola+mRwPEUVDp+TKKh2cxtGmlEHwwJq/ExURAsMQZ5sE/TiKRzAueYDzye+AZyfnBL8DdXwK/Bbdp6PzgCJkJAB5QCwSE+9zLXRB/n/dbdXrQys2CtCrQDlTG/XBIobUkZttI5Q2CZInNCanZKviLF4qJ+G1GxyTozBLKecEBL2qWW0m0AaDWsyHSGUoRSBcnvC+iO+p67ISGxOl0UJFQc5z8WFD+PsThnBfBorHnZG33JQn43ve+17a1ssvg4CCuueaaRGUzmUzbSYkB4E9/+hO2b9+Oc889N1G77bJlyxa86U1vaqsMIQQf/ehHGybjrodp1g+r+otf/KLt+lxe8YpXYOvWrUilUm2VW716NX784x/jNa95DWZmZhK3v9BceumlOOecc/Doo48mKn/99dfjggsuEAZXAaamphKHH1qxYgU+8pGPtLTtaaedlqgNAHjwwQfx4IMPYsOGDfirv/or/MVf/AXOPvvsts+FheC0005rKYRXlPHxcXz84x/HDTfcgPPOOw+XXnopLrrooqPCo0gikUgki4feKTBCRCy4pNh00eCGy2aWBaOkQ58tQZ8pg1m2fDs9GnAFHQDMslBQWboJeRUuHnyhFAcI8fIGSW81ieQoJqKPiFvvzdI8cDNuHrXJ39SRaEdF6EEzklaCUtWqAaJbiZ4SjvB+xVZO0KzG+G2j2xMQcJAGil5CIp3gbrlIEyCOPiC8vZtH228vUA1E7jAe2UcCAk5YuNuuEiSqwSJ+XcEuNn30cup2n9+8n4O6rchuN6uTR/7GfZN0h07CR01OTi5ISKfbbrtt3hQY733vexOVu/DCC6EoCmzb7ko/isUinnnmmcTlv/rVr6K/vz9R2Ze+9KX4x3/8R1x77bWJ218M/PM//zNe85rXJHqe/f3vf497770X73znO0O/f/WrX0W5XE7Un+uvvx6Dg4MtbXvMMcfgpJNOwq5duxK1BQAHDx7ELbfcgltuuQXDw8O4+OKLceWVV+L8889PXGevueCCCzrKT2HbNrZt24Zt27YBAM4880z89V//Nd71rndh1apV3eqmRCKRSJYpPbXx5W5AAA4R112GkPKpE0Fi3tpzFjf4gFnUUR6fQ3W2DLOsg9vuK29c7GrerNpkfZP0CHEcbdOGUdBRmSqiMl0Et2xfoNLxQaxDoD5XeenlxanTTiS361FLbPTwgPKC191IIllqzOfNrkHT7Xah3g0uWldowmo0uUV/jmYRiAvMwyNhpyL1eX9JTN+481/cZNp4YvHDOhHvOaF2IaFtAL+rnnDdGZf41qK/BmsP/hoOk0SC/xzPlprF7Zn3HYHt6+eCoF4d4b2jJNiqo4yIrSvo2UHEUzaFl2cs2tFQaKc2wjoGz5ToqPljj7CCo+4IJ7/NyNtTe/z+97/Hjh07FrobbfPAAw9gYmKi5+1QSvGGN7whUdnh4WFs2LCha33ZtWtX4hBAr3/96/HGN76xo/Y/8IEPYHh4uKM6FpozzjgDf/u3f5u4/Cc/+Unouu59L5fL+Pd///dEda1btw4f/OAH2yrT7vaNmJ2dxT333IMLLrgAJ598Mu65555FGWLq0ksvbZrgvB1+85vf4LrrrsPGjRtx1VVXYe/evV2rWyKRSP4/e28eZslV33d/Tm13v713z2gkjZbRCjESZrUBQ/IQlgccsBywQaDEhGAS4SfGPGGTMQGMZKNHzmNiHsVWgGBhsMHY4BATv2yvX9tgI5toRWIkzWg29fTed7+3qs55/zhVdetu3T23b0/3zNS3nzNzb92q3/mdpU7V+a0JLjychSAlgUWxr5CejBLUJhiEsyuxFIaOo1xfrrB+dJlWqdFjrbkxgS2UfjjbCpwLGlqiJBDIlmTt6BJLj5zEq7kYpkkigtgj6CsMTZDgfMAuL/TbedZsqsQIP8iuElYmAAOh+pTuh6TS5w5kQtDOWRVrkEAglAjoiliqjaCGAe0+o2f9QIRC/gFPko4+ivVJv/YT9lZYVFDC7705JbSSojtHhf4TsSu1FiFQEhhBCb8a4aE+OS765I3oqL+DvEAII1CUtJNoYwqUiS5xlkLvj1hGjWhMeubsVl6sukZFbX52vJre87ZeX4KtYTveF7uJVqvFZz7zmR2vZ3JyclvJfYfNW9APTz311NDXvva1r912/el0ek9b6m8Vt99+O5lMZqhrjx07xic/+cno+//4H/+DpaWloWjddtttZ8zH29/+dm644Yah6tsIjz76KLfccgs33ngj//AP/zBy+tuBbdv8zu/8zsjpNptNPvvZz3Lttdfy/ve/v0MxlSBBggQJEmwVI1Bg9NnY9DMUlCoqicB6I4y4cwYJbQJLQOlJWtUGtaUK1fl1vFpL7+S3gmGVF+GPYQbxBDsOobSgRklJfalM6dgy9eUKbq1FFLpolEMxjLCwLaOjj/Tm/EdgoItUKF9eUE1PcAFAdXlebXd+b8U74qwtIZtU0sVPh6A6tMrvsanfALFHaO81WxA6x38a5MKwFQzwJoi+d3Ej4jwOUD501x/3amh7VXR6PLTPi9MImyna9XV4SbRphryJQO0hQk8MQ2jPia4ijBideG/HxkPEqu4tXfzE/oh4ifVBn7/udsWH9oxex7rGc8sJv8/QYyQBVKtVPv/5z+82G0Pj93//93fcAM00zW1d350vYTvYjsfJ8573vJHw8PznP38kdHYTF1988dB5WwA+/vGPs7q6iu/73HXXXUPRuOyyy3j7299+xtc5jsOf/MmfsG/fvqHq3QwPPPAAP/VTP8WHPvShPeWN8YY3vGFbY7YRms0md9xxB895znN48MEHd6SOBAkSJEhw/mIHPDAEMdOyNqQCP7EsDqFQyKD0ZiPY+f4RQmAYBm69RXWpTOX0OtWFEl7D0wmek43peYMo+ogAX0rqK1VKx1cpnVqhtlJGSTBMvRQkd+buQ0mF8qReMxMkOJ+wE0qFXVdebAcCFZQeMbMiUPIbgdl+/2dy/2aLWBmMs9c9bfVFl/NB9ym9Qv74KX1l5n0E7bGK4uSi8FAQ87KIe8UESgUDMHURIcNmrITHehrQ5rNDn9TVhi3hjDQPCc4F/NEf/RHlcnm32RgaTzzxBN/61rd2m42zhmHzLIBOxD0KjIrObuO9733v0EqA1dVVPv7xj/PHf/zHQ4cf+vCHPzy0Z89VV13Fd77znR1LRu37Ph/96Ee5+eab8TxvR+oYBnfeeSf/+T//5x2j/9BDD/HiF7+Yv/3bv92xOhIkSJAgwfmH4RUY4ea6Z3cVt7ULbMiUFspJX6L8CzMPRkcS88DiXSsRtCKhw9ptBIKXjoTp0VjRttyUoDxFbbHMyuPz1JfLSN9HDYxVneDcQxBoJJCm6NtVzwuv1mLtyDJrTyzSWKvhtzw9B6M43SNmRYAQQVgPYQQWrEZn3O+4UWnMxvRCgVIK3/cTL7UEexAb2XWfic13TMw+ijmutl5z2Iot1bulZ3DXSaGUfAuSZ6UUUqn+1UQH2+9R+qHdbfsRq6OPZX/k4RCz0o8s6zflcPAJ+nVC6NLbC52Xxtb2Dv8BEeSboNf7Qj+HgudELAzTwLwVRr/jYZO1MkIYdB4LPTTC502HF0TIsoj9Hu/jztLpE9H2CIk6IvwYveNt3J6Qty09/EQ8wlRbO6PiZQtkEpwd/Pf//t93m4Vt41wNgTUMtmMRv50wWHE4jjMSOruNfD7PRz/60aGv/+QnP8mHP/zhoa697rrruPnmm4euG+Daa6/lvvvu45//83++LTob4Qtf+AJvectbdoz+MPit3/ot/uf//J/kcrkdob++vs6//Jf/kh/+8Ic7Qj9BggQJEpx/2KYHxmY7rHAXppUXYQ6MjvDQFygEYAgDQ5gYpolhGggz2LyO2jq1YxeNFlJLhd/yKR1fYf7+p6gulTBCa8ME5w+CoZdGW99oWAbSlSz/aJ6F+09SnV+jVW1q2cdWw4cNwUiovDAMA2EYWvBk6s/dSoxQGHQhqTCkL/FbHviynw9bggS7hGGVF31mcb9E1LuBrbARTwwwsEjaGas3V6WEJCVbY0HE/h/1erA1mttzZQmVGHGhfIeyYKDyIh6maSPlxWZKjh59ToyPgMfwe1hfpPRp/y5EZ19F0S87OjE2Wmfz0dWtsNh4CibYJdx333384z/+426zsW189atfZX5+frfZOCsYNm8DMHSehm4sLCyMhM5ewC/90i/xEz/xE0Nd22w2OXz48FDXfuQjH9l2aDKAmZkZvvnNb/K7v/u7FAqFbdPrhy9+8Yv86Z/+6Y7QHhZvfetbeeCBB3jZy162I/RrtRpve9vb9pT3SYIECRIk2LsYUQ6Mfjuk9jElQfk6rnvogaG2uTE+lyGEAAl+w6W+XGH1yQXWnlqiOl+iVW0hLFNv4rex84z2rgItYDFAmGG9PuvHljnxD4dZfWoR2dJW36pfGIZk83vuQ9BO6hr86ZyqisZ6lfl/OsbpHx6jdGwNt9wMlAvbm38dwqogPJXX8qjMrzN//zGO/X8/5slvPczJv3+ClcPzNNdqGKaJQGtaLriVQYDyJF7NRfpKZ5VNwrglOIfRM33DB9KZWJhvp/4+ZTMo1c7V1e012b8Awf/9S3iebBdiRcmO87v5j7cj4LCn6H7uv1rGPQ46BP/CwIjcEgZ0Vrf034gpGTbo30gJEStx748ORYVhBBm0+3hc9PNU6FZ6bNBG0VVPmxcioX+oYxMG3R3dbktsIne0tePcLaiCQu/bLc0rtbmncvBQVz2lS5mRYE/gfPC+APA8j09/+tO7zcZZwcTExNDX/uhHPxoJDw899NBI6OwFGIbBnXfeeVbrfPazn81NN900MnpCCN71rndx+PBh3vGOd2BZ1shoh3jXu95Fo9EYOd3t4IorruDb3/42X/nKV7jmmmtGTv+HP/whn/rUp0ZON0GCBAkSnH84O/b2KvDA8EMPjAtOPNljNYhSeA2X2nKZ1SMLrB3VCgy32sKwzHYon+FrpGNHLkAYBkqCX5esHVvhxD88Tun4MpE5omjHy050F+cBIrNTAntooWN8K22ZalgGbrXJwgMnWPi/JygfX6VVbmEIYyR5UCJb1sCrw2t5lE+vc/r+Yxz7m8d48pttBUZjNVBgdHliXBCTMBDAKV/h1V2kJ9uCtgQJdhVxIfGZlVC43pNwea/f24FSIhI0yw2Kimew6n6vCX0tupQWSLTnxmA/jEFd1L/LVKzfe70a6HO88/eNSlvBECoxovBPXWUQfZ0IO0YnTIYdV2ZE53V7Z9DLb48SY9i2BWGwoo6OtPx90aOoiUonn+HI959X/WlvD2Fm+EFlJ+pMcCYolUp84Qtf2G02RoY/+IM/2FMJh3cK28l58Fd/9Vfbrl8pxf/5P/9n23T2El7+8pfz6le/+qzV97GPfWzbe5l+mJub4+677+bxxx/nPe95D1NTUyOj/fTTT+85L4wQr3/963n44Yf50pe+xEtf+tKR0v5v/+2/jZReggQJEiQ4P3HmCowon8ImlwbnKRVuMHUeDK/p6TApvf735z2EoYWUteUKSz+e58T3D3PqB0+wdmSR1ScWmH/gGCe+/2OOfPtBVo+cxmu4CCUwTUsLOOOWeWqTGOICHZLKMhCWgddwWXviNKd+8ARH/t8HWTl8CiVVJAAYaiza8pnBZaOdfLLBPmvoiIUds84UhsB0TFqVBqcfPM7xv32MJ7/1IIuPnqJZaSJ8gW3aWrlgiE03AlpBZ+o5axh4rsfK4wsc/97jHPn2I5y67wiV0yWdc8MwaKzXWf7xPAsPn2ThoRPU16rtpeVC0XMqgcBEuhK32kT6PtiBkC9Bgl2Fiv1/pgU6ny0xk/cRLvpb4iZukd73cRcL7CQkKihsWPq1sRvx9gbJGKIFeJDZf5xfFfCigp+C8EZxoX2gmA4fpz3os2ZHr3Eiflrv+h7/FukPDNW/CDqzZIcKqy767XeOAYj93OsFGA/Cpcer81t/3vuRj8/Gnh+CgwM9IeLvYEFpn6vakdIGPL+27HmhAj1XvwZGr1EBw92uF0kSjD2De++9l2q1uttsjAxHjx497wTr/XD11VcPfe3XvvY1jh8/vq36v/KVr3Ds2LFt0diLuPPOO3fEc6EbL3rRi3jVq161o3UcPHiQT3ziE5w6dYovf/nLvPa1rx1JuKq/+Iu/GAF3OwPTNPn5n/95vvOd73D48GFuu+22kSQ4P3z4MI8++ugIOEyQIEGCBOczhvDAOHOBd5gWWkkd4913/TOv9lyGCvfzAikl9dUqa0cXWXj4OIs/Okllfp3yyRVWHp9n8ZHjnPrHx1l/aolmuY7f9ILwW22LzzCvRWj111NdsJmWSiGlzj3SKtVZP7rM0iMnOPV/n2T95DIECozumNScwb63ezZseVYMdVGCodB3kAJBk2EgLBO33oqUaCd/8CSrT5ymvlbDq7koT6E87UWlfAm+AhkLkSIVSmrvqihMnCfxmx6tSoO1o4ucfuA4p/7xCIuPnqK+WsV3JRiCVrVJ6cQqa0cWWXn8NM1yPbDCvXAg0MJM6QUeGL5EmP3v7QQJzj62IwENxcRh2cU5vemzJhCFd1uwb6gaETFBdSfxdliooO2R4L7dF935GwbyG1P0hyqMSImhfetiR7uVBbTX+66ycY6J+KmdxzqGNBzWQHmhoucLfevobVvnmAzsj+j3tsfJhuMZ/TbYyyXqm3480ato6EY8GGp0rPu0jhO6Xqw2VdKHCpFwMnWpaEKPShV7h+v6S7D7OF/CR8VxPrapG7Ozs0MLZlutFh/4wAeGrrtarfLBD35w6Ov3Mq677jre/va373g9v/mbv7njdYRwHIebbrqJr33taxw9epRf/dVf3VYC9n/6p38aIXc7h0OHDvHRj36UI0eO8Gd/9mc861nP2ha9c6XdCRIkSJBg97DzJhDovaAiUGC4Hr7raaF+4FVwQSDYf/otj9pyidpKCYTCSlvaKFMJTMtEIFCuYuXwaaqLFdLFLKlCmvREllQxQ6qQxko72rPCaAssFEAQ1sKrt3ArDRqlGs1SncZqjeZ6jcZajWapgWnZAVPJ5jaBRuiJIZRA1iWrP16isVDHzjk4+RTOWAqnmMbJBvPPNAP1Z6DJkAqv6eLWXRrrNVrlBs1Kg1a1SXOlTrNURyiwbBMjJpgShkCYArfeorZcRjZ9TGHiC9VHEnSeIhBQ+q5Lo1xHtjxMYXT0U4IEu49wPu6h+3Kkt4jBBm6DAypXW2IiekZvdt7AdyItVY9GQGhBPgSKgpjyIpSRR2dvwF5bERL7JjrbFekACIxROhKxxxQkm1XW54ydnkmD+n0rnhpbx9bmQMc5fbpwIOluhUfyWDin8L3vfY8HHnhgt9kYOf7X//pfnDx5kgMHDuw2KzuKl7zkJdx7771DXXvvvffywhe+kP/wH/7DGV0npeStb30rjz322FD1ngv4L//lv/D5z3+eUqm0I/Rf8YpX8JKXvGRk9P78z/+c173udVs69+KLL+auu+7iLW95Cy95yUuoVCpnXN9TTz11xteMGg899BDpdJpDhw5teq5hGLzuda/jta99Lf/23/5b/vAP/3CoOvdCuxMkSJAgwd7Gjikw2skDRSS8V77Cb7pI12dPCUHOAkKBgvQkzXKdVqUOgGEHrqaRJaUCX2mvjKfXcfJpUoU0hX3jZKfzZCZy2Lk0wjGj5Mj6erTluytpVRo01mpUF8vUVyrUFku4tZaWdQgRJEtOcCGjJ9SI0MoEpQTSVdTmS9ROrSNSBmbWJjuVIzuVJ1XMYucyQUgpdJgVpeesW2vSrDSoLpWorVRpVup4dRfDsLQwXoBhmX3rlb6PW2+hfBmLX35+rxGRoFIohFD4rkezUke6PoYwSSRVCfYW9th8HCk7nQL4SB6/wRIUBnXqvGAA9dAjYSClIMym6iYVcwcQIhDIq8j7oJukAJSIgkwF3/vw00E9UHV0VBwXtqvgNa5df+faHHdZiLepm5Rqfw/DH3VVpatTHedHdQ4J0SX1V13/t1u2FQxSh2zh+ljF8e7brGUh9UgZc34/Fs87bMdT4aabbuK2224bITe9eOc738n3v//9M77O933uuecefuM3fmMHuNo7eN3rXje0AgPg1ltvZWVlZcvjWCqV+MVf/EX+9//+30PXeS5gZmaGD3zgA7zvfe/bEfof+9jHRkbrr/7qr3j961/Pz/3cz3HPPfdsObn7jTfeyC/+4i/yB3/wB2dcZ6vVwvO8sxJqqx+UUrzjHe/gkUce4bOf/Sz/6l/9qy1dZ5omd9xxx9AKjPMp1F6CBAkSJNgZnNUno5QKv+XrEFIX2iZMxIQFQrTDLRDb+hpheAEBJqAUfsujtV5nveVTOb2O6ViYtomIEn1HhFGej/R8ZEvqfm66eC0P6crIW0NT34rpX4LzHn0NfVU7JIipX2L9hkd9sUKr1MCwLQwr8KKI5WVBgvJ9fM/Ha7XwXA98FZwbCtYG12vYJnZOexbJM7KCPj+gDIXv+bjVlg6vJcIk3hdWPyTYixB0Rps8n58bXZ4EG4Qoisvl+y6lou3RMFiBERenC+1V0bFOdilW4pUHj3+tDAjCQUYn9Qkg1F1/oCwxNhvPYJ1vcyLaYUHp7KLOfggDLA0g28vOpkc2U2rHAntFRwa37kz9MDrHqteXpD3eAz2LNw0ZtUUWtu7wkmAXsbq6yp/8yZ8Mff073/lObrjhhhFy1Iu3ve1tQykwAO655x5uu+22kcT836t41atexfj4OGtra0Ndr5Ti13/91/nqV7/Khz70IV7xilf0DS20vr7Ovffey8c+9jHm5+e3y/Y5gf/0n/4Td999N0ePHh0p3Z/7uZ/jOc95zkhotVotfuVXfgXQOUn+7u/+jo997GP8m3/zb7Y078fGxoaq1zTNXVNeAHzuc5/j7/7u7wCtxPv5n/957rjjDq688spNrx22zQCpVGroaxMkSJAgwYWBs/p0VFLiNV385gUSQioKuxBY+AkwTAPLsbEcG6/mBvveTivLsG9CrwrXlbTqrbYBo9D/9CgwfIny/OBL+DvQk3y5eyOe4MJDr/Ammg2i/UEpUJ7EdX1a5SYdEhRFoMCIUQoVIAZgCAwjJh4TscvbNwdKgZ1xSE/kMByzTfNC0bMFt7F0fVrlBn7Lj0kmEyTYbWwgxd8Q59783TTBdB+oDYXk4bO8y7Oj44T2WthBKFoC2sqL8D0iLshvp3BQbf5jXhgdng9tpgOjijMR3LdpDFqdeo73684gj1fUXNXnWqF6hf2xNsffGzuaJdrPstDDI/7+1cvvxt4X7fembr+NqAPjtRM+F7sqaf+uQm3+wJP6oq+i6Hx/Lp4H+NznPke9Xh/q2ksuuYSXvexlI+aoF294wxv4lV/5laH4PHHiBF//+tf52Z/92R3gbG8gm83yS7/0S9x1113bonPffffxsz/7s+RyOV7wghewf/9+isUiq6urHD16lB/84Ad4njcirs8NpFIp7rjjDn7hF35hZDQNw+AjH/nIyOj9zu/8Tkcor/n5ef7dv/t3fOITn+C2227jF37hFwYqGprN5tDJuCcnJ4e6bhQolUq8973v7Tj25S9/ma9+9avccsstvO9979tQkfGlL31p6LqnpqaGvjZBggQJElwYGCKJ9zAIQhBIidds4TXds1PtHkK44RamgZNN4WTTGIYJMkjAqLo28/EwAwY6V0BQMHRCUBXQVUHuCxAYholhGBim0b4mZCKe+TLcDSeb4AsXsbFXMcmannuB+CtK3CowTAPDNIMSzDHTwLAEhiUQlgBLBN5FsYS9YbiQbseKUNajDJxsmvx0AStlo3zVR8Bz/iIUNErXp1Vp4LcurE1sggTnM9QGz9qOBNlGW2AeT5qNUMFvwfEu3wpBZ3r0tk2DPhLokjse+drzAgQSkCilC3SW+HEV+4sv5arrKhWV/o1WfQoE70hh6eeEN8T7SvDW1VNXu39G9RI0ID5UNwyFEirMvd03vFc31QvnSXj+4fd///eHvvbmm2/GMHZ+i1YsFrccHqYfLoRk3u9+97vJZDIjoVWtVvnWt77Fvffey6c+9Sm+8IUv8L3vfe+CU16EeOMb38gLXvCCkdF785vfzDOe8YyR0Dp58uTAUFSPPfYYb3nLW7j88sv5wAc+wN/8zd+wvr6O7/ssLCzwta99jZ/5mZ8ZOo/J1VdfvR3Wt4UPfehDnD59uue467rcc889XH311bzmNa/hc5/7HEeOHMF1Xer1Og899BAf/vCH+eVf/uWh697NdidIkCBBgnMDm3pg9JUjbjEQb2QtFsZulgqv4eI33U7CkeFa3Kru/JOsK6UQhsApZEgVstQWyihfthULXZ0dWg22rRK7LPA6iAe/9Wx4VNfntjXoaGIZ7CaFPjS3Q7TPlOv2EOrXc6KvWe25jU4r2s72hc4BSoCQgf2x0GKiyBY5vGaTARGBcMkppMnNFbHSdiDIGlVL9i6iNS6QUvlNj2apgdfy2gc36odR9FHvstP3nP51qU42zsCPKSO+AAAgAElEQVSYu8coe6Pzh7ip4yyLjm4U0XNmcx+X0H57g3V3JBgFVT1fRstfW/yrRdbdP2824F3zI3a85xViB5dQ0fMpNM3v319nykp/78au+6JrAomO4yK2XKqecFORT0HcK0t0rR/dPIj4ahyMYzeb4TquwnGKz/Tgt673h4hi19iJ2Ode9OnlAf3fb0Qib4P4q8uWFi59XlxpoT/EB0P0aWcXnxETXXTDfouNRXzGRxlDehjYCrr7Pc6Q2vTREJ0dm1f9mFADv8RohGtmjISIvZgO4uM8fC06I/z1X/81jzzyyNDXv/Wtbx0hNxvjlltu4Ytf/OJQ137jG9/gqaee4uDBgyPmau/gwIEDvPvd7+Y3f/M3d5uV8xJ33XUXP/VTP7VtOrZt8+EPf3gEHGn82q/92qYJuE+cOMHtt9/O7bffPrJ6AV74wheOlN5W8fDDD/N7v/d7G54jpeTrX/86X//610dat2maPPe5zx0pzQQJEiRIcP6hv3lPZB7W7/hwFQlhgATZ8PGbOsyREOJ8z9PbAaUkwtDC2lQxA4ZASokK3S0CJY5CntFmd+s2hKGt5FA76gsLF0LXRJKhrsMCbSVKr5UsBAaywTmR6W9wUuhJtCXb1uBcQ4CTT5OdG8PK2BeE8qIT2gvLb3k0ynW8ZswSr3sQ+g3ICKA2qmdLdW1GYKsEu9eobTS467L2V9HxfRD1yEJadJ6znW7qPjdUO/TebWdW9P0aP9bfJj46JrrKBi1RQqI66PRa6vcvnTwpNB0Z+9xh7d9TttD2iP9+bdh4ZFQ3jXj+iRDdi1lMkC42WegiZ8cgpF54jXZQUzrcXr9ruuuO8ywA0RX2qIe/9hgpIdseIH3apcL3jo73j+Av+i12L0Rj2n+8+x/vg64HhRIgQ8+EsK1Be1UYkrCnDYJRSMnD9mx0F7bnMKiYUmTze34Af1t9SMbmZHyt0AoFtaWFO/qmestWFy8RU1aKLe4DLiAnyr7YjmfC8573PK699toRcrMxXv7yl7N///6hrpVSDpWk+FzDBz/4Qa655prdZuO8xAtf+ELe+MY3bpvO2972Nq644ooRcATf/e53+eM//uOR0BoGr3nNa3al3ltvvXXXvIFe/OIXbyt/RoIECRIkuDBgtAMSdIcmiCkxOjY4W98whpeE1oZKKtyGi9d0tUVYfB8Ukz+IeL3nMiJhR5CDQgFCYGUc7FwK0zFjybVDqQjtLg426CJWoljadO99B+2Eu4+LruNbH8+OPXc/AU4fAU+8dFhqdrMzCnTM0X7tHFS6+OjXLWcqqdzziAl/4tNuQPf0nUJ0d0VcIkXf/op0dWExBKZtYqdsnHQKwzTbko8zm557FgOnTtg+pRN4S1cXpQJhaofkcCdKjMHNzu0rHwuPB63qJyHrKzEjSv4eJYEPSvfxziJ7j8XFjl10QIXh9mPtjIvVRfuSgeMkAoFlu0RNjc5t92f82ODSpqs/6ptBDSi9N02/Eg5TP3VI15+I/R+Fs4l9jw+viNOQvbTO4C/se2LKluhP9XLa/h4K4uOls90DixG/jdrtRaiO1FPtaxQYsRLWEx3reqYZRMcxg1BPpi6En0Uw2kIhIrp0PCc7eejDl0Fnmw2isFJ0j1/8e2zkBquI+vQjA3gSnbO8827pnlehQmLAuMbndxcPbSV6eG+E/OgO63gfEt0duUV03ft9WxYuLbFziN1fHRdFX9oHw3nf0YVxlqNjgUIrXAu67pl4n8ruUQzXxUH3G6pzHeyLNoc9+4Gg87v3B6Lv5Bg0kS4sLC8v86d/+qdDX3/LLbeMkJvNYZomb37zm4e+/tOf/vR5HwIpk8nwh3/4h0mS4R3C7bffvi1v9nQ6zW233TYSXjzP413vetdIaA2Da6+9lpe85CVnvd4vfvGLfPe73z3r9YZ4xzvesWt1J0iQIEGCcwcDAqyOeDMSCMikL3FrTdx6K5B5BTu4mMTkfAwdFYcwBHbaxsk5mGkbwzIQoXlhuLvdudp3kPY2sE35a39iZ0q4iwS9h89LnEkbN+r3fl3ZI7HvOq50wj0rbWGlbGzbwRTmGTB07kMAypf4LRff8zuTl0dnjLIM4mKDojYp0FZibAkDReuxX2MKDehQYnRMLEUHnfZPcWEpbcoqJijtamePoqKvAqmtsFCIIHdM+7MW2PZXWnQrRdoSzJgCwwiKUO3ztzi0ocX8Rr9HtEW8x7vO7bJ2jwvBI7G3ivXtRlOjD3+hwLw7AlX3DNKCW9nJc4dSJ3Zyv/aGyotIydCpiCDKMREbCwOUGSsGHf0Sz1mBITQtU4EpwaRdLBAWCFO0yRvBb/F+Nvrz3gGjfb7qUK4Muraf+qq/2iF8DevBRstF93nE6hSx/zsSeYVndcy6XlKC6B4I292eowMYGvKdSXV8FgO4iveV6DqiGRbdx5Xs4xF1JutjrO4eRVT8Loz/tY/Er+1u5/DYynMkQYjPfOYzNJvNoa51HGekSY23iu0oTZ5++mm++tWvjpCbvYnnPve53HPPPbvNxnmJ+++/vydk75ngP/7H/8iBAwdGwssnP/lJHnrooZHQGgYf//jHz3po4kqlwnve856zWmccN9xwA294wxt2rf4ECRIkSHDuwNh897yNjUvMwi38Lj2J13BplZt4DbdToCTOYLPVZ1+4/e3izkMIgWGbWBkHJ5/GzqYC7wzdUVvq4X6Wh5sKuEa4Ad2iQG2wgGUEdLr7ouOHM62o+7ILYIO+yRzpsK/s7uLw+0a06KUZ/RBMdykVZtomM5XHyjpt4STQIZQ+XxCT77XlXwa+69NYq9Oquh2K3I3+hr9p+q+QZ14ffekMhXBOxQ9IhfQ8lO+jPB/py56iZJBguIsFqSR+eJ6UgbBdT1pltOuI2jZgbVTo8BjK114x8dnYnqexG2HDruitQ3t/KKSvgva0H5aR8F7ERJP9PDQ67pP+v7U9HrpY6WC8q3QLP5VChf2uFFKEIX82Eo33Kqc6eqRrDelZYwyBEEaHB+JAy/s+z8Duwz3DYXT9Hzsfhc5PFc6x8OEcFBXZHAiUVChf90+76QLlK6TnI1VbuNw5u2L3T7/1lEBpJxXS91HS13yFcyc+Phs+2jYYo66E0v2VUBvMsw4aA26AgY9n/YIopb5PZb88DdEzpH9L2ufpwes3V6I5MxBx5rqyh8ddtDrqDpQG0tdjE3hBRPOk63br7oaoxq0s2d0shkzEI3cNas7AZm/9uTG4P7f6zLmwoJTaVvLu17zmNUxOTo6Qo63hmc98JjfeeOPQ118IybxBJ1e/8847z0pdQohtjcm5Aiklv/7rvz709YVCgfe9730j4WV9fX2keTTOFDfffDOvf/3rz3q9d955JydPnjzr9ULbu8noyeGZIEGCBAkS9GJHnxY9wgulUJ7Eq7s01+u41ZbefIVChOg6trT/OSe3RwYYtomddUiPZbHzaW3JqQCp2gqdBAnOZWwiiFFogZ+VtsnvG8POO8hYHLnzSG0xWJwjBMI08Jo+taUqrXITgaGF6srod8UOM3Qm2KbyYsDcCC3clVQoz0O5PtKT7eJLfL+t0CAULgfm/kpooa+U+nzlSS1/DLwjIkFnqDEJP/djTqEFlNLTwmMpgzraltFaZhvriy63gs2EqNJX+J5Eel0C8Og+CJQYXULk7j9U3DNhkBq/33h1CdDDugMacf8R6Qd9GrIZCLdB9dzuG4qLo/4wMAxdtKKi/V0fE/qzMPuU/ooNfU1QRFDQOWZEh1sI0XuHiJdg/qEAT7+vECjBokTNceE+6Nxerh/MtcAQQQYKDNfDlz5+mAtChWMU75ANzC4CRYryNP1IORKFSdsE0YCooUuopIh7CvX/fQuD39m0SEHjSy9QNsYUptG9ys5bowYKlXiQtzajwXtZeF5wrlISKX1830PJtiahrfTr7JahWevTdtHX1SnBXsB3vvMdDh8+PPT1Zzt81Kjq/uY3v8kTTzwxQm72Ln7t136NT37yk5jmznkMG4bBPffcc0FYpX/xi1/clsfDr/7qrzI9PT0SXsbGxrjzzjtxHGck9M4E/+Jf/Itdyyfzy7/8y7uSONxxHL785S/zzGc+86zXnSBBggQJzk1YO0lcIDpFkUIgDEGr0mDp8CkmvVlmiwdQht7Ahpuw0Div/aUX2xSfnRWozqbHf8F0LLJTBRqrNRorVaSUmIYRU96Em+Zkc5rgHMagmzQIB2TnHfIXj+PkUtrSXspIp7nZGnBOIC7XFnRHVEEpRW25zOIjx6k8vYphG4HAqjv5bXcn7PXVb2voMCoOhJnKV4xfOs3MtQcwLAPDMttnC4XvetTXq9RXqlSfXsetNnX4LalQCooXTVC8eALTspC+ZP3YCtWFEqHOol15e40VxMSPQguhnUKKicsPkCpmaJZq1JarVBbK+J4XrdFS+AgZU4J0DLAYKGSWvgQDigcmyM0WQenwiutPr+BWmzEvg437b0NVn4idIdq9LTp+D/rdVximwLANlI/2KAjOsVMOqUKGyUP7wTBYfvxpakslrXCHDc0gRLvyzuOBkiBuKD4w2fTA5rV/jPeCEl1HzPaw6G4IFVQqEAQHfgVCIQNPk8x4juK+ccy0g5l2WD0yT/npVQzTRJjt2EbKV+Rnx5g+tA+/5bF6bBGv7uG3fKav2c/YwSnWjy9RWVinVW3gtbxgHm7QsIAfwxRkJnNkJnKkJ/N4DZfVJ07TqjYILT9GsQqc7VAVYc4b0zZJj2VJj+fIzY5TX6mwduQ0XtON5lR7xu4kjyp6U1VRfaFihsAqVCCVDJZlhZ1xcAoZJg7MUJibZP7Bp1g9vthmtY8+c7gWaCVmx9ViwP8J9gTuvvvuoa+dnp7mVa961Qi5OTO86U1v4j3vec9Q+SxCz5Pf+q3f2gHO9h5uvfVWrrnmGt785jezuLg4UtpjY2P80R/9Ea9+9au54447Rkp7r8HzvG15PExOTvLud797hBzB29/+dp71rGdxyy238Oijj46U9iC87W1v4/d+7/d2LcfKvn37+O53v8ttt93GXXfdhe/7Z6XOL33pS7zoRS/a8boSJEiQIMH5g5311wusGeMhSIQQeLUWa8cWqZ4uoVxtrQiRwd2WaXfva7dqAboriBlZKqUwLFMLJsayWhYSCx+SIMF5hW5tY3gfAHbO0R4YOScITwN7/E7ui41DlfRXtypf4jdd6ktlVp88TW2phGEaOlZ+/DoxoAPjpZ/1dL/zto0YnZENj7YiVhB4XvgU5iY4+NPX6fKiazj409dw8IVXc+kLr+bi5x1i309cyuSVs2Qni1ipVMxrQ5KbKjB33cXse9ZBZp9xCZmJHEiJCJQJHV4TMR1B+/mjhYZWzmbqmv3sf/ZlTF29j+xsQec0MARWysJKWZgpE2GKPklyN+7rMORMbrbI3PUHmL52P2OXTWGmTaT0QcZ0IRskOG/XFkpMCWXbPSXK30As71IMhm3i5FOYKav9mwAzZZGZyLP/xss58JNXkJ0uYJhG27MhygfRWTYL59MdEmrw+fQpg6+NFDPBn7AEVsbGyupiWNrVInwvESrwehJ6Dkpf4uRTTF45x8z1FzP7rMvITBaQrheEk+oc5uxkgQPPvpy5Z1xCeiKHlXUQtsnE5bNc9tPXMnn5HOliFsMyY2O2yX2pFBiC9HiG8cum2fesg0xfexFWxorCmYUxhM4Vf7UOZZ5SCNMgPZFj/OAMF914BVNXzGHaBgofRG/mmJ1CaC8SfotuIRNMx8TKpjBTtl5jAiWrlbLJTuSZu/5SrnjxMyjsm4jCOQnVqZ7anoNE5xyJPIRiXkPn0GPyvMfCwgJ//ud/PvT1b3rTm7Bte4QcnRlmZma2pUD5zGc+Q6vVGiFHexsvf/nLeeSRR3jLW94yMkXwK17xCu6//35e/epXj4TeXsdnP/vZbXksvfe972VsbGyEHGk873nP48EHH+Tuu+/m0KFDI6cf4rrrruPrX/8699xzz64niHcch9/+7d/m4Ycf5s1vfvOOrUWO4/DOd76TH/3oR4nyIkGCBAkSnDF2POBgxxZUAUIn8/bKLuvHlzn2jz9m9cQSpuVgWBbCMrQAJA4BhmliWTaW5WBaNqZhtmUzgWeHMASmaQXnWRg76N47NJRA+WBYBunxLKmxjD6sZBIJIMEFA8MysDM2qUKG7GQBK5OKWb/uoZsgWHtsy8G2HCzL1utKXEBrCIRhYJo2lmljWw6mYUXXK7SAzFCalmGaIKC6WOLk3z/J4iOn8Oo+Haa7PehWYOwmRidYjCiFEkQDhG1gpkycjE19pcTT9x/hyF8/wmN/+UMe/38e4OQPnqBZqpOfGePyF1/Hpc8/xPglUzj5FEoEYV08ie96SM8NwrvEOTYCy/suh4mAm2gGKp3DQNORKNfHb7XIzRa44qXP4PIXXc+lz76K/MyYVjr0yccxGFowb6UtnEIKJ5/CyjgYGCCJ5YM5g34+0yEJyFspm/xMkX3XH+T6Vz6XfddeiulYOoyTdk3QCkfPR7meVgbpJmz+BjESoU4fbUzXr/EvkcGEr/AbHrmxPJc+7yoOvuBqLnneIXLTBWTT1d5eAqTRtpQXEpACw7Sw8ymcnEM6Y2Napm5s0BeghceGY+i+sh3MlIOdSWE4Jgod6sxzXR2CDJ9wbrVl0oPHVymFMAROPk12MkemmCGVcbpiRAu6Q5adCxCBoggBlmPjZFM4Oa040545YbvCCbbTbQwUWcGf8nwMIRi/ZJaLn32IK376Gex/xkGcTCrwOgqVHNpz2PddpPJ1SLlQCSpGxfXZ6oMEo8CnP/1pXNcd+vq3vvWtI+RmOGwnjNTi4iJf+cpXRsjN3sf09DSf+9zneOCBB3jTm95EOp0eis6LX/xivvGNb/CNb3yDgwcPjpjLvYlms8lHPvKRoa/ft28ft9566wg56oRlWbzjHe/gscce4y//8i954xvfSDab3TZdx3F4zWtew5/92Z/x0EMP7Tll1TXXXMO9997L0aNH+fjHP871118/ErqXX345t912G48//jif+tSnGB8fHwndBAkSJEhwYWFHQ0jFEU/tEMY3ry6VcP0WSiiyU0WEpS3LopjDkSWlTsxqKiP4KYyDHrg4BlaXQghMQwteMEILTKkFjGYQ4Dq0AuyWG0TMDTg+kk5oVyIMAzuX1ht3y8APY7SPrK72h/BjRD4e4iB+fijEifVnnN6Gcbc3kr1uilE0PN7KIRnp6ajdg+r+EJd1hfdF11wWAoRhBMJ1eoWHSofYUVKBDGPnt+kNbHZAZ0sx1wc2oPM3wzYxMzapfJpUIa2TLntxl+VtTagt8hO0q0/7wwTLOlF05/+e9HQ4kWCJUgIMYWAaVhDHXyCRSCHbfat0vgQldH4Gv9mifHKNxUdOUZkv4buSdiCjuJB2kz7YaK5ueR739sngc7TUf9sj0zGX2+GKhKHXHsM2MB2TZrnOypEFyqfWqS2WsdIWmfEsk4fmGL9khonL5nCyKdxWC7fVolGp4rU8WtUm0vfxXVfPq7BZQiubhBAIU6EkSE9pS2ZTtL0dpE6q7dabtCoN3FoL39Xhf7KTeeaecTHKVbTW6tRXq1QW1oMqRGCl31Zs6XVU32s6KbiKFO+mbWKlbfAEpmPqc2WffupGcK8LU0ThmFSQtDzy5hMQhSMyDO3dE+SaULLdTjvlkJ0sMHnZLAeedSVuzWX5ydN4LRfp+ghAej5upYGwDJTrx+hrJoVhtA0GhPZiiHJ6RF0fPIfRVuyCTs8JnchZRjRDVUTnvIkdEQQeSwIV9LGS7bEL78/MeI6Zqy9CWAae51FbLFM+uaLH29B6p+65KQyBmbaw0jZWytTKnFAbidJ8BMrLMMyZYZtaCG8aKClxGy0apTpuo4XveahowdDeFYZlBML8IF+GL7Vni2GC6yEQ2GlLr5EZB9ex2vk9bDMYcxFLIB10SvDuFPaNMIwgTJiMPAhCmJYOiSU9X3uGGkb7uRHMj3a3dHpQqGBMDcvECIxOdMLxzg5VAT/h+AtDgKv5M20T07awHEsriYLcKMIwAz70OIY86/q0QF+HjFOgZKQLEqYI5rmIjGVUkMMkSmNhiOAdUbTfCX0Jwf0j0PdlYW6cqSsvojA9xvqJZdaOL+LWm4TaO891adUaNMpVfNcNtKHhKgCEvBhaMSJ7nnEQ5kESwT0jFO0+EkKvYZ4PXX3fSaPnQ2ffJzhreN/73jeyZMK7hZtuumlk8+Zshd+J47777jvrdYJOgv75z3+e9fV1/uIv/oJvf/vb/P3f/z1PPvkkjUaj5/zZ2VluuOEGXvayl3HTTTdx1VVX9aW7k3Pq7rvv3lbIs1HUf/z48aGv/+AHPzgShcJmMAyDV77ylbzyla+k0Wjw7W9/m+9+97vcd999PPLIIywsLAy8ZzKZDAcPHuSqq67ihhtu4PnPfz4vfelLyeVyQ/MzNzd3Vtb2iy66iPe///28//3v5/HHH+cb3/gG3//+9/nhD3/IkSNHqNfrfa8TQjA3N8fll1/O9ddfz3Oe8xx+5md+huuuu27HeU6QIEGCBOc/zpoCo8PUVQCWQHo+zbUGS48+Tf10mcxMgfREFjvnYKVsEALlSdxGE7fcoFVu4tVdVEtqgacfCIpCi7cw6adjYuccUsUMqWKa1EQWZyytN4aKSBh51tHVB8IQWFmH7HQRISr4DTcICBEKb3ZKkh4KvkXQFxIVJM80LC0UMx0rEDaFlyi8povfcCPBQdv6PL5tP4vS/7iMO5L3xg9slZfQ4jMwyY4J9fcEVKx06yS8UCClY4pbKRsrY2OmtTeTMIzoEun6eE0Xr+ri1V18N0hOaxCNZShIHhWECo2Wow8gJXY6Te6iMVLjma5G7iKEFjgpCcqTtMpNGisVGqtVWmt13LqL13ADQaCMxkOFa49p6pAjKRtnLKPDyWRsTMcCz8dveTTW6tRWNc3aYpnaYhXZDAXC3XN20BwesXIH2NqkH/0YhfMjlOt2khdIqfBbHn7LxW20kJ6H3/RoVBpUF8sgBKlChn03HsT3PUqnVqmtlFl6/CSWY6Gkolmq6yTeUmGYYOUcnJyDXUjhlltUF8pYWUsnkm/pcXIbLVo1l7Wjy1QXKjTXa7h1l/zsOLnpAoYtUAYYWZP8XBG33qJZaeDWmni1FgKBkwk8K3KOVnw1WjTLDRpr9UCAb0R9KkB7hYSm25sMhWGYpAoZUvk0RspC+ZJWpYZba9EqN3XvmQZS+ijfxyo4pPIZ7Gwa0zbxGy5e08VttkiNpcnPjeGMZfDwsIoOxUsn8RseXsOlWapTX6mwdHgewzRorNe0UFgEWa6UxMlmyE4UEabmv7FaoVlpBEm/9aBaGZvMeA7p+TRKdR2yKpvCStkYjkVjvUazXNfCdNnr3BFf4kWgiEoV0qSKaYRlIqWkWa7TrDRQnoeZssjNFMnOFLEytlY0GpLcbIHxy2Zo1Zu49SZ+y0d6mketHwkTw0MotldKIX0Fllb8CCT4AoJk3Sgd80sJUL5eZ0snVjCEwdrxJeprtUgBplSYAyuPaVugFI31Oo3VKnYhQ3osR325rM8PWZHBjWKa2PmM9iLxJG6tRbNSp1VtYBgmQgl830OY4OSzpAtZ7GwK3/VorNdw6y3cug7xYghBqpjFKaSpLlXw6q4Os5W2EQj8pkuzWkd5qj0vCaas1AMhLIPsRB4nr0NftKpN6isVfDdUGur5IYSBMHXYzFQ+TavaiJKmK99vtzPom1Qho6vxZTBXtQLIsAyykwUMw6BequE3XaQfKO98n3QuR3o8q+eUZURzym95WvEGmCmHTDFYm1O2XltqTRqrNfyKR3osTWa6QGY8h5NzELbAzjsUD0xgOAZey8N3XaqnSyylTtGqNKiuVEAEOcwC7xk77ZCezGM5OgxHY61CbbkcKXSUAsMySY9lMFMW9eUKKMiM5THTNoZjUF+rUlsq6VloADJ4hnbl2tkQo36oJ0iQYCDGxsa4+eabufnmmwG9pi0uLlIul2m1WmSzWaampsjn87vM6e6iWq1y++23D339wYMH+ff//t+PkKOtIZ1O8+pXv7rDa6LZbLK0tES1WsXzPGzbJpPJMDExsS1FxV7CoUOHuPXWWzs8XpaXlymVSjQaDYQQpFIpCoUCExMTO5rgPkGCBAkSXNjYcQWGiP6JHQskVkoqZMOl1ixRXyyTX6+RmS6QKqZ1HGkMpOfhVps01qs01qp4NRfV1MIE5auoktDiXBgCwzFx8mkyE1kyk3kydZdUw9UbVsvCsI3IgnGgR8Yo2r6JdbQQWqiTnS0gXZ9avUVkNhpjabitZ/8GKbT1ve4rsBwLESQmNSxTC8EdSwvAo2SlmpYXCD98z0d6vrZu9CRK+j3Wtt2NH/n2OSKoejtK9RwYde1nAf14Vm1LU0ML242UDklkWAZW2iZVSGPnHMysrS1jY0oov+Xh1V3ccpNWpYXbcPFdD+V5ehw9LUWKem+b4V9EdGuFlscEVr0KK2OTmyuSKmaipK7h2SOJJbzJkEczQ7RZk56PV3Pxmx6tmquVDKfXqS6Wqa9UcKuBAtWP8xvQCEPYpUysjENmIkd2togdCGiV5+M3XaqLZWohvVoL5QcMGF1S64FdcLbnciDYDZUXOyAHC/u/Vy0jtKeQ1MJJ6ftaEdDSSbyVlBQOTDKdTzN20RSrk4uBAkrie9qCHRkkzBbaot4ppCnMjpEqZLAyNq283njZhRSpiTStUoNWpYlc0gpyvyUxTR+/5WNYBqmizltkWAIpwEjrOP6FhodYKGkBrKktyrMTWZxiGjNto9DhqGrLFUDg1po67FRnN3fmURGhUiN2kgAnnyZdyJIey5EqZDBsM7D4z9JYq1LxS/hNF6UUlmNj2CnyM2NkpwpYaQfDNCOFULNSJ1XIkJnM6zw0Quq8NLNjeq2otfAark6U3vRRZqAwFVrFbmVsnLxDZqJAfp7SjooAACAASURBVGoCTM2nk3VolmrU1+raG8b1sdMOxf0TKBT2agVhGjoZci6NnU5RL1SpLpeCe6M58AEqgFQhTXpMh2B0iulAWaNIFTLUVyuUT69hpWxy0wU9Xo6FEhJDmKTHsxT2j1NdKWnlvS8jBUbHYIT1CTAdCzubwkgZkQdA6EFiOlYs7GWwfga5XGQzCD/ma0WOYRk4uQzp8Sz5uTFMy0R6PlbawbQtctNjZCcLKNentlJqqwyVzseQmyrgZFPkZsZQUhsVlE+v6nvDA3yw0w5OQY9hZiKPadtIz9d9s1alulzW88OVZCbyjB2YxLBtWpUG6bEcdspGuh6N9SqtWgOUDyLw+EB7zhiGgZNP4xQzFGbHcAppUNCs1DEDZVSr1kRJXws2Chky43lykwVSxQzNUg0pfVLFDFaYXyIwjLCzmncE+K5PbamMW3cBrdwozk1g2BYKaJSqyJqvc9JksuRnx8jNFgPPJ0jl0tTXalSXSniihZ2xSY/lyE2NYaVtDEf3v9/yWBfLKL+MU8yQGc/i5NP6HNvAytrkZoogoFlp0Fiv4Fbq+E0Pr+Yh3dDrSr9TZcZypItZ0pMFLEe/ZjuZFJZt0yjXaNW05aphGWSnC2Qn8tgpB+VL0sU8Tk4rPlP5MqZl0ihV9TWqnUA+8vjY9EVxJ41hEiRIsBGEEMzOzjI7O7vbrOwp/O7v/i6nT58e+vrf+I3fwHGcEXI0PFKpFAcOHNhtNs46pqammJqa2m02EiRIkCDBBYaz54HRB1pkFwruDGrLVRrr9U7Bq9KCJOn7SE+CT2Sth1SR4bwKFBgokA2J3/Jortcon1jVYSAyNunpPNnZAvmLxikcmOgINbEbckGlFHYmxdil0/hNn/LJNS0gcIz2SaPYeIpQCBlYKroSwzExUhb5uTEK+yZwCmmcQgrTsTBtKwhrEBctBtd6MrJcrS1XqC2VqS9pC2VtaalDQJxV9B27rQhcQ8+L2PnB4T0FEYnFtPDcByMlcHI2Y5dMk50bCzyXdBxx09YKDRHESw9DzMhQEOxKLaSvaqvT6tNr1BbLVE+X8X2JMLevvID4sLQFsqE1s5VNMXbxFKmxbBS+5KwgrrlQOhwLQvdNo1Rn7YlFqvMlLTSrBkqeVrj2aOWfpFPhopWxWiMpPYVX92iVGpSfXgtCwmivDukrZMtDul4QSkT05vvZk9jZBVLF/u1BoOgNFUTCCH29DHzXpzK/SmGyyMT0FJZhIQyD/L5x5v7ZxThOGr/p49aP0Cg1KOwfY/KKOWYOXaSFgoEngbj+AFbewco7VE6tU5lfj7zNZq7eR35unPLpVVrVJqalE11jGgihMFMm6WIG1ZS0Kk1816NwxTj5uSLZ6RxCCFqlBlbaJjNTpL5cpnxihcUfP83ascVg3ncnqg5CT3XMjeC+N02mr97P7PUXY5oWQglataYWhE4UqCyXOHX/UcqnVqgvVxg/MMHEZTNMXDpLbmaMRrmO13SxMzZKaoWKMAxSYxmstK3vzZRDdryIm2pi2U1a5Tp22mbfdZdi2hZupY5XbaKkZPzAFBf95JXYmRSmaeG7Hr4nmb3OQvo+8w8+xcoRfU852TSz119MejJLs97Eq7fw6i6pXAYnm0Uqn0apwrHvHWbt6GKPIh/QeScMg6lr9rH/WQeDsDxBaB5DkMpnqa9VOfYPj6F8SXYiT6qQRtg6BJQpIFPIoiYlfssPvKo8wOuahyrKQ2HaJrnZAhNXzGCG62owWIaA3GwRK2XRqulHrWka2GmHqcv3cfGNV3LiQRtlKKrzJSzH5uLnXsn4JTM4mbT2cijXUJdpb4Z0IYdtO3jNFvVKBUwdWlIZkJ7IcclzrsSre6AMnFwKJ59m4fBxTj96nMr8Om69xeShfUwf2kdh3wRWyqa+UsUwDdLFLOWFNRYeO0Hp5AqV+TUmL57jkudeTXlplWa9QSqbRXmS6uI6q08tUFuu4kkPYcqgZ8Ik9g4HfvJKpg7tDzxEgMCbU6JYfnKe+fufwmu0MGyTfddfykU3XKa9jnxoNOpI6eNktQLLsAVK+Sjlk50eZ/aZBzEtA6/h8vT9R6ksrGOYJqlshrlrL8XOOfrt9YTCbTQpXDTORTdcoZU24zkqp9dprtfJXXUpfsvlqfseo1GqMn3oIor7JknlM/iuT7NaJ13Mkpsq8vRDR1l58mnMlDYEcDIprQA0tXFAdjKvDTUCRYXpmOy/9jLmDl1Kc61B+elVrFyawtwYlzz7ELmpMfxW2wPJOKQVPyfvf5KT9z+BUpJULsX0FRcxc80BmrUaXqOFX/ewUilyU0U816VRaXDivh9z8p8eR1hCh42LpmnMaytBggQJzgGsr6/ziU98Yujrr7nmmj2RLyZBggQJEiRIcPaxqwqM7o2X13RRddkhdxYQhWhpW6eGAtG26Vnkuh8oBqRUeEGc7zAHRqNUp7leD6ysfZx8CjvnBMLeXdgBKrBSFrnZIrXFciAI1cl+RyXPVZ3/aGXOmIMzliE1nqEwN67Dh+RS2LmUjudthvGn+1BT4DVaeE2X9ESOzESO2niZ2lKFVqWOW20iW76OJd1tVX620F3lIBY6OlltKXTLriHwXrAzDnY2RXoiQ3Y2R/HAFNmZIs6YtijXMfGNIC2M6CKhIjrSl1F4mHQQ7sjOp2msVWlVG0hXWw23lVhthMqNrSodOiKnmQIsm1QxQ26miJ1LdcQ3j9raQWCLfdQHkT9JRENEdQkEXsPVipz1OpWn11h/conaUpnGahXf9XXO1sDS2jBE1B+hsDOIkhL1rQ4tpfAa2nMgXK8iz5mABWFAmLtnb066mIJmJzwvBgx13/PCfgo97YLxUL6kWanj1loIpXMHCNPAyafIzhRIpTJ4dS8KqTZ2YJKJy6ZJF9O0Kk2qiyVA4eRTZGZyWqntSrxaC9M2UZ5PZiJLfl8R39dKp/pSBTtn6/tDKfymT2O9RnWxFIWqkZ4OedUsN1CeT2O1Rno8p8PSTORJ5dLUViqsH1+infeEdgM7nnVhL4nA+FqHLPIaLp7fwm/6tKoNnFyG4v4p8tNjjB+cwW+5tMp18rNjzF5/MVZKh7Fqluo01uukCg4IdN8Fc9rOOPqeqLvUl8u4jRZeo4VSCieXJjudx7R0zg4742BnbIoXTzJ5+SzNUoPSqVUd6seXjOenyUxlGTs4jR/0mzAE6fGsDulU0+fXlsvIlgQpyM0VSI+nWfzRSSopG+X7kYIHCEJHEXmztGqtyCvQa3mB5f44dsaheNEk9ZUK9bUqTjFD0fUA8F2P+nqNysI6zVIdr+F1eA/q6D7tB0GY4yE7VWD8MleHqBRBzo3gvSQ7ncdwrPZrSXBNaixL/qJx0idyOLk0XsEjlU8zeXCO/EyR6mKF2nKF+lqF1FiGXLFIbrKAk03j5NOx/ClagYcEr+XRLNdxax5jF0+RncpRLE/QDMKHoWDiwCwzlx3A8109NxdKWGlbe34UskxduR+v1qJ8cpVUPk1hbhxhC6prZdxKi8ZqlcZ6FbfWjMIdhdNQKUVqQr83jF06RX5ujLWnlqitVMCT+p3iwASt+jjlkyt4TRcr7TB+yTTjl06zfmyF0vwqrttC2AZOLo3p2Dp8YZCnwko7QXgtE7fW0oo1qVCGwrQt0pM5HTosr3OIpQoZCnMTzFx9AN/1qa9WtWHFahXDsrTyPvCAka5Pq9oMwsS5NEo1DNOkuN+isH8CDCg/vUp9tUqzUifTyCFsA6/uUlsuU10q0yg3MExIFTLkpsYozk1hOw5CQG6myPjBGQr7J7Acm9UjS7RqOqRb8aJJihdPUV2vUF0tUV/V90RmLEd+ZgyxJqh5PpX1EqbVwkpZZCcKFOemWHvydBilLFoqVOg+mCBBggTnEO68805WV1eHvv4jH/lIEqIoQYIECRIkuECxywqMOGQQEzmwGo/kjm3r7R5hnxAD5WtCBMJSgiSZQKvUxKu0aK7UqJxcY/zKGcYvn8bKO5i2DVJuQaI2OiildAzkYkbHWrZAxENZbBoaYKsVRRWSLmYpXjJF4cCkthzNOjrxaExJFAlkY1XL2HfDsXBsEyuTIjc7hntpk9pShdKxRUrHteWvcn2EMPf+BlucxQEfFgqE0iHTMhM5xg5OUTg4SeHAOJZj6+SnZtDRPmgfAQFBEunucNmhTNBKWZhTedLFHPn94xQumWDtyCKrTy7QWm/gtXww2F5uDNX5v5EysbMO6fEMqWJWh7/pyaK7gwjXFVOAr2PPrx9fYe3JRSon13ArOtyNkkFi4i4nKNXdoAAdXRPcRyo0qUcgQg2GGkRhL2I3XNN60daltgdDK46CxMRSJ/EVhkBYRuTVooRCGSAsgZNzKByYIDuVp7y4xtqRReYfOIF0PayUxUGupXjplPbu8/U9I4SmgVAYpqBVbrDwo1N4ns/UNfuRvqS53mDliQWWHp1HKR3336u1KD21rPNu+D6+55GbKdKsNhm7ZIqJy2dwilmEYWmFBJvp7GJCbGD16BKV0yXtyeP6+K5PfnYMJ5siPZ5l7MAkrVKN2kKJ7HSB8UtnWH58npUnT1M6vkptqRIkmxZIqUM7ZcezGKbJ+GWz1JZKLDx8XCfx9nzsfBo7m9YJ6JWPVAo7pz2o8vsmMGyT0vwyR/72R8iWj8DAdVtMXDlLbraIAhYfOaW9M6Svc4Gs1ll9fIGT9z1BejxHfnaMAz95OWOXTGvBdD5Fq1oPQvMEfSBABkYKy08sUJ5f12Hxmh7S98nO5HEKGQr7xinOjeNWmjz94FN4vmTiyjkEOuTS8hOnWXjkJHGPsHhfh055QqHXANskPZFHSoFhgGGEQ6JPTI/nMG2zg4YeV4VEJ2A2TSsKFZTJZ/FqLif/6UnWji/SrNQpXjTF5BVz2I6Dk03poD/KACkiPhqlGqfuP0rp5Cpe3WX/jQdJz6QxHIPivkkqp8soF8ampshmCjx1/6PMP3aM2nKFVC5DdbFM8cAkkwfnKJ1YRUnwlcRHj0tjrcbph46xdmxRK+FcT4diM4X2nJD6fsvNjDFz3cU4eYd6qcypB59k+fF5lCeZvHwOO+dgp2wKF42hlA6dlB7L4jd9Fh47wbG//7HOVzGR45LnXIXlaIUYov2uppREKqHnW7jqCv1sk3j4ykNKH9PReU5y00WyxTwnHzjCsR8c1sqpeovFJ+YxTKFDTXkep+snotww0tWh4WorZZRQZMZzTF6xn+UnTrPy5GlSRa14ylmC2nKJ+YePU55fR0lFbrpA8cA4GAKJwpf6WTl+YIqJg7P40qd0fJVjPzhMbVkbpxx49pVkp/NkZ/LM/bNLWHjkRPCbwvdcKvNrrDw5z8LDJxEGjF08wYF/diXFqUlsJ6UVtIN0Fnv9PStBggQJgMXFRf7rf/2vQ19/ww038K//9b8eIUcJEiRIkCBBgnMJe0iBEbPAhi14xbd/7XUWUJFgKE5HejIKOeHWdYxwv+GSOzBGdragYzGbRjtBbzdPo0SMvmGZOIX/n703i7Eku887f+ec2O6SN2+ulbU3u5rsptkUx6JIw4SHEjwwPR4DAw+kN70IepIeRjD0Zkg2rNWADQEaGDDfDBuWXvxkQy82x2jLNE0ORZCSaHWT7GZVd9deud+8ayznnHk4EXHj3rxZlVWVtTQ7PjK7MuNGnDh73Pi+/9KgtdV1yWKHE0A+Vdzi2bAbEr8Z0NpcYuniCksXVmjksaiLpKs291YpxIup4WthITwlD2Uemkj6Ag8fPwrwogAvUPiNkH4ekijuj7HGIIUox+PMerPCAhdhxMirKwqSvjh00k3njs87ZLxQ7rZiEYyxRMtNmutLLF1eoXNllcZGm2i1VUl+WszZ2Q455iVR7ac8j4YXKrxQ4Tdc4tOgHXL04R792wcufFKqsU/poWTz3BdBGNI+v0xzYwnpK4QSLnb7GcNOE3DM/CukmyBxb8Jkf8jhzV0OP9xj+OCI+HDkQtSBi+UvC/FHzBbCSfN4Vkmd9RYDrCM1j11bNat9ESgS1wOzAZ1O8LyoHnsWa8RCwSKfuAdalyvFC/Mk6dK68GiJs6gv8iwJYVG+y7UQLTVQnmK016f/oMf4cEg2TsBaxgdDbGrA2ONOMbkXiNEmT9SdOItwbTBZRjKcMOmNkJ5ABQrRcFbkXjNABU7EjZYbZX4aL3Jh3oTM8yjYqmCfzxsx1/KK8Cg91x613HChFq0j0b2Wj4oUUnqEHWfFH7YbedLkmN7tPUY7fRemUeVCjwDdylxi7yRD4Kz8J4MROs7c/h35LoGwcEK2MQYReERrLnlzlqSMe0P69w+w2iKlYrjTI1ppsnx1jajbRPqy9FLSSUY8GLsExXsDtNYIT5BOUhdyzVeoQCJG8+suF6ZwuST8ZuCeYZ7LexItN/GbgfPCaUUoT5H0Y9Jh7IQucgv8wYTJ4RAVei4XlnQhvNx0rzylhHD5V5KM4YMehx/uIosQPpVzOuczmt2lqfAvKvO2MLBQgqjpPN2EJ0n6YwYPDunfPyRLUlTgEzRDuhfW8jmRzw3ytWCd181wp8/gQQ+TauKjMekkBZHnImmFmFi7Odb0CTst2ptdl7w9CohWXF6HIHThvqqedMlgzHC3x+DBIcOdI+chJmfbmjvvEXUadC6u4Dddbo3W+hI6SbHGOm/OMCSVEr8dIaWisdREeh7xwIWd7N8/cHPJGIb7fZenrNtCIJ1oUzG4qOZHyr+e5J4azsNNepIoahK2G0ipGB8M2b1+L1/HlmQ0cQK8MajAKz20vGbg2m+gfW4Zv+ETtCOU72GNJRlMSMep86jSlixxiecnRyOkEOjlhjO2EQKLQWCRStLoNmksN5kMxs5jY6eXCyQwOugzPnLzrr3V5eCDXdgdAGC0ZnI0Yrg7YLTvBA8VKNJhgic8lFJTj9aKV1JlWTy776o1atSocUb4Z//snzEYDJ74+t/93d+t97oaNWrUqFHjY4yXSsBwmJIWtrTHEzPHqZIDj1OqEFilsBZ0nNL7YJfxzhFr8QWk5+JcS19h9aNKO3sE7Yjly2scSUEyGLt2P+WXNJuTfEJKom6Lzc9dZel8lyAneQBH9rnfptdVXo4LwlVWzyq4YZ2z50K4eNxXN2istIm6LfaDB8TvuhAqPCtX3znPgmP/Pq5iUtVqXhZYi7WG5maHrc+/QmO9RWOt6Sz8M7OwC+YbflI3lGOpXaJVvxnQvbrO8uU1vNAnHSeM94dkSeqIpSeZj8VStY5gVqHH0oUuzY0l7FnGSluA+Z0DCtFGcbQ7YP9H99l/f5v+vQMgJzHlgot42JRYMAJVQY0TvEtEQZa6P16OKfcS1KIiXhzH1JvFGLe3Bc0GXhTkVswZ6TjGpqYkf4XEhT0KfJRS2NzrJhnFTkCLJenYhZQxSeaEXFkQ0BXiVliQAuVJpCdcnD+Rk9VKIjwXtk0pSXOtzfKVdTqXVh15rwTSz4n50M9FRTvNeVHxKnFecHPTTxS1cA1qbi6xdm2L1lqHaLkJBaEfBRhrSJMEGSqXCDjwcGGhEib9McYaZCidRX0BKaD0XHHtxwOhBULLXBB2VvDC5knRJXitAOFJ0uGEbJJSOFJZCckoJj4aYc2KE2uUKHPrGG1IxpPcur+aayn/ryiEM5knMTe59f10fbXPddh4/bwLJ7TcdNWWCq8ZYlKT38+FQ5RSIuSU9xWycnxB2LrCGxHhjB6yccreu/e4/Z3rqEC5eUM+blJw7tOX6F5cr+xldjpslUK9ZuASpVtNliRkiTOiUJ7nwpPlgprIjRfKe2BLwwurHYsvcjcQa5zxgcHmYaJCrGegJVj71BadS11XESlc8nEjnFeLruSrsjDpjRjtHZElGULm4prAPX+EqLQKvMijsdxA5M/1C5+/xtZPXUUg8DwPP4gY7PYQSqJ8hR+Fbt31x+hUu/KV81JLBmOXAyQzMy0uu60qrFqLi+k3FV6cWBUiAw+jDek4Ie6PCSIfFXqu/pnz3AkCn6WtLiuvbLJ8ZQ0v8BG4BO3OeEWQxVkeclTlIQPzsRAyn09yOj/cCLgfKVDKrW/pSbJRQjqI3TpXLuyYSTMmR0Oaa238ZuA8J00+UywuLFySld/NjAFrcu8Xirwr+TrM//cS7Ng1atSocSrcuXOHr371q098/Ze+9CX+/t//+2dYoxo1atSoUaPGRw0vRsA4btA891nVIvgYBXnytY+4Qcm/FoRMZoj7Ew4/2MFojdVbLs53VMRkrnASVYO3+Xs/hc5gcZaEYTti9doWOs7o3dwpY73PN6MkeCq09JReEDPkn5ACvxnSvbpB59Iq7a1lgnaI9JR7sTcFaTTXoEeQt/NBcBzX5MK3hJ0GnUurjgRTMHxwxHh7gEvuIefKfEqfjCpRPDNFcitFUfn7NCiueU4ihl3whyjqkccDD7sNWuc6rH7yHEuXuniRj/I8JwicRP5XCc/i7xNOnfHQkALpOcvczpU1hJLsv/uA3oe76NR5LxXC4qlaV1mGQkpkIAiWGjQ3O4TLDReSZIFY8DRT4qRihBAgJZODEZPDMfvv3ufw+jaT3PJeqCI5b1Hl6YDYYwvwhJsUQfqL4wumd84JzlxTriJRveCkPn42k/NEzaDAGRu8LW6BzUnTitW3tRidh4rKNEYYpJR4DZ/m2hIrVzcIlxuMBkPi4dgRnHZaYQvOij7TLsyLEoTtBkEjdEnZAS/w8HwP6akyTJOdE7MEoiQuhaVMqO03w9ILAilQoU/36gbLl9ewxtC/c8D4cIDfDGlvdWmstfBbYTlP3FJ34cqQwiV6zyxCgVRiZi17kY+/1GD54jrdi+uMD4fsPrhHNooJWiErr57DbzlS1uZehjrVWMBruMTEOk4xqUZKWyYKL8IGulY6slUoCUa7XB+mut+7BMakhmySYLTBW2qiAuel4eL052R6O0II4XLJaEvhJFZY0BdeNLMoFc/KoeJZbAhaEUG7wfKFdboXN+jv9ji8dROTaIJmyOpr5wnajTwPkCy9WVy4RonK8wRZozF5vqlCMLDlM1cU/y/noM4y0kmCMQqZqel8kAKdZtNcHfY4CV+0J4tTl9RaKJQXIH0XQsxkmcuZsdRAhh4GgxEm32ltyd2X34uqyqy1Je2vk4x0nIAWMIH+rQN69/ZyocTkU1QiURzdP5yKR5bcm8iUz0BRdkAhodhyS9OxJh7ETjAzhr337jHc7ed5gpwwFA8njHp9WusdwkYDvx0SNAKkL11p2i0yN54RqNxDB/fcc/mDlMvHZSxWa1TDCUDKU8h8bG0uWpg0Q0iXbDvshAgDNnNjLj2F5/m01pdZv3aBRrfF8EGfZDBm0h+zdG6Z7isb+M2gUBELTQErQHrKlSMLYcHlvxG2WDc236dMngPH4kchfjNy4oN2zznleUStBkIosrELe1b9iufWhJnWoVz6uZhUWSuLtuOTclJVvyXWqFGjxovC7/zO7zCZTJ74+t/7vd87w9rUqFGjRo0aNT6KeLEeGAvfqabEYSWIwuzHj+LvSg7xOHtYfS8UngsX1bu5x3h/gBf6SKVobXkIpbDZvPX0Q5jgx0WFpzTWEiw1aKx0GO0cOYKnUtfFt6wy97k1cEHc2bztyoUuOf/TV+lcXsMaXZIVs3WZ7adHvuqKY78AThCRvqS12SFoB4SrEbvv3GP0oI/VFhVUa/6U/fgIHesj6WJcmdvWOEvn5sYS57/4Cu3zXaJuE5s5ghNhj+syC5q8cKjmTpj24VTUWrq4ysrVTdAQ98ZMehP0MAZpT0eIVEUUK1zy20ASdhs0NpYcaTWrw50pjq19KZGeZLTTZ+edexz+eJvh3UOUr/B8f67e7vqCQlykny48sHA8ZtcIVlTWta2shSorWTn/GKpK1zNA0f5i6B527lnrKML1T7VtNicvbZb/ACLwCNptWltd1l7bQgSK/s4h48EQmSdbL6pnrUWnhizOyLIM4Qva6x0mO0P2svsIYwmaIX4jyPd/Oe2HKZftfiyIglDOLCryCdrKJQrWFuFLvGbA2qubtDaWuff9m9z98/fZeec2zY0Ol/7GJ1l/Y4v2+WVHqhtREvWFx4DJwKTFXlntCAiaEa3NZVYurdPZ6LL9V7f48Fs/Yni/R3trhTf+z4DlS6tIH0ycEfdciCELBJ0GrfUOkwMXGgpfoXyRE9rCrXsDEoWUCqWUSxI+0c7qP5+jQoL0BGZsSI7GmA1NuNxyVvCeRCcuf0y01KS5tuQEjDh14RtNsZ5EGX7HtVLknhZ5U4tHmS1CGQoMjjT2myHLW6usXNygs77G7e9c5923/oJsFNM+1+Wv/V8+G29colCfCu8OYwzK8wlCvxQdjLIoCb6USCkwgpm8NYWgVuTB8Bo+yndkdlG2ECADl4PIClGKITOeXMKJL3F/hPI9JB5B2MCPQoQUZEmGCjyXx6QdOgGjKmKU5eQ+Cnl+jekMESgk6Shh0hshYgF9y4O/uMX73/kB44MBOtUUnpKN5aYrSxU5qixCSKSQ5d9gc6+7XGDLD0sk6TBhtDOgsdoii1Nu/Onb3PvLDxBCoQIPFbl/hafY+OR5WmsdWuc6zvOg4WGMhlQgjKS5ukRzveM8X8t90JQ5iJTvvChMpvEiFxrNC3wnlAiJyTLSyYR0nDgRqNOgtblEvD8m7btjKvLwmwGd8ytsfvIyyTDm/a//gN337rJ/c5sLn/8En/q7n2Npa4WgGVbmocuaLXLvKSFyMVTr/DvU9BuqNSb3AEvQSUa41KTRbSOExObPaz8MaK8vk0xShnsDsiQrPbncPGM6b8o9sTKJytVS2aCqHz1Db8YaNWrUeBrcuHGDf/2v//UTX/93/s7f4ed+7ufOsEY1atSoUaNGjY8iXsIQUs8bLqSM1ZbD93cw2qAiRdRtlpZ4QE6s2flfnxjVywu6zWLwmgHtc8uk/ZRsnOWiywIRZ0bcqZA/wr38K1/RfWWD5Svr+O0Qm4eys6qjdAAAIABJREFUOX7fs4c1BuEpGt027XNdOldWmRyOSPqTMlH4gkwAj3ePs3hZnyuiqgEt+vzskVsiF6SXmFpy+q2QxlqLzpU1GittvNDH6vz8Z5h4fMqFGLIsY+lylyy7yPb/vMOoN0B5sgxfUrRhccsK5s7NB7/p0b6wzNLFLn4jcB5O+tkTLi4slAuhk40S+jcP6N3YJR3EzhMpt85efPHJ9TudF8pccfmV884xlULdOhWVAycX9Ewxk1fm5LPm/n0aCCji35cajaS13mHj05dpn1slPhrjhT5+5BM2W0TLDXSa0b+zy/YP79C7uYu1wnl6CYHM/2etJR3HHN3eJwgDWisdvDc9gk7o5mYYsPzKOmmSOnJV2jK0k0DmZLvL3iyVRzKI2b/+gOVLa3Qvb3Dlr19jZX2V8XAM0uaiScbK5XX80Gfl6hoqcnHvo+XI1UoKhLLOUn1/wPLWKuuXtlA/63F4c4ft928RjyaAKKdhOo4Z7/ddXH9Psf76Bfx26PqlEdBYbyH8/HlmIEsSBts99t67TxBEXHzzVZbPrTI5GuH7oeu77X3G/RHj/QHj3oBMZ6xc2eS1L3/OhcEZp/T3DrBYFCrvUUXajzl4fwe/EbJ8ZYPly+u8/pW/TpZlWCyrV8/RXGpxeGeX/Q+20ZMEb20JpbwydFNh21+Q8wqFj4cSxX5IuQaL54aepIz2+yTjGBkqNt+8jGwql2y84Z6dJtMIcPk0AkUapxzdOWD1yibtrRVe+fwbrG5sMIknjHoD+ncOmPRGTA0U8vFXTsyRSKQVFe+eWeMKYV3dVRFo0TrhTVqJsh7SSkxqXJ6VGCajEY3lFle+8EnWXz1HPBoTdRs01zuEnQgAaQXKupwQohCAS+uLaVhHKZzXkPUtVluSQczh/V2Cdsjm6xdpbrQZHPWw1hL6kfOsEZbdGw/Yu34fYSRK+CjpIZWHEIqCGLfz+58AlGWwfciDd25x8ac/wdJWl9d+7nOce/0qmU5QyiOKGkz6Y452DwHDzru3iToNupfWOf+ZTxA0Wlht8BoupJPne3jKQ0nn2ZL0Y/p3Dli9ukVnY5WrX3iD7oUNvLZPY61N1GmQTVKEJ9GpZrjbZ9A9ZHBwyPK5Lm/8rz/N8LBPPJjQXuogDOzcvOdy1QxHhO0mV774SdaunWOwf0RrY4loqenyyvhOvBMGRjt9hg+OWD6/zsqFDT715f+F0f4RcTwhjVPS2HkgSZyIaYGDWztIX7H5+iVWrq7z2v/2WeKhszheeWUTvxWxf2uXe3/5PsOdnhPA8r1qqnaJUtCQQuChkNaFzqs8Vqcz8DRic40aNWq8QPzTf/pPSdP0ia//3d/93TOsTY0aNWrUqFHjo4qPr4BRMWRWnsJqw+H7u2RxSmurgwpcPO2peOBCBhSGonOGwk9dFSdfGIJ2yPLFNY5uHZAOkzzpcHHmnPXdMTkiZ/+ERYaK1VfPsfrJLVTLnyFcZ646YzK0CA8iPUljuU26lbL86gjx4R7x0bgkpk7Fjz7WjfN/BY9niTjn0SNgNqzRM2QFilj4lTu7eWAtfjuke22D7ifWaay2kZ50IT6wObH0NCneT1E346yA25e6+J2Ao/sH8KFxnTOj8pxUgDvHWjDGoiKPzpUVli6tTgWMCiF49g3IaygdoZ0MYgZ3Dul9sMvRjV1U5DvL6cLL4qQ2PLSZj1P5qVdAaXUvROWTaXkzOVrPcJAXtXMaOmcBHnXvKnv2mDhetCPubG5pbzJDNnGWzKvXtsguutAsUSciaIT4IiAbJxzu7rD33l3ufvc66TBBSg8QLuSZMtjYYLUmmcT0PtwjDCM6b67S3ujQvtp1icCVj8GQJRmmsLoGMAKbGkych1KyLpxM0p+w9+49wjAkfPUS5z99hfOfusL+7W0G+0dkk5RkMGFpY4Xli6ts/LUt0jgmS1K8wENPMkdCK4j7I4bbR6xsrrO8ucrK+gZ7W12O9naIRyOwquyfdJxgjGZyOMRozcorm6y+eg6dZWRJQjyeYIxBWoVJXZ6Fwf0Dtt+5w9brV1h7bYuVq+sYown8JuPDIbf/53ukN+66BN/7feLhiPZml5WtTZdPY5Jw45t/Re/uLjZxuS/QkPRjRrtHBEsRG5+5TGujw8qldTKr0TbDxyfruz7f+cFdstgl6EZbbGLyhOlm1sswA2Lr/rWV5xn5bicl2SRltHvEuD9E24zVa5t0r20ALsdONopJ+jEmzQCLDCRZnHB4c5elbpfocoNLn7kGn36Vg519dm7cJT6aMNrvlyGCXJ4BF7rMpAYrtfOwKad6lTh2wgGxgcQZD1hr3P6Zuvln8jk03hmSDVIGBz0a6y0ufOYqxmiSZII2WS70BvkdJMJIyCwmMZhEuxwYxd7hHBUgAyud55xJDUl/wv6dB0QrDTauXuD8516hPz5ASkmr0UEnKePhkGQUs/PDO5jYYCZgtRMQReG1UhGPSkgQWAY7h0x6Q1Zf3WTttfO88jc+jTWWcTxAoWgHHQ5u73LrnevsfnCXnXfv0L20jv1rV1m/domN166gbUqWJcT9kROGMgPatTw+mtC7uc/S6iqNV9pc+Ow1tt58hUwnGKtRyiM+GmOtJYszRrt9jpYO6N3fYe3cObauXKbf7zEej1ldWic9TOjvHdHv9xge9Im6LbY+94oT6eOYdJKQDGNnWGGNC09lBcOdPv1uj/STCd2r51ldWyeJY0bDIQ/eu82d798gm6ToWJd7+eHNbazWrL6yyerVTdrnlnPBIX9uWsHRnT3u/eUNpJK015fc/Jhk2LQIrUUpUtjMYmLt+qdcKJUddO7PGjVq1HjZ8M477/DHf/zHT3z9P/gH/4AvfvGLZ1ijGjVq1KhRo8ZHFR9fAaP6wld4NHguDMPuD+6jY83Gpy+iPA9tsimfeEYuDMcuNUBmiJabrH7qAvEgZrh95CzEH+bxUSERHSEsiLpN2ueXidZaeE1HFmN4riEGXGJRTdhusH7tPHZi6N/aLzkpWyRqfWTw/ce98TM+/wzhknBO/W8AF0ffd6Ewli50aay0HKFmTkgG/YxhARl4rHxiEz3OGNw7JO5NXLiLU6hQuXE1fujRXF2i0W0iRJ4I+Jn2fR6QxFjQlsH9Htt/dZvRbh8V5iFf8vqdtI4fXb0TOmD+kK0s08pndmEOgOm5Z700qhWqSmBPLiaekcpXGOJLwAiQcHRvjxv//ftYY1x8/jyHggpdjH1SyCYJ4+ER/Z1eHv8+ACEY7h5x/y8/xPMVJjOM94cIA5PDEbvv3mW83cdreOCDjjOSo5jNNy9z7rNXQFuySYbVlnSYsPvDu/TvHRD3J4x2B3k+BM14f8j2D++QjBKUpxBSMtrtEw9comwv8mkst1GBQpuUdJKQjhL8ZkDQaXB4cxcMxIcjDm5sY8YZO527CKEYHfaZDCauT/KwWkWCY5tZdn98j2SU4Dd8pKfQSYpOM7Iky5NCK45u76OkYtIbsXf9LsnRiN3rt/OQNc6zJBlM6N3dY3QwRCjJcLvHjf/6NlGrQRCGaG3QScrBBw+Y9Efc+f51hBQM93vOy05Khvd7fPCnb7u8Is0IK6wTgWJDOkw4uLVLNkmRShEfjbj7vesgBaODPqODgQsdpA1xf8z9H95ksNdj/+a268fMzM5OC1YbtLHs/ugO6WiCCj1niZ9pTOrGzSSabJIxOhiAtehJyvDeIfd4n+GDXj5eguHhgMFuj6Q/QniCIkSTUq7fHrx9Ey/w8TyPwW4P5Xsub0hZIydY9HcPufm9H5OlKUf390mHE3SasHPjDlmWcnBrh+H+AGsc4X7vzz/k8IM9POFyGWmdoSKF3whYu7ZF58IqxhjiScLh3X2SNOXw9i7pMCYZJ6BcRvL+do9b3/txmRh9fDAEKejfP+ROdoPDm3sE7RAtUhACTwRkcUo8HHF4ew8vCtj74D5GGPr3DxjsHJKN09zjaG6zqi5W6UTpe9//kMGDHkEUuXloMjCgjGJ0OOBo55Dx0QAQ7Lx3Dx0b/DBEBR5ZlqLTlHQUg4CwGdG/f0CWabTWHN0/4M73bzDYOUR6Aiudt5LONEEjJJukHN3dY3LgPGdGuwNu/3/XOVjZobHUItUpWmfs2Ftk/ZTevV3ieMLdP79B7+YuUafh+j5JXfLscYzXDpG+C0eHEphM039wwIff/hF7P76H7/voLCMZxQx3e0x2+tz/wYeMewP62wd5uCjDaG/Ah9/8Eds/uIMXuZwrCIueaLJRxuHNbSfcGUs8SLj/zi2O7h9weHuHyWHfiVQGkqMxD358CxMbtj+8mxvNuHBrLrH33DZco0aNGi8h/sk/+SeYJ3yHkVLyO7/zO2dcoxo1atSoUaPGRxXi//693zz2llpY+bnEjy+O4XWUzdzbWfle/ZQhiKrhT+zUCwJcuIa1T27xib/9GYJOiBH6xHf5p0bFEwTA5uGVPvyvb3PvuzdcombLbIxwUb2m8LoAkznLze6rG6x+6hyr187RXG07sticYGn+jF5+izkkpUL5Afe/9z4f/Ok7ZYzoPHsq2Pmgz092n9nQM08HK2xhXDsVjo6N0/T8xSG+Kp+fdJ8qmZ1fJD2JHwWsv3GBS1+6RrTWcsk+q+eWHhhn7sdyHFJgjWFyMKb3wR73vuMIIBeKRJx4+8K5RAgXM3/l2gZXf/bTdK6suTA91pak7LOYg0W1TKLJxil3/+wGt/77u5jMuBA7OKtYIawjiReWcZrdb+6M0kVr+nHZvCJ58UmYS3pjnVPCk/naLLDMtfmELj+yhYhWGpMfH4tHrSlhz2j8pnH2rQUZKmTDQyFRQrlP8xNc4t4UnWRonYAQSM8vva+EVGWCYgHY1K0fFeYJnFONyvNVxP0JwwcDPvW/f45P/R8/zYMf3mTv+j32391htN3HizykEi6HQx7CzVpAG4QvUYGL9y896RLzTjJ0phHKlS99BcKSTVLSYeKI6naAmWj0xAkOFvBCD+m7kEXGaNJJnCcyzsffyryXcit8IVxCY19hkhSTGrR2YyE8QINNwWiXvFz5rp5e6LtcFWmGjrULUwiOwLeAsfjNAL8RuMTnqcEkzjvAa7tE3ekodYnvcV5UOg/fFLQjkK7Pk36MjtM8p4QbYeUp/EhhsC7UVGZxj1aX58ELfaSv0OMUk+kF7oGFEJ9PFAF+K8SLfJdgO9VI6XIVGGMw1uY5VAxo6/rXV6jQjVc6StCTlFLszO8hEY509nGihhDYxLqcWBXvPEueW8NX+GGIMZYsTpzYbDUq8JG+h544Dx6EQCqJagRIT7pE08bNqWi5SXO1xdZnr7J67Rzv/b/f54Nv/BCZ519QSoG1ZIl2op5xzwrlu3khjMBojdEuYT0WpO/hRe4HQGeg45R0NHEhqTyFinxUoMiSFJNo52VqLVa6736i9Aqb9xPDzQsLYTt0Yyc9TKZJ4gSdZS6PhWAaLtAIgnaE3/DJ4sTlIUlcn3qRcgJPnvAdY/EChRd4qFAhlGDSm5AlGUE7QkqJTjJsZjDWOE+F1OA1fbymy1UipCDrp26didzHVYMKFGE7ctM9NW79xCki8pCBQufeLmVDpUAFCj/ywYKZZJhcMPOaAV4zcPtRnAEGaw1aO2+tsB0iPSdgpIOE5Mh51Uqv2NwlXsNHetNcMbbSyV4Q4EUhaZyQTCYIhAsnVXz7kwv23zKJTGXdCMmLwB/9P199IfetUaPGy4Hvfe97/MzP/MwTh/39xV/8Rf7oj/7ojGtVo0aNGjVq1Pio4uPrgTFHTLuwIXnImzQjPhrRu7lD68Iy4UrkrBKfcVXAkeFCSRprSyxfWWewfUR8NHZxw6WYajfll8EK6SkdEdXcaLN8eQ0v8ivk9/MVoqp0hzEZQTdi9VPnOPxgl8H9Q0e/y5fUdLAgcp9bl+Ukkbb4SyGrr52je20DGXknfumXM1c+QxiQVtJcamHWM/ZaEUKpkls8+f4uBIsKPFaubbL2+hZBO6qIks927IVwQmQymnB4fZvhg6OS+IWChzyNRdhT9LCd4TvdtHpYzo3iboK5+POP31czUkhF8Fw8tWfUsce+15mgooVZgbP+H6do6wi74njZrsx1rlRebjkvnECEBaOdYCvy/TJzyZ9XXt2gc3GVRrsJSpBlKcIIpPVobrZIx2MG9w85eH+XZDhxlubauGTbxVRR5L97gEVnBqtdXGebi8RCuVrqOHNeAcIR6UIJ5+ExSPMyioTuTgQw2iBEmouxdtondlbCKgRTPckwsc5DFjmC2OZ5dMrOUnm+DYDUkJkUJLlnEtOk5eDELQU6TV3idEsZEhBsmZPJGkpCVEicF4V1Hituwlus0Yhp9KtcGzGkkzw8ExZRLj83djrV6MyFlppTCmbmyfRXSxan6CzDGKd8WVOIf3b6iJT52daR3EYnuReYxmJKKbhcHAJnOJAV3ZITwpKpUC6K8ZNYDdk4pQj/58rzMClYrTHaVUIIiLotLn3xGq2NZUgNJtXoLCNsRzQ6TZIkZee9u4z2+24OGBeiSufPcZHPm2muFeM0RGun1fdkue51YjA6dn1iwWon4BZzyCQZNtMYkwtjZV8XguKswOm0c/eL8F1eBpNmpNoAiRtnk/dpRbwSMl/XqcszY7UTxYr5nSX59xSTz2/l5qiOczEhDzkopXB1zsfTTU3h5mHg8t24sE6ZG3NtHMmfTwSZr990lOTLzLi1KBVW48I1GecZV26FFkxiSHXi+sxYsAYrQaca00+wVlNsEkIIlHTnpOPEfaez7nkofUX5BCrGIM4waT7n5ia6TjO0diHJZMXronxOsAgif/bMWLrUqFGjxnPHb/zGbzyxeOF5Hr/1W791xjWqUaNGjRo1anyU8fEVMHLYyn8LIsUaSzKccHR3H9X0iFYajyQen/j+cxxN8c7fWGnRubRKMoiJe6PpOQXTIkqOi4Kckb7CbwZE3SaNlXZJVL2Q99f85bwgFP12SOfKGqO9PvZ+xfr+CTE/Fo9T0vwoLjQ6nxe4Trj2qVAVoqyzFPZCn6ULXVrnOiUxWMzRatJYeKgDxNkhn3Re6BG2I8J2hN8ISuvXBSdX6uRyobQvdOnkuS9sTvAdSxD7xHVj8eDn9c5GCUe3DpjsD+fOfZhAYE/+6FF1mR+RqtIzpxzMG8meUEJezGxFpvN/2gnHDNarxrmiqILIx2Cu/Jm7P85qEk+4jBePf7kfWovNXNioauQBUW4beRplqSpdPw3IVroAFe2VAhX6hJ0GrfVlhBQk8QSlfMKoSZqM6d3ZZfCgx3hvgNXOetxagy1ElOLe0v3i9jaLNqYkuEVeN2xBXtuyvlK60D82MdONvjjdmOl+LnAeapUOyVsyJSVxHgGm2pe2aLrNhyVPBiykI7gNWKunZHRBMhfl5f3rvCoy5z3BtJq2sA6v5mUSzmMBCybVFF4+QuT9Yac7l7UWbU6gXEVONuf3E9V1c+zk4rgL82M1FOGfSl+K6sRHuGehdf1M7klTEPEVqrfa5U6osS731UmeRkJIMBZT9GvJMQt33FimXoZuP3RzcAmhcV4uWYYXeARRyGD/iL0bD5gcDqc3MbacztNpI0rBLP9/OXcKw4CivVY7txWRj5fLq+UIequty68gq929aG+c+6Tk3134KrRbJ8X6KJKuU4yJEKVnDEaTT0ZscePMzpXr6q+NExjK3FkynyfFZM33AVu021pMZqdCk6QssCpU6USXzyDhMmm7fq5oZ6K6XxvnvVF4xYGdhkLUuRdfaVUg8r3DrYmp8CPKEGQFLEUZxTSZnYvGGGym8/6Ule3+pAdJ8ffTfb+qUaNGjafFN77xDf7Tf/pPT3z9L//yL3Pt2rUzrFGNGjVq1KhR46OOj6+AUSGPZuMBuRfhbJTQ+3CPxkob+QkX41xrnZ/07Ghja1y4p+a6i58+eNBjuNObChd2SgpMRQBXn6AZ0troEC41UIHKyQIzV9vn/FKbk49BM0Rc7BLciFDe04WN+kmEsI4sFZ4gWm4SdZouOetMou8p3NFnLl9QhHfJrMYoiNZatNY7DLePSJPk+DBWl5Jw4Tqaa0s015cc+fQs8rAs4Nusseg0Iz4c0797SNwbTwmsyrli/vqiSHFMb3g4ipOP9UchNtoprz5f7olc0ynWyIwgI0592bHzqnV/nCE6s2U8JcvFlIbOCVEo80CUpHO1rlOxoqxQYS0PCCnJ4pT96/cZbveIlpt5KCVBkcwl6Y+J+yPGhyNMaqZCAoCwFWvsqjiSD3tBXFb7ohRZmN2phZiKEyUZWrmmomjPyAdiSlS7v8V0/uYCgrvMzpKg1bGUs/1cllucV6hDwuYizVxziirOHZsltUW18ONlPGSKluuzWvlqWbbyochnybHbzdVufi5PByL/PCfyy48ts0vp5AlePonF/LG5uuf1stYyORxy83/8iKjTwG+GzgMmFxJMphnu9xnu9dFxhvRlxUuxMmdmNJrpWJVbq5neE2nddLOVtVFtkpzbQsr2VEe60ollMfkvlnIOFSLDtOur14vZ6/Ljx4Zsfj3MT2QrH87NV1WHSteJyi9lCM6ZxVcRIKq3rO6L1SYUeSikAQzWBR6r3Dtf89bOhQIszjHT9Vbkji/7s9q+qafq/BZd9UirdMBs22vUqFHjBeE3fuM3nvjaKIr4x//4H59hbT6++I//8T+ys7PzyPP+1t/6W7zxxhvPoUZPjizL+Df/5t+c6txf+IVfoNvtLvzsJ6lPatSoUePjho+vgFFiEcki0KlL1JoMJhhtkEo8Ey+MY++Z1r34uwStksZam2ilSTrIc0c4k9rZS6wjSP1GQHN1CT9P3C2meseLhQXlK2QYEbRCvEbgQi9khQXk05d/ikOPfKdfOLLihA/mWbynQU7MSeVi6nuRjwo8tE6fDeH/2LBOXFEQdptEqy0mByNSG89a2RYwjiiKVlq0t5aJuk1UWIQzW9CeM5+jAqs16SBh0hsTH43IJmlueTzlL6fGrHb+cqYEteDUQtGCdhTWutXPjpe2iCyeYT+P87An8LNzly2u5qKcLVWy7xiBeTKeyezMb13k56Ak/4p6LmhcWd3cE6WaJ0Y4K+fJ4YikP2FyOCzzQRQW0OkwJh1OKpH55sj0Sl4cB1uOU9UTalbDqhLs+X+tLT0bZsnanJmeZeOPnVYtep7sLfaKedK9eKZNidvi+BwxXf2jrPe84iZmj1fKLwuvTM6imjY/Z+7JNb1+XjHMKznt7rkGi2ldps+4eeJazPZTlRTOjQFstZLVPWH29Lk+mMeckEKlLFv0s/M6y5KU/p19RjsefjvME4O77xtZ7JJap5MEIfNk9XPtKnPBLHj+VEMeVYWFst8ti+deWefjao8t12JV2BDVgWGm8YvcwKrnVdbNzN1EdV+qjH21uGruh/LUYs1Mr10oOi1cWjODNFP/Iod5ufdX2lXcTliRj0e1JfnflrnrpseOtesh3kbFvexDvsjNPi9YMAY1atSo8Xzxta99ja9//etPfP2v/uqvcunSpTOs0ccX3/zmN7lx48Yjz7ty5cpLT9ZnWcZ//s//+VTn/r2/9/dOFDB+kvqkRo0aNT5u+PgKGA9j3oRLXKyzlGQcE48mBCJEeuoRF54hlEAIRftClyxOOXhvm2yYIjw7DdFQvNQaF1/Zb4Q0N5bwGgEWu5ArflEQUiI9Qdhq0FhuMe6NMGlydi/bZ9nWOR7suUCA3widp4pXMR9/4ZgSL9JTRCstGmttjm7tT4kyyZTEsW7tCClYfe0c629cJFxplIm7nweEdKFZRrt9xvt9dJJirMlD6cymPl+kXRQEU0lYn1TtKnG1aB5XeKkq9zfPtx6rx5NMvIWE5OMWMc/0PrqAx5B3TlmJvMNzZnqGdJwRD+T0ePHLQ6rrQhq5Qc1GKdk4IxFxWZ41FkQeZb4csAoReUzYmP1sZk7NkbYnVqu8rgwOx8mTbfH15RyzdvHlJ938MebGAk3l1GUe02VOfbPpn3bms3nl5hQFnPLUR1fsSWd6LtSUYwU6yTA9U/GeyEN8GesSdlc3j5lSSveKE6p/XDSbacKJtXvUQ89SCopUtj4xPz6zv870WNWbcGaTXXTf6R506vn3MDzJOjjNfM83drGgxfb4iY8u7zFQLofiwlq3eGmgteaP//iPX3Q1+NKXvsRrr732oqvxE4Pvfve7fOMb3+Du3bsIIbh8+TI/+7M/y5tvvvmiq/ZMMJlM+Pf//t+f+vw33niDL37xiwD85m/+5hPft91u84/+0T8CXJ+//fbbp77253/+52m1Wk987xo1Hhe9Xo+vfe1rvP322/T7fbrdLp/97Gf5yle+QrPZfNHVq1GjRo2fOHx8BYwcJ9kLFkgnCeP9AUIKwk6D3BzumVfIkT6C1noHNEx2R6TDxIWYKs91JJrRLhmoCjyi5RZe4OfJTB9F9jxDVIkX4ZJiCilRkY/fDomHsWujeLI6Ho+RfvoxeWne8+2U6hRS4Ld8/FaAUI4WnqE/X3SlrUv4G3YaRMsuRNmUl6mQUtrihT5+K2TpwgqdSyuoSD0/8QInlplMM9rvMzkcuhjjRf0Ej06iPfebWLjkH6M9YmZRU12XVVJwthWPgZP0hnlx5oQ8GqLqQjCX9+BheLoRrRJ+UxaznO8GJ4TlHmcC64Tl8tpKHy4UQXNieM4i2xZixUwDXNnimHX6lFwuJ0A1XFRlLi3ur+OSUNULYf7awtq92oSiFUVLqkvtIc1eeOw4ESwWnTa3Poo+Ps2NZo8Xu9vJGvUCxnvuwDFyvOTZF0lnixSUBX8/dOIWCeFn73esvMed/O5hV15srRNZFxZ+7NmWy1yC6Ryo5EZyl8ytbYopOjv4x7eIqgh7Qr1P/B5RFT0qHWPnRFsok8GL/J5CTOM1nbTuqnecb9/8MIj5Tx6ygT1qHcwW8/Cd8OTP84lmizCQ+WIux1Y8tI4FrLVTzUow9TxZeO2L/pJQo4CTPF0pAAAgAElEQVTWmj/5kz950dXg4sWLtYBxBhiNRvzhH/4hf/7nfz5z/Pr16/zpn/4pX/7yl/mVX/kVfN9/QTV8Ntjb23usefztb3+bL3zhC/yH//Af+M53vvPE9/2H//AfsrGxAcC/+3f/jjt37pz62i9/+cu1gFHjueHP/uzP+Jf/8l8ymUzKYzdv3uT73/8+f/Inf8Kv//qv85nPfOYF1rBGjRo1fvLwsRcwToQA4Ul0nDHeG+I3Q8LlhvvseXCx2t0kWm6hlEfv5h6T3pB0GLvEkAURKsBajUkzpCcJ2pHzFDH2+dTzMaF8D68RVLwMXsKX7mqVnkMfWqwTd5o+qumDApee92UaQJeE2G8KgnaI8lQZqqckVQyQQbDSoLm5RGO1RdgJ8yS0zy5jxwwVLgRSCExqmRyMiPvuS6UUU8+LxTHrq7DHpuZJJOxjt6k0Xc4JTcuxyC1n2k/zcYNOrNeTVeBsQtRN6dayMtZAZrBKItRx1thdcaL59lzpOdU7jZ00E//ecYmLGl05x0CeqKYUfMpwPlS93abC1Omx2EWhIKCLxNiOtBZTTlktonprPA2Kx+qJqI7zI0/OL5nTLatG8wvdF06uWSkAlKKawKVxWbiATzsvFrdhNv/SbPlT8WTqDTVfSvVva6xLKJ6rGrJIeP8TN3ULgYlS0C/Cn1kpEGruvEe135J7hzHdB2eUHV6urwk1avyEQWvNP//n//yhXgBf//rXSdOUX//1X3+ONXv26PV6j3X+9vY2b7/9Np///OePiT2Pg9dffx2AH/3oR48lXsDj17lGjSfFX/zFX/AHf/AHLt/oAhwdHfH7v//7/PZv/3adjL5GjRo1zhC1gJFjGuu4IC8F0kqyUcpo+4jGWsslITZVxvHZv31LJfAaHsuXV7HasH/9AVmclQScxaICn6AZES03pxb8L1P8KCi7SvoSL/KR0hFxT+iAcdrbzeKkLhFzJzyple0ToNShBHlcfs8ROy9agFpwbytBCknQiehcXUOnGYMHPWzqLImlFMhIsXx1ldVPbtFca53G4PnJURi5UqW/XXJVmxmSownJMK6Yq08JIJu3UYjpcq4a5D/c0tV98Kh2CSumpDlTK+Jyr3mI8fNM2YuElGKqPiym/fGSZqtYrc/cOjzd1FtMvJ8e83PclpbJfisibEUk45hkPHECQrWOp6lgtT2V/VDMfYYg9/iw0w8r5LT0JF7bd7kKjMak7scJc+TdYF04oNLifI7ffkg/VXM4HOOMc0N1awQq8FCBB8LtnTpO0Wl2io6o8biYEUUxM0fnHxen0R6sBQqHi5kE7LNlzIz/zCQXhUE/0lN4jbAM3aeTjCzW09qJ6jXzG8bx6pVixPyeAhRuQcX6KROnV/bTY0uxuunnDQrbIX4rdLfRlmQwIUsyxLGO+OjBVhpQ9rZ1Rgl+030fQ4CONVmsK5sDM3ua23ry50ouUHmRhwoDl+RdG0yaYTJzTO+tUaPGs0ERGuZR+Na3vsW3vvUt/ubf/JvPoVbPB4eHh499zVtvvcWv/dqvceXKlae+/1tvvfXY1zxJnWvUeFwkScJXv/rVE8WL6nn/6l/9K/7Fv/gXlbxmNWrUqFHjaVDvpjM4/pKvRwnj3T7pKH4hvWXyfAKdi6usvLKJF/kuJI4Gq0FnBq8R0N7qEq20UE0f4b+8wyp9hRcql1D5ZRNZKngunECVU7d5ovOcoHxZrCoLUszm/0OCvxSx/OoGS5dXQYJJNTZx81S1FN1X1jj3U5eJuk2MNrPeF2Lu54xr6+QLAZklHcRkozT/aLHg8LBj9hE/p6hO+c+ipp4m/UvOdT55dz3qQlG5yfw1p/l5WlSTPgMFSxsuNVh+ZZNopeX2XcnxOj7J7RYeE1Nrac0sMZ2vy8ZKg8Z6C38pQkX+lKTV+XUZjy8CVfllcs7S2PLHWIvBOi3TCFToE3WbNFZbNFaaKN8HU7OXZ48Z34Hp3jez8iu/P3JDcMmejbaY1GK1cON2qk1k9j7WgFQe4XKLxkrbPfMjPw/RVFzyGIv1NBuaAXTuQWGrL+uFwrag7EK9y4n4aLlB98oa3UtrdM4t44U+pZPhS/KsOzPk61h6gmC5SWO9TWOj5YxLTulYaXEiiNcIaG4sEa00CZohUikw+bZZtaWpUaPGM8HXvva1U5/7X/7Lf3mGNXn+eBJvhm9/+9uMRqOnvvdkMuGb3/zmY19Xe2DUeB747ne/y/7+/qnOvXnzJu+9994zrlGNGjVqfHzwsffAmE24yjFrOJ1q4v4EnWiXBFiY47z7s+CQZsg6gRf5tM4tceFnXmX44Ii0P8ECMvBorDRpri3R3FxyOTJegpfaOdvR2d9zzuN5UW+l9bW1mJwcLGJTT/MAgJASIYUTV56DpUShU1StNl1okGd+68eAqPDLLtGs8hWtjSWkuEC00kRPMsgMKvTwmwHLV9adF5Bhmly4Ul5R1qktmE9C5frCowJj0TpD68yJJ6Zqup/LMXZ2rdtqjgp4KN93GhQWzK6YOYa6+NwWg79gIcwcmjXln3pNnKYip6inyOt1GjVlwQ0edtXD9cnKmNjCUNsiPIn0Jd3za1x981PcDz4kG8Uko4R0nCDm73nKas/nzClKsdpgtGHlE5usfGKTo3sHDHd7ZOPE7fkSom6Lc69fJehEDA6P6N3eZfdwSGOlzfKldaw22NTQf3DAuDdCSjntTvHoSlrAGoMX+nS2VvCbIcYYJr0Ro70+Ok6xxtDaWGbj9QtIpdCTjO13bhEfjVw+ATHnZfJE4/nxw7F+KrUCO/Pv7CmncwUqNC6rnZfk6uubhO2I4fYRk96YZDDBZCYX8/Nr8jBhU0wFA2stRmv8Zou1184RtAIQsP2DOwzu9/B8H+F5c+2o1ma+oYvrXnpbWPesbHZbdC6sYrFooxnt9BntDkBaV/e5skuvSilZubLJ1uuXkJFEhoJJb8Jwt4/0BY7NP0EA+chgUd0Lr5MG5z/9CtFqE6sMO+/cYbR9C4tGerIiNBXXMN008+8C7XPLbP3UVbJRyuRgzN6P73E03Mfl7Hl5DVVq1PhJQJqm3L59+9TnX79+/RnW5vnjSbwZkiThG9/4Bl/5ylee6t7f+ta3ZvIKnBa1B0aN54EbN2481vnXr18vQ6PVqFGjRo2nw8dSwCjCICwM1VD9VYDONHqkMYlGWEef2WMnP6MX8EpdhC+JVltcWF9icjimf+cAgKDTIOo2c2t3jdHziUFfEObI+eJVvYjf/dQG3CfyRgX5MledPGRV+blxv1trp0JCJdn4NM69zc+rcJEzltaCajLumXuW0+N4S4UowgtNCX5jXJiIwt73heKkwbEW6clcNGuz/umLmExjkgzlefhBQJalZDqrEDSmolwV5ZxdVcvbZI4J1xZ0lmG1zfm/gqAXCJsndS5EgYo4MDOmTIdtETG8KERbIXoW/8q582x+vSPrK0qpWFBO5ZqZgw9ZNCfV7WFktpujDwk59zAi/BEeVI+meHH9YEShYCCkwAs9llaW2bpykcnBgMPb25jUkI4SZhbfsbVYrfZcm/K1b6t7uLXOKj42LF/e4JW//SZ3vvdj9DspowfGCXO+IFyKWL26RXOtTbi3RzKcoNOMaLXJ1l+/jEkt2SghHo0Z7h4hfL8MG1Pcu0xo7Sp3rL9NZlFtj9VPbNLaXCbTGfvv7zA+GGF0jDUZjW6TtdcuoqQi6U/o3dwBY0AVQlAxj6flV5NdL058vWhMyv/MHnoUymfq7ICcLpzhybNlVgwvBLwFz+x8LU89ER5+p5ly5s6t9l8pdM7IAVUhcXEZBUxm8RqSc29eonNxhft/eZOD93dIhokTWGdCNi2obfGcsAabZfhNn9VrmzS7LYQQDO4douMUKRRSzQmchZpg5+pYaJczLZhLeG8MRmvCbpOtz17BAmmSsJ3dZnC3l4sQAoEC8ueZBWEsKInwFOuvnueNv/sFhr1DBnt7GAOjwyFIS5Gd+mHz42H7bVnPBUnnT+jMh9znkdtZXvqMtHzsHtNjlqARsXb1PO2LXYynGe0Mwdx0+a0ECOT08uqaq4jU7c0OFz53laSfMHjQZ7R7xNHtXew0Ac5HWv6pUeNlRpIkz/T8lx1P6s3w1ltvPbWA8STho6D2wKjxfPBx3xtq1KhR40XiEQLGqSiojygcqSnsSbHsp2+WorCWxr3kzp//vF4grbUYY1Chorm5BIAKPaQvHWH7koVkWtQvJjNkscZoy0PJ0dPgZCNSF4ZFO4Ja+pKw08BvhfjNII8r7SM9ifRUTtQYTOosqZNRTDKISQZjkmGMyXRO3KspgVEQpycQqNiTP5qpfwUmcUIZVhwnYF8yGGMc9154jUiXeDzVKcbmItqx+BbPqD3FPCis0LXFZM77womV8hjdOKNfnsJTYV50m7l+rpiTipsZU3maCfLRx6NHPN+Hi5j6xjrvgvdukw5Tjvb2GR4ekU5SsAaTi1RC5l4HJhcYsQilkFJRGiYXvG/uiTOTaFu4RMIFSSukwAt8lKdcfp5Mk8UpCsVw+4gPv/UDvEZAEk84uneAHmskCr8RYqSG1M07PUlzzxZven/rhBmp1LTV2rh9RTlRwWQp1mhk5OG1AkgFUgpMnGEzg/Akh7d2MV//KwQSk2YcbR+Cl1uwa4vJdD5PK+nNpXQJ7JUEyTSZcv4gK84tQgMJC0JJF6u3yL9ReKtpk/f3tH+lkgil8nOLRVgR/3DPTKN1vh4LPj73UpE4jxU5XaFWG/dTiE6QC8uAVAipZtZNuRaLfcCa0tOuEJ4FzrPOCUv5/sp8YpXZaWkyQ2EJX9yHXOCWnlMATObaNRVKK+cINzZZnCA9DyV9wqiBH4YopVzuiiRFBZ5rg83nozHT7xzCIqTLe2GtcD+mcp4okrwLjDHoJCvrIFVlXRXrwLoxkMrVsfBAsuhyvxZCIpTEapdfAwt+I3KfS9fJ2TjGJ0AKD1FxorDGkqWGIPAJO028RoAxmp137/Lhd3+IjjPScUraT8FKjNFYm82EQhPKzVchpJNUrHX5Zkwh7ReiRi4EeHIub0e1PsZ91yi8AQsxXbg5Jz1Zjr7V7jtDmSC9GPRiPJU8po8Y4+ZqvtBLQVoiGR8O+ODbP8BvB1hp6N85QHoCqyU2MRhr8zBa03uJoj35OrEG992kWD+5h85P+GOjRo2XAq1Wi2azeeqQSBsbG8+4Rs8XT+rNcP36dW7evPnEeTDu3r3LD3/4wye6thYwajwPbG5uPtb5P2l7Q40aNWq8SHwsPTDmseh10AIIQ5FU9oWIOIt4FWuRgSJaa1UPlp4XL0vokOO1cCSKSQzZOMsT5oonFjGqhNWUTC4IhFxs8CXCl/gNj/bWMs21No3VNuFSA38pwo/8PKeIJk0yslGKHqeMdgcM95y1o9gfoCepI3KgTNxbeGiIRUz2rEnw8Z4o2b1ZYUyPM/Q4A+NIPV0lUl5CFIQkANIRaVrn/TTT9EUm8mdQgbzvqzqJFXm9zJR4LG4m8s+n1+VWw+J4sdWDzktHzDpLiIeTSNXzKgXNfige3g2LRv5MZ8MCz6Hj5T/6jie14bR1LSNZIbDGomPN4b19+rtHaJ2iTYqwwokLudV3EfZL5gSuFTlJWRCCxZhJgfC9uX3RCUnGaqQPfhTgN0OUrwgaAdFSg6TjvCywlrg/Yue9O65N1hG+QbNB2G4QtCK00mAEYTsiXIoQngIpnceeEG5tGIPN8vWSi6Eq8LEYjNUoXxC0Q/ezFCITSdAOCdsRQlistIz3B0wOhmUrtDYgnUdJEXpLCIEQhVBiy3VQ5NWQUjrRQ7q6idwTyErrxAvjPMFsnrsGYR2hLCVS+FOr73zvsgaMtVj0TPi7md5WEi9U5WYprEQamctOOk9QPLU+V4HnyP9jGestRrv6TZWQ8iO3Jj2BkMqR80xPE1bkQoyZI8NnBYyiRKkknufnEY4qfh+5ImPytew3AjfGhvLZU8BkFptZvDAgbEUEjZAgimh0mkTLLaKlBnFuhC+lLBM8zjbN1VEbi011eX9yIapI6I4F5Xv4jSCv51SoErmAppRywpoohCxcPiolcgEjF05wZRrjQh2F7Yiw3cAKAx6ES26eK+Xl/Vx4vbiwRtIXhJ0Gy+dXiToNtM7oPzjkwTt3kUq4tiLzkFf5BmBlLmzZfD4ZjHH1lEK457l0a2m6iiXWGjKTTude0W/5+Ejl9gwpVC4sOrHBIvJ5rkuRSvoKGSgn+FE8nvP/FQYR5YRya8IPXL1E5cuItS7XSTpO2Hn3ljsmLWicUOp5oEQektT1H7lA7tqdC3g6z4Wjc0Fubr8WC36rUaPG2eILX/gC/+2//bdTn/uThKcRA9566y1+6Zd+6YmvfVLUIaRqPA98/vOf59/+2397KkPDIAj43Oc+9xxqVaNGjRofD9QCxqNQZkp8yYjkl5jYXgT38m3RSUo2jJ1161O9d1fUC0RuYUlOuEFrq0Pn4gphp+G8L5oBXsN3Vta+Qnoe0nMWuVIofCnwPIVpBQTtkPZWh2yySTKKiY8mjHaOOLq15+KW56SHeFiejEJheaRAU7K3pMOEtB9DZihi/X+0RvklgMgtyD2Zk2Vixmp8pk9zcvJRvXxm3jDzuTZeMB4tXjx7zOa0sKXAtP6JLS68+SrbP77FzvXbeIFP2I7oXt0gbEdM+hMQ0FhuoQKFFdC/d0Dv1h6TwxFJf4LfDom6LZYvrNHeXHbW1lIgtWLcG3J4f5egHdG9sEbnyipZltDaWkZFHmufvMBwt8/+9fuYOGPl6ibCk/R39/GjiNbaCu0Ly/itAC+y+C2fy1/6FGuf3OLw7j5xf4JSHkE7Iuo2GWz32PnB7VII3XjtEhuvXmBw2GMyHtJcbdNa79A5v0LQjvBMwOYbF1laX+Hw1g6927v4zYBoqYlUCpMaDj7cZrDTQ4SS5lqb1de2aCy38EPfkbsGBruH9Hd79O8cEPdGNNc7uZi7hN8I0JMM5SnCpQYAOjPsv3+f/fcfkE1SbGaIVts015ZYvrBO1G0hpAUjEJmg/+CA3Q/ukgwnpMOkHL9Sq1OSaLnJ2qcu0FpfcnuvUAgkyWjCeDCg9+Euhx/sYgV4oc/6tXMsba0QtvJE5daSxi5EV+/WHocf7jjPhxlrfJebp7nWor25TGdrFb8RYD3ACoQR9B/sc/TggNFOn8nBGBf5aG5/zsns5rkua6+e///Ze9NYSa67/P9zTm29b3edO5t3x85GEhJiVv1YIsELUEyQgkARiFeAAAESCIkgCAghIKBIIEBBgCAvIAgJxQnKnwQcJyQQTBzHjnd71rvf23t37XXO/8Wp7tt3Fs8dz9hx7H40d2Zud9WpU6dOner+Pt/v8xiyoVqekh/h0CfojWg/vw0alu5eo7LcwHJMMF8AkR8QDn16F/cZbvc4+fZbWbx9jfJqFRxN7WQTr15k6c41uuf32T+zhVN0qSzVKDYqFCslkIaU05ki6I/ZfWaT0XafNBQzQfS8z5lGJYrWLUss3LFKlmVEA5/ehTb+/gitFcVGifqJBbxyEbfo0Vvfp3txn+V7j9O6bRlhmeC6VNLIk+10UTqjUC9RP7mA1yyAADfzOPHO22mcXKB3vs1op0/Q80mCGLTAqxUoL1do3rJM65YVyst1Mp2yfO8JnLLHcKfLaLdHMoqxHYeF21cptapYtm14xyxjsNulv9VmvD8k7PsUWlUqSzVKizXcapE0jLEth2q1ybg7YOOpMwSjMVmcThMZdGakIStrTWprTSoLDQqVEuRjmsUZ/Y2OqfTyQ5IoYfHOZRqnlilXK3jFAmA08IPxmMGmmXfRyGiy2wUHt1ygdXqZ1qkVQx5K0JkgGgTsvbBBGiVUF+vYRRcsQffiHp0XtqmtNWndukypXqdYraClIWx0qgh6I/bPbuG3h4S9ANSr41kxx/VBCIHneUfePkkSlFLX3hATDDtqopI1U/U3x0vD+973Pr785S9f04+hVqvxwz/8w69Qr14Z3AgZ8PnPf56f/MmfNITtdUApdWTC6EqYV2DM8UpgdXWV7//+7+czn/nMNbd973vfS6VSeQV6Ncccc8zx+sCcwLgaJt8bLRC2MDrjk6DBjEnvqwWXmtQevPGKd+WKmEiQpEGcm5dmN14tkutuaIXJZrQldtHGrbg0b11k4a5VCo0yhXrpoEJjkqmpJtmVJinVwkJ45nYQdTE1xlWJIhqGDDd7CEFuwOqj4mwqN/GSz0PnmZt5hU/mx6Tj2MgfzeMWLwnTbHTbmgasZ3xwmVrk5oFWMXM/Xw1XrMC4rk5Ngqx5Bu0lx7tSm5fW8Fw2HV5kDbrWfLyUjLniVLsJ68aLNyEO/tEH9yIChITaWpNT77qbNI7pre/hlDxKi1WW7z1JabHKYLNjfChqZbxqAafkUayXkZak88IOQWdMuVqgdrLFwu2r1I61yNIUlMbGwesUiJOYYqNE85ZFnFoBrRVO2UXaEt0sY3k2o50euqBYuPsYVsFGnBXYnkt1aQGvXjBVXhZIW1JZbeCWPKIgQghJoVamvFyjcqyB9ZzF3lPruRSMprrcYO2tt9HZ3Gaw36HUqlJolKeVIEJLZLOMVyqSRjHj/QHlpTqNk4vYjkMaJvjtAeP9PoVmmdqpBZbvPUmhXkIobfwQhEVxoUxhsUIaJMTjkEKjTOP0Eq1bl3HLBcZ7AwSCYrWEXXCxig7YEIx8/N0BSZJRXqrTun2F5sllvFoJlSagBJa2Ebagv7dHGiVmPRMHifACEFJiFz3qJxapH2+a7HYhEcIiyzKyLEFnmtF2H43GrXos3HmM1q2rOdEhQAuSKCIKjaRf/+L+wcQV2lTjaFPF4FaKVJYbNE8t45ZdlFBI28K2HQr1Am7VYzfOCDr+tL+TtrTWWI6FXXKpn1xg9S2n8EpFHNedyjeGgxGjPZf+egeUprbWonlqCWHZyLyqJUsTE5BEEQ7H1NaaNG9ZxC45aKlwKi6251Bu1UijhP5mm2KjRO1Ei+pCnVK9bHwSLI3lOUTDgNiPSMOEZBwfJkDF5NmqKbYqLNyxgrQtU63TCxnvjkBnuGWPxTsNIWNZNkplREFA645lVt98ijSK0YnCxibsB2QosiyjvFiltFjF8uzcJ8KivFzHKXnE45hw4COGYloBIW0Lr1aktFilvFzHrXgonVFsVpCWeb6mUUKxWqZQLbHyhpOUWtVpRaYQgkKzjFstoLN1ot4Yt+pRO9middsqpcUaQXeIzCxq1QXsdZfdM+tEfkA2W4upFUJK6msLrNx7klKjglP08kooUxmjM03n/I6RchPQPL3EsbfegucUcGzHVEWqlCSJcCsuiR+hsow4iPFqRZonF1m6Y42lO44bKSylEdrC744YtntkccLibcfwGiVwJHEYs/fMBk6lQOPUEpVGg1KthhIZWoLt2AS9EZk2lTbRIHxVy0jOcXU4jsPHPvaxI2//oQ99iMcff/xI2/75n/85jUbjpXZtjuvE6uoqv/zLv8yHP/zhq+rYl8tlfv3Xf51arfYK9+7lxWAweMn7DodDHn74Ye67777r2u+RRx65IeJkMBiglJpWNM4xx8uFn/7pn2Z3d5evfe1rV93mO7/zO7n//vtfwV7NMcccc7z2MScwrgoTUrMcG6fkYrnzoboRaKXIMkj8mHgUoJKjZZu9eKOglUBnGrto4dUL1E+1aN66RKFRotgsIR0rl9c6iFRdzch2EoQxqirZdA+n5FA73sCtuoy2enRf2MXfG+K3R3mRxdUkko4eBTb655osVaRplmuDH3n3OXIIYbwGpC3ByqU7rlFkcaMSSEfu2yXtvkq4xRvCzTiHqflvHlAWlkBbmkwmaEsZksCVSNdGOkYeKIkTRjs9dp9ap7JUZ+H2Vdyyx9Lda4Q9n/bzO5QXqzRvWUI6kv7mPt0zu0R9f5pRHwcxUX9E/8IeK285zcpbTjPYbdPf2CfsjPH3R/jtAcVWBW2BVXDwKkXGO0P2n9ymefsyq287hU6Mb0f7mS36F9sEvTGW5+DVisiChXDIs/0nJ4yp/nFBOAKVZuw+tY5lWxz7llsoLVbIsozhZpfe2X2GWz3GnSGFetF4HrkWUmVgC+ySx9IbjlM/tYh0BYOtNp1ndpC2RaFWorLaYOnO4/g7A6Khj1M03j+Wa6MyRX+rQ9A2MnmNU4usfsstFBolWrcskQUJyTiieqxJ4/QSYX9M59wO490+WmmKtTLhKCAJE1R29fU8ixOGm22C/T5hf0yWmODywu2rrL75NOXFOqXlGlor3IpHqVVDCMn6V55nvNdHSAun5OBUXML+eKaiaoa0z+V+siRl3B4Qj0NUqohHAV6tSOVYk1K9zPKdJxltDeie2z9sZ6CNh0ixWWbpnuM0Ti1SaJYYbfVoP7c99dOwi+Z5kqUpOlP0t/bx+yOivjmetCSt25ZZeeNJKgt1BrUee89sMNrqceq+uygv1xht9Rhu9xnt9Bnu9Bl3hiA17v6A8W4fFWekUYywJUv3HKe8VKNxcoEszhjvj3KJJaYkLNJUT4S9MeOdPgunjuEseLiFdVMVoC2ccoHqWhOtFIONNk7Z5dibT+NWPMadAbtPXGS01cMtF5G2JAlj0jChe26X5ukljr/9NpCQxjGdF3bpPL9N2PeJRhFZnJlgkYbEj+idb6MVZFFK4/Qi9ZOLtF/YZuur5wgGQ5TKOPG2O6ivLZJmKXsvbNC72EY6liFMmlVW7z1NNAjor+9jucazys79q7IkY7Q/oP3sNqO9Af5gRJqk6AMdL/PHETRWWtRaTToXdxjsdIhHAdKyKDQqjPcHpEliCJfGIo3jC5QbZXaf2mSw3iHLTOVK645VqistkrvMNemc26G+tsAt334vWZrS2dxhsN4h7I5xywVUluF3BkZezJNYRRstje8IQpCGCeP2kNFWnyzMiMMIy7NZumsNt2o+w2RpTP/ivpE8m38QmIARUG8AACAASURBVGOObyje/va38/u///v8/d//PY8//viUWJRS8o53vIMPfOADrK6ufoN7eXPh+/4NGw8/+OCD101g3Ih8FJgKjtFo9Jojk+Z49cFxHH7jN36DT3ziE3zyk588RPi1Wi3uv/9+3vOe97xqpL3nmGOOOV4rmEflXwwaLNc1ZpSec9jccY4XxyUxJhVnJEFMNApI/AiY6D/f2DGEFFi2TWmxQu1Ek8ZtS7RuX0ZKgbSk0ZOeiIRf0rVp9y7pxOTLySSLWDoSzyviNYt4lQKWtOgXPLJMGzmsOOWwn8d1fljRk+NCFqdEfZ9oWMYpOcZ8dz7nrgMTk13LBL4ciyw3N87fPnR5XuxK3ZCE11UqJPQVXntp0JM/+eHE4fcOOvKirRgvlytQetdx4i/lw/n0iJeXlhhNeYHR3M+Ds8Ys2pj7aq2JBj6DzS79i/vEw4BCtUhlrUF1rYlXLZhKC8+ZSiPFo5CwawK84dA38m8ZJOOIcDCmcqyJlBbRMGSw0WG43iVoj0BgMqgFCEdiuy7xKGLv6Q2sgs3SvWuoKCMZxXTP7bL/zCYgTEAebYg0OTkzDgSzJGAbskYpxWCjg840zVuXcaseaZIw3O2x+/Q6Yc9HpYosMZ4E0jbVZkIKnIJL4+Qy1dU6o3aP3sV9dp9eR0hJoV7ErRZYXjhBaalKab+CXXRybxBJFqcM1jv0LuwTDwI0moV71rCLDpXlhpF10sbnwasWCXtjopFvAr9RQjj0yeKUxI/zLHY9e4bm8ipFGsaGiBCCaBgY+UApqB1v4pQ8CrUixVaFNIqwXRvbc/Nr4TPa7eXEgYNb9kz1m7rCfM19HRI/JhBjQtsnizPSUUQap1glh0ou0eVViwhbMq3Im1SNKI1T8li4fZViq4JKM4bbXbYeP2c2k4JCvYjt2iShCeyM20Nk3ycaRGhlCIzqWgOvXKBQM1UG3TO7DNe7rL7pFJXFGmHXp7++z/7z20TDELTxohjv2+Y6RwlpaMy9qydalFdqlJdqRKMQy5WXEfBCGCnEoDtisNmldfwYxUaFQr2IVyugFXi1EqVmhXG7T3djn3KzSnW1ZTygVEbY8xlsdbFLYyzHQitNPIoIu2Ms22L5npOAJg4Dehf22H78PNKyjYH7pApRQBqnJKEJ9Fuuhdco0TgtGO702Hr8PEhtZL6W6tRWG2w/eZH2mW32n99GOhbVlTrHv+U2lt9wgspSHa9axC64CNvCcowcjt8d0b2wx3C7Szw08k96Kr+TW1zn67xbKOAVCmRJht8dEnRGAISjgGgUkkQxlZU6S284TqlZQWWK3sYeW18/j0oUtbUWpSUj79Y4sUQSxAy2O5QXayzdeYLtp86x99wGvXN7+O0RXrWAtCXxKKa8VDdeF46cErNCCBI/YrTXJ4tSsjAhCWLsokf1WAO35lFeqhL2RghrLiE5xxyvFpw+fZoPfvCDDAYDtre3EUKwtrZGuVy+9s7fhLgZUkyPPvoo7XabhYWFIx/zkUceueHj9nq9OYExxysCy7J473vfy4/8yI+wvr7OaDSiXq+ztrY2Jy7mmGOOOV4mzAmMq8IYnDoFl1Krhl100Tq7LHgwx4tDIEBKEt9ntN0lHoSoRJnA3o0827Uxc3VLjtF/v3OFxTes4dUKIE2AVCn1otfrCO4UMDHwzLN+3XKBpXuP49VKCNuif36f4UbHBBUv0Rs+2unlgW4NUgqyKGG43sEpOtROt7Ady5iHznFNTLOSMeatXqlAWPDIQj8fYHF5bP9FLtLLMeo3zdfkmo18E9d55EYKEolAICdm9xrQmixKGG32GW/2UH5C4seM20OcehGvVUJYEsuSJOOIoDuivFijutqEDEaLVWLfJ+z5jPdN5cHEyBdMwF1nxqhbZcb8GC2N3NBkSJWGFMg4UKXTCq2MAa8Q8mD4tUYoEBMCbfa66YN/Z9cYMH4LOsuM10OmjQzaJddcSInl2BRKJaSy6J3dp3+hTRZnqCQiHvmM9/tEowCraFNcrmJZVm4OrEiCiNFmB3+nD0jSMCX2Q3SmsVwHgUCniqgXEHV9Cs0KCwWHQqOMvz8gGgTEo5B4HINSzJ72VN4vzchCQ1yXF2ssrqxhuRYKRXm1RkaGcCRO0SEeB4RhQOrHlOpVlu8+QXmpRhLE+J0ho/0h0SDMqz0OpPsmxW46U0SDAIDqWh1vpYhdcHBKLk7Zw664KEuBLbAcK5fqy9vCEBRO0aW4UAat6Z3fZ7DZJej7SEsiLUkWJACkSYpdcMjCjMJiidpyE8uzQWiqq7XcIF5ilxyQAhWpGaI6f6akB9c8CWP87pDqcoPGyUWE0FgFm/JSBaRAFmwjYWYLM86ThWQy4BKC7pj+epfwHp/icoXSUpn6qRYqySi1SkhbEg0DOmd3SfwEYdks1JeoLNZZfsMJCtUScRgS9MaM94ZEA99UmqBy+TuFUqY6MMsykBaWhkM+QrlXiLkXMtAqf9n87haNj4vjuahEMbjYpne+TRokECT0opTGySUyleCUXSpLDZyix0QqMh7HdF/YpXNmhzRJ8ntjQgvP3CAKdKpNhVAcUz3WwC7ZhD0fvz1k3BmQjEMyP8IrF1i8+ziJH9O92Ga8Z+a2yjRBd0RvfR9pW5TqVcqtGl6liO26SCxGW0O2H7tIFqeoLCPs+8YAPcrwykV0dnDdBSClJh4FjPf6VFeaNE4tIQTYnktluYZTcLFtByltI7V26do4xxxzfENRq9VeF8Hxm0FgaK353Oc+x4/+6I8eafuHHnrIPFtuEHMfjDleaUgpOXXq1De6G3PMMcccrwvYV9bYnYR9v7GB06v34UbDgLlZ7Gwb+nBu9CTI4HgupYUKTsllksj/qv5S+Y3q3NUuR65rHY9C+hfbRKMgj0POpI9fbeerxGAnU9ZyLAqNUi5TsUCxWUa6Vh44OaikuKxLVzuHiSz/pRUZk7+10bq3PI/ScpVmkqGSlDQISeKULMmmXgnT7PjZX/KxmJIq8pJTl5DGCaPtLm7NpbRcwS44HJrvR0+uf31gRkVm8j+tNdKSuJUCTskl6PsotEmEF5jo3+x+V82SOcjQvn4cfX06ypZmBsxsOTNJZ+3eJ38feKtMZFVmDiYm8/GgrUvn/JFxhbF5cZL3YOCvWBOlZ383RIbOr8P0qaQ1aZ6lrjKFzjRZkhnyYZLpbEmC3pjuuT2SYYRbLhCPQyzXplSs4RQL2LaH3xky3u2ateJgWqAzQ2TkbOjl5zYNHk8qDkwlhU5N1cjh9S0nObJJIDg/1iGJ5tlotJh6P6hM5/24wjAKTHDbNhneWZwaI2NlJI5UpkwAWiiENdnuYCapTJGEZhyl66CUQmWZkfKSAqRAK8Vwq4PlWDi1AsISpFGCXXRxSyaQq5QmGYekfjStZkCDFAKn7FGolagdb+FWjDFymqQmEJ5kuVeBWVdVYioB9l/YMr4PYYjWCsuzKbWqOKUCo50ew+3e9HrPelhYrkV1tUFpqWb8SaSR58riFOlY0+3Nz4Ffx+wlEEJg2YbcUInZV0UpOBZoRZaabdxqgfJildrxBbxKwRDmWR7cT7LclymXDcrXJD3LsGptiJjcd6PUrFBfaxmixbXIEnMts9Scp5SmykLM9NX0N/9/pkmjlGgUMu4OKI0qxqfh9CIq1RRqJcL+GL8zIhlF+HKAFMa/JR5HxsvClhSbZZyii+U4jCw5lQubjJQQYip1KC19INF3pUeUnnRzcv6mw0JaU5IvjVLSyFTwTLZRiQlgSctU0omceNbajHHsh8R+eHCsCZszXVrM/7WG7vqeqeKyMdJbSuNVi7hlD9t1SMIYp+RiezZpaPqRJilplKCVJs2f61rpnMSy8rlj7lEVZ8a8Xpt7W+XnoGNtKo0m62zunyYElFtVWqePYZddLNcylRj5vWvkLt3DJOish1JOkkqd+45w+Ny/Ebj80N/E5Pkcc8wB3JiB9ywefPBB7r///iNlo9+ofNQEN6vvc8wxxxxzzDHHqw+25ura1a+GaoMr9uAlR9sOt6wnXyyn37Tl5B0TMMsUTsmhslY3ppI3wbbh9QbjSSCJ+j7tZ7dIghjsSeBBYYysr/LBdvZ78EywTyuNsARO2aV6vMHSG41EihDCBIVeru/NwgQRsjTBqXos3rUKZGgy+htdxnsDLMuCnKQwhtFiJqicRy3FQdjTjMPBfE7iiHhzjFW2aNy6hFspIm3rVUEoflMgzwAWtsBrlXB6BfSORqc5hXFp4FK8yHQR+tD6cEOjbxjTF1ltrwExCeUf7sVl3wn1JNg/fQEQM18eZ9KBr/j7S8GVSfBLX9bTA80QEzOBe61B5vfL5EwVGjX9bYa0kQKsSTTaBDuFnJTfSIRjM9zpMW4PcDwPx3OxPMvIxdxzErdcQKeK3acuMtrpojM1zeQXAhMoTDNkXklg+j9zzzKhGcRBMDwnUoTFQWa4FoZO0pBlCjWNbOfnkBOcWmiQeSUAEiFs0BKdmoClIQYOD+jk+aXJwAK76GJ59lQ3XwhhzOw9I/Oj4wzt5I85AdNovmQ6vkIfzBORr3f7z2/SvbCLWzFyPtKVNE+vsPrWU6RBhFN26V3YpzsIEZapWtFaI2ybyrEGrdMrrN5zmmjkc/GrzzLa7aMShVP0WLrnxPR8sihlvDfg3H8/iVV0cKuekQZr1mmcWqJ5ywp7T62TjGPCoU8aHdxNWim8isfxd9xB85Yl/M6AwUab9tkdpGNRWqpRalaQQiIBoRVC64O4uwaUnlbfCGmeL7Zrm2x+pdBKGkLFsamt1Vm66zgLdx4n8UO2vn6W0W6fNEpxCwW4C5hW3hyevyIvoVBpCmjsgs3yHce45b576W+12Xthk9FOnzSMkY4xxRaOY4glLcwiks2QkJlGJwrtaCM9tt2lUC9QqBYpL9cQCGI/pLtuZJdEpgn2hwSdMf2LezhFF+lZFBcrLN99nOatqzSjhPazW0SDwMxlbSoEpbTzviuUrRCWmt4Hk7vigMWaWVQMH4fKNCpR5hkuBdIxfkVaGZ8qaUmktJCYSqGJ18g0ewTMZwFpxtcsqAfMiSEvNFjmntt8/Cw7T5/HrRTwakXccpGF06uceOvtdDf3SFJT6ZEGIVII3FIBKYSpfsrMMW3XwXLt3CclJUsmRIPGciRuwSUOYrI0mfr5TM9fTOhlPb3nlu86yV3/723sXdhk79wmw80eKsrye8Yy1b7ka4eeGUOYkpLCyte/6SR4eXGw9hxOHrrykScDMDfRnWOOb1bcrCqGnZ0dnnzySd74xje+6HbPPvssGxsbN+WY8wqMOeaYY4455njt4ptPQuomfVe7NPPStD3z5TMPKFmejVcpYbnO4STZOQwuG49JQNTorCdBRDQMGO0NiIcRGoU8JDkxySTkqkFWMQ2ITJvGKTo0bl2kfssibr2AdORUWuQyXO+cEYd/mf112tc887m0VKWVrZAEMWF3nPdeHyIppvvmfwtmqjDgIOiRt69STTQwFSvStigv1RG2nJGYuTIurTh5PelvHrpGSiFsSaFVptAp5dndM+9Px1tcvvNRDnC9uDQz+QZw5UtqgtVTr5fLtpm9eS5/+6bPEn34/r1SNw6TF7MVf5eWJeVbi4MA4OQkxIRgyoPwCIEUZn+lEsrLDYrNMqQ5oYAhD5yCi1vxyNIM6VpkmTIZ70mCVytRO7GITsB2XLI4xS7YhiARh+P+KstIxhF2waXQKFM7vkDqJ6Rxilt2TZtRgmUZQ+3qsSYAdsmlvGzuaWHl56JBpYrEN546bsmjslijfmKRoDciixOcgnt4WJQmi1OC/gjpWVSPNVFpSjoOEQiccpHKch3Ltkn8hKAzNlVqUiLzwOqkegRyslnkhKsWCCGxbIvqapNCo0QWq1yWz1R02CUXpMb2zPhMkZN+WoB0LONfUTHmxsKSxgy96uJVisb8OZ8yTsmjvFTDqXhGZkpnpurAsbBLLl69aI6ZG6IfhG51zn4Z0sGtF0niGKvgIKTALjimgqPozfQxXweEniGuBGmYMt4eUGyUKS/UqK21aN6yNJ2hdsHByQ3aLcfGLXmARtpWLlfl4VYKxh9CTsbV9DGNYrI0pdAqUzu5QDSOSPwQaUu8egmvXsTpm8oA27PRKJyyZ46hxMzEMz4ZQoBb9qgda7J053HiMEJpxXCnh1NyWbpnjWKrbHwXopjhdhe/PURrTWmhQmmpauTEMkWaZaDAch3ckodlWdieixAWWaKI/QivVqDYrFA71qJ1y4oZeaVJgtgYs09u4enaOqX4plcqizPicUzkRxQXKtROtlAqo7e+Z2SaWlVKi1V0ajw4gs7IVCFOqh7kpIpITJ+zl1VaalOdKR2LYs1UlGhl5p4QhjTxqgW8cgHLcYiHIe0XdqitLlBuVKittQiHPirVVFca1FabOJ6D3xsxbg9JxjFJEBGHAaWlCqtvPU1/s0M4GOOUPIQQhP0Ap+JN5/cBaSuwCy5eo4TbLmC7rvFXk4aociue8b3KV0WVZqZCyrYotsrUTywY6bYgJA3jGfZtOtFuOg6P77WeYjkR+vr56DHHHK9J3Mwqhv/8z/+8JoFxs6ovYF6BMcccc8wxxxyvZXzzERg3C4e+h02+beXZ+zrPnLOFCbaUSmBDptJXvp/fjNAHlRfhMGDv6XUGGx1UnOUa3gdRVK0OwlCX4RIiYRIYlQK8aoHlN5+gfnrByKdkKZdTBi8ftNKkKqW4UKG0UGG8M2RwoUuWZVND0asRCIcCAocTGk0Q1raJhxF7T28iLEmhVcFxrXkF0BGhtUY6FuWlGmF3jO1YZOEl5NiLll7M8cohD3jNkhQcaL+b4LI+ICswgfWZQo4pqyCkQEqTzZ0mIa3bl1h9863Eg5CgM8ZvD/MKAhukIFUpaWokYtI4JQ4jigtV3GqRYqXCsNVhuNPFLXtYTh70z6stsIzcW9Ad0zhVpLhQYfke4yPgt4ckYUQSxiTjCMd2qC7VWbxrFbdSpLxcp7rcmGaf55wLKlaE/YAkiKmsNGicXiJLMkY7PcL+GK9anJEhEqA0aRAx2OniVD1at6/gVQsk4wCvXKJ2fJHa8Ra2bRMPAobbPSMPZFl5JcJBtrwZQmF8fFJDskopsQsOx77lVhbvWqN/cZ/RTo9oGGDZFlgahTJr3hXl+kQuq6NQaLAlTqlAteBQXmxQO76AbduAQKEotMoUWxUapxaxPZvOxV2yJMOtFpCejZaajAylM5Qpb5gcJbcI0WQ6RQmFLNjYFRe3XqC8UGfhtmMUGuVDlJmWxtdBaOO1IqQkHUd0X9jFuvMYK288bSoetCYNElSqKK8YP6zhVpc0zqWFbAun5FGxLQrNCtXVJrZrz8gNmZGO/ZA0iakcb5gAdrFgvERGPk7FI5OGUPMqJSNFpVKqy3VKjQrRKDJz3DKyXoak0hQaJRbvWsO2XPaeW6dzYY/BZhelNK07l/BqHmDI7+F6F39viNKK5h3LnHzXHURdn6gX0NvqoAVYtoUUFmmaQGqqALIkIxiM8RpFqseaLN0dI4VF2PcJ+2P6m32yMMw9hhSTajNTICSRejLXNFmYEA0CgsGYKg2W7jlOebmKLFg4RZfWrcuUG3WyWBF0xgy3+xQaJSOBJqW592eI51lZucmLQhuPE69W4MS33k5ttUl/o43fHaG1QLoWWuSVekoz3h0w2u5R/p4atduaLN65hizY6FRTblVZvHWFNE7ZP7NDf6NNPIiIRgGBP6J6qkFprcrmo2cZbHSoHWshLYv22R0sxzZeXzqvttIClERpjRIK23Mo1SqQGNm5+qkWpVaFLFZTYiKLU+JhSHmxRnmhShZk2I7L/vMb9Db2bmzpnWOOF0GSJGxsbLC/v89gMCCOY4QQeJ5Hs9lkZWWFlZWVV02izObmJhsbG/T7fZIkoVAo0Gw2OXHiBIuLi9/o7h0ZSik2NjbY3t6ejnuxWKRcLtNsNjl16hSu6167oRvEzaxi+J//+R9+5md+hlKpdMX3wzDki1/84k073s3se6fTYXt7m3a7TRiGpGmK67qUSiWWlpY4fvw4xWLxph3vWtje3mZjY4Ner0eSJJTLZWq1GktLS6ytrb1i/bga4jhmfX2dnZ0dRqPRdLwqlQpLS0ucOHHiFZm/rxcopTh37hx7e3v0ej0KhQKVSoWTJ0+yvLz8sh47yzIuXrzI1tYWo9EIpRTVapVarcaJEydoNBov6/HnmGOO1y9etwTG5DP39HuvBoQ0X4SVyfAvLpQpNktoYeQNXroe/usF+WBawmQnD2PG2z26Z3aJe8FUg33yhWcSi9P6CsH+K2SRT7wNSks1aidaFGpFLDs3pdUmwGlyuAX6ZbpO075PKzGMLnblWJ3WXSsM1jsEnVEebDkcJL9i7uIlL5qArSCLM8L2mMHFDm65QG2tRXmxlpuozpmMa0FaElny8OoligsVY1ocJnn0UuSZ13OZi28sDjF3gEBlgE6J/YhoGBKPQxI/BgW2HRB0hlgFh8SPyGKTNZ4GCf7+EK9qPBkSP0HaDuPdAZ0XtnHLBbxGEbfmIYQk7I8ZbLYZbnUYbHSxPZfhZpfzDz1F5ViD0kIVq+TgNkqw1ycahYx2Btiez2izTzyMsWyboDtm+7HzxEFE7WQLLEFpqUoSRIRDn6gzBKXxygWcikv9dIt4FDO40EFFGUkQMdzpEQ78XN9f0Tu/h1YZGo1dMP5LaRgT54bZw+0uTsFFJRlplJBECfvPbRCNfOonlrA9m8ZtK1iWjbQtOme32fr6WTpnt0nDhKgf4O+P8MololFo7gDHQkpJFmYEeyPjjRHExH6ESjIGmx2kI7E8m+qJJmVVR1iS3oU9Rrt9Omd3CXt+nh1vqiGEMEHz0a4JJFiOqVqoHm8SDQOC3hBpC6QjGLcHxIOQeBSCUnhVj9JClfqxBZDGByHojRn+T4fuGeNpopOJrJaZOsKCOAjZ+to5gu6YymId23Vp3bJKGibsfv0C1bUWteMt4nE09VmYDYZLS5IEMZ2zO2RRQhLGWK5NeblOlma5R8eYwboxeraLDpZnU2iWKS/WiIYhwf4QS1hYls1gp4s/GBsCRxm5sngU0LxtGadcoNAoEY9Dhjs92s9vEY0D3HKBQrOIcDWxHzHY6DLeHSELNkF/jEYTjUN2n9k05y4N+VNaLSM3bDJl/BviYEw0Dgj7AVmU4e8NiQYBaZSAEIx3B2w/foFivYxXL9Aqr5jxVJruuV2Gmz16F9uoTBEPQ3a/vk46jk3FpNJU1xrGMyLJZj5ETap4pCHjBgFBd4TfHhnTcGX8LHSq2H38ImF3TOVYA7vg0Dy5hJASlQj2n9slbI/pnN3DkjbxMGa0PaBYq2C7NmkU5zJphqzQsxn/eT5EFpnA/3C7g3QlbtXDqxfyzxqw+fUz9NbbDHf6ZHECSrH59XOEoxDhCWprTdOW0uyd2TLz/Pwu4/0+Qgr6622e/f++RnmlTmWlRrFRwS0VQEE0CIiGIWgYbveNbKYF8ShAWJruuR3OPPQETsXBq7kgKiRBQvf5XYaFHoV6hXicoJEMt3pc+O9nWbx7jdZtqwgbCs0ClmdxKWc4/2g6x41iY2ODL33pSzzyyCOcPXv2mobKpVKJe+65h2/91m/l27/9268aoL4WHn74Yb7+9a9fc7s3velNvPOd75z+vrm5yac//Wm+9KUvvWjQenFxkXe/+938wA/8wHUHedfX1/nsZz97ze0KhQLvf//7r6vtCbIs43//93/54he/yGOPPUYQBFfd1rIsTp48yVve8hbe/e53c+edd76kY14LN5MEiOOYL37xi/zAD/zAFd//7//+b8IwvGnHu5G+J0nCww8/PJ2T16rmEEJw/Phx3va2t3Hfffe9LNfjzJkzfPazn+Xhhx9+0f60Wi3e+ta38p73vIc77rjjpvfjavB9ny984Qt86Utf4tlnnyVNr57sKaXk9ttv553vfCff8z3fQ6vVesX6ebPxwAMP0G63r7ndfffdx913333F99bX1/nMZz5zzTaKxeKh9aXdbvOJT3yCL33pS1edE5VKhXe84x18x3d8B29961tnKkJfOrTW/N///R+f+9zneOyxx170vj116hRve9vb+KEf+qHpdU7TlH/4h3840rHuv/9+6vX6Dfd5jjnmeO3hdUtgGOipCaaeREO00T+2Cy7V4y28ZglFilAHWcFzFakZTAdFTxVghCXIEkU48BnvDRhe6IDW2J4D6rDnxWzw4RCxcUj2ZibLUgrKS3Vqay2cgotEoPIcXK3V1FzzIDfz5T3tCUrLNVpaE/Z8/N0hOJeQMkZQ/tq9yvdRSUYaJAw3e0hLYjsOtdUFdIoJ3lyhD6+WTLhXA4QUWLaNVylQWqiQBBFJlOQcZH7TT/wt5qGfbzAmRJ/JLs/SjMSPCYdBLrMTo2KNFDZ+Z4hdcs1riSEwsjDG3x/gFD0s1yUJUqTtMNzskvoRC3et0Vxcpdgoo1JF7+we3Rd22X9qw3gxFDxGm31GG12OfeutWK6F5bnYZQ8NRIOQ0XYfaUlG2wOiYYR0bMKez3i/T6Yy0jSlfnwBr1FCbllkYYK/1yf1Q9yiQ+P2ZaqnmnSe3aXz3A6JH5GEIaO9PkE/QClDYPQv7huDYteicXKR8mINt1sAJUwAOk1wCx7o3AA5Tuic2WK0a+SBWrevsvLmU6hYEXUC9p/bZPvJc+gUpLCIBgH+3hC3WCRNUhAa6UgkFipKTYa+UiRRTOJHpHFG99wOcRCw8uZT1E8sYpdc/PaQ9tNbdM7s0F1vI7XAthzIayGM1JFmvNsnGgZoAYt3HqN12zJI6JzdIQ4jcATj/QFRPyToDkgj46lhFxxatx7DLReJgoi9Z9bZfvQMYcdHRQosQ1pMikiEJUj8kO3HzjHe6nPyXXdRPdakeHqFzvNbbH7teZIgxi55hsCIFeRm75MpKCWkQUQ4GBH2R4zbfRbui8FRYwAAIABJREFUPMbKm06jhMmG757Zof3MBtEwwC65aCFo3XmM1q1LaKXZf2qDNEjRiimBkeWk897TG4y2uggB9dNLuLUSwpFEw4Dhbpf2uW1W33zaVH64psqo8/wuQWdM9VTLkA/ayDntPbcFlsQuexSbJdyWh3AFmUrJsog0son9gKDnk/Qjgv0h8TgiSzOEbTHY7DLe7XPs7bdQWqlSWVhASMnwYpvuuV32n9nG746RjiQepew/sUEWpkjXobJcpbRYoXd+jzRKLvGdEmYu5dVEfmdkKpLGsanGsCRawe4TG/TO7bP0xpO0bl+mdesSKlaMdkbsPb3FzhMXjKSV45KOU0ZbfYq1Cm7FJY1jlMqQ2ByUMB4QUYAx344TY+Jtw9Kdx6mtNkBAf7PDxqNnGW71GO0OjEk6sPXEBbqbbU6863YW7zCZ5ePdATtPXqRzdge/O0RnRlquv96h/cIOS284zkp+DsVGid6FfaJxSNgdo1LFsNEj9iNDsI0CpNR0z+/gd0asveM0y/euGYP5vTF7T26glWbh7uPEfgxCMtzuMdruAlBolBESnLKDsGcl2+bPrzluDE8++SQf//jHeeKJJ65rP9/3+cpXvsJXvvIV/vZv/5bv/d7v5f7776fZbF5XO0899RT/9m//ds3tLMvine98J0EQ8LGPfYzPfvazR0ro2d/f55Of/CSf+tSn+O7v/m4+8IEPUKvVjtS3nZ0dPvWpT11zu0ajcd0Ehtaahx56iI9//OPs7R2toirLMs6dO8e5c+f4xCc+wenTp/nABz7AW97ylus69rVwVBmmQqFAHMfXvA7/8R//cVUC4yjyUZPqn6MQHS+FwPB9nwceeIBPf/rTjEajI++ntWZ9fZ319XUeeOABbrnlFt73vvfxbd/2bdfdh0uxv7/P3/zN3/Dwww8faftOp8ODDz7Igw8+yDve8Q4+8IEPvKxVGVEU8a//+q986lOfelHSbRZKKZ577jmee+45/vEf/5Hv+q7v4v3vf/83VZXUBP/1X//FmTNnrrnd8ePHr0pg7OzsHGntq9frvP/970cpxQMPPMA///M/E0XRi+4zGo146KGHeOihhzhx4gTvf//7b2hePv3003z0ox/lwoULR9r+woULXLhwgU996lN8//d/Pz/xEz8BcKTzBXjPe94zJzDmmGOOK+L1S2BMpTgmaux6qtaMI/EaRVq3rlBarIAQB8UXlyYNv45hjCZzpWthTHSzJCXoDBluduie2WW8MzDyEUKCmtgQK4QRrZ8aKs+SGhMVL2Zfz5uwPJvyao3yWgPp2mRKT7X/D4TAXjmKyWROZ7iVAhWl8epFrIK5rdRE80lPSAcTqD0K0SCkRNqCLEgYrveQ0iKLU8qrdcordbRWxlj2kuzXOQy01mRZhl32aN11jDRO8XeHaJUHPud4FWCSQq8PQpBCoKWkt7nP85/7KqOdHihQaUIwGrHz5EWkLQkHvtnBgixJiQYBvQu7jLsDwu4YKY3kWjyK6b6wh787xi44aKWJhj5RPzD3ojMrQKQNgTAOQUpUnAf088C1kILET0zQVhsfCCksgr0hOs4YXuwgbYuwMybs+ygtSKKM7oU2fi/AfXaHsDfG7w9I05jxXp84iIiDEJWkGGUsiyxI6b2wy3h3gFN0iEchYdc35KxtKr601sTjENu20FpCJgj2RuxHm4x3RsYfI0oYdwag8soIYWSM+lsdwlFgjMrjBMs2a3E49Nl7bhOtFUqlxEGIdCRZnOLvj9h57ALdM7sIxyINYsKeTzQIsISpNtMTCcYZCtlIHmWMt7tkQUzv7B6JHzFuD7DaFkFnSDqKiYcxWZSilMlaj8cx/XUznllqKh/icYTWGm3la52emUf581sKMy7bT1ygk8v4RIMxkR/SObdD0A8Y7vTAnnhvHKyfSgiwJBIblWqCrs/+s1v4+0Oj/qM0450+aZKBJciyjOFOlySK6Z3fIQ1i/L0B4cDHbw+J/JDYD8niGKSZL2mcsvvcBr2dDtKxiAYhfn+ASjOEhP1nNhjvDVBZRhqlRH2fNEqJ4xidqdwk3pyvv9dn66tnsQoO0pYMtrpIS1CoVqgsLlBtLOIIj/2LG/Q322ihkLZASOMHkylF59wuQc/H9jwEkngcEvZ8kjAxFTU6Jxctm6Drs/31izhFY27u7w0I+yEqVQfPHkE+Nho9ium8sE/YjRjt9pG2beTSAOFYKKUYbLaJhgHdF/ZMlVwQ43dGuVyWWRuSJEb3MnafvoDlWMT9CMlkEddX/EwmhZFuivsRvTP7hN3AeIloiMdm/iV+bDIS8ywWlZjx3vnaBfrnTTAxCWJTQRIkCGFNnx3CkliuqcLafWqD/kYH27OJRyHRMCD1E7TW9Nb3sFwbIQTRMABtoTJMFc1Txqxdp5o0TAm6Y7TWJHFsyKo0Q9oWoOitt0nizPCCccpobzDD2+jDJz/HHEdEt9vlr//6r/nf//3fG24rjmM+/elP8+CDD/LjP/7j/OAP/uBNyfi9FFtbW/zBH/wBm5ub173vhDB49NFH+dVf/VXuueeem96/o2J/f5+PfOQjPP300zfUzvnz5/nd3/1dPvzhD3Pq1Kmb1LujkwBvetObSNOURx999EW3e+GFF7hw4cJlfdza2jrSGNx7772Uy+UjzdXr9cD4whe+wN/+7d8yHA6va78r4dy5c/zxH/8x99xzDz/7sz/LsWPHXlI7X/7yl/mzP/uzl1yZ8pWvfIUnn3ySX/iFX3hJ+18LzzzzDB/5yEeOTLxdCUopHnroIb785S/zUz/1U3zf933fTezhaw9hGPInf/InfPWrX73ufdfX1/njP/5j3v72t/OLv/iLlMvlI++rteaf/umf+Jd/+ZfrPi6YqotPf/rTPPbYY/z8z//8S2pjjjnmmGMWrw8CQ8/k9E+z/Q9e0Ln+ulAmc1t6Fl6jRHWtiVN2SbPksu/Jrxm8CCEzrRaYZq1PfhGH95kx7kwDEyjunt1j74mLZFGGbbsgRX4ZJgavevoF/EDv/qAC40qxeGEJbM+msFCmuFgx5IGaHtwQKa8UwTRTeaK1kRyzbIlX9bALNlmSodOJyahCZcrMratEz6fTcfJ77sGSRSnpOEErRRyFLIuTlBaqXFrLcRl5ca3J+lqPdWjQmcIuOtRPLeYZ9JaR59H6EpLschPY/I3LX7sarmM8L23qtX4pro7J/T5Lwpm/Rvs9hrsdLGE0+bNMkfop8TnzZc7IFZkmVJahfEXkh+hdhcTKM701SZCQDHsMVS+XmNNTMlTkvhnT9QgY7fQZ7fRRSpmAuGEcicYBEyPh6eIkBVJaRP2AqBugUEadjHw7KUjjjOF2n+FW30w4qcFSxMMAVO7lITVIbdYHJFmcMdjsmqoyoafeHgIBSqB0LpUnQFqmDEFnmqgfEHZ9emfb+Tnma7WUTOT1kighDmIjhSNAWmJqwB37YX6e2uyrJcKWuUeIT5gHWCdx04mpspx4kkzfkdMqOnMdFEFnSLA3RGViWjGBUOjtDKktBBZamiEKOmNDGui8miO/3lLa5nwscYX7UeSPJkGay0BNbDKEBdiaeKtDf72LkOa6X+rbodEgTaWKVppoFBINfAbn9wzFJfLrIAVYoHRG0B1OjbEF5F4HAL1pdcjELFxKSZak9Nb3zVzR+oD8z03Vexf36J7fNddUyPwaQtDzAYGQ5NdLEfTHBJ3xwXywwXIkbqlIqV6nWK4iM8lwq8N4v2euhz1D82sYbvcYrPcMyaWFUdST4mAO5ychLAiHIcEgMHNAa2SetHBZpaEwa2+aaAYj076wZD5Xdd4P4wUx3hsw2hmAsbBCSGXmq7QMmYk21YhJSNgfgQYpLYSUxsfiKh/MRM6UpKOYZBQy2OqaJBSdp6nkJJghMMy1V5lCjUM6LwR5QakZ+0PnmZ+rlBLhCjNHhuE0iWKyRkiMsc1orzdT9ScRwgKtScOE3vk2vbP7eX/zaiChGXf6udRmvo4JwWh3wHC7n38Wy6/RjJfYHHNcLx5//HH+9E//9KYEbWcRRRF/93d/x6OPPsov/dIvUalUblrbGxsb/NZv/dYNmzT3+30+9KEP8Su/8iuHJKleKTzxxBP80R/9EePx+Ka0d+edd95U8gKOTmBUq1Xe+MY3XpPAAFNp8VM/9VOXvXYUfNd3fRfPPffckbY9at/jOOYv//Iv+cIXvnCk7a8HTz31FL/2a7/Gz/3cz3Hfffdd177//u//zkc/+tEb7kMQBPzhH/4hjuPccFuz+OxnP8tHP/rRmyZnHIYhf/mXf8mZM2f4mZ/5mZeF+PxmRxRFfPCDH+TcuXM31M4jjzzCb//2b/OhD33oSN4tSin+7M/+7KbcI5ubm3zwgx+84XbmmGOOOV5nT4lJ2FdPSYtJcIFJxnbRZfHu4zRvXc6DFHO/gSvDBO+MUakk8WPaL2yx+dUz7Dx+ntFG2wSdXBs9/aI9G2nIx11gvoxPYiUwIwOVI49j2UUHp17A9hwsyzKxEqGZxhVm2njlIA76bEm8RoniYsXIZU22yA3NpTRa85awsPL/yzwAKIT5d/Ij8sRQYZks8TSKGW8P2H38Auc+93XaT28S90JEJrBtx1wHS84DGpfAGBHbVI81WH7TSUqLVbI4yT94X2OwJtNVi6v8vNy9f63DrAEH929OLgiQQmAJi0kwfOKdY0jAPMA6qZkTk2ouI19D3tZ0bbAMGYglEZMfIQE5ZbGmrU0CkHmQlMnaJWR+HHm4/zANqpt7XCLz9SznYqYBc2HlAW6kCdJamJ/JawiUBC0n++RrxnQcjLeP8Rk4mLsCc646JyuELfPznbTNofEy64v50ZN2JwMvJuM5q8+UvyYlwsrX/Ml6IzEEzFVuBhPXlQf725NArWlTCBukRB8oOZkgsCWRlo20bIRlI4TF1Y8yGYj8mSK1uQaWAFvkAXlzzYU1Ia0OSN/LHhuTF6TI54uFtE0webqPNm1KkV/zyZjk52kup5jOS0FODkwC50IaM3XrYL4wmSv5HJWTfufEhZCTK2W0GkVepSBtiWWZ7bQGpCLJAnrbO7QvbOJ3TQWBnn6WyYPp2gTaZX5+wrbye4Pp8+dgwCf3qSFcJh5P4tJ1cPYiicm5iEsSEvJxya+JtATCyeetZSOkNZPKMDluPmcnc272oFd47k8+22mJmXfSMve0JadzYkJcTAnUyQ1r5fPUNpJXU6JghpDScoYItTB+LnZ+Lvk9oSUzY5UTrhNCToh8jkrjDTZ9dudrz/QDQN616RqSky8zXuZzzHG9+MIXvsDv/d7v3XTyYhaPPvoov/mbv0mn07lpbT7yyCM3TF5MkKYpH/7wh3nyySdvSntHxcMPP8zv/d7v3TTyAuDHfuzHblpbYIKlR838r9VqvOtd7zqSMfPnP//5Q/4ISik+97nPXXM/27b5tm/7tiPLfmVZdk0ZqPF4zO/8zu+8LOTFBJOM+U9+8pNH3ueLX/ziTSEvZpEkyU1r64EHHuCv/uqvXhYvxptF3LwWEYbhDZMXE5w7d46/+Iu/ONK2f/3Xf31T75G5h+ccc8xxM/AaJTBmSIpLMtWnX4oPvaQRQmAXHAqtCq3bVqiuNY0ZtTqQ/rns5zWPKaNw4E+hQWUalSjSICYehASdEcPNLp3nt2k/u0n//B5hxwchTNalnHxtvyS1XRwERabjOZvMOQlj5BmfdtHFqxWNJMMkSHjJzyt3YQ4mgciDfMISU9Noy7UPG31LiZWTF1PiQh4EPGUe1Jpm4wpxEDe0BSrJiPoB/fP77D52ns6z2wwudPB3h0Q9n3gUkgQxWZIZTfJpbHUSqLkE14wGvjYgpAkslZeqLL5hjcpqHcu1DjKwJ+OgNVMfl1noK934M3PsdTCGLx9mJuGh4RXTwPBsdvf0Hp8SolMKI1+iDwiIQ+1aIvc7EAcB7Gm7M4H9WQJjttpicj9ycNxDXT5EOOTtz/YxD1bK/NgmyH3QF3HoWHnGft6eENL0bDIPp/1hZmxmznVClORkycF2MxTGJPN8Kp9z0PbkmBPCZDpGk/cmgVR50JeDtPArXNfp/TMhQOTMeZvA8rQfswH/ybZ54Pmgr9MuHRxzUtknyEmMg2CvmJI4hwPvlzZz2VNjOtfyfufX8GBDMc3mF9N12xA1k+qbg4Zn5k5+PQ6R1RMyYnL9JmMzIS+QV3jG5b9MA9v5cbX5ghj7AZ2L2/z/7L15jCTnfd7/eevuu6d7rr2Xy+VNiiIp0pS5tkTZhqE4CUQqCAPDUQ7HBp0YCODkj1wIrECBYRixAxBQIEd2LDuGFRhOJMPyL2YkWhRskaIsUaIo8Vouyb137um763jf3x9vVfUxPTPdu7PcFbefRe/MdL/11lvvVV3P8z1W3r5Ap9YiCsIhzr9P1OkTB3rrZ2g4h4h3Y9jzYgt6gmMqWPXP0/61agiERSoy9hP3ybc3LQAZQ/3aN4iMeC/2JknX79CaS+dTMnWG+lOLF0Zax5bvgbGe2b8mUu+eRGBI+7e3vpM5IIzeOfr3gXQ8+vc4kXj/MLR/9c2HKaYYA88//zxPPfXUu0IknTt3jk9+8pMT5RR4NxFFEb/xG7+xpyLLTnj55Zf5zd/8zR2THE+Km2++mfvuu2/P6oPJckgUCgUymQz333//rmXr9Tp/8zd/k/794osvjiVIvf/97yefz48tYMDO1+D7Pp/61Kd4/fXXx67vSvC5z32Ov/iLv9i13JkzZ/j0pz/9LrTo8vDss8/y+7//+1f1HF/+8pfH6qsprgzPPfcc3/3ud3cs88wzz4yVYHyKKaaY4t3GeziEVP/TuujFi+ljcwXEOQwUpmtSOFihfHiW7FweO+eklpo3KjTl0CN+pJRxbO6AsBPQWWvQXm/QXW/RXmvQrbXxmx2UFJqYSqJEpd1oQBq2Iw1EQo/aUOlZ9V+DP+2Mg5f3MCxD54B4NzphTCRCmZ1zcYtZhLmOkhFCWCm52Ue3kuRcSQOyiOGrGWZlhObhIgg7EZtnVmlvNMmUcrilDM5MFqfo4RQ8nKyL5dlpuBOVEPQ3IJRSEIGd8ygctGhtNGjXGnTWW/j1TkpAaYi+fhomiUaNz43Zp+8ahvj59D2l0g9Vmog9/jAh/dSWwGDb1z/qb7HNCE847DtyvNs2SzAsu+9UWqXle+K8nsYj7BOu8JaWagUDYzPKDmJ40K4CBrooPkfqVaEHqncHEf2l4uPV4GdD+noCGW/PyedKEAtKfV8rtkD0qunn7ZX+rdf0YaGH3t/JzbM33eNxFdt0qdBJ7VshraCJv9kFBWE3Qoxlq6L6GzkgXAy0NT1dUn6C8U3W68hDtlHV068FKt2ik9OOtVZGbeNCpE0ZuOY9wlW5OyQXfYPey6e4cpw+fZqnnnpqS/i8q4nz58/zW7/1W/yH//AfdhE9rw0ajQa//du/zb/5N//mqp5nZWVlz8UL2HvvC5gsh0QiKpw4cYLnn39+1/LPPPMMDz/8MKATe4+DEydODJxrHGxubnLgwIGRn33mM5/h5MmTY9e1F/jd3/1dDh06xJ133jny8yRUj+/772q7xsU777zDZz7zmbHKzs7O8pGPfITjx49TKBTodDq8+eabPPPMM2PlrvmDP/gDPvCBD1CtVq+02VPsgC9+8Yvce++9Iz9bXV3lf/yP//Eut2iKKaaYYjy8hwWMISSkg+rzvohjUNuegzuTo3zTHKVD1djK39SW7O917PQ8ET/gh75Pp60TS4bdAL/ZJWh3aa80aK/UdYLVRgclezG9tbWh2lK96ieYxBB3MEwkDHFglmfj5D2dGDaNjz50hnf7+UjEFFjcVjvj4uQ8RByDvz/WtkCTJpqXiomX9BL6CbbEO6B3MT3CUMUJZps6IfBKHSfn4lZzuKUsXjGDk/ewsw5uIYNT8DDtOHTGjch7xI4VpmNhZRwKBytEYcjGqSVkECFDiZSqZwGdCkrDFSXCU1/FfTTVsP50I3b1JNh1mY4SL/o/G8UcDzDQ251BDPwYPl0f7bz9GKbDPuocI4hy6C1gtXVPHH2KraX6pd6tbez/rH9f3N5SXgz9Mg6plZxFpSKGGNq/+1s1Yn/eUt/o8ejVpLb/eEj31ctXbB3+cRbjME+fClhqy/uJACG26X3ifX7wXmb0BJCtB/Qf2Sfm95cRQ38Pvd83FpEvibpdfNnRThpxPomRl9L/jmKgQ9N8MQNzOhZ8tr3tbjM/+w/Z5th0/u3E/g93C2OIGAPXFRsRDJ9K9F/jiMaNgZ6X6vCcjeu+rO8m/eoXPVHr8po4xQ0MKSVPPfXUNSFIX3rpJZ5++ml++qd/+l0/9zj41re+xUsvvcT73ve+q3aOz3zmM5cdsisJzzQ8dseOHeOBBx644rYNYxIPjERUuP/++8lms7RarR3Lf/e732VtbQ3TNPn2t7+9a/2u6/KBD3wA0N4e42I7EeYb3/gGX/va18auZ6+QCBS/9Vu/heu6Wz5/9tlnOXXq1LvernEgpeTTn/70WKGoPvzhD/MLv/ALW0KK3X333fzMz/wMn/vc5/i///f/7lhHt9vlj//4j3nyySevqN3vZRiGwZ133snhw4dRSvHWW2/x2muvTSROv/zyy9RqtZHC4B/90R9ddgL5KaaYYoqrjfewgCFGGNapPnpAoaTCsEyy1QLFw1XKh6pkZ/Paav1GEC92g1KoUCdgXXvzEu31JkGjQ+iHyCAi8iOkH6HCCBCI1MhTpYTFAP/SR2KkFqGKncmHFALDsTBcWwsYe3aRe4PkO4NpmViOpQUK1SdeiH7i5ApaHxNVSfJRGUR06h38TkDzUg3TMjFtC8MxKR6qMHNkjmw1j1vOoqR6Vy3vricopVCRJDdbxM17IHUi1c5GC7/R1aHOkvm5nSn2SOxiETtUxY3Z+3uNvk4dsFzvWXbfeP3cE9Rk38Z7NcjNLcR32tvGUKl3gVpNbyjDa3Yba/5tqxgSDkYVStUbBnSbfuws2YjRFv9DBwnGMLLvF9D6T5gIDkIgxNAk2Gmbik+oEveGLWWNuI+T3EFxIcFW9XZk/QMN3x3bKVbbem9sgyHBY1jXSiOzTVDlFFP8MOLZZ5+dOIZ6knvgrrvuolKp4Ps+Fy5c4IUXXuDNN9+cqK7/9b/+Fz/+4z8+VuLYcXH06FEeeughDhw4gOd51Ot1Xn/9dZ5//nlqtdpEdX3hC1+4agLGiy++OFaS634sLCzw2GOP8cADD1Aul4FeHPyvf/3rfOUrX+Hv/b2/dzWaO5EHRiIq2LbNQw89tGtOiyTvhW3bRFG0a/0PPvhgSvhfaQipKIr4n//zf45dR4KDBw/yIz/yIxw6dIhMJsPm5iavvPIKzz333EQk7/LyMn/+53/OY489NvC+UoovfOELE7erWCzywQ9+kFtuuYVCoUCr1eLUqVM899xzrKysTFzfdvjrv/7rscSVO++8k1/6pV/aNgm3ZVn8/M//PEtLS7uKV1/72tf4uZ/7OfL5/GW1+b2Me++9l1/4hV9gYWFh4P1Tp07xm7/5m1y6dGmsepRSvPnmm1tC0K2url5W3ou9WidTTDHFFLvhPSpgbGWx0gfYmDBXhsApZcjM5CgerFI6VCFTyWF5NiqSA5FkbkQIIQi6AY0L69TOrbN+aonOZouw7SMjiZIyjtMtekm4+8QhVGKRqFKPg4Qo6hkQ7kz+ai8FSGJYp+KAER93PbGUcVibXhsNkNr8PxENtKeGNhtWql+2Udt0w6CdaSJe6B/a6lRKhZIhoR/oPpcqjeMtQy0wwSJuKTuy/tTo9Tp07d9TxGbjdsbGybmUj8yjJNTPr9O4tEnUDpF+qJOzDhCKO/VLwvSNF/JH7WRZPMXY2DIi6VYQk7aJiwBcrtnzDw8SQVTFJLQAtX1co8s8xyhXgR4rroQCJdLzj8WY71nTetbuakAYUH33iMQCPiHpSffltLmid4yKPTOHLed73yHUgEPD8FXq49WWA5Nu7N8r0lBUvWam984kR4/qTfCh/B2DFv+9D8SWYUjEc0V8ff05IIbaFEfWSj0uejkp4pxiSqJzmqgtfTMKaU6ay5gOauA41b/ljvh8y8Ej3tB7dc/z5wb/ojfFDYM//dM/naj8vffey5NPPsns7OyWzx5//HG++c1v8ulPf3rs/Bb1ep2//Mu/5G/9rb81UTtGwfM8nnzySR555JEtn33oQx/iH/7Df8jnP/95vvSlL41d5/e+9z2WlpaYn5+/4vYN43//7/89UflHH32UX/zFX8SyBh/RPc/j9ttv5/bbb+djH/sYlUplL5uZ4nI8MECHehonKfczzzyDbdtj1Z+Ejxo+124YJcK88MILXLx4cew6HMfhn/2zf8aHP/zhLc9Ijz76KD/3cz/HZz/7WZ577rmx6/yzP/sz/s7f+TsDY/vaa6+NFVqpH3/7b/9tnnjiCTzPG3j/xIkT/OzP/ix/+qd/yuc///k9MVr74he/OFa5xx9/fFvxoh9PPPHErgJGEAR885vf5NFHHx3r3DcKHn30UZ588smR/Xzs2DE++clP8q//9b8ee19eWlra8t5Xv/rViXIkXY11MsUUU0yxE96jSbwHkRhNKgVS6heGQfFghX33HWX+rgOUDs9iOBZRGN2wVur9EMLAr3W4+NI7LL18mvZynagdgooTTFoGmKBMhUQSyYhIRsgoIpKSSErCKCSKpM6dIWXcr/20T18/i6GXbkSaf8MwBKZlYFrGQAiI6wpKYVgGhmPpsBZSIaUkiiLCMCSKIiQSqSQKCfFr9HxTbOmjEUiTDhsGhinAjhPXCkVrpc6l75+ldnFDz+uhLyTXYxfuFbbrPSklURhSPFDhwP03s3D3YWaOzuNkXFSkBSGpJvFUSSasMfQaIsSGraGnuGLo8ZVIZI+AVUmotCTE3Hsb2gBeIZREIEFpkVT0hSy6UiilUFLvW309rT9LWzHqwL05//YNi+/pkUJF6CHvO7HZTkU2AAAgAElEQVRSoEJQUUJyxzuCivdcCUrGJLihSBJdKCW1EUMsQNN3qO5fNcClj26Y3uOVofo0Ji32JCLCyIMFaSJoI3FplAoVxteohm+SWpwQUr+QI+6NfUKGknosk/BQEIe7ErrqVARS/W1UCEN3gPbikygVjha2rhb6z7X7bXGgWJLlZDgE23QrnuJGwalTpzh79uzY5T/4wQ/y7/7dvxspXiR48MEH+dSnPkUulxu73nHI7d3gOA6f/OQnR4oXCTzP4x//43/Mz/7sz05U9ze/+c0rbd4WXLhwgVdffXXs8h/60If45//8n28RL4ZxtcQLuLwcGAD33HPPWCLDpUuXxpqP+Xx+IEb/JCGkRokwzz777NjHO47Dr/7qr/Loo49ua+BVLBb5lV/5lYlCo9VqtS3eOC+88MLYxwP803/6T/lH/+gfbREvEliWxeOPP86//Jf/cqJ6R+Htt9/mnXfeGavsrbfeShRFu76OHDlCqVTatb7dkkzfaDh+/Pi24kWCarU60b43yjNikvl4tdbJFFNMMcVOeG96YGw1iUQphWmbWK6JV8mTnS1SOFAmt1DEyXsYtqE9C9QgQXMjPOT2k7VpHGepiPyA7mYLv9GJI0eINBSSwkhDXSQWuJDkgkg8J1Ra5/CrF/6il9ha/500Sv8hEDqJqiFSAuZ6Qr/9ZiJaICWGEBiGqfOBABKJoQRKJFHO+70utpllPbPi9O/RxHovlEgi7ggF0o+Q3YCw5RP5IYZlYlg3woxmeyPw+G/TMjHyJoV9ZRDgZj0aFzdortUI2oEOi9Zn6ZuSosO6xIjfRp5whyaOh51Zu+trVeyOLRxrwhMPWI1vc6zS68DNeRQPVhBC4Ld8urUWfq2d1p5y1v1nHeJBxcDvw540I1rQNx8GS+5sDT9I+quBY0Ze4wDRKra8l0wHy7HIz5VwCxksz6Gz2WT97UtEYaTz3sTXvKOF/C6fWVkXwzK1R1cYoSKZ3iszlTylg1VUGBF2AtrrTVobTW20P5R3YdvrHFFGDJXq/drXz0LhFjN4xSxeMYeb8zT5H++bURgRdHw6tSbNtRqyq8MdWhkHK+OQmcnjFTKYloVpatExCkL8dpfORpPmao0oiJCRJD9fJlspYJjas61+fo1OvZ3eE9M2oXSuppxLdqZAppBj7e0lurUOc0f3Y2dd1i8s095sEHZ9vHKOwmIFJ+di2mYqJAilBYlIhgRdn856i269Q7feRkZRfMPpc38YmmTaGab3eeJ14uY9vEqOTCmv2/bOErVL6+lBQoEwDcyMhVfKkp8t4mRcTNNCKkkkQxrLNRpLNaIg1G3ZcSb3cj2lgR+Hxnu7KZKspUwpS3Ymj1vIoICN08t06q3tvS8SrViAYRm4+QwzC3NYtkVtbYPWZoOg5ceCviJt2fBe0X8Vl+vJdaUbczKE17FRzSi97Ppt7Y2JScIXzc7O8su//MtjWVIfOHCAJ598kv/yX/7LWHW/9dZbbG5ujkVebodPfOITHDt2bKyyjz32GN/97nf5/ve/P1b5V199lZ/5mZ+57LaNwjh5HhLMzs7yi7/4i3t6/svBuB4YpmmSzfa8uw3D4Ed/9Ed3zXEwLh5++OEBIcfzPBzHGSuPy7AIE4YhL7/88tjn/sQnPsEtt9wyVtl/8k/+Ca+99trYIdq+853vpHk9AF555ZWx2/Xwww/z0Y9+dKyyjzzyCK+++uoVjccke8cnPvGJyz7PKLzxxht7Wt8PO/7BP/gHY+3LH/rQh/jd3/1dwjDcteywp0USpm5cXM11MsUUU0yxHd6bAkYflIitLEOJmXPIzGaZv+sg83cfTsxXQQlUmGziNwjBuxMU2pMiiIh8TVKYVl9eBzQxloa/SKw206gMOvyFEioVK5Ln/x4R0CdgpB8OPvgaSiCUwDAExEmoZRTpsElXtQMuA8k1hBEEEaYwsG1LJ/MWsUXr0KWPUemkjRikkmSEkBLph4TtECujCZ0bGQm5IqVEIMjNFclU8wTH5qmdWeXCt9+mcXETv97Rc93crqYbTea8StipG0dpNjERq6TCLec48sE7MCyT+sV1Vt84x1qjpS3ut1SoRv+6XSSZHXTCyXafgYA1Yx05bv1KSizPYf6uw1Ru2kdutszK62fZPLNE0AkwTZsBJ8tJp2wcEsot5XDyGYJ6m7DZIej4yDAiCkPy80Vu/vBdBB2f5kqNpe+fpblcA9voO8ferw8lJRiQmyswd9sBKjfto7RvVgvHCAxM/E6XZq3G8smznP32SbprbcJ2hDubIb+vxPw9h6nctIDnZLAtB4HAb3Wpr9VYfuMMZ7/9Bt1ai7AdUj48x+L7bsL2LIJ2lze//B1aqzVM29biOsSeKgonl6F0sMr+244yd2QfL//Z86x2lrj5oXsoLlR55dm/Yan7DmG7TW6uyJEfvY3SvipeLoM0FBKFiYFQglAFtGsNVl69wPqpJZZfP0/kB9oDclhUHRXXSiXvxWJApcDC+w4zd9N+qocW+P6fv0BtZR2RcPkKTMvALXtUb9/HwfuOUZydwREOkYgIlc+ZF05x+htv0tqsEbYCDIyBcFRDI8Xlxs5TSiEMKO6bYfGOw1SOLKKU4vv/3zfotloQbS/sIXSTLM+itK/CbR+8j0wxx6mXfsCFk6cJ/XVkoL/zieT/UdvGlWIvvqhcx7eXAY1/h/emuLaYJF/Fxz72sS0JeHfCww8/zOHDhzl9+vRY5U+dOrUl5vq4KJfL/MRP/MRExzz22GNjCxjjXsMkeO2118YuO2nfXy2M64ExyiPixIkTeyZg/NiP/djIc66uru567LAIc+7cObrd7ljnnZmZmWiemabJxz/+8bGFvOH1OMm8e+KJJ8YuC/Dxj3+cp59+eqKQQP04efLkZR23F1hZWSGKIkxz2wexGwaGYXDPPfeMVdZxHA4dOsRbb7018XnOnDkz9ly52utkiimmmGI7vCcEjGHrtJQsUiCVws46uPkC+X1ligdnKCyWMS0jDhUDIymjG+gJLI23nb4BhmlgmAambWKalk5yjEjjhuuwEzFhnjpU9Cws0x4V6X8kNs7pewqUAahezG79M6YUEivS+F4qQ6lzOsQxvPXQXUdShoAolESBAsPAsCyEpS1QlUyMZnuMqUDEzR812Ua8t4O1acLMp0WUAlNgWBaWa2NnHN2WxCtmWOx4j2ELd9rn7ZLkSVBC6TlsGliuTW6uxMJdR8iU1lh7+xJ+o0PUDUlYMZVW2M+sj9uH792+vnzEC16p3n6RsLHJuoj3aIVM10wCwxSYjs45Y3sWhmXoUEehzs8TbyD0NijZO0d/DgQUShg9MaPP6jn1+Iqfn3RYofinEAhzkPgUCP3lXymUioVbU/VdT0LMJptdPJdiK/l+DzNhCDB6QkCPBlZpO1UU0d5oUL+0ThiEtNbrcUP0/U0oiepfBEmIITO+TkHf54OLRUmJMA2KizMUD1RpLW/QuLhOdMlHhiFEUucmsi2U0p4HhmnoawglyhAIDG0JLxIuPRlrEecIoreHG3HBgbYMQikJkSJbzZNbKFE5tkBpfxW/0ebiy28RdQMI9I3FdC2skksUSJACy3MwbYfZY/uo3DSPYZvUz66xVu8SdUIEAjvnkpnNU9g/wyHrOKsnL3LplbMErS6d9TqZw/M4roud8RCmRSIQqTivkZQSN+8xc3QBu+DQDTqEKgJDYTgGZsbAdA2ErUl/0zawMzYyCmmu1agvb9JYrmEKA8M0MTwDy7PIV0vYtoOwDGoX1mksbSAjmYon6T00nTy9OWQYJrlqgcrReXKLJfL7S3jVLMITCEuHqpJxyDWlJHbWZfbYAcqLs/jNLksr5+iutLDzDm4lg5fLcuS+Wzj90kk6jQ7CGB6toXmE0mGw+qO6xXk1RBxiKxEdhSHSEF1IiVKCTqNNfWUTy3MRQhB2fWRf0nG9ZpKwf3E/xBMuCiL8VpfaxjrdsEOrVifsdNOyIHW7ZC+ZrDDom4fbz8XdkCx5ofo2iMuoZTdBM80Dc43y/Wz1wBBTEeM6wyRx//stw8fFAw88MDYJO0lbhnHvvffuGlppGHfdddfYVvt7mfg4wYULF8Yuezl9fzUwrgfGqHBRt912G3NzcywvL19RGyqVCrfffvvIc44jYAyLMJPMu/vuu2/iefb+978f0zTHSkze35bNzc2x5ibA/Pw8Bw8enKhd5XKZ48eP8/rrr090XIJJ5u9eQ0rJxsYG1Wr1mrXhekGhUBjL+yLB5SY/n2TdXu11MsUUU0yxHd4TAsZ2UErnXnAKLuVjc1Rv3sfszfuIwoAwDHShYYeAa9LSa4/hh1/DMjQx6NhYjoVm8DTLpnnfmAIXsfWlAb2H7a0P3CoNsC37wiIQh6ZKKMahQTC0Na2MC6lAEnUjXYWIScg974nJ0G/8CiBDReQrhDAwHQthxiE0oh7RqYvuZLXaJ8r1d0nMiPTI3qHPEq0n7ljDNDBsEzvn4uY9naNE3aBfGobZvWQOSglKE2e5aoHywhzr88v4fpf6+XWd90UoMFWcHFnP78ujaEQfmXU5F7C9JfPl2ThfO4j+PaAvWUP/PEcYKHRenWQ9GHHiYGEYcaggLVIYtv5bRhEyjHvLAMMkUQ77TogmLcOETlf6TmiagzqsSohVFQsYKs6NgF7PpoEwEykwlkJibysZKojQSeGRGDGZn6bmEALMRHVU+v1IxaGZ0FbtltBkap9s09vx9HVHQcDmuRU6jRZOIUN3o4VSBkKYmuAWCVes57mMUxcYoK85VTGMXv+jx0RJhTIEM4fn2Hf3YdbevoSSIc2VdWQQAAZC6hwUKDBNPQYquX4hsIw4R4+pxdMoTVQRGw8EyWBIsAziBD5D9+M+6UZKVBhRWCyz//6bKR+Yxcm6vP3c9zn34kk6G038hg+hoHhwln0PHsevdSEEO+thZ10W7zjC/juPcPblNzn/8imWXz9H/aImO2Zv28/xn7yX0oEqC/ccxHBslt44T3ujzuaZZSr758hV8ziZDKZtxxuuRBlxrqcowi1lmbtlP0G3y8b6Kr7fQQmIRIAUAdgg7DiXlCEwLGjXGtQvbHD+229z8bunEYaB5dnkZgvMHt/H8Y/cQ/WmRbxKlkuvnKG5UkN1Q4RtapFIx50aGEMpQFgC07GYOTrPHR99CKfo4IcdMMGni0Jior0EJRDJCCefYf+dN+EWMiy9eYYLL73NxW+fpnSgwvxdBzj0vuMcOXErGxfXWXnr0uD66unzpPcqI87fEaqeaGUYev4beqH15mochjGKhUsJzdU6Uin8RgfTtujU26nQp+hfNyoVH3ToRkHUjWiu17l05gyWZ7NxcZlurZWuQaWASEKQCNpg2kmYzL71cVno39cuz1wg6a4fni+myZ3xh6bBNwTGTehq2/ZlkYX79u3b87aMwuLi4sTHWJZFpVIZi8D2fX/PLb5rtdpY5S63768GxhUwtstJ8cgjj/CFL3zhitrwyCOPjCRsx03kPXwNk8y7SeZzAs/zmJmZGUsEazab6e+tVmvsc+zfv3/idoG+nssVMK5kve4FRuVomOLqYZL5eLXXyRRTTDHFdvihFzBGxQZWAFJhWCa265JfKFM5ukC2kiMMfG3FOcWOkFLiFD0WHzhG/dw6tbdWCFo+MlIYpg7rJISBIbSVsGEYySN/7BmR2jbHVpLJWMWEYWyJrC1XR4gX9B76k4eJqBPg1zqoUPY8MK4nKAgaXYJ6B0MIXM9NI7goqXQy7yAYyH2REH67Vq36ycv4vcTCXKDDdkUKGUaYromd1fkBSgerlA5XiWSk532qfVxbq83rAcPdLpUkiLrYBYeFuw5iORaXai2ioM+Yfzgcyq7dt7NF+fiNTX4O1TPEr23xprpOMdjEPoI6IT6VQkqfwnyZwuIMdsbBsCw6tSadzSat5ZoWMIVOLh0FEq9SYP6eI9ixhXxzaYPmcg2/6RN2AgQCwzZxci5uMUOuWsRyHUzbpLnaoLlWo1tr4zc7CENgF1wK+2YwLIPGxXUM26R0qIrp2Ahl0FiusXluBRVGyDBERSBMg9x8kWwlj1vKgVI6DFEcdsmwLEzHob1Wp7XWQKkIYQi8Ug6vnCM3X8R0LaJOSGN5k/qFdaIg0vtsv1CSSMVxrh3LsnEyLmHLR8qQTCVLfrFMFESEXR/TtrA9G6eQwTBNOptN2msNWqsNnS/DQHsmEYf9E4LZW/dRPFglv78EtiAzl2fWOERuvszGmWXW31rCcM2UgI6CCLecpXrrItm5InbGob1Sp7PepL3aJOz6Wrc1BYZpkFssUdxfxbRNhCl0boXlTbr1NlE3jEWPwQluWCbCMcnOFSnur9JtdVg/vcTm2XW6mz4y1B4hmBC0O6y8elbncmq0qNy8SPXWfQhXsH5xmZU3z7P21iXCdoDlOoCgu9Hh7PNv4t/Z4cAHbiZbzrNw20E6my023lmmedsm3lyW8s2z+EGX2ulVuvUWCIHt2nj5LPlKgWwpz/Lr6yy/cga/3sFybDAEcsh7K5FCEXruCMtAOLp/hAl+q0Pt4hoXXztN+eAsmVKB4sIsXvEs7cTTB+1l0+/3kAhxKpJEvs/muWVO/tV3sTyLSIXMHtvP7M37tNAV30uFYWBnPJy8h3AFgd+hfmmDbq2NnbUJA5/1M8tUjiwQGgHZuSwzhyt01tuE7WBwi0u8KOJk5PmDRQoLJeyMzqfSXK7TWm/SWqlh2AZzB+dQUtLZaGK5Fm7BQwgDpaDT0Hk/TNfC9hztdSL19wYn75GfK5Ip6/UTdnz8Vof6hQ1aK3Wk1PPZzjnYWQfTskAJZBBiZSy8mTy5mSKFuRmdYDQIqF9cp73WwG/7sTcXl799p8ft7kmx8/HXF3rea9t8h7gOjEum6GFca9fLDV80yXFXYnk7qbVvAtu2xy671wLGuNc7SRuvJoIgGJvE3E5MOHHixBULGCdOnBj5/riJvJPrSHJ0TDLvrvY6UEql8+x6atcoXGtL+XHyOEyxd7je5+MUU0wxBVwzAWPcR5vBUAQwSHwNPzj1J9NU6ES9bsEjN1ukuK+ClbGIwuCHzKLt2kAphZVzqdy2HzvjEqw1EVIRBioOCSIwhNAWpKaJYRhxWm8AIx0npXqhNWRMoKfvKZlaXo6CiBPgEnt8RN2QoNFFhr0b7JUEZ9hrKKkIW12CRgehdILdZE4Kpb9QJ9bk25HMI99XPSEIBue9tvAmtsDVJKTlWngzWWaOzrNw92EMW2jL5T2+3vcakvloZS2qxxYI2l2WXz+nQwJF/RRh32zbtVP72a/hwuOMiBj5e+qgIwaJ/x59qUYvDrXbWSefJVcmgiWip+g7tQATHfpmocjs7fvxClks16Z2cZ36+TWCeicO+d8LSuSVc7jlDJlynuxMnpXXz6NQyEgnZteeGhZuOUtx/wwzNy3g5jJYrsvG6WXM0yabkcJvdcEAO2szc2wOy3OQQYRpm1Rv3Y9byGKaFqtvXKBTa+A32kRBAAgMyyQ/X2Lmpjly+8uoQNI8t0m33iIKAyzPwc5lWZVS54oAhG3izeQpHapSuWUBJ+fQbXQx3tBCg2x0NGHYJ16JpM+EgWma2K6NnXUxG22kjHCLGao3LxJ2fDr1FpbrYOdd8oslLM+hfm6DjbeW6W52ifxoC0krgOKBCgv3HMYrZcACt5iJhdEqVtajvdHCsHqeeQpdpnSkSuXYAl4px8bbS2yeXqHT6CA7+hos08LO2BT3zzB/z0Fsz8EwTVbfvIiUkqgb6FBQKRPfJ2DYJqZjkpnJk58rsvTqGVbfukhjqUbQDDAsHboPJQm7XTbeWUosGnBLGarHFwjbIbXVddbPLlM7v4bl2Fi2Dgfl132Wvn8WO+OweM9R3EKGyrFFLnznLWoX12lu1ilGVQoHywRdn9ZynfZmE8MQWK5DdiZHppTDzXj4tTbrpy4SdRRuIQdGz3suEYmSFaDiOFvCEhi2Dt+IAUE3oLlaY/XtixiOyaHFOfLVEm4+g9/qEPm6n1QcFm/AOU9ooj+SEfXlDbptPV7CFGSyORaOH9YCBolDgr4Gy9XhBsOuT2ezRdgJMD2LSEpaa3U67TahEZGpZCgulohaEWEz6HkUxVcplP4OZnkOxf0zzN2xiFfKYToW66eWWX97hW6tieU5zB5fREnJxplV3LxLfr6IEAZRKFk/vUxrvYHl2dg5R4s7gDBNvGKWmZvmKO6fobivQqfeor3RRIaS7mYb/FCLkTkHJ+fpnCXofFpOxqG4WGbmyALVY/uJgoCw08UwDaJAEgaSKPB3cpQc3MlG3bsTB6fB5TU+YqFgnF15J+H6cvbocb6j6O9no677evhGNkUC13XHKtdsNgnDcGKhYFwvg0naMgrj5ma43OOEEHtOro17va1WC9/3rzm5N673BWwvYBw5coSDBw9y9uzZy2rDvn37tk3UPq4HBuhrSQSMSebdJPP5co6zLCsVya6ndo3CuO2rVCpj52iYBKVSac/rnGJ7XO/zcYopppgCwNrpwSax5r0qFr19lmn9j3bDrVGD/+n3VO+BbPinrkMQEeHkHIoHKmSrOW2pK2/Q8DlXiMxsnoWHjtFda9HdbNNdaeGvxRY6QltYaq8M7ZEhhKl/JtalCqSMiGREFMWhNqRERgI//nvUOKZjHluIRp2AoNEhaPuEfghDpNu1hJIgA0lrrUljaRPlR6ho0NNHAbbrpHG7k7WllERJlSbOSt9Pwqz0sc7D85wwLm8o3GKGwoEZ8oslMrN5stUCou85dDsafYpBCKGJUq+Yo3xwlsbFDTprjdgNo1+82G0CDn4uiInKgf10t9FI9sfR51F9IdjSkCjp2UhJzWHHjdHNvQYzQ/WSEfdyYYRUji6y731HsTMOlmvT3mjQbXSwsy6ZmTxeKafzuiBwsi7FQ1Wal9apX1jT4qphUNxXIVMqcOl7Z5D+RaIwxCm6VI8v4hYy+PUO3U0djsYrZjn00C04+TOYrknQ7OJkM2RnChT2lfHyGTqbTZpLNaJOSHFfhfLhWTIzWc6/+Barb14gv1Amv1hm7rYDeKUsnUaLoNlFGZLcviKZag6BCVJQP7eGiiT5/WVKh6tUji7gFjO0Y6t0y3EoLlYozJZZe/MSy6+dRwZRPEYKJQRKSQzLwJstUDhYIbdQIgqCOMmzIDNf0H0koHGpRmezRXejDXlFaV8F23Xo1FuoJaVzvegbq54yStG4tIH9ukPl+CI5y6Jba9PeaNLZbLNxeplOrYVXyqEkOIUMdtHFOG/ovAP1DkIKyvtmyebzBA1fk+lBpBNi33UIO+ci/YjmZo2oG+AVsxx84GaWPIfNc6s6B00QDXxXMCwT23OxLRtDGgStQIs8QYhhGb2cEDFJneQyEErnwHCKGfzmOu1akyiUCMPUfSkAJApJFEYEXR+/1QJDkanmMF1Th+s6s4JbyFBenKVYnWEpY6OQhIEin3OZvXU/TtFjc3mN+lqN1mYL07DxSILrxNO9Px9E+mbiY2OkPzH0sHTWO3TWtAhmWAIn52E5DmErTL+fqUTASLzyjPinAhVI/FqAlUV7NxjJulM6zUqkQzdFbZ+w1UUFEjfjMXOwStjqsvZWm2w5T+lwlexcQWuMWQe74CFsUESxAUPizSEwLIPqLYvsf/9RTMfEsAXNpRp+s4vtuhTmyzSXapiuTXYuj1vIkD9Q1nmuIqnXwkYD0zUpHZyhdLiK5ViYjoVTyFC5eYHC4gzFxTKhH3Lp+6cxXRvLc8nPlkBCc7mGnXXJl8t4BY/Nwir+TJdMNU/5cJV97ztM0PZZfv1cnO/LIDdbxHRsln5wlqg9XmzyLdta3x5/2eJFOkJTTHFlmJmZYWlpaayyb7zxBnfcccdE9b/yyitjl61UKhPVfbnnSXD27Fnq9fpYZWdmZiaufzdUq9Wx+/4HP/gB73//+/e8DZNgEpFoJ2+IEydO8PnPf/6y2rCd9wVMLmAkYW4mGdtJEq8nOHv27NjhlvrXwCQE/alTpwiCYCJvHaUUr7766tjlhzE7OzvW/J2fn+eXf/mXL/s8U1wfmGQ+Xu11MsUUU0yxHQwhdIzqUa+d0fdoFRsTTmp0JUQcz1yo9EX6oo+YU4Ov9Pht2h0fZ3kO2dkCbimjYzyrPiv06ZPhmFDYeY/yzQuUb1mgfNMcuYUSbjGLnXGxbEtbRAojjnVuYFqGtry0TWzbwo5zadi2hW2bWJZ+mZaBkRBMIhlPBuaf6vsXBSFB29dhmlpdnb/gOoEMJGE7oFtr09lsEXR1LN0weYUhCoVlWdr6xjJ7L1O/DMNI529italQOtl8EjJGDXpiaOtrBzfvkZ0rUDm+SPX2/VRvWSQ7W4gJPdVHoIiBfzfyOhBi6EVPCBCmgZ1zyS+UcHJuLDhtqaHvtd1nfb8l29rweXd4DQq9g/WmfyuBUiJOkisGX+lk2u3F4Otdgej9jD1IhNCJsYv7Zjj4wC0UFitEQURjucbGmWX8ho6Ha1gWhm0hANOxtJeAIehsNGmt1mmu1HDzGarH9pGfK2JnHQxbk6pOztNhlDZaNC5tsHlmGWEKKscWKR2skpsv4uQ9LMfGzWfIzhYpHa7iFrO0lmq0lusEbR+vlGHh7sNkZwsYlkluvkTlpnmKByrYWVfXv7SJ3+pgehbFQ1UK+8tkZ/LYnn4Azc7mqd68QOlwVYseG03q59bprDdxsx6Ldx1h5vA8lmMjDFLvwkTXFKbAKbp4lRzZakHnuokihClwix65hRKlw3NYno3f7NBaqdPdbGtPiYNVMpUcVtbRQ5CMu3aooLXWYOP0Mn69jQolfrNDc6XG+ttL1M6t4Te7OvdAMgbFLKZtEfkRndUmnZUGmUKO8qE53bZiBifvUVicYf7Ow+TmSvj1Ds3lGvXz65iWSfnQHKUDVXKzJR0KSqk+Zj5J3G5hmKY2VOiGuh1S9iVGF1oYi+9JIg5zaNomdsZBSdMkiiQAACAASURBVEXQ6miBOU7ersPxSZSSaSihIA55ZRdcDNtARhGNixtsnl7BwCBXyuPkXAxHe6HYOY/ykXnMjE1taZ3Weh2/pT0GB7zmVG9fGFwJPREjWbfCMFBKEDR9goaPiiKECZZrYVqWNhowTSzbwrRtTMfWP22rT8wRqFARtkNkIOP8MfF9Id1rBEiF9EPClr7HCiHIL5S1gFfKkpsrUj5cxS16SCUxbQMrY6HDlMd5TGKx3TBNrIxN6WCFgx84Rn6hRNAOqF/cZP2tZYKmr+97cWgzt+CRmy8wc3SWTDmr51CtRWutDgIy1TzZah6vnMWwTayMQ/nInE5MPltEScn620s0Lm4SNLta2CtmsDMOtmuTyWXxclr0dPIu2WqB0oFZqjcfwPJcNs4sU7+wTmuljhACr5DBtMzkLrk9EoFYqYHXexn9d6D39pW+d3DgwIGxyz7zzDMT1b2+vs63vvWtsctPmoS4H6dOneLkyZMTHfMXf/EXY5e9nJjuu+Hw4cNjl/1//+//7fn5J8VeeGDAziLEbtgrAaNfjJlkDfzgBz+YONn8008/PXbZ/jXgOM7Yop7v+/z1X//1RO164YUXJhrTYRw6dGiscmfPnk2N8Kb44cUke+DVXidTTDHFFNvBuqIbjhgy+dtTjH40Sgiu4cfKAYu32JLUcEycghcTNCLJ9zrFmOiJPRIZgenZeAsFTM8mu69Ie7lBd7VBWO8StQMiQ6AiiTBkL/wTIiVLeol4Y7HD0MKHjGMualJOpZyV9tSIQy4Rk0tBQOPSBsI1KR2excnb6fHXAsLQYbQ6q3VaS3X8RgclJRIjTsDbQxL3dOB4oUkqbTGrrWAlfWG34lfitSGVhEgfZzkWuX0lcnNFcotFMpU83owmaQCd6HQ653eFnp2DUEphOiZeOYPlWMhAYtiJWfOV0DbxnpkoGmMdciU2vBMgFhHePU8M1XcPASTYWQe3lCFTymOZFrXTq5z5m9fp1juEbZ/2chNhCPxGh8K+GSQQdgP8tSa106tsnFpOE9dni3m8Qg7TtTSB3+rQWm5w5htvYDoWAoEMIsJOQPHgLN1OG8M1yZTzdDdb2jMKRdDssv7WEqtvXKR2eo1WtkHQ9Zm9dT+5+RKWa2G5Nvm5IvmFEn6jQ/3iBss/OEtjaROBIGgF5BcrcagiBYYm4nOzRfILZdprTVordernN+jUWjQubGIKi9mD+3CzHk7JJZIhspWEVuqz5k8Jft3elABXOiePX+uy9uYlbaHu2JT2VygcqOLmPdxCBifv0K23Y0FYIJS+v3Y328gwFggiRdgJ6G60aF7coLWsvSaQ+lqCtk97s8HayUtceuk0hinIzOQpHZojO1vAyWfIz5cxHJPsQhEpJI3lDZZeOU3YDVFS4ZzPIGxBpppnRimaq5vIeoRh2vE1aVci1bvwePrE9L/oWyUDy6VvPveLeiLuxzSfkuhJvf2qpj4TUgmaqw0sb43u3R28UpbCvjLthhbNvFKWwvwM9UsbrLx+jvZaE0OYcf39rhF9uS/S9grtLSK0aN1bgrGQEbc1EbGllJiuiVvO4uQcLbLGxwnDIuqGNJY36Dba6Tkx+ubGUPf0C2OdRpulN85TCefIVnMceOAYleOLcfjDCNACUOSHOlF9IqCCXjORwC3Y5OeLeCUdvmPt1BLvfP11glaXyA9pLjUQQtCpNSnunwEpCNshnXqT5R+cY+n75/CbHaSUlA5XKSyU9RaR3NOEgWVpkSYKI4KWT3ejQ2u1ydo7S0g/QgYRSijsvNun0UoQCsPS920hBVEnpL3ZpL3SRIX6u4aMIrr1zuRWOXuM6b17ir3AHXfcMbYw8bWvfY2PfOQjY3lhKKX47Gc/i++P56mUyWS46aabxiq7HX77t3+bT33qU2OFWnrttdf48pe/PHbdt9xyy5U0bSTuueeesUWUF154gRdffJH77rtvz9sxLibxwNhJTFhYWOD48eMTC0433XTTjsmqx82BAYNiTLVaZW5ujuXl5V2Pk1LyO7/zO/zbf/tvRyYSH8bJkycnEp/uvPPOgb9vueUWvvGNb4x17B/90R9x//33jyXkNBoNfv/3f3/sdo3CXXfdNdb8bTQafOtb3+LBBx8cu+5EkHnkkUeueei0KTQWFxcpFApjea1d7XUyxRRTTLEdhilWBmzet7Um6/eCIE3+ORntNU5pTXD1W+X3vCy2PtoNWMAJhWEZmJ6FYZu94tMnwomRiAnCMrBzDt58ntzhGfKHyuQOlsksFnErOaycg7IMIikJgwDf9wmCgDAMieLcD0loC+2pYeI4No6nrSRt29KWpJYO5ZAIHHGwewBkJGmt1Gle3CBs+5qkv8Z9o0N8NKmdWSNodHprRw5aZW6x0kx4JUPHPLdMff127KVhWzaWaWEaJobQ4VFM18YpemTnCxQPVZg5Nkfl1gUqty5SOlLFK2cxHXOI2GTQWWC6BjS2eB3ozklIQsMU2BkbwzJRkRokjlN5TqW053avbU++00F7ME7jedINHHFlJ7wi6DVh2jZeMYeb9TANk856k7VTl2gubdLZbFO/sE7t3BrdWlsTqAKiMKJb155P7Y0mzZU69UubOpeFQOdFsE0d/1+IdCwNx8RwdUgahE5eZ1gmdsZFmIa+oykI/YDWWp3m8qY+x1qDxiVNDgvLQJi6bqfg4RQyBB2f5vImjaVNGhc3qJ/Xlt1BQ5O3CqX3QNvEyWdwS1lkJAk6PsLSngKgiPwQv9UlCqNeeKQ+L4mB3kvCbyWEfszYRt0Qv96htdagcWmT5lKN9kaTsKtzFpiupT0dRC/cX0Lmh90gzu8RafE0kDpJcr2tQ2PFuYiEgMgP6W62aa3WqV9cp35RJ1APu4HetxxLJ06fL2lvmbi/Zai9RSzP1mJJQydQtzxr8HrjNul2hFrURWE4JnbGjsVfOeAlpVD6Pal0MusgIuoEGMLA9tzYuyHx8NAiBUIgLFNb+Ts2SkHQ0vcZIQyClk9rrUlzvU4YhuQWSswcnae4v0K2UsDOugRtn/W3L9HZbCEMcyjs3zC2s/HXA6GkBAFWxsbK6uuUkdTjJ8DNu+TmipQOz1I6VKF0uEL5UIX8QhnLc7RXVnLiRLBRg3tY0skKvWfo9i+xeuoizZUaURhpgUQpWisNgpYPSocdC1p+7MkSa0Nxm03LwCtlsDM2SkqaK3VWTl6kfnGDzkaL2rk1Ns6s0q13kKEEpb1pOhstahc3WHt7ifrFDbq1FpEfpfNTafUCGUa0Vhs0l+txzosIN5/BybpYjolSijAI4+8H/fdAgYwi/GaH9lqD+oUNHS4u42FlHCzPRgFhEMV5o659MupxbhNX6RYyVrtGo6/fp7jmuP/++8dOTC2l5Dd+4zd4/fXXdywXRRH//b//d1544YWx2/GBD3xgLKJrJ7z11lv8+q//Os1mc8dyr776Kr/+678+URLgqyEc3HfffWQymbHL/9f/+l9544039rwd42ISa/3dxITL8cLY7ZjL9cAAeOihh8Y+9jvf+Q7/7b/9t13nz8mTJyeeZ8PtmCRs2NraGv/5P/9n1tfXdyy3ubnJr/3ar40dvmw73HfffWPnRfi93/u9XduVoNVq8Wu/9mt8+tOf5l/8i3/Bl770pbGF0CmuLu69996xy17NdTLFFFNMsR1GZGrrZ0i2eUQZaT08gTfGFT9VDRMbg80Q9IWYNkS/ISWj7a2nGAtKIaOY3jIF7mwOu+gRNQP8jTbttQadtQZBrUMYaK8BQyiU1KE/hNmzAO0PXSGUqcMkiYTYl/p3NDEnJEiho4IoBd1aC8Mx8OstnLyrSUTjaj6q79AlkUKFitZKLGB0/DTuui4Q/xSkogbxdadUoYhDcKEJGqkUQiqUkNojRcq4swy8co7sXJ7CQpnC4gxuUXsY6TAmjAyrNZ3tO2NL/yTG7ar30m/HdtOC1Op7IqT75qQj8m7N7WtBOg3ux8IQmLaZhgjSjgUK4jw7qj/Qf0KYKolUEQqpvfOEFj8Ta3YZi4Wma5Kt5lm4+yhOziUIfKKOT9jyyS0UMSyjJ4yrvntMbKEOgKkTLSOGaWe9PoXQXh0yjDCEgWlZ6bGhH2C6VnqeRGAyLFOLkrKIlXOQfogAnKxLbW2NxtoGYTfQe0c/96MGzq77KU72nTpoxPuHQIf1MUwj9fhKyPs0IfxA1UrnMTDMnpGC0P2eeJ3J2FAgzcsR52Mx7Ni7zkzEZ5EKtlqgthBCkJ3JM3vrfn2PNgwsx0IIaK83aK3WkX6kBew+yDAibPuEQYAUEjvnkJnJ0VlrIn2JaQmdTFooHRIp0HsoQhC2fDq1FoZtkKnkMC1Dk++WkV69MA0sQ2BnXJxclnCtTmu1QeRHqQAmw5D60jp2wSFbzWNlbJy89hoKu6H2oFmuoUId1mogfFRqlDGsQPVeIvUS0cKLwMKbyeJV8hi2hYzDeckwws7buIUM2UoRpUJAYmAi4rknJGCogVMM3JQGZzAYBpEf0VjapNtos3l2FcvRAlfY8fFbXdxcltmbDxC1I7rrLaIgjJdNuiDBAGHpeSbDCBVFKCXBsMAyIYwQ8bgka1kLM1p0MBwTFYKw4v6L16MQIEzwmx0uff8MudkCxYMVbNdm7rYDOHkHO+ey9s4ytQtrtNcaIBUiToYtDIOwE9LZ2NQhtUKF5VrM3rwfO2tjupb2Jtpocul7Z6hf3LjWThg/lJh22fWFYrHIgw8+yPPPPz9W+Xq9zn/8j/+Rn/iJn+Anf/InOXr0aLqPtdttXnzxRf7kT/6E06dPT9SOn/qpn5q47aPw0ksv8Su/8is8/vjjPPzww2ncdqUU77zzDk8//TRf+cpXJgppU61Wuf322/ekff1wHIePfOQjfOlLXxqrfKvV4ld/9Vd54okn+OhHPzoy38H58+f54he/yN//+3+farW6p+3dKw8MgEceeYTPfe5zE4XVe+SRR67onP0YFmMmGQeAr371q5w8eZLHHnuM+++/n3w+n352+vRpvvKVr/D0009PRMrefffdW8L0PPTQQ/zO7/zO2PWcOnWKf/Wv/hV/9+/+XR555BHm5ubSz9bW1vj617/O//k//2dPkiV7nseP//iPj2U5v7S0xL//9/+eX/qlX9oxoffJkyd56qmnOH/+PKDn3O/93u/xhS98gY997GP81E/91NQj4xrixIkT/NVf/dXY5a/GOpliiimm2AkjBIwY23zf6HkyDD6iaOvN7QiwEQ/Kl43txYt+e0IB2uqyq+M+G+6VWf3ciBg5TgnZZQisjA0ZB5V1NYmetbByNt3NNkG9S9jydSgHX1vBGolwkQxFEkZKgVAKGQsYQghUGoZKaO+PKEIqQ4dU8kP8Wpva2TWEYZCbL2F41tCX5P7fr87jtDAEfr2DX+vQXmnohMBJ/pYR+l7i2TRAfSb8jRFnpVAAEmUCGJiuhW14mJ6FmbXJVHJk5go6bFQ1j7A0wbjlQW23S54yDNsiEdZUpAjbATKUKenb212u5ARXftAknhW7lb32Mdvj9gmFjEL8dofQDwCBnXHwZnJE3ZAojLBsGyEEURD1jQkpSa3fEzEnqsMhJbEDnay2Vp+5aR5QrL19kaDtE3V1ImRhGr08CjEHniLRTAyR1t8TO9CeAX5E5EeYjq2TLHs2lqu9O6yMg5VxYu+KnrAg/ZCoG6a5DCI/JGz7+hrDCL/dpb3RIOqEqaV72p7B3uuRwf38dJ+3F0YimjBiLvdX2KfgxT8VYHk2ds7FcCzdV4peXwyMgdAibtJXMWQU0W218fys3vcRyFAhVYSUiqDtI5TCb/t0NlvIUPUJGLotURihIklns0Vro4GddSgfmqW72aZTa2vLhbgvLM8kW/aIghC/1aFbb7P+9hL5uRK5aon8QpnmSp0wCJGRVl+cvEt2Nk9hcQbTsgiaXWpnVwhaXUzL1HM0DNk8u4yVsZm/8wClmTxOxtO5WpbWaa7UCJpdhKGFNIXq0zsFQ8Gjen+l4xZ7nZoGlmOSqeg8Ivm5UjwfmgTNLlEQYRgG7fWm7mulxRqBgd/o6hBfRm+uGrFYZlgGGCI18DAsMz23lEp7xLg2Sioal2qxKK5642+aCCXobrZorNR00vh4UiVeHCqSBO0ukS8xhIWb88jN5okCiYwUtusiQK9j0+hpH5H+DilMHZZKe2AOdJeuXynCbkBnswWmwMm6OBkXr5whN1dCAXbGYdk/p7slXg+J0KIk+I0Om2eXcQoZvKKH4RR0vq9KicLcDLV31mhc2sUaOdm6+hp57ffTKabYio9//ONjCxigPSyefvppnn76aVzXpVQq4fs+tVrtsmLd33PPPRMnB98Ja2trfPazn+Wzn/0sxWIRz/Oo1Wp0Op3Lqu+jH/3oFXuHbIePfexjfOUrXxm7bb7v8wd/8Af8yZ/8Ce973/uYn59HCMH6+jonT55MiV/Hcfj5n//5PW3rXnpglMtl7rrrLl5++eWx6rvjjjt2FWQmCSE1LMYcPnyYH/mRHxk7XBPo3A5PPfUUQgiKxSKu617RPHviiSe2vFcsFvmxH/sx/vIv/3Lseur1On/4h3/IH/7hH5LP58lms7Tb7bET1k+Cxx9/nK9+9asEQbBr2eXlZf7Tf/pP3HrrrTz00EPs378f27Zpt9tcuHCB733ve9vOh6mQcX3gvvvuY3FxcaL8Fnu9TqaYYoopdsIIAWN7gWBrbHTRFzcgeU+hWQTRK0PfZxPFVx86Ftg1HrzQFoxKKVQgCRpdwk6A6Zg9K/1RBPMUEyMN3yQUZtYimyniVnNE7QB/vUX7Uo3ORov2WhMVgQrAMLVYoS1TdZ4IwzAQsZWkSjwxEAihMDCIZERogIi0uCGkQnYiNt9cRiiDzEweO+sSRe+eui8MnbC8u9lm89QK3Y0WSIUyBieW6PsfSANSJOSq5lNEOm8BDGXoHLSWwC462CWP7EKR3GJJk6KerQkeAajBsClT7A1EbDncrXWQQYRhmn3E7rVt23sKSUJ6AKEIuz6t1RrdRhsJOOUsxUNVHUJqo4lT8DBti26toy3iE+Ev/ocwSL0CRC9RvWEIzHwGt5jF9CxaKzUuvnRar1sFbiVH9fb9WlyNPQ5U371KpKx/z2J88Janc2X4TR+3mCU7X8bOL2HWWhimwC1myFYLWpho67A7kR8QtLr4tQ5RNyJo+Ky+rsP2GIbAK+fIzRbxG11UEGkPi16D9D4aiwjC6AkTSgiU6LvXJbfpvvtvL+zZ9iGMlNI5d2QccsvOZ3DLWSzP0cKpZPCemjoyiLRbEpdIYQjCrk+31cbNewhjEb/RZfVNLSIFHR8n42BndN1J2KpBQwmFjCRRpGiu1KmdXSVXLVA9tkhno0Wn1qZb7xJ1dNiqTDHPzNFFus0W6+eW6NTbLL96nkw+R25/gcqxBcJuwOa5NTrrTYQQ5CoF9t17lOL+CgpJe1MnMg+aAYZpoIRCyojNc6vYGYfZ4/vwFnK4mQy182ssv3me1mrP6lEJFTsP9OZj0uOD/S7SrpMGOlG3beKWMhQPVqgeWyRT+v/Ze9cYS47zvvtXVd3nMve9kbtL0kvxIvOlBVFiqNeyJFpRZEZOPoQCHDmxLSlwIDkIBUN24MsrG7Zlw5AB+4PlwHAQWwgCwxYMQUEMJ0ASJYpjJ7YC2ZIoXkSRWnJ3ubuzc585Z861L1Xvh6q+nTkzO7Mzs7O7rB91NGdPd1dXV1f3Of38n8sEnZU27aUNkn5M3I9I+hG99Q5cEEiTzXebaiuJY4Sy50VKgQwCgpot9i2kdBFLEukiLIxIIYoJmzVm7zsJBjoLGwy7A9I0YeLUNKfefIapU9Okw5heq0N3tWUnl8yuP0AJ0iihv9ol7cWEYYOpk7Mcv/8Um0ttBq0ezdkmKgjot7o2KijbVlKMlwHcMRWiGxgjCGohU3fNIgT0VjZpD9ZsyrNBhGrWCBo15u47Revymq1l4SI4jCGvPQOGJIroX+2wfH7A1F3Hmb7rGN/1toeZnjtGbaJh0wf6wqSeO4D777+fD3zgA3sqap0xHA73lYomDEM+9rGP3fD216Pdbu/L2/z48eP8g3/wDw6wR1Xm5ub46Ec/yu///u/vabter7ej6PTlL3+ZH/qhH2Jubm6/Xcw5SAEDrDf3bgWMJ598clf7zETs6zHuWD760Y/y7LPPMhwOd9WnDGPMvophA7zvfe/bNsrnQx/6EP/7f//vG/JS73Q6dDqdffVtJ06ePMmHPvQhPv/5z+96m1deeeW6aei2IxMyJiYmeN/73ndDbXhuHCkl//Sf/lM++9nP7nnbg7hOPB6P53oEmXeuKQkDQlAxio6tH2GKD4p1nTKQGUvMyAaZQWjPHsimup/M6DXqWZwZrIRBakk6TOgvd6hPN6lPNxFCusKsRR+8I/o+MO7/hE2rIqRAuoK5qhag6gG145PUT06R9mLSXkzSj0iHMWmSQpygwwAlZdVO5bynlTPSC2OLbepUo1OB1jbiINoYErUG6MQUJ/LwAy8qhx/3hja//SDO05XZXRcGnaIborg0KJV1TVKMNMiatJ7aEzWCyRpBMyScrhFM1WnMTVA/PmlTcUlpvbddce+8dZ/rYl8U6VXsw1HcHdJd3CDqDqxgJLOlZUFqVKLy7J5StJIzxOskJepoOkstVl+bRyrF6Ufvp3+6Q9TpU5tqkEQpq+ev2fmuBSYxNsorMfbfwhY5RoPQ9qRprdG9iKgzJBlEBLWQ4/eeZnisT5om1KYapFFsPfGzVHlZ3RMj0InOl2GMTQ+kQeHqahjN5sI6QkmOP3iaqROznPl/7ufYmZNooZk6PYuOUqQQCGMNtkYbNhc2qJ2fZ+LkLPWpBifuvZupuVkbtTFRozZdR8cJHWGvdbHla9Wgk5Q0TrF2VkFWNiiLDDGxLnwK8jRFEoG0IoEx1fsmFMZiDd2VNu35NZonpjl29hTBMGBqZpnWyqqtF+LOgUlSSMspqUwu1KIh7sX0NjYJajWmTs4BglNvOks8iCvptfqbPQYbXfd7Qefm/rKRv7PcYuFblznxwN3MnDnG1JljqHqNpBvbcUZRm6zTODFBe3GNzZU1oo6Nlls/sYQKAsJmnVOP3MPUyWMknQghJI3jE0zffYw0Slh4/iJrry0ybPdtDkNpRXVjDFFvSL/VZbBpUyiFtRrJMGb94hK9tc08hZkdQ4NJDcJIpFHoBHTi0gm6FIRhs8bUqTnueiSlPj2BkgoVBoRTtqi9wNBZ3GDpO9fYuLxCGqWY1JDqFD2IMUbn3znGXUsykPlr6vgsJ77rDPWZJuFEjamTs2hSjt1zCt4GRmv6rS4rr85jpGHq1AzN2UnmTp8giRJ0mtI8OcHUmRmi3pCrz75GZ7GVR6dl9UoEdpySOGHQ6rK5uMHa64sEjZCzb72f/lqX4ebAXsfDhKVvX7Hbpe4c63JhdjuD0iQliSM7hkpiUoNUkomTU9QnGpjE1XoZDJk9fYzaZN3WqlnvMuz03RwvrmMhIJysU59u0pybIh1GRN0B9Zkp6tNTNpJmeYO4NzzyGlsez0Hy4Q9/mJdeemnPqZ/2y8c+9rEdCzMfNR//+McP3dP7qaee4sUXX+Sv/uqvDqzNOI75sz/7Mz760Y8eWJu7TSHVaDR2NWbvfOc7+dznPnddw7yUkne+853XbU9KyeTk5K4M9uOO5a677uLjH/84v/u7v3vd7Q+Se++9lx//8R/fdvmpU6f44R/+4T2JBDeTp59+mueff57nn3/+puzvbW97mxcvjpB3v/vd/K//9b949tlnj7orHo/Hs4Vg1OhpxokDIrMwjWJ25/19ANa9IpvFVmNLuVitzFMZ2CKTveU2zeNTcI9N55C6wqM+sfHBkekYaGfOlRC4XNQTru7DcL1Lf3mT7kKLeDkmHiToKCVIUlSgbGSGlNbA53LJ2ygHgdQCpQWp1KRKY7R0BVkjawgy2qVmunnO8Vn++DRKifvO8CkFshx5JLDXjtnq6ZwV+sYAqUFKSTAREhyrM3V2lom7pq23eD3IIzQQrpi69wg9XLRBY4i6fTavrRF1Ild7wN8zDoxxYriwKWSMtmLAtRde48SbznL6kftdMWZ7nXdcgWwwkNgogWQYo+PURXFpdAqkVsAQSNJYM9zoAzBs95m+6zgPvecx4iSiH3WQgSDpDkmjFJ2CiZ3RWUvQNmWVjhOcKgBxgkgNigDpBIz21RWiTp/Jk9OcfNNZ5t5+Am1SIt0n6g8ZrnVRYUg4UQchMQha82skScS9/+/DHD93mlNnTyNVQAokSUQcD4mHQ8wlF8qdqRciE/UNSZTY2gGJu9eoUt2AxGCG2kVviFIbCowtTm9SbesRbDlHNo1a+9oaMpCcnZ3i+D13c/ep+1g/vcSF518imAwhBRNpzDBFp9qJfDqPoJMoSCHpxgyWO7SR1CcbnLz/LA89+VZb/0hrjIBoMOTaixdIen2E1O4kZvVQpK0doQy9lTb9dsfWwjAwe/Y4px6+F6kFUkskAUkS0eu36Xc7ICHpR8SdiOWXrzLo9Dj16L2cePAMwcN1AhEikcRJRK+/ycqr81z92qsM2z2X1qg0aZ2DRxIl9Fsd+q0OzEwz3BzQvrpG1Bva+g/ZZqmdp0pbAcMk2FRKWtvxjzT1E02mTs4xc+Y4Z9+aIIWtY2FkStTv07q0yvrFZRa+dYVBq2+zdUmFwNjaMFjVKj/LRljHAqEIwhpzZ+7ize9+O83ZCYzU6HqKJuXUg2e5+977AMPG1RX6q126rRb16TonHrib5uQ0KgjAGOJ0yDDtM/+1C1z52gXiboRSQT4lc8cQl/4sjRI2ri4TvBhy8v7T3POWN2FSKw4mRtNeWGf98jKkBhG7+ZoCGiSCFHuNJElCNIjQiUZKZcVDIajNNDj+Xac5fvddyMBGaoqaQNYE6xcWmP/mRfrrXZqz0zZaKAXtInuCmmL27AnOvPVBAqUQRQ6CCgAAIABJREFU2oBSaA2rl66yfvEavTVXh8U5Z3g8tzuNRoNPfepT/NIv/RIrKys3ZZ//+B//Y/7e3/t7N2VfN8I//If/kCeeeOKm7OuZZ56h3W4fqBH4S1/6Eh/84Af3VBtiJ3brPb3bVE6Tk5O87W1v42//9m93XO+xxx7bdZszMzO7EjC2O5b3vve9LC0t8YUvfGFX+9svJ0+e5FOf+tR1i7k//fTTvPjii3zzm9+8Kf3aC1JK/tW/+lf8yq/8yqELoPfeey+f/OQnD3UfnuvziU98gp//+Z9nbW3tqLvi8Xg8FYIsFLP8t6AkXIx4uNv0NePc3UXViiysnFBEZuyFclRI6a+pflZ5X0qdoaOU/nqX3som/bUOjbkmytXCMCM2Cc+Nkw9hPlWELeIpnRABYJpIJVATIY1T06T9mHSQoIcJ6SC2YkSckqYpGO0K8do0TSaLWHCpSAQSFIT1kKCmbN77sb06PDkjEyRkKFENheyLrMatnYuiWDNLfSJw/Q8kKlTUphuEk3XCZt1GW0zWCKcb1GebdlmzZvODG1uI2Aoe5XgOkx+hj77YP9LVUIi7Q7rLbTbn14m7A3RssAWaKY14ySDsuXHyMCRbIN3ZXRlu9lm7sETciemW8tAbYNjpM2x1Manh2jcuIqRh0OnTW+1gsgaMYeU7Vxlu9NlYWCFq90kHEf21DovPXqI9u0qt1iQlJdZDdGprK8SdIXE3YtDqgoFrz72GagS0rqwSd6M8+iLqRqyev4aOYe3ikvPsltZoe3GJaGOASG39nsRE1I9NMHPmOADDbo80jkEY0mFMf7XLyrev0V/tUg/rSBWggTRNSJOE9vyaNbxmtSyyaSeFXX51lbg3oH11hd5Kx0YxrHe4+rVXrZf+wH4P2hoOguFmj2vPXSCcqNO6ssRgs7clM6MorNEMNrqsawMaWnPLhNTobXTprLQQLUgGETpOiPsD+mubKKUQEpJBzNXnXmP1wgIbV1YYtLqkia1fsfraIvFmRG+x5YJa7LElUUJrYZl+q2udDSr3taIGiNYa09dszm9AAt3FFo2ZCYSxIrJEkSQJw6hHZ6VF3Its/Qw0/Y2ujebTms1r6wSyhpIBAkiShMGgR2dxnd5qG52kW377CGxB+bgXsfzyPL2VTWr1OpsLLZKeixSQokiDBMT9iEvPvkJtssnG/ArJMEZISW+1w9WvX6Qxs0hQD0saiTvRQhNHEd2lTbormwzbA1v7QqnS2Ij826742skGCnRkI5ouf+M7BI2QVGhSE6NNTCBqKEIQhn57k0GnSxIlrL22TLQZUW80kEphMCRJTJQMWL+4Sn+9j1UFSj+kSuFBTrent9Fl5TvzRBsDOldaZEOiMQw2eww3rUD0+rOvgBT0O5v01mwOb6nsoEftAa1Lq+h+ilSSQatLEie0Xl8j6SR0rmyAEmij85O08uo8neU2aZwAwh57vUZ7ftUWQI9iMJJkYFBSuHuPQGtDe2mV7soGw17fCdcez53DyZMn+bVf+zU+85nPcOXKlUPbjxCCH/uxH+Ppp58+tH3sl+/93u/ln/2zf3bT9ler1fj5n/95fvu3f5uvfe1rB9LmcDjkP//n/8yP/uiP7rutJEl2nYpoL4LJe97znusKGLtJH5WxW6FjOBwyGAxoNBpbln3oQx+iVqvxx3/8x4dat+jee+/lF3/xFzl58uR1181Egk9/+tNcuHDhQPaf1SQ4iLQ+U1NT/PIv/zKf+cxneO211w6gd1s5e/Ysv/RLv1QpBO05Gubm5vjUpz7Fr/7qrx5qijKPx+PZK5UaGFuiMcoCRr5StrCctmck/0/lif9GhItqL8oRFraop8y92bf74SGk9ZqNBhHd5TadhQ1kIJicmLYFV433Yt83FYNXcc5zE6+LFDCArAc0G9NMnJ4FKTGJJh0k9Bba9rXUZrjRRcc6T9sglSQIrIdnMcWyZYLaREitWUNJhUCiKRtURjp40LgC5GE9pD5dJ9kcEGsDSlZSSaFtYVSd2sgfYSCs1wgnQ6bvmWPqzBxT9xyjcXzSjRnoxHmbGzBJZrRz0S15ontTHGGu3e18TXhGqBgLQQqJUgH97iarr8zTen2FZJiAsQZLnZ8DUzJmem6EopyIyee3y0YHStjaEJ2I9dcWSQYR2YALFSADRVi3qQval5YBjQgEMgwRqmYjuIzm6nPnSSNb+Nvm/YfhRpcrX/kOOtGkiUYoEDWRB1ZIKWy9jFqAEYYLf72MSTVBrY4KAvu9om3KpvnnLnLp/75C2KwRNuqEUzWCWsjq+WvMt161qeX6ESjJfe/+bk49eg9Rb0hvrUUcDRHCpsvR3YSFZy+RJglpErnvJolQChXYlwxVSRh1M1ZZg8P6xSXWzqekaYxUirDRpLPUYuP1ZZASIRWqJlGBtcb2NjZp/dUa2mhUTSGVQsrAOQe4oRYyT5c03BwyaPVZO7+Ajm3BbREowkaIQZPGkb0fKokKQoIgBGzKqFf/6nnSVFOr1VBKgZREvYjBhSVWX5nHRAlaG1f/27XRDJA1ZzmWEmMk9ndArnZZo7qB/mqH3uImaZyg4wRtUncAEqkkMpTIUCEDK1DImiIexMT9mI0rKyRxbO+x2lgRTUlkGKJCiQpsOiSUKNUfscK1UoK4M2TxuUt2v6lGBSG1RsNG5Lh1s5DEYXfAS3/5dbTWNJpNgiBEKsXmUpv119fQrjg5eXJBi1IKGSg7f5Wy9SyCIHcKMeXfW+4njRCFfmFSWwtj+fxVrj3/GqkxaKuLILUVHYw0IA0yFIQTVrCY/+brpFGCTuO8iLcdmwClQmQQYpSxDZXiDsHY3wKueLhN29Vj9aVF0r7OoyWDMLBzugnGpKxfXQIDUilUENhC4VIANnJqsNJj+cUraJMSTNixW35hnoXkdZJ4YFNpGYMkQIkQ1ZDWWUVAd9Dmlb9YBQ21iVpetHzttVWSwSsYnWBM4grPS5sCM1Q2kinwd3jPncepU6f4zGc+w+///u/zf/7P/znw9ufm5njmmWd4+9vffqDtPvLIIwwGAy5evLjvtt773vfyL//lvzy0wt3bUa/X+bmf+zn+5E/+hD/90z89kN/r//W//leefvppJicn99XOXuqI7EXAeOKJJ6jX69vWnajVarzjHe84lH23Wq2xAgbYiIf77ruP3/u93zuUvP1PPvkkP/ETP7Ht/scxMTHBL//yL/Nbv/VbfOtb39rX/qWUPPPMM/zN3/zNngqX78Ts7Cy/+qu/yr/5N/+Gv/7rvz6QNjMef/xxPvnJTzIxMXGg7XpunPvvv59Pf/rT/MZv/Aarq6v7bm+39Ws8Ho9nJwJD2ZDvDKDZP/PUHuNuNrv1cN+DJ7wZfVgsxI9MxBBIhFC22HMpcsSMiwYR1qt62Oqz9so1K2CcnqmuW3IgvEmm7zsSs4tzbFyqkCyPPApqsw1EIKgfb5IOYnSUkMYJcZygBwm6F5NGKekwwThxQ9YlwWSd5qlpJs/OIWoCrTPD1c3BGE2aQH1ugmMPnXb1VSDpxaSJtpEZNUVQDwmbNYLJOqoRoOqKwNUJsREYDWSoitRmmTcyI3N0NMxlzLH6HwW7YMxFbrAe02mSMuwM2FzcYHN+ncFGz5qKt/zgOtzoHg9k934ZKIJGjXy8pbRRSRIwAhEqENbjH2fwxKWtk4FykVyq0PyEQNaccTK07QhVGO5FHkFlv2+CoIZRpjCoiuI7UgYBQROEkmhhkKGiPjvBsQdPE9RCdD+23vhCMHF6hnSg2by6wfK3LtNb3kRIBQhbnkJJpAhsrRUXDiGkjQiS0gn2o9+PxprTpZQYZ2y3xZlt9Juqhbk1WwhZeMcjrZHYGFerYTT0ovo7QAhr2JW1AKEU0hXvFsoKx3kBZrefck2sIAiR0o4fUuTt2T4HICTSZPWA3DGrvBp4EYmQ90eUltl9omwtEqkExqh8PSuIuGMv4w5XBpJAhJjAFA4Z0vZNKFcI3WDrX2y5HdvJIoPAzjFt7Nhn95TKbwy7RRiGGGPsfMy6IoUVkQJXj2TEaUQIaSMMlQBZ/iIoz4Xs2qj8q1hNWGEgaNTsWLufdMIIjNAg3WeqEMGlEoiacnVVrKgihD039vrQ+Q5Gfx4aJ2LYu6Sw5zjM7qM2olAqO8Z26khUGLrjdemaMp0e7HEH2LRa2Dluz7vtk5Ih0gT29yDSrqeyguZ2u8ClahfKbStAhAKFIkupZtwcVkqV5oz/Jei5M2k2m3zyk5/kySef5I/+6I+4fPnyvtsMgoAf+IEf4J/8k39yKB7Ux48f5xOf+ASf+9zn+PM///MbaqPRaPCRj3yEv//3//4B9273SCn50R/9UR5//HH+4A/+YN8pec6dO8dgMNi3gLHb+hew+ygIsKLNO97xjm3FsieeeGJPRv69Chh33333tssff/xxPvvZz/KFL3yB//7f//sNFdEe5b777uPDH/4wjz/++A1tn0U6fOELX+BP//RPbU23PXLy5El+8id/kkcffZS/+Zu/uaF+bEej0eCnf/qneeKJJ/jDP/zDPc2bcRw/fpyPfOQjvOc97zmgHnoOknPnzvGbv/mb/Nt/+2/56le/esPtNJtNPv7xj/Ov//W/PsDeeTyeNyKBMVl1z8Kre4eq3WOejkfXG11uthpexq2WvylHb9i/uXjhDB9SKFuUdaSIcfG+8NQUUhK1evTXOtTnmpx89Kx9kBbZg/e449rhkDzj2cV4GXCerJk3tSCcrVM71rSGNAMmTYkHEYNen2itz2C5Q7IZQXuI1jEGjWqG1E9MMvvmu2mcmLRT9ybXhTDaYEipz01QP24Lf6ZxSn+5C90hqhFSm6rTmG0ycXKayTNzhNPWS1sG1jvYpK4wMNhUJSXyaKjRKZ39w8/PGyaTH0p2VoQQpHFMZ3mD9vwa7avrpIMYGZSNqRXT4s3t9B1KHnyRRxdUR1gE1gM9n/P514MztNaCfGV7ybgaDAabu9+JE0XKQIEIQYSykh2mqg+K3HiqwtpI+0UPZaiQoXR1HwwiENSm6px4+AyzZ08SqgAlrHd6v9tnc71N+9IqSy9cJvtuyrslJQoJBOODHp34MqrAGbBCggRJkNfVsQKGHJmmhYAQBDI/1K3iaHmTYhsl5UjMZjYQqjKI5TOoghBVWlY0a0URwjHtVdcc+X1QvM+C0WzUjsKgqptupzG6AZWBQgRqh3VsGyKLVCg37cQjpQKMyj7L1hkVme1vmDAveCrzRqSUyFo2D5wA56KRso6MVnfKayntdN5KH9mi7hIZFHM5X01oK2KMbCyVAKVQqFKUVLlpM15Uc+1nBeczAZjQoEJR+erKNxWKoLbNeQA7XNK4+0BprgUGaQQyCCsHNXIbsb8b6+4+XuqyFBIZAKjqCOcXhsdz5/P444/z9re/na9//et8+ctf5pvf/CZRFO2pjbvuuosnn3ySp556ihMnThxSTy21Wo1nnnmGH/zBH+QLX/jCrlMxTUxM8Hf/7t/lgx/8IMeOHTvUPu6WRx55hN/6rd/iK1/5Cv/lv/wXXn755V1vOzk5yRNPPMH3f//389a3vvVA+rOXKIS9CBhgi5hvbm6OXfYDP/ADe2prLwLGbozrU1NT/PN//s/54Ac/yJe//GX+8i//koWFhT31KQgCHnvsMd7//vfzd/7O39l3ZI9Sih/5kR/hve99L//hP/wHvvKVrxDH8XW3m5qa4gd/8Af5R//oH1235sZ+efLJJ3nHO97Bl7/8Zb70pS8xPz+/622llLz5zW/mqaee4l3vehdBMO7HpedWYWZmhp/92Z/lueee4z/+x//ICy+8sOtt6/U6Tz75JD/0Qz/kU4N5PJ4DQXzi1/+/TLWo1pnAPceZLE1A2dBfeqjO/r2d97fYxaPglidaXL55bR3tlEJK6f4GKBnY3OJJkgsZo4JG7kmbGa+MYfrsHMfedBcz504yc+5EXhh2xz55bgpCyjwyw6Z3SUiHCUk/Rg9T9DBx6aU0sh4QTIQ0jk8RNJ0F7Gafr1LkDkIwWOsy3OgTdyPSKEGGAlUPCOohQdMWNFe1cioYKxhm83X0+tkiYGT78uwPUxIwRvTZ3lKbxecu0rq8SnehhU5Sdx4KEWM00kjs4aTYbUfTnF1nm8wD/BBPfhHxk4Uh7G13W1IPbvNdYMoWc6tmkptDR42x2brGrSuqRlRh7DnJaidYQ2lJHBWuUdfw1i7tJEWJLWuJ0sfFXlzKH20X1qfq1KcaNE/NUp9uErjoCYB4GBP1hnRX2mxeW6sY5ceOVtkAm78ZFTBGja5jxr48ZiMKg9myTtVIfhARXZmgsufpu0W02OphUHTbjF3jOh3beZuygOHuGNv5OOQyRcWBovgrTODma3bzyQQINzAG93sq22ZUwBjd7+4EDDG6/hiMHCdgjDa1zZzbeaPKXsujvd0Z3aaZ8WuPDNe4RVtx94J82+o9YPTOPubPrhh33dzoHfxArkHIhcgD2bepnsdxZLv7o9/5gz3v13O0RFHESy+9xGuvvcbVq1dZWVmh0+nkxtNGo8Hs7Cx33XUX999/P4888gj33nvvvvb5h3/4h/yn//Sfrrveu971Ln76p3+68lm73ea5557jxRdfZHV1lXa7Tb/fp1arcezYMe69914effRR3vrWt1LLReRbk+XlZZ577jnOnz/PwsIC7XabJEloNptMTk5y/PhxHnzwQR5++GHe9KY33fT0V28krl27xre//W0uXbrE0tIS6+vrDAYD0jQlDEMmJiY4ceIE99xzDw8++CCPPvronqJI9kqv1+Mb3/gGL7/8MvPz82xsbBBFEWEYcuzYMe677z7e8pa38Nhjjx2ZGHDp0iVeeOEFLly4wOLiIt1ulyRJqNfrNBoNTpw4wdmzZzl37hxvectb9h0x5Dk6lpeXefbZZ/nOd77DwsJC/h0RBEF+rzp79ixvfvOb+Z7v+R7q9TpgRdKPfexju9rH7/zO73D27NnDPAyPx3ObEowGXVSLZRsQWz0A7SNx5v64/05sfSDPmhYuJ7kVL5QKCFSACkLSJK0IFlkaqSzU0v6us8ZHm0tZ0FvuMFztI6Rk6sysq1VQLbbpOQIMLv+3RQSSMKwTTjTghFuuC2O/KJlkdpO66lCoWGcMzeOTTJycdv10RmpZuk6yHOruWPIJN3Ld7bgfz41TsvQWRml7bkyqiXtDuost1l5dpL9qvcRsAXUq94a9CBae3SNG32W29oontXtjyBTuynVTsXlnX1nbni6xw7+K3ey0de6J7voSdQYMN/u0nEBRNi4YDaTafjFJiahMrDF73/ardbsD2q63Y2bsyNftTlJOvmQ/BtAdNt19uzs2cr01qv3ZwzblSbXtursxMpfFp4qAaUAUNSSMGTk5O+95tJPVbpWWbDfMuaa4Y+vjxduigTFNjG3z+tdcpWlTlnV2Ov97+5qsCoK774/H80agVqvx2GOP8dhjjx11V3bFzMwM73nPe+6I9DOnTp3i/e9/P+9///uPuitveM6cOcOZM2eOuhs5ExMTvPvd7+bd7373UXdlW86dO8e5c+eOuhuem8CpU6d46qmneOqpp/a03fr6+q7XDcPrhmh7PJ43KAFQSdFULZhtjfvCWBFj24LZOxgh9uJBNtqOlDavtRUvbBSGTSElSUm3aSXbr7ViWbuBq5WRGpIkYXNhg/DleaZOzzJxatoal3z9gFuK3HM31zWqRp+totrRYoyxIkzZIqQLIyeGvTjeew6JzGxspU1sirlexNr5RVZfuUbcGWK0caKnx7MT2Syi7O6+1eCceaO73P3FHPRz7A2BME7B2o6dlu13joxGhWy33OPxeDwej8fjORz2Upj+sFOgeTye2xcpZKm2RFYwtFxzQsidvSRLBU+3E0F2w+h6wnmwFn3KikcW640W786KepfJ8lLbdW3Ngf5qh40Ly/RWN0mGiTU8e1vSrUdW7NsUAkCRJeaIDS9m3MuU3oviM22PIfvP4ifcUZJ5zSf9iP5qh/blVTbn10iGMf7ceHZFdp2XQhqEyCLERHEvcAuK4sTeaPyGoSRs7e419otlHy8Ak/9WGn0d+feox+PxeDwej+eW55VXXuGzn/0sa2tre942TVP+x//4H7tat9Fo+HoZHo9nWwKppE3VJOQW8cFkRlcttn3Ozc034mByZhfCBwiXPsr2yS63tS7ivP5FWcTYTjTJ0koJbJv9jS7RYIiqB4SNGo0TU9SmGyUPes8tSSV3+K3BljQs1+mbly+OBlM6UdLYujpplLJxaYXV8wtszq+T9KIiLdGtM8U8tzxFeri8srQRW+4FxbWv8xRY/k5wp7NfkeAGt99lgKK/zXk8Ho/H4/F4tuNb3/oWX/ziF3n++ecBWFhY4NOf/vSe6r58/vOf5/Lly7tad791lTwez51NkEVg2HoTpWiLLM/Ktk+4ppTX2f4t6wZZcdVqO+MaswvLqwmqUSFlQUJrjdGGNE23FTDKfcj/08a1KdBxQhrFdBdahBM1kALVCJyI47JJ7ebJ3tueDobdjOMeLS1lMe1G8rffEHvajTcdHQWZGJoMY4atPpvza7QvrzBs99GJBjU+sc9Bni1TWLL3stUB9iDjYK6LvQvX5ez8h8ANDNWeNjHjqgGURIxy1h5RWVqU57hZl/92X7nbLNppM8+NMzpjttYfGbNwj21WmihNx+u2c0PX72gb+9n6DmVkUPYlY+3nhrEfQX6776rR9kaz5mWPBx6Px+PxeG5Lnn/+eb74xS9uSf306quv8jM/8zM888wzPProozu20el0+Pf//t/zF3/xF7ve7/d8z/fcUH89Hs8bg1zAkEIgRWbsNbkR3yVgGinImVlmNEbYvxmF/mFG7FTlJx6T/S//WAhb10BgQNhc4ZmAkW/linQnTrzIIivyfbsOFAaivPeVgpBC2Xfd5TZJlCCkJGzUqM02UbUAUSoY7blFyOfdrUV5Bu72ed0/1998MoOeUAKhJN2lFhuvL7NxeZneShu0sDUKcjE1u+8cWZn4Eods8D8SDnlUD6D5kW+s3Z+B/AsoexV5hIwBoWX2ZVWsf0CM9TkwIyt4bh5GuBlwBCfBTYadhaoDvlA8tx77DQC63ufbTGf/G9rj8Xg8ntuPb3zjG3zxi1/klVde2XadxcVFfuVXfoWHHnqId77znXz3d383c3NzzMzMEEUR8/Pz/O3f/i3/83/+T7rd7p72/853vnO/h+DxeO5ggizCQSqZp5EyxoDWaO3qRphCLLB1KMr2F4PGOM9Sa6DJPjejOcHtx64sgIue0KV1XKNCCpAglERIadfVBm00aZIQJ0k5Z0+e9qp4YHI5noV7PDeuz04Q0QaMTomHMXq9S+vSKlJJjj98GjWnKs60o1Se1Xw+oMNlnJvqrTLWe+mHGPvWc5iY6lvhhE2daNJhQuvyKqsvz9Nb2UTHGiEVlAp3l+J3DrhLpUlduofttMVY8W5ftqGjnIWlY9rF/dNGxGU1Ja530DvFzmy/k92KE9t6y49LBSVG35REjAMWLA4L//W2e8Yba3eec9X/d1sIuP4EMfmW28a0lqNf3e+rG+VWMESLkfdH36O9caMS9IGMfe6MdIOdyNrItjfO0WjH32W32xnyeDwej8fz3/7bf+Nzn/vcrtc/f/4858+fP7D9v+lNb+Khhx46sPY8Hs+dh5TKihdKKWQgkUraCAUpMMI48SIlTdMiXVPmXSqsSKDReaqmXKFwy61wIBDKpm+SMhMqrDiS6oRUp2hdRFWYbBv3MgJSo0nShCiNieKIJE0qj+9CCJRSVsgAtLC9ypZlKamUUhWhJU0TWldXWXllgajVh1SDiwXxphuP505BIJQiGSYMNrq0Lq2w/soSUXsIqipejNn0QPtRbVCWPhv32uZjmd1f9/iq5Dg6YnZ1i81qMgnsWGWv6zW6Y5KefbG1xZ3O38hBCmtc3q+IsdNedtGL3e/Ec4BsPQvmhk/QGPHCtWF2MxFueFIcHZXbH7s/vJvx2q6ft8Twlm/7mv1HZBhKE9fj8Xg8Hs+dwvd///czPT19ZPv/kR/5kSPbt8fjuT3IIzAq1o+8lrXAhkeUi2mXHlrKXs5O2Mi8nY0oL3bJobLmEEiUEzakS18l85RRQRiiggAlFEZj07sYXOgGuTdZliZqHEU/hft30U97HJIsr0yaaAYbPZa/PU/cj5i9/yRBs4ZITdUYsP3uPIfJDTwn36y6F/4R/tZHCIFJNUmcsnFxhbXzC3TmN9DaWFG1Mle2OaMHcaK3vXeMa9xUFo2uUUS6HcT+bw7jL8ndHMSNnpM9HPCYzuW3/Ot5QbvvtMqedzpBgopL9l5P4zjTYe4gPXocZuzb7Rse3dbf4G6A7Qctmydb5Ifs9wlQpJsqt7PHi3ecVX0ct8nvmWxebz0Ms83nN48biQu7Pbhez7MbxN6ijzwej8fj8dyaNJtNPvKRj/B7v/d7N33fTz75JG9/+9tv+n49Hs/tRTAqStgoC6xoAHnkghF6vHGkJAxsMZaUi2874cBAHlkhkQSu/oYSKhcwarUaSimSRJOmGlKscKEFwgikUcU+RgslZgW9KQQKMLmHdZYKyx6TNSQIbFHf5ZfmiXoRE6dnCSfrkGgrykjwD2Qez+1HJs6maUrSj9l4dZErf/UKqh4SNOp2pVsgRYrH4/FUGX9fuhWq8ng8Ho/H4/F47jze97738dJLL/Hnf/7nN22fDzzwAP/iX/yLm7Y/j8dz+xIIIYq6FFiDvXZFrPOUUVAppl3BONO+ELkYAO6zbJmxb0pyglvJJWpy0ReZWJJtYvev3WtrYe3Cy9W4Y6jmlC4kB1GslkdguGLfRWgGaZzQW2mz+OwlZu87wdx3nUSECm1ShNvObjRqQPDihsdzS2FvShhj0ImhfXmN9fOLtK+sIZVwOUjMdjZCj8fjORp24/ju8Xg8Ho/H4/EcAj/xEz/BYDDgK1/5yqHv6+GHH+aCPAVMAAAgAElEQVQXfuEXqNfrh74vj8dz+xPktfi0QZuiaHchHmx9Wh6t3ScQtt6ELMQQmSVjNoArpj3alpSyJGIUkSDGGFcPw+SFxMtCRrkjRlT/Xe5fJe1AOVJkdBmZV6Nh2Oqy9M3XSbsR03fNEYRq2/ZvNAWIx+O5CQhIY03ci9i4uMzVr57HpJpgInTC5bhULR6Px+PxeDwej8fj8bzxCIKAn/qpn+Luu+/mz/7sz/IMJgeJlJKnn36aH/7hHyYIggNv3+Px3JkEWWiEMQZTjrygGvUghNgxF7hwERhFLmfh8j1nBSvsp1tiF2SR0TsXMMj2r52gsjX6omioXKdClKIjSkZJ4Zblak11jSzplHDFYY029Ne6LD1/hZlzJ5i8Z8427W7eAuHTOHg8tzqpob+6ycrL82xcXrZXrASnfFIJC/N4PB6Px+PxeDwej+cNjpSSH/uxH+P7vu/7+Hf/7t/x8ssvH1jbjzzyCD/+4z/OAw88cGBtejyeNwaBjZIoohy0NjYCoyRgCEMeJVEx3Gd1Zp04kNfIsLmZnJEwE0SKBFIGW8sCUbRdRrswDhuFUY2+KAsZuQ91Zo/MBJS8zZKYMVJrsFx0NYvQEEJmHWC40Wf12/MIJWgem0I1FEKSayCibPy8FR24R/WeMX3cttt7MerutjjorThGtytbas3Y92LM8uqC8W2MXWc0zOpWZWw/BTpJibtDNufXWHzhEsNW39535M5CbJlSdrlD4FYe1Fub0fO3pTbTOPJ0YXf2uG8Zmzv8eG83tnV82OEes9v7lcfj8dyOZPUPd7Oex+PxeG4uDzzwAL/+67/O+fPn+dKXvsTXv/51Wq3WntuZnJzkXe96Fx/4wAc4d+7cIfTU4/G8EQiylFFpmpKmKdqkaONCJsx4o09mDJdUUz9V5I2S8S9LH2UwWTkMcBEao2mjhBAuEgR0aqMvrIiRiRcGI40VQCgMNLk8kj/sO5lBCIwo6nQIsb3BK1smpbRRFkPNYLHDRm2ByXvnaJ6ZxqQGDiGMzuPx3DhlfdJow2Cjx8rLV2m9vkrSjTG6vOZeOCyV0huWbx6Z0s6BiBjXi0b0FPixKuPHwePxeEb58Ic/zIc//OGj7obH4/F4duChhx7ioYcewhjD66+/zvnz57l27RoLCwt0Oh0GgwHD4ZBarcbk5CRTU1PMzMzw4IMP8vDDD3PPPffszunM4/F4diDQRmO0RuuUVCekOi3SRpUKdBdu3lvrSozWk8i2rWaQcgKEMyAJKZFZ8W4pSoJHEXmhS2mt8sLi5f2WxQtj3E6pRmSUfFAFIKQciQgp9dmJF7mAkcJgtYeJU0QgCKbqyFAiAjneLXuMZ/wtc5sescNmtUu28wjNvc/d2uXjMKZYkq1VZPTyPr8Hym4iIUZt7JULkfF2s/IFfKsa1nYVJeJq6LiP0jhh0OrTurzKyrfn6a1skg41xcUotjS0/Y8pbXciDjrVlL9CbirZqTflO1U1baHn8Cg7KYz7/Kj7cVBtV6JDR4WbbQ71oO7AfhZ7PB6Px+PxeA4bIQTnzp3zURQej+dICKAkMGTpnnKhwQoBMg9hcOa/8nO5dFEOMGLULyI4hBFgJKCdSKBQShIEiiAIrGFBkO/XGFtQXLvoEBfC4YzjApV3pNSPPLVTkVYKRlM2uKoco6moXBRJJl7kLyHRScqwNaB9aY2oFzNz/3Emz8xuaaPk/13q3K1jGM56k6UCw0WblFNhGaz3OsIgpCzOK1k2MAO6tLLnliGPRxKAzGzu7tM8TZxLCWdDjbZcd2yZ07cR7l4w7Ay49uxFWpdW6K12SSPtrXueA+W2vUY8Ho/H4/F4PB6Px+PxeG5DAvtnTESCi2IQgBGlSIZSUe8tHpRCIIxN/1TUobCGVJsuSjqhQKCURCplc5oKty9dGIdsPQ4bhSFMZnovRV2Ug0JwxnmnmYzWAjAuDCRzpC6nq8o8JcsiRn5s0mC0Ta/VX+0S9SNkTSJCSThZRzXCraLNyBgemfFUbH1r03NpdKLRSVr8jXU+Jia172Wg8ugYEShkKO1nSiKlQChX8DzPzmJyg7nnkNlybp0wp0Gnaellz7VNe1ZEMyHtNSgChVASGUikkqhA2WgoKRFOpdzOVHvrlMgwICU61fRWN2ldXaP1+jKdxRY60Vb0lNV7lfe697xRMMYceR2M7QSfXEy9ifv3177H4/F4PB6Px+PxeDy3H0H2JjNyCGflN6KUOaqaT2gLxsVwkK1eXliqPwGjkQ72fR79IaxYknmKa1dcHCMqUR/ZfrbEVpiiNyMdtN7lgryIrykJMXkbJREDYTBCY4xAIBl2B+hOjySJ6W90Of7mu5m+9zjj91iMx9Gad4u0X0LaY4/7KVF3wHC9x6A1IGr3iTb7JHFCEsXoTMBQEhUGhI0atck6jZkJGnMT1GabhFN1gnpQRGQY4QSrWzwl0R2IkDbcQiAwSUzUHjBo94g2Bww3+0SdAekwQcdpEYGhBDJUBPUaQbNGfaZBfaZJ49gEtckGQRiQi5ra3LqnM7u3SEHUS7j2zUusvXKNwWYXnaaILBTF43mD4GtOeDwej8fj8Xg8Ho/H47nTCExerMFFSGShCphcwNjJBJhFN7hWqrZ7h3C557Mi2ULK3PBarb5rXxorXpQqgFflgLzmxojBPIumqHxa/L9AFMXEXVxIOfqiGlVSSruDRqcJSZzSX+uSpimqGSICRWO2SdAMrdEoF3qOJhZhiygjBCbVJIOIZBARdYdEnQFxd8iwPSDaHBJ1B8T9IWmcksZJHoFhPfMVQS0gbNboT3apTdUJJ+rUZ5vUZprUJuuEEzVUGCADlYfBlDKQ+ZiMG2WcDbIU0WMjLgzDbp+kH5P2IuJeZAWpzoCoFxF3B8S9CB3baJts7gsprEBVC1D1kMFkndpkndpMg9pknWCyRm2qQThRt+sE6miMoiOTJ++DKOpeGG3oLm/QurpO6/IKvdV2ab0dvM9vnRASz21GXkvhFlL2yvUdvIixPQc5Lju1Nc5BwuPxeDwej8fj8Xg8Hs+NEWSigTBWvMhMM4WAYQphYoQ8d74siQmj64ly3QWXzkW58A4pXG0FbL1cl+ZGmxEBo9RwkTpqa1QGVD1QR2vv5umpqIoYYwUMA2gJQpOSYjBIIUgGEXEckyYpUXfI3W+5l9pknTRNqwd9xMatLF1Xkmq6y5u0r66z/toiUauPSVI7zhobZSIMaJHXH8GAiFO0SIi7ffobAiMkJrUppyZmpmjOTHL8wVPMnjtB4/gUqhba8+cNZ4ePu5ZMnNC5tsHm1TU6VzeI2n1MXAgVrpL9mHrzhhRN0ouyEAb7qdaopkJNKObuv5tj5+5i8uQM4WzdpqQyuujCTTnQHZDSip1xzNKLV7j6tddIo9hedVltF+Bo87h5PB6Px+PxeDwej8fj8Xg8nv0QaK2d0dkKFlKKvGa2yNJH6TFhFSWMNpmttEpRfMEKBS4XvUTa9C7Gbas1qda2PkOqMWmK0dqKGrkdtmSUFLbeRbnGRvbORpIU/xyHQORFuqWQKKVQrh5HJoAY48QU95/RhfihU82g3YMrNi1T1B0yc88xVDO0/S6PzchQbOnXPmyro80IaUcojTVxd0Dn2ga91U36rR6D9R69lU3SQWzPZyYESVEIQoi8SLcBK0hgSuNha2Sk/ZhEDOnNtyBO6U93aByfoHlikmCqPj5qp3IunMDj7cpby6ZsqW9h/byFsGnX0mFM3I/oLrXpLLYYrHcZtnoM17okg7jYUthtsmiFcnRQaSa7KKPU1c8wpGmCjAStSytE7T6NuWkmTkwzdXqO5vHJfD5U+nzAOebHaKBF28LV+ohSNq+usXFphY0LyySdIUhTzD1TbqsojuNiyyhiOA6G0THZeRy2O8Ly8p3vuTeOKAla49o/evG1jMm+jI5k3zd5v9ebFrcpO0WE3OmRCiYvyrX98YnRL/J84+wjUYn22S44b9v2d9NHzx1Pfme/0dOdT6RSA+Uf4h6Px+PxeDwej8dziFgBIw/DAKFcQeDSM4oZ/aBEnv2p9KBeTtuSiSBCCKSLChAIhBGgwaBJjX1l4oVOnWCgnZHROCNI1vpYtYTCKJ/Xw3CRIa5vubGkVItDSVURMLJjsTU4XCHxsijh0mulUUx3pc3ApeyZODFNOFknLa27nXlwp+V7p1BrpBOIkm7MYLXLtWcvsvH6SqnQtjMUKZFbTcToQ+kW+6XMHP6RQYgQgrAWoqRkuNYl3uihagHNU9PIR88STNQQSto5kw1FyUKTGY7L6b0846nYtVzRbd0ZMlztsfTCZZZfukoQhiilwKQusgnsZJfjG82t+6V0TC6KSillz1Vq6F7boPX6CkGzzuSpWe773jczeXIGbdKtKd0o0ukcasFgYaO3TJqihzGr5xe4/H+/gzSCQCkrNGaTLu/YeHOfQR5uX6+LpnTDGuGwxAu22d/oMn9dHh3+HLzxcL+5dvjFsP1yj2f3jNMgdiJfbTSdo3C/4rx44fF4PB6Px+PxeG4SQZJar+1y+iSbfih7YLYvk3l1uw1zE1teU4Lcez9vDwFKIqRwnv4iFw8yo7atEVwynkth6ymkAo3BODtfrlnkYsoYT+Hc2zz7wKWgGvHulMrW4FBSoWQRgZF5iqZpaoULo0l1mkdf5J6kmXe3NuhhQnexzbWvv8b0PceYuec4qhEglKz2sPQkeFCGCOHc6YSUSBnQX9+kt9qmu9ShM79Bb6WDjrXzxhelaAtsBIwTJjJDbp5sZ/Rh1Qk6QRBQq9UIw5AgsEW8BUAKg7U+Sy9cpbG0QfPuGZrHJ6lNNW3R9HxeXM+P9A3GmCHI5ozAnV8pQQqGrT79lQ795U06Cy36S51cBNRohKy2YTcWI61mOy3khly4dNvk9wEV2PZTQ9TqsfTC6ww2ukydnqF5YoqgaYUqdGaGd23lgoGo/Nnx2DO9sRQYNDooQkrSKCHZjNi8uk774irtK6uQGqvV5MJMaWtduluVh8J9ls/GXMtx10FZ8Rvt9p4iLbbbPhvzkf5WGt5Ts3vpwXV2cvtdlzt6kF9nGt6pVOblEZ3SLdfTETHuGj2IPu10fJWokx0u82yFcguViLZ999Lj2d9c37K1MIVPynXntsfj8Xg8Ho/H4/EcDEGSxtiUSspFSOTWwELEECL3mS9TljdsxIRLSyMMEokUynqFu8ZsiqPMXK4xrtWKl5e0xcSlEBiji8gOke3TFM9MI4bHrI2s+DjOwFvpszE28iIMUKIagQHktSy00aTG5v032hTGCpdWKU9VZQzDjS7XvtFlsNGlOTeJqilQxSHlXctqEgjjPNgOxmNdCkkQhAw2+qy8skD78hqday1kIJFKFh3JjNyl6BRZsnxn4sWowSeLQgmCgEajQRAENt2W63uapkSdIRvX1lBXQk4+ehYZSmozTVACkxc6oRA9PEB17o/a2G12BolQiqjVZ+3b83QWWnQWW0gpCeu1PJJlq043GiPhPhOmunikN1lkkqxJCEN0mpL2I1a+dZnW5RXOvP1+gkaIqoeoULropHLFiRs0lpSzUowuE7boeJykDDZ6rH1nkWtffQ0RKFQYYNA2MmSb9mz/bqBfezDO3D4pcKrzopre5zAjP46CO+lYPHcq2wWUgnMdKdfluoncKgKU5wAYVzDuOmz5bb5dcJgXMTwej8fj8Xg8Hs9NIAAnChjtkpoYa2gXmdHZSRTlh5dMkzCZD3cmRJjciprl7FdS5eJIoSzgnLWtOGCyKIyscLdbLshEFErpiLKuu6gQyEUOgUFogckiPEy2lsgfuirFukvFMrLUOJmx3kYdlAQaA6KSVyt743LKa01vZZOrX32NmXuOMfemU4STdVQ9cOmojC2W7bY5iOc9Ia2xebjZY2NtmfULS2xe3iDpRVZEcecmGzeRDWYWNeMiKyrFyxlvsJBS5i+wooYgi1hJSHUKUhD3Y1ZfXSQexuhY0zg+SX2maTugMwEq8//3T705xpQvLSskhoK4MyTa6NO7skG01kekUJ9o5ClF0lS7EjXaFr7P28NFUimEMC5CQ4zscnwkUxZlZLI+KWmLtKewcWEFnWjmHriLqbtmbCSGS++GKbe0t3M8IrPkH0qpSIcJg7UunavrtK+sMVjepDZRy6O6jBFoI2wNHTen876YkT2MEz0r41G8v14tnXGUr6l8z2PfH9HcL90/c7mp8tmtxp0mqnjekOxCBChLi+XaIKP3pbFcR0D1IsQbmH181Wz9dVAi+/K4AXHE4/F4PB6Px+PxePZKkL2xAoLGGJEbust6Q64klD7LstRIBFpodOaxVxIJpItysPqFqBrRta0zUTaWFmJEsS9RjqIo22grtRUKG0HFG1yUjAGIioBRWc84Ecfo3AhZ7CcTL0Sl/YpLmjEMNroM1vsk/Zj6zARCSoJakBdE167z8sCMlzaeZdjus35pgfaVdXpLm8hAWO/4xFlxJZCLFMU4Z4bWTJTIj3eksGp+LksChnEiiNaaNCu6LiGJEgZX10iihLBRA6A+3UBk22XKlKfy7D8uI4MB4p4tlt5f3CTZjBAS6s06RqfoNLUiW5raevfuvBXzHRsJJQVSFobzPI0RW4vrltOe2GvRzhEVBpgUNq9tkEQJQSMkrIcE9RACaa+f7KLdq7GsqjSUjHH2XpH0I/qLbTZfX6N1YYUkigmbYS5UaG3vERogEzHswZSOcVRE2H4SFuNYjS3Z0yFVRIzsesqWHa14UfTjNjA87WKo/O3Ec7tTvhK3CqnXj4Q4yGvg9okm8+yKQz+dO8ocHo/H4/F4PB6Px3MgBNnTTZH33TiLvTUeyhFzvXG2v8won5v4pIvCcOmChMwiMOzLRjzY9bOIj8wAbvef+wPbFrO2jRMqRp+PRh/KysJGXlCcogC5KT2Yl0UZ9+xlAJ2CTk2euioXMgp7b3WXZc8zk/Vc0F1uc+WrrzL7XSeYue8YzRPT1GebkOoDMxoKIUgGEf3ukNbrq6y/vEzSj1D17AQVHRPupFlvTjBURRqttU0J5bw9Kzm4R8SL8jpZtEqqNYmrGYLQCCWI2j0WX7hMMoioTzcIJxsEjRBhDq4GyJ1EPrVcqrV0kBC1BvTn2wwWNjG9lFq9UZw7IUmFINUx2lgDvsmFQPufQmDN+uW0cDtH2mSfGycqCFNKQaYMgVSYYcL6+SVMYgibdcKpOgT7t5KUPZClUqSDhI2rq3Tm1+kttBlu9ABh66+UPPOztG+5ICekjUYpaSmjURGjIt04svubVZlG70E3crxjbiLb7thfI1D6zvF4PB7PTWO7byF/O/Z4PB6Px+PxeDxHQSAoDMomM9I5o79A5v/Obf5mjBkuL84tcwFEZkZvZQUMsJEZWaocDWhjIzCsrTTrh3F1GUqpj4zY8tS0xZM/tzGOeJTny1xkh8wiMLIADyfgaNDa5K9CvDBVT+4xT2/lksgIGLT79NY6pFFMEsXMJdoWDQ8VMpA7G/B3/cQoiDoD2lfXaL++SvdaG1mTyFBitD0emyWrJEBlXvUGRr08y977ZWNvJmAopXIBIxMuCgHD1QoxGrDbxoOYQauPqikm75ph6vQck2fmwAhKmbTe2GxrEBfEvZjetRb9a22G6z1MalCBwmhbG0YLa9gNAjvmaUpFXDJod026XY1EZxgXcTS6fzMyN/KYI2EjbBQSHaf0FltIpZi6e44JM03tWNPuS48ey7hjLa0yZr6b1BD3I4YbPTYuLtG5vG7HIDGIUBbXrWujHEEkhMQIg5DGpaez13N2zKPezLlIUe5aJeVT0d/SrXGXZPeNbQ70iLkdPK2LRIH7bsjjOTK2m367mdujQvNBX7e3w33Asz9u5Bxv/zNtjEORx+PxeDwej8fj8RwyQWYH3NYo5yIqTCXXjcgjMXInZUQuVAiwBm9VeO3nzVEYT22h7OxhqJS73miXV99kKgPlxyhB9d9Fv0pG+WpG6dz6qAJl63IgEdJ6V2f6RrkuRO7JXhJG8j7hDKgjz3F5fQcJMhQMWj2S8ylJ3xrzZ+87wcTJqUI0udGnQNfh/mqH5W9fpb/cQQS2xkgmxBhtKmNfNlxnUS9ltNaVfNvlyIsgCAjDECllVbwwtniy1vZlx82KVFJKqIUM2gMWnn+dU0nK5N2zN3a8bySMQccpcbtP59o6g9UOJrHVaaS2YRZ5bQopCMMaQRDYNF5ZWjatc0HJaHfOkyLqRjvhIkuplhd6NyK/BrLrXAttryDbheIyk4Ko22f1O/MkySlOzp5FqOuIczshrdhiUkgGMRsXluhcXaO72CLeHDjRxkCSFgKqu/9IKd39Rtm+l6JRbIozPVLbpiziQKF/ZGIDJQEvXzR6mvI2RtneVmQYs/rItll4287reTwej8dzmOwUC1g4Mnk1w+PxeDwej8fj8Rw+wTgPZItVDsqSQClri/2ktJlwURhSFEZRoSRCWqGgXFjXGIPGFbYuGdOzgt5SiJLfrTP3j7jtj4oYJivIUbYQ5k9fVqjIjPFSyIoHdyaoZB7bYxHF+nkKrfLjXf40l4kugrgfEXUiTJIy3OyTqSG1yTqqEe7ghV6OcSmMyQBIQTqMiTtDOostNq+tk/QSgiC0zWsqx1EWJcalhhp/qCI3CmevIAgQQpCmaUnESG30hbafuQAMQLgC44KkH7Hx+gqNY5MM232CRmjrgviHXioSlosKSocpUWtAd7FNb7lN3Bnaa0mqwhvd5W+zqZYkYM9VLrxpO5/TNLXpvZIE7dKXaW3yQtfSpatC28iFLRFMWS2JkWmeXTpJP6J1eQVVD5i55xjhRB1ZD0tRS24m73Sq3bVitMGkmsF6l97KJuuvLtBdsAXp0SYXR3RaEt9E6X4jJdKGHFkhxvVBa41UmjSpihhZ2qnKJZBf4mXxwlznALae09F2q7eU0giPNJtfq2LL7c7j8Xg8npvOqIiRfzWJzLHnCDrl8Xg8Ho/H4/F43nAEAkludCczcpeViRGhInuTGzttSiAhsHUvsgLZ0okXZTu8MyqakVe+e2fgN8ZV3jDZo1NRSjePyDCiiGQw5egIu82ocV4Ia+QNg7Awxrrc9lnKpVEPbbKHMynz/Rb9EM62KaoGyXwFe+xCwbA7IB7G6FQz7PQ5/sDdTJ2eRYTKeZ0Xco0pNVPRN6zLPVIKBr2ItYtLbF7bgNSm7hpNAZW9L2qMmPz4ssiMccW7y4W9ywJGOT1RmqbWKJ6JGKnO027ZoSnMwNqATjTDdp/2lTWmz85Rm6jlBnVPCWlTb7VfX2XzyirJIM4FCYE7j1nxeSdilMe8KLAOYJBpikzTXKzSGkSqEVm9EnRpjphSgXvcvsb00UVbIcGkmrib0l/r0p7fYPLuWaYm6/bcpobKxT/aSPa563cyjIk6A1a+fZXWpWXizpB0kLj5yBbxNMM4I4o29hoSlfuXQUhsLZBAIlK7r+wasMetS6nhDLZeSLnPpfuUS6m387S9njXHjPwtX+mlgyrycXk8Ho/Hc2RsicQQIz93PR6Px+PxeDwej+eQCayx2RrLKoVus4CGcVtlBj/nXm29sqve0EW75KmVsvQ1RZoak++g9LZk48vyuWT7sm+Kj8c8PpWetITrV7aekjZ9VG60dSJM2bivtXa20iLyYXyUinDjY6oiRllwcUOQxinJMIHFFjpOMdoQDyKaxyapTTUJ6qHbVo8ZcLd/ITHaMGwP6Sy02Li4Qm9l04ofJUN29TRVRY2ysTuraVFeni0TApRL/zVavNum5ElH/uot4sUow1aP9QuLhJM1Jk5Oj13nDYeovjHakPZjekttBqtddKLdTKq689tLQRQREhTjnosQbrviGrFRDqkwufCkTUpWsD6ft0JaQYwdag+Uu5NqBq0eGxeWMNpQm6qjwsCmNDNU52X5OnL9TAYxw+6Q/mqX7mKb1sVlOosbeXorGym1tbh8MRw2qkTYMJJibEb6K4UTXVx0hr1PCbS2NV2qURb5HYCKgOHEkXL9jOq+totoyrYDV4gmH+9inW3UIh+p5PHcOWyj6Vaufu/S7rkF2DZAuLzQ4/F4PB6Px+PxeG4SQWE4K55Ico/k3KhntjzFZI7ORaIY10qpPWFwufOdgCGK2hKZYuFqfjvH55JiUslkY0rmvlI/y3ltMgVEZH2360rkFuNuUd/Cem5XxQtTardk/h0xNmf7zYugZ7nrK/1zAocrHK5jTW9lk/5ah/blNU48fIZj958iOD2HUAKdphUjb2aMtREtirgfsTm/xvqFJTpX14k6Q1c7oIhEGT0Po8W6y4JEpTZJZpSV5MXXpRKVdbMxKqfiKf8dl5ZKSgGhZLjZZ+38ItNnj6HPncRjyee0AZNo0n5M0olI+wkikC6qSVbTlVEVp0YFLGv0l0hhMFKjjITA1keRMjuHklSLIv3XdebO2L5LgQwVcWfA+ivXQBvqM00mT07TmJ1Ap9qmqyrfE2zLSJdmrt9us35hiY0LK7Qurtj5J1Ql2un681qTpuUIElFZ1/7DWCFBCISRZPVaTDlQBANCl8TLqvBXfJ6t6/6Oi6IYP2Lu3JRrYWyfys3j8Xg8Ho/H4/F4PB6Px+N5oxPkqZcMuSF01ABoXCRCZtCj+Gc1CsLZ8UQpzU1WKNdg614UnszZNqLUXmZgFPk/zchqlcwr+UcjqofrQ2b4zb25S8ZNo12PzLgCv+OL7Y4akYswesGIJdSNm01Rkw2MMQaT2PoEg40enavriBTi9pBwukYwVUOFAaoWgvOMj4eJTa+zOWCw0aN9bZ3uYptkmOSBMNdj1Hu9GiVTfGYwSCFtpIpyxc5dDROrz7jxKo1ZeeyqBuOSp70W6CQlilKG7QGDVp+wGaJqwS56fydTFpA0UbtP1OpjEl3UdZAKKVyaN0pRF7kIV6QZstETIJ1h3bjrUyrF/9/emzbZcWTZged6RIJksbpL1WppzHrGbD7IbOb//5f5MMy0Fa8AACAASURBVGOSzGRtarVatbBIEASQL9zvfLire8TLTIA7cU9VIt8S4csNdwdxzl2osaZJs6iFBurAcZyjc+5F9Ewjt7OiEbgP9KPjuz9/g7/8f/8D7776A774h9/j1e8/w/7FA2jfJJ0cGP3W0R8P3N68x+31O7z509f45n98hbd/eY3+/hHbqx1t38wqKUKLcRLd3A7zNXke9trMTf6efK/bvdKm7XL2CJIQi+DnZNr8L0A+vCIyhmm+/SxkVPRFoVAoFAqFQqFQKBQKhULh08YOmMc2nUloRRY5HFfkeaMTiT/G8Dz1OZURadYWIqcLNS0OXByZ6TtLwXKHM1yzxlg6qyUCAzBSUol4JeO9OPVzqsApO805iiHAKkQ0GGlJDSDaMG4HXv/zV3j3b2/w9Zev8OU//QF/+E//AZ///e+w/92DjHF0vPvqDb790zf463/5N7z5t6/x+PYR3IcIDXt7kuK8R0JnMWf13G9tw7bt2NruaaZIo3EYmnqo96WYd9TZmNZPg4TXNAI/Doz3Ity8+dNr/P4//j32zx6eJco/CTTGOAbe/flbvPvTG6APbA8bWttA1Lxug8chqSgoBbm9cjoaMYg2oDGIBohYn6nu70Eueo0xQLco9r3ipSIGMWNsBNoavvvra7z5X1/j83/4O3zx7/8Of/xP/xF/90//Dg9ffob9YQcz4/juEe++eouv//lP+Ot//lfc3r7H7f0NDYT98wcJAfLaN4JVYInfjKHRV1dRGrkuyHV0Bum5FGcCgeQMwDl6yVLK5T5eqGBkq8HqbFTcRaFQKBQKhUKhUCgUCoVCofA0diAiGJ5NZfJUppQcSbGQgFEDw4hASSeFBo22CIHEM8+biMGuZ0Tzqd3rIYpw0UjT7yTyMkh3SR/VVbwwQt7TQZlNLMVLTh0DdgKU8tyxigZCHFvaHGrkhc2bKBngPjDe3fDuL9+CAeyfPWB/9YA+OkY/8P7Ne7z79h3e/uU1jnePXvOibTB95PJRkKeiIh2v2LKJe/4UcbIW/rYaGV4nw9LvQEnzi+iLuX8KU8Eya4mQcfvuPd7+9Vt88cffgVoDS0jAeRKfAlJ4ETPj8fU7PH79FqNDohYo6soQU6Q4S5FChjFYn9Nwc5IWqW8sKcFgUUlK7DM+kxRVWtA6iwSWEuxe7YnoGLrMm78Z7294/NsbfPPf/oR3f/0W2ytdRwz09x23t49499W3eHzzDuPolitNxAvkVE1LxBMWUcGuuxyepmuyNahdDLNbuscEIhE0xNa2D2Zxh05L9Uq0tBR881FJ8yv6kNRThUKhUCgUCoVCoVAoFAqFwqeJfSJCPetPeBmfCLqcuimYUhhZD5LUKNwZrB7iUrgbTjYSlGxtKgEMAKze0Mo56kchYiQFg/W7VcCwor8mijRqp5oPRrr33jF4nASMaZ5tiAc8b/E5j2QnI26vSUix45bsS1I3AA1ba2jbLoICM97/5Q2+/Z/foHcZ33Hc0I8OVpJz24G2SSFyaoS2WTounAlc54KjmHMYX8Zqc/aC5hAi9+HhAa21k4BhBcZFwDjXwjhFc7iIEaR42xse37zHd3/+Bn/4P/4BtDUQj09WvwCMzJY98P7r7/D+b98BraFtG1oDmspxPHgSFCQh21CtjQAmiUaApS2z14xt37DTLmJIE3FKRLANe9+x79tJlDKs0RkeQUUhEHip8G1H2wEMxvHtW/ztb9+iHwPchwhtQ9diA2jTHxXLZLOTCJtg0Ii0Sy5qpr79m/noSmeXfWpRJ5HKjgGNXAE2kkgjaw/oYG5TVJH/jHO9ivV8ZIuIYVigRXrWp/AtJOstbX3Cm6JQKBQKhUKhUCgUCoVCoVBQ7MNd+Hki2M7phdIfelmDeHWTFuRtlFoQTtUjHQD17jfn5+SAvPohcx6PEZLryI28X2+mSBeT2yMVPDoPDGj+qhzW0UyMsBRSwkBSYiHZ6lyYaEDAIItuOHuqmzWIxDbNakpQw9ZIimRbKqfB4AYRfTDEW36zFFEUgkQijkVLCRnnRN6yptjRCIxs4TxWESkgokqqdzFGRGoMdNyOpeBzivS4LiSu13CTGgxg9Hc3vP/bW/THY32inx4sQOAY4McOHBDxbYjQgC4yBbjP6ddIRQ1bN8SX3v4sObzAg9APxmgdjaWAt6SFk+eca6Js2wZmdoGrtQ1jmNAlCz9ElCSgEenztigj0vRXspO5Mazogx8ja8TXrHIun+vMcgoo2/9Lqic/u2iEMBISh4suuZ+IWpJoDNunRJvYmBm8pX3DFlEmZ5yloLI287Cy4CIvcoTSVSSZfEZ6RrnIVfhN4SRRnaJ2nr/XZb5PWQUuFAqFQqFQKBQKhUKh8JvGzlc5iBQngjEXwyV4fYmpKLQRozAajiePbkZEUxAT0NiL4yJfZaSoiQD2jROIEVdwGiPRiSD1iI1MvoJjLJTYI0vvop7tEzIHaQTkvVoTOEeCxI/VwzD1hL07T0GzxezPRcjJx3L1mCRdjnmEhwVl2OdIidYI297QNhmX1LsQAWQMiVQ5NGLD7vMsOPdA8owbicxCG9AfOx5fv8O4dVCRbgAAPgb41oEhtWGaZmPyfcS5wLw9M1kbgwbAw7IvzevVyHt9hmSE+7KWrM28l4/jUAK/o3fSPTxOY/GuxpBUVJCYI1uz8tlIYt71gmHmrENoJMY1prHzBf1/EREkF11EQNiB5G1SOkKk8Di34QIFw16LeDRGFjAlOqvdYaJnm7Uk2Mxz0xgRmF76clq78KtE3rLp76RnLi0UCoVCoVAoFAqFQqFQ+CSwP/WlE6aXaYoooi+oOVnPC6mdBQeLmhBtQtnDAb9h8kVWgnwlbIzgmzywKb6DNpc9tYPr5PAkj9rHPnbzdGZS0p+XHDCzdSb+8dn6IXksPjGNpDBC2H7CYMgWWAsZP9FT2Dvdu97npHWTaI9tk7RWhOZe5YMHRh84Dinevd6f8+Sc0utcjIyHtNdvXVJklYghRPgxooi8xguwktheOyZDo4Ri7XhrEUmle2RKuaS1Lc4Frc8iRmsD9nxbI4whP5aKbRUy7LXVz8B4pn7G2RKxxn3EiKAMrd/xopaWg+h8m/WAOUVdfCXnSOPIyAVJF6dZ3yRSapVRaY12ujvAaXxnUASCFWtdKBQKhUKhUCgUCoVCoVD4RHFXwMhEYib42KIb7EslwL1grkUQOAGPYAlXUl3/IIs8gDmQm3ih7S4eqk7M+hgizYrXwVg8zG1OQbymFC8aJSHzm9vn5X7tJo9GhRtvbMJpLElQsRZybn5O/axjv0rT5P1kb/qIV3FJaM2/PxPVDW3XuhdtA0AYfXj9kt47juNwsSXm05Cf8xQhMtkgzZalbe4d4xjKCdNy3acFEXW6RrwMKSLtMUxXQhDH2kcIGC7U6VW2J/yNpXLDXEviXjRGayFOWRBVvi7Xici/LSphFuTg16xF3n0eYEgtHPvS2gU0ruHCeLr/bGbzhk2HhHx/2icajcVsSmjY1tNK+TInkIqaERlBMDXUIjGuXOivhJ551XP6JI+xUCgUCoVCoVAoFAqFQqFQ+HQxCRhr6pUpBZPlM1nzBqmAYXyjiRdOqhqfl53EkQg9i8RwElI/RpD/E2Fv17twoWWqV+GCSAIoksBhuf0xpMA4NBoip2mxVFImpth9M1L0h4oPpzz3WIjI5N3ukRicijJfeLRf4aVRGC5dULxfx2PixbZv8rNtLkRZofM+JFLi6IcIPe0ccXF3LFFeBTpddV1nLepsa+rJqfzmYSm6Rrd6I0NFsbGsJhOwklhha8aDJQhke9RNa6tB07sNEzwGLFopCxZPvbczwop7S12MqIti0RdG7YsgMUJHecG6jbRQk8JxmnsICHrnuCfunfuxOYmgKXORNHi6tdeAF7K+cuQKi3g7CNQbxghR1uqDsIsbi/ioc6ST2BFzOn9XKBQKhUKhUCgUCoVCoVAofFrYn/Lqj6LVHN7MDFgBYCeflfWzIrZBQrLTcULs5egLD9dIIgbEyxlAqA4r6RnkrFyVojUsndUdUnyM4ZEErsNMWkyIKe79rew7+3xISVzMooxNJ/VHqV3/QbpP01k5gZ0LZF/guciL9TpqBKLN50Jsz4k9ddS2SeTFrgIGD0j0hQkY+mOCzyqg3F0/FymPKN0PjYL5xLULALpjNAqjH11stUVcge+z0+MPA0obJkACgKYGW9eHr4Hh+9O/0gil4zgmMU3Ei82va20D0aFCBmOM7veLGOe7W4d9JbrNgp//6eQ/372Sux05hKj58bSYdhZnm6fmijoxFl0EDF2r42odN4ui2tBYU2u15jUxLP2a7BdyESOPJc34FG8RtTj0POWILHlpCq1CoVAoFAqFQqFQKBQKhULht4D9SULMIieGEoriMgxkMtC5uJSP38hGd6Cmk3hh9J055ee0TZmvtWLZZ+rTPMxTBEb6mT2lw9v68fFRhQ5h1P0aCMFvgoiPlILEjDGTp4rxXPvUImQjmQ+nOgH6DSPEC410MAEoz/Luo7moabFi2zf3mgcADEY/unrJAwTxOG8k3udba+jMTmTbT7ZhxlWKrmdBAGvB8lIvAgxG19ogIjw0IcqbPCATvkR8Y19bjQA0YKTnZpe2QeDW4tnxuotmYWEkEc2jLnQcIgxKW9wYrTX0o+GggT4aeodHZchZoK9MA8W91FGTEWKSy8ekbYo4YFEVc82Op8S89b3ZqffuYg6jxdnh0SpDzgmkiAsS4Y+oAdjcZqPrM+wjpdfKIobZ9lq+cFFWRRTmeEaFQqFQKBQKhUKhUCgUCoXCp4j90nOZk8RgZKkFW+i1qey1kHSav38kwj5iMJKn/tR9eGp7VATg9+Tq4eZVnmHjaG0mMt0rfcj9XT3TRx/yWcuRE0ElNrTUBQEkYoWlwPe7Up57G0NEhciMPDilEdqmRLR+b57fY7BGOBwYYNU/5NpchPg5YeCU6itmEGKPp6xJtS82I8mNINaaF9wx9Me96r3dWYhaa2tM45oHIaQzEWhTwnmjMNsnDEt51o+O2+0GJsbGG5rZqTVZGyTimolgko1LBDZLn2Q1VcDAaLKGWxbzpENIC4IcBDU0IsHEi9EG2mjTXoQKXth3EDG20dC3ht4P9N4xOkvNjCRortFJsaM06sKJ/sU2CKGRR4rDcBEjrX86r0YTGxlI5xJ835OKj5Y6zU4stvsYae8ADYyxnpkpQkKElCZ7aQwwk4gu0eVSOySSzqlk4vaXi6YYGRFT84Pzic61dZ4ET7+uv1zFpns3/+gRIfTs+ffM7fnXz4YrKco/W754SsLOl65tPmclvvdIf6W4uy7McLx+bOdedij4cayxtnvv+Rd+A5gcXPJ/Qs6OOx/d/N03L8MU+foDwNZyrd9CoVAoFAqFQqHwU2K3fxBJEWv9Bz4NgEekWIFGRRBho008sVtDIwJxFHzuY4g3MyuxzRZVQWBqmtPd/umjv41IZMDc8j3YoyXBwAUMIw+l3bZ4YOeURjxkXBYpwmPgYdvD+38Y8aj/ADV22KIztHAwk9kkCm4rGy+e6U6ymajBPiZL0dRasCpiV4twEKEARlBTQ6M2CQenOiCKJyMwCOAhkR1eGFmb2Pcd+y5js3GZ9/0xOo5xoKNjYDjxm4lomwcmwlk913NEC0iiLRriOTegbQ3tQWpvGJn7acLWsqzdox94fHzEwMA2ZM2I0LS5kOHxAbom+iH7jnJ7+qcIIxo5gc33DGs8gO3v2JPN166BmdG5gzCcJA8RTH6YG3beMcaugtxIQsbwqImZOoQLA0Hd2+hN2Mqp1wi8sZ8pYrdIkTWnjcvkfwhvue+h4lvb9igPTnBhUo6B2Hd2hvQBdDqAWxIMnM0hbNuGh4cHEQCH2IDIxKbUv0arnEWdrqKH2oEBkU1Saj4/V6VjZtlXtIVdVsznByU7ZhEk7+8YEUYWlEN0ApOLkit+qFRXWcj9iLvxS6Lrr2ZxRWy/FKt4cdlOEpB/y3jShudlrf/p8dOujY95DD/2o/vl7I7fKL6niHG1r59yGrlsg/nqiP5e+ASOlEKhUCgUCoVCofALww7YP4hSiiBj+Gni84VMVZLd0g/J9Zb7PrQGC1LwihYUEQCAfOf/MOMYg3vc2gWUqMdESFq6JyAI1SxgDEsLw+zRIWCZg5GBbG7n7lRskzVOLpPxLYpQK/dHft3Mnc4Eb0tEb+THnyIbtM1IhxXcopGo90SMjOl7Dk/vEH9YhCcjw7O9hogVQjwPL/RsAxHKe/2ncxaWpocpz3EqQixzbduO7bN9Sm/1UxNJvxzIAt8eNuyvdqAx+jhAA+DOaIOwtU32VAPaZmtE1wNEIOAxIs1UduvmWKhz+iZb9xoj5QVh9AmTXzhFU5GR5RbphFT4mrO40UFE6DRAXb4fbOmlzuvU97cfOy7TTIKJ1McZvtw8OgO4JNEnpP0cr0wcscL0k2+2XDGJGHrNSvxbmyQChkWveBV7ApibnIdZHBrjVGPDrC0PXUSlMwO70t4vp5Mi7Z/dQ6eZp8neF4GslgrFObze/vPX6/hxz5Uno89esB5/0HPv5zb1z4gPs6Ks559cvFjXgx3HLxjHj/loP9W/eX9UpMg8efuB5/PTjePDV8QP85Sf6vXnP+sLhUKhUCgUCoXCbx27/YNoLbBLGgVARipqQWCL1Jj4QBMrBsHTPyn5LQSpXmO/AaFDOZf5TuQisZPflsIliLSnSSMjdr3Jpv8oXB397d90yYPYuV2Ki4KkSmSikb4EMDEm//FFTDlFhaQfS7fFYCmwPcyQ3/8fnO4lPU2c0IjFW1vFKROofFydgcGgQSBuKlxAPfPNDleYRQyrc2FUtFmoPWzYP3+Fbd818uDpwuWfAraHBzx8/gptA5g6hhfa3gCSSCIomS87IJHgPAAaoBRBFZ70WYQSjERGXyMEtXvIopPUU5kvNvEO2NCI0QfQ2AS8+Xln4pz0nPH3Ph/yvRnRTvC9ev9UsHXISSRNrZ/I95nc/NB1eSXGifArAg9zi88BDNIzMIuZ0pBrUNNz0rOKbCGwijpEKkLFOHSGaRyrAPr0WTpfd4EirAqFQqFQKBQKhUKhUCgUCj8B9isvUiO3rZis/W5tEwGDNiVThR21vPsMEQoY0AAKixzA4jScSMv0ijXFlDkHe773nPf9BfAKHepd3VTEyBEJ4Wl+j6jUVC04Rz9YGp58r3CLcy0OF1QSIerixRhg1gLCE1EpiZuYTfRI83oBaTgXDB8hINnEPXJGiN2BAR4UkSqDp5Q/Xutjjb3wbiiIVveMz89LWeYhRY5fffEZfvfH32N7tX/i0RcGsRttDa9+/zle/f5z8K1r8WigDwZz03UiBLivXX2O5wLrVveBAR6iiw29M69lIl+3NhZJuTYu2pzXlgkXV2vSUqHtWwM2wg5JlWb1XlYhwyKM5jRwHiMRwQAX62USM5JJ42WeK03fTWuap6s/el0yDxz9FpFZamPWD1prklZLf5st8g+7bVpqg9D0LAa1VMg9xtt7iJW5GDmBpNaNztCjMFah1NbMBSLaTd5dqL2FQqFQKBQKhUKhUCgUCoXCD449v4lIAXstykMjYPN0SJI+CkQY6CnVEKvzNrsHcaQ/skbvefPCCTWPvACk7eEMHTKhZ7/vkqjQaBGSKA8j7lzEoJTDfoHnXV8IVuuT3Xs5xupxGomItd8AnKwMwlLFC6lGMAkV4Yk9R6d8aCoCSU/DHmVhnzOzZoDRgsXMQAe4Cwnqaa4AzHVLrA17kQnMeO462EnEYE2L9Op3n+P3//gHbJ/taoNPXcAQ0Nbw2R++xBf/7ku8++oNjnc3rbvSMIjQBmG0NotvTeqZbG2bUxOllGOshdgHs+xbSKEE259AXleyNnvvvtfn76P9SdBL7URqKcK27djaDmrAGB2Pj494/35MbdiPp15K/cr6aSqqnSN18r5cow44/TnZWT+mtD6j3RBkT/elff/ULhxj4Dgetf7NJrt3sSPrdb33Sbg4jmP6DCo+ttYi+qI1PddkNnbeHkc629KYCZvXKkqWC9skYflyJ94NwBAhBcClvQqFQqFQKBQKhUKhUCgUCoUfArunguEOr0Ox8FFkhYBb0xQvRpxFZMNg/VHP/vCfJokmyBEYCwMo3JyR5KT/ZwziyNqk0RRW8Hgt2g2cSVGp3CCRBUbaG/HnggNIyPzV+1g5vktqLkduKIlqtSVkXA2tbW438aiOQtkiEoy7pOza2cf6OJvJW4uokG3bnCxmC7UZUb8k32xFgtcUWPLGCN1Z3MA6HwBEmuufGdurDfvvHkBbK9rToGvo1Zef49Xvv8Dj63cAbrAnyAxfx5OAweZ537Hv+0lwyPVTgjwf3qekn8JEfGdRILdl39lvFwzWyI8BFweZWYrIDwYw0Brh1atX2LZtjjhIERkSpdG9PaA9vQOeOFP44j4XNjg+WWtyfMiGWwUUBmN0AFpzJ0exYbFlFnzWZzbG0D0GWC420rF2HjrQedOugpTGbWjfFpEDHYdG/qRoFxt/jNHWymwQjy7BXX2jUCgUCoVCoVAoFAqFQqFQ+EGwSyoYC5CIgrZzlENDo20m4SzSYojXfh9D0hE5UUZRqNsjMK4HIRwag3jO49+a5v8nSwklKaxyhIPhJF5wJLiStDhKtTWaiovHmDjIQBt3IjmDyD97o8c8QihoOlZLN2XpmUy86EOFFUsBZN15FIMSn08wqvc8n12MSSmCWmvira8CBlNcC00ZNbTYuVnO08UgUt/kNDVhG7vOCG6ELQkY6L5W2oMKGPvzRck/FRiZ/erLz/DZ77/Ad/s36UtE2h9FTn/We58EihMhjizyhee97Qt3xF9Ei3uRTad6DekeXUpoWwgYY3QwBqzw+MPDA169enVKm3R7vOHx9ugRCJECyQTBea+HHZb94VMlj7bIn3t9HBNgwUHUW6tRvGXaefcivab22c4bwhis4mEIGFf1QrZtAzP7Hs31RXhwOis0imt03aMD+Qi4evY2sNAgtCLNJHZoOA/b+shCJdxO+fppvVgfd8TYKvBaKBQKhUKhUCgUCoVCoVD4WHgEBqROMEDDRQwHW7QBhOyGEplWM2GJXrC860gE2l0QNG1UeAlP8REaceFEZhIwXHnBTKyaR7lHYXASClyAmYbgr5zvzE7aiQl14n8RD07peJLHs4wj5fy3Nlu2lTd0IfYYSRufh9iRyNz0PVlx30aazieiL5oSpj0R2BKlYmNc+yfTq2I8Tu5mr/LZJ9upcoa2Pbxwud1ZEgYA0iLnPTzv5wfA8y+LkiD4uo60ZwRYQW+LspiMbAuI0ppZxQurkyLfX9VwsWtDWIg2xuiShixYfb9+jKjJYmMm2rDvALUNvR8Yo0d/GmSQo0QWCcCOjjSP8x5f4UFhNh/CvA+T8JFr3iw9T6/tirzPuSeBdtrmmsKO0mm3CEfMks6NutalGR1D04GBh9pXbBRtLWcJ4tmdxu9iRBKOIfuYaIBz5EXSrk3QKGGiUCgUCoVCoVAoFAqFQqHwY2MHLvhyyhEHgHvlDk3DBMZgTAIGNILDcSJhE05e0dGHvGoRJUEkOdxJam94jvyUTz6LGDktTaTHEqK/bXrvSuapopD4PgQfGpEEMb7U9yndjtmNPZrCCiOfBAKLkBAm2oWhKUeLh4Mkg9nzoOyNbp7lcW1rm6aNIrStYds3bE1SgY0xQGMRfjxyg6emjOiNeWbjhfe1aC9pLqSqh/cxMHpHP7R2ShKgwqCnpn+1OEUpXE0ofdRvB8aj2IZcTEsEtBjVWo9+gLRmyPee74EBTetmqlNcG475JsBFtAMzpn2UxcFIl0Z+bb6Gnbk/z31Kq6QE+r43bADG2FQAGV77oveBcYzQ8Ux60ekQZy//2cZPCWRMkJCR03FwIWgsOAuhKnMkO9hvGfJ8yDYVF6WcUJNEWTT/MDe0xiBijM4YRKDRIYXZ5Tl7tBNZ5EicuyG68LzPYhZJvIj5yLZsKttwfJ2Fp7x3S8goFAqFQqFQKBQKhUKhUCj8SNgBLMRZJgMDwlUlkeEUeWH++PfILJqJwqWg99SKs4CJOVvJvegSSERr1L4YqeEzeXfpT50+tBQ4E3O3zlS5zzaRsdY6+e8gdQfCYlkQkDocYCuqfUXDkn7OIQwgdbOOL6WzoqZ1MNykMorVZpZCxtJPefMcz33WHMw2s3c5q23MPuKLz+AGHO8e8fjNW4x///ewQsC5/U8RQlYPvH/9Hd59/QbjZuFQw5/TZB5WGwOgjbBN9VeiOL2lM4IT/E3uCf3PBuDjuMg25W09m/KLbV2JAGERAbRsfRO78pkDIhlda+k7W/cEbljsoCKNjv+l0QC5LghYk1txFnemCWkfz63NtAmnNk4bebKBCVIEi1a5jmqQfTxvdLLIlFmmiG51s0vNkzgv1pol+Xybv3O1aE4nFRNQLe3ja/QUCoVCoVAoFAqFQqFQKBQKz2FfvX5zCpOpgK+S0EL6RX0J8/8PSlXxBKs1FYoFw/PV6ydAePK7DMDzfURC3sUdcXfwnEHCXY7HxQb7Re6gvhYzjg6EuCO9vlEUyaaFvDTRh0eKvshigPU4caeU/pwsEu9MdMiiQSZ2lThujUCbkp+WcsttFCmjRkofZeJIspy2vwgulv4nDd69zeWiSTxiIqARbm8f8far7/D3/3SYz7oT8mGXJGD95tlREjHw1vH4+h0eX7/F6D05xJ/rEcR+IWxtw66RNVZvwQU9jV6IgB7WSCbyNYvLPe+7/XK86wq1SA1b8xYFFTVzmn4XazjXi5H8dU3SnYHAtKnwwfI5BgiUoqty/7GmdSGl72w9zadLFkGzLMB60OS9+Lx4EXARNoIZ7i5f2xquRaqga3Yj3T+2y0Tb2fR5AdRE2GkdGKTp/NhsY+mftINoSERSmp/dtfaThY3z6Oe5XQkghUKhUCgUCoVCoVAoFAqFwvfHJGCE4WjvegAAIABJREFUB3eiptzl3qIaNH0Us+dIbx4lMTeeRYATAS6fpqsbYPn61XV/yi6UUrLInYmYSwifYkpZrNRb/QMItvXaKa+9ezNDU+7QlE7HrhezDQxOaa1yHyrQWDCKpQ1aqWPRciSuhFShYWYJ/Vg8oJ3WJQI2Am2QwuWNPP2XkZ02LqtZMLSY8SqemPf9aW2YWELRJyvBbrVSTDihRti44f037/DNf/8Kf/w//wPYChy4YJHm/Slwobpljvc3vP/mHd797S3ev36v36Wizfa8zbZa32TbNimK/dkrqW3SLNpHBKk+DhzHTZpTIaFRQ9vk9bbtkZItQda31ZyYVuwkXlzVxmBmL8QdUSF23ZjWUmsSfdNa8zXU2oZGDSBGY2BrDN7lntvthtvtpsWt8x61yA+k0d3RLHO0CcgjtSbBMolHL4svMCl3qAByFiGtz1Oh6yGi4ZR+D/G8pO6P1rKhhg0bwJtHdY1Nnvdx3DCOw/d2qCm6BZPYGJEa7BJNHluMUaNnpto2ZuTUQaFQKBQKhUKhUCgUCoVCofAjYV/Fi0xmuviATFAq5ae8s0UiPCVe5LasPafEjMS3NpG+ALA27OmGOL1nnMk3Je2uwjOyeMLp83m8udezCAOCEIpbO5HAPpbBGANKKC9RK+u4Uk8nn++Uqmuab3Ihd6LVBBsiSRXUEmmpooqTzH2o17YWe8aZfM1zPqcQ4pkYTREY7EQxgNYgmfYJ/Nhxe/0Ob//6Ld7+9Vu8+vIztIcdThc/l6boV4R5zZ9XNJFEX7z9y2t88y9f4f237zD6EIFBn6On/kmRNRY1sW0b9v0B+74L6W+ynglTfaD37r0SCVluhPR1qrg5VdQcnQGPjBIv/7OIkYtt5zYvhdGpzy4Cx2AQbZPwaffu+y599AFOmktEqZgYAf/tvURAwTzv5TNaPr+Hq72QoyWuGsndTkJsOstygXSAJT3dZqIVdA2IKGRptloTeZmxgcYAdZzEUjPAfPbEqry/z3XdZBGD4u6SLwqFQqFQKBQKhUKhUCgUCj8m9hx1kaMIZoKR3GNf6jhACDXGyXPYsBKUT8PIx5Vyu0pvZYQlnH9bydPE62s7+ts5Tp45vIXEC09sbWkNDCBJddPahm3bsG1Nyd2VzGX0ziESJCHhngNztBED9PoUmeic2pljMEg9tqVAb7qHGdyF2B7qJb/O+R6pffU6G2WOcDGCHE62R1qcBnTg9b/8FSDGP/5f/zu+/MfPMYYUr/7tyBeCK0EqijQD4zjwl//6P/Hn//df8P71O7RXm3rMq62bFIEnMm98/WlNBIxtR6MNgC6RwRgd6J3RU0F7IdaF5m4NGEzovZ+KcUP7lYiOs6gJQAtrQ0Wva9FjbRPA5Wd2X+9dxRYh2httuDqbHh4erHKP39/7gaN3jLGMqdshIQalRtM6N4EzCzR3hY4FSX6IeWs/rYngwH42mtLLc6QD5vMi22MMFXn1XGmtYWxqD7axssyJCQ/7jm2LNFsWhTP6cpaks0N7upxnjm65kFQnKxQKhUKhUCgUCoVCoVAoFAo/FvYr4vqKZJSCwEs+/nF9/Yo1jZTS28knmGailyEiwdbEG9tqTEhjLmDwSAQg2Mk+7XWtEx5jSCTlPUTND21rSS9DpDn7mxXYFWQP9JxWJ9frWMd1t96GX2D2MQ9oD7uYvKCt2O8q/LjIMxijiwe7RYUg3Zt/A1E0HMQTMRzXncnN+ZOk0hgRrp7kj397h2/5azxsn2G8PfDqD59j/+IBW9t8Xi7cALgUTn7hcPI9e78D6I833N69x3d/fo3v/vwaf/tvf8Lbv34re0KLrUNJamIl79HQaMPWNhcdtn13ISTSN0X0RX7GDAaxRGAMBriz18cIYUHrKyxpwpiu1ueZdM/zBqCpoNbaMPB7VsEv2pIxSj0IeORJaw37vquNNF0ZEYg3bBr9M1gFAimroUKn7JI1wsFPnaZ7hJYoCVwLUPn7M2L/5FoxWST182oOh7CjJo1P7YyBwQx0i3jSteG2EXFmg6wNT1fHjNFSBFj+eUZ8sO8JJrzkOdO0tQuFQqFQKBQKhUKhUCgUCoUfC7u9uEdCxvfyxyRg8P3rn0bOLD8LJ0H4NbRtQ9u2JS2LdT1m4tP+lx2HXzaU07WWCmkm8Yzgb04e3pvyKc+9R1HM8/2wAVIiEcOrfLrSRQvx0vev1QGcB0mZESs9sfY0RVBQkNdScOC+iMDZmzzWiuXXHyBs2i5thEaE/vbAu8fv8Ne3/4rHr7/DH//v/w1fvvoDtr0B26aBMjLYHCnya0AWg9omNR0aGswi71+/w7d//gb/6//5Z3z1X/8N/X0HHwxsaVFpHZgsBph4sXnkj1jV7GPe932JrtEGXQgAw6OpAJ4EDC9I71S5PNscPXF1Vpw/k4gRkKU7wjTGqzb2fU9ku4luNqeBfX8AEc0iF5twt0FESyH7/X9a2Np6W0UXEVngeykLgvb+4gn7n3FGxA3MtGzNLFrYeGJQNDWQxEc/akU8lhujng+BQG0ABE9lZ9EaLmCQCBjZ7veFo3nM1xbI6u9qrUKhUCgUCoVCoVAoFAqFQuGHxb5+sOaeF3DyH4dmewrhANQmj14yEUKZOa+rQDM1yOZKLG+1LfP8x4mI9x6FFXeyXOIb2Bo5cW5Tr8wn5WG63OeVIiUoRx2Yl3oa+gUJyD6UAc/Rr97SJrK8rEBwHi5FOiB1GZ/Sa7WmxHH6zG2WokFGMtLicc8msnAiaYlA3Jx8PWk+pIJPTDpS1Zg1zVs+k7PHwO31e7z5128ABr77129AewPtDWgbaCe0ndD2hvawoe2bvN430L6hPTQl3Jt7o/uzs+iTPMyTvVehaX7H65dZvNMrLCUX9yFRDbeOfusYx4F+6+DO4KNjPA6/9t23b/H26zf45r9/hfdfv9PnRl7U3R8MYt15NAdFzRWvb2I1L1zs6RhWUwLqtZ/0LluPEi00r20j0Ecbkh5u2LqIWhS2jq4EjbW+BCHWRhYwpueSBZ8mutXgEP5MwCDqGpWRxRWxk0UfjSFSETcGaGCMJqnJnKxX2SHtbZpPt3h9frGIDXekWBs7jfk9snixiEuLHaYUTgzYHgYTbMQEAg2AmtWoyG00j6AyAYkaJlvY+TlUyJqe4zTzmKXFzQ2+KvL+y8W9Uf6ahNFCofDLAqefp665h+f+C5Cea/wlKI25UCgUCoVCoVAo/AYwCRj36h8IZWW/CcG/DUnTMisE1tpEe02YHP3NT3/9XgrYtoUgjEgQOKmc/3fZHxYv8SRieEop6VK5QiPc8/3RTvgg0/U/Xq3YhSst58LGHwtJCWUiRgOxkfc6SAoRYxqSjoU5eb9Tpm7T9RNxa3nCtA6C1kAhvT/I4BB+jBw1spj8R1eFiysEHgPv//Ydbn97h0EDBx+gvWH77AHb5zu23z3gsy8/x2e//wIPX77C/sUD9s8/w/75A/bfPYCsZsQSEiMEfypIn/4n47xiBTyOxFeTCQAmyiRlCuCB0Rn98UB/Lz+Pbx7x+N17PL55h/dv3uH29j1ub97h9u07ueaxYxwD45CC1WJXpaPv1JOxZ2U/WdBgDIAGGCJYzLVqQsDANO8BqzljooK1NwbNHvwqNlwJQjG06/0GsEdKwNfFnMbpukH5IRfCEuk+JA0W/NGqkEIARkNrM0HfbI4T4U5xhlhfeIInSutbQ0nSdqH0Z75FBAt/r2LSdL7kJuxIynYmTHuLOK/QNOLT8alnA5ELOUQDxA0Daou0jkmfs0WFzDMinGdnc3rmORYKhcIniud0hxe6r1ymQv1g1BFdKBQKhUKhUCgUfuWYBIzsVR1EPWFAiVNLjTKRd0s+dSXGgucLgSJIUNZ885qiCJTIOmnDIgqMTLXxuRe3pYpJAgY4+Qq3eG2k+ZS6xvhIJzODVIz+bB5S1Fg84HUOTb8De8FdHyOMG2R/7w1Otg3y75xSZ3rnv4TPJLWxjQdeJ6Q1SenfWr4+jU2JbTix7YPzG1yccve/Mz0rf47lHrgtjesn4VJhtQ3in/U2b/mujwN9HDj6TeqFvIZHYzx8/gqvvvgM28MukRgPG9rWgI3Q9g3bLu/bppMnETCapljy9yrC5JokZmupsWD1JcSWfXRPxzT6QL8dUvz8GOAxgAF5nyIwjvc3+Xk8cDzeMI6OcTswbh3cB9CjJoF1bqT24JEsrSvIH6AU3GZIcWhG8zEzGiKoQdax1TthZq1hsUTEpLUV+1K96wfQu0bbEKG1EDjWKAHZn/N6zcJHfGYvZiEkr/9IcQRVReUM2ImwNRnr0Q/00RGp3MgLmkstmnjGYBGJRhsYnab2B6Xx8fJi0fLojuXs0nV3jDE0+iLOhTkyhSYRw7/jODPyZ/aMsF3xUNJI7z0JCk0kMRUyt9ZA+4OkkmoDfcj6daGrW4Qduxgdu3SWaEPgM3FsO43oh8RVhMTPLZjcO7MLhcKnhx/9FDgddx/YY4kXhUKhUCgUCoVC4TeAUwqptaB0+OGqd78Tr8kTOHkGB5Ef5P+JcDLv7PyRfa58ued4X8iiORVSn77zhux+a8OEjDW9jQssOuaJiQ975N+WqgYEMIVA0dLYYpzDh8SAe9jfEy4ubeUm0+sJALGISUTC108phrSOgUXGMJK9zHY99ZPrjgRBCZyjRtItmNwCmdxs2WM+s6Hs4/CbpG/1CLffgweOm5D/QsIPbA8P2Pd5qfJgHLcONMLDw47tQdJL0daADV4rQpzRSQSolshyJfaBEDcetgfs246HhwdsW8Pj4yMeb4/oR8fxeMPj4yOOxxtujzegi4ARBLUoWmN0sHmzI54Jbc3Fr7YB3eyThIcQecKWtrckgqirrZuvU2qbyQ6yknmAWUQIcbSXdEohhKxCGWGK2DFBSaN1LCpjSkumooHUnQg+fno+aZ9aNA6hQczU8pWn6wVtGlujDcfRcbvdXBilJmt933ePuGk2F2YwN3BjtDGEuE8FzkUMQoqKSGubGfkjUBJGTxY8z9tSMtkazyLGdLIuW33wfH7Azy61RUtCaY6+YKTUXCTRFqRF37fNBZ5tY/TWQb1hdAZxB3OH1xpxFflCsMwHu4xWP294Clf1UX5SLJrUfSnqKdjfHxfNVxRKofCrxvc5n67uXNu7FGGfuH++kM/HcaFQKBQKhUKhUCh8gtitEPBcB0HJd2Zg8ESsKZ+mhH2DeO0mb34ndDC1Z78jAkPJOOO/LCqBLO6DwDzcs3jNoT/GEC96Tp7q6jFuhGOuWWFkudQNMCJWBYzU9jrmjDmPPkBpXN3aOFFkUcfjzHGJnXMKH3sWq7Bhqa7YjEtwD/TVO14ISYiPNDMwxpL7ni/63IJgJSgxOZOZ1EjTMjEiNZE/AkmJhBiXE/itYWs7WtvVSz7m3odEMHikw2AwE2jbQQ3Y9pwOKqd2AmjXNEqkXu8Hg/oANcJAx4HDh99ag0+O4GICNcJoDLSBvilJfAMGEW63d7jdHtF7Rz86Rj8kiiLvBZuwLAiPDPE1ovKCi162tpKgE2veVaAg/knGvu87Xr16JcW8aVMCnvH4+DgV7rYoA7H7JjoTkQ8xhAxbZ7G+8n4JvnqOCFjrYMgZIHs6nyE5DZW0k1PKXdE2q6h5JoGICPu2uyyQRcqh9UVAqR+W59yIsLUdW2Ofr9n/sd9w9GPunjVqzB6jPx973BFVNtfXiBU62dBWw5U28AQm8RWABTylgU5msmcrtmIVWDt6J9ARYpWsx4YGAvMGEUiaF/s+i0lpRNp/0/RsdGdCV1E2L8FzZOIHCQUqnM0N+BcfgPzgik0sFD4tZHX7/hn1IafK3OJ9/OwCcKFQKBQKhUKhUCj8QrC3VCcgiCtJzwRNNSI8WRD/c1Hr5R9hTtzRFN2Q2xelwSIXjISEk5HWNg9GR5/udfGCc+HuEDCM6HSxwll/OGHdjBq8EAzOqW94+p2+sQvEe1kFlZxQi7hJpEQTEnW6DTNRyCyk4Cokzb/Vvg1AW4SL9T4VUwYPoJuNTDAx0SeeY1PbePsIT3knvfU6TvfGA2bnC10cIgLRJuTxJr+hBYWl3YHRO45jJt8BArXNo0mcOIiK0iqiteibIalw/LHkaI95qK017NuGzdNOMbg1jO1AbwzcRJx5vD3idnsvIouNTfeDLXQm476jyDU1icYgS/Ul1tSIjzSuND57hKzTsyiRTdNa7btEhjQlm6HC2+PjI47jSHaQPiydErPt1/xjfdK0DkSE0NxoNNsvr39Lx2ZRGK7fpOiM/N7XEFPaHd6aXpvFnPzEQmQBABFcCSuRPQZr4XKG7XBA0ie1ffcUcNGrCCzjkTHQ9ewgYLg0cTVMaTcLhWo/tvXRkIISyIW+SwpqWaCir6UxuuhzPhOmdZRFUxNniAGZmZqRdd0/YNv2JGZAz3Updh77UFNNJZGEwggxzPRsfpAoBIKcVW4PnB7FhzW2fpKEqWcwmdeNXQJGofDpYPk7+/LbeD1/uZzvH3A+ZmeU73uulgBSKBQKhUKhUCgUfgvwvDwrmZ4JYEutpH7WiVTjE58TaVKU3b1C8gi//vpphsmFDiO7/Qufjc7p+l4jx9YoBx2aT41H/EPSxAX3QId5WxuRCSXeEv3JmtKFsyqQf0XB5FMEhdv/whN66BPYKJhv9xS3+eggmAAOscTmOJPFzzF6fLKVzdd+SYSIFupu8Yxas3RDLaJjyK6f150MhcP+ZOvNnvOVN/XsBW7Wz0uP0ud+ldqLmogYEl1CUq8DA4OTCMJpnLm9tD9MBDMH7zwdthd6NdmzaiK4mL18iARI5A6n+g4bGu3afggPUtOk5ycVwt4HIJbOfbLGr5hIlRCr8vfne2xyHzQqP0PmKJCn7oizJcQsnCKOTEDctoYHvJrPvTFUp1JhbFn/OQrF32/NBUbRtq7myvb/9MeFCGzv05qJP80udns66zjEWuh+zBtB7MAAOkj3me2dRpJuywQoxgCpkDHtAx2J/Z3wMYR+FmHm31gELlp+f6BYogfd05E/L8T3ElIKhcKvG37g6vsSMguFQqFQKBQKhULhp8QOLJ7ELOmHzOt0uPepkshspDJOpE5EUkC8mf2CJA7g4nr3qE7ex3cIo4j6yOlVEkVtpJqR4hORbaLHE1yUd5yI0ORZ3qw6NmSOkkImyO4g+MKGTA3QegKnrhAe6+nO5HW+jFQbnrz7CVK8WmdpXtgwj/KLtub0Wrj89/hKRE+i1kUYwcle1Kai2E7cmyyRxQuOe23NzIKFdUzB6y4Cxuo7H/bm5ZeSt1lc0bohRFKLwNJZGU/Lqf3oD0oUG8Euo5ZUPykSx+7leJ9ZdrPP5ORNItxsbcO27ZqCa/N1zYB7ylutFX8uJjAlgtqW/pXQkJcl572eBLXzPQ3A8HWUI4hWsWC+72rBBYl/tTtd/OAY7FVqDQI5X28CmkVjcVoC0o/YrLUND625LQczeJAWW0/kfTof10gpi/SatxADlCKG0hR4Wo+2bpDayJT7vIrSakptnF7i3qeRYkzTbek+ncUdBqOBuGMMSeVnNVWsaAsPEzHSfvgAT99YZ2aR5Vnqc4P/PXJ1P073yG+/CMmQy/0vHmrcY42VR/OvEs/8zV94AU5n7g8RdfWLx4+3Zpb/lCkUCoVCoVAoFAqFwh3MERhG0gXTttRyngUBYwtJ2dKooUB6Y08EUlY6/A9IEQyCJJPKJOfz/6Rribh2D3gjNi11TnKakxRAksrHUv8YkW7tRGIrTPVpQ7yYzcFMQnqm/zmB7GS/Ja+/EmoWoldHwO66fwVz8yZwS6mwTMph6ZMHXbYRZLnUlRAPeiVcL2y/ihUnb3r38iYQ8USKWkFxapw8ym2MWaiJufl6ujsOJU2v9I1EBYsEcBZh1vz8RCSpmdSuY3SpezE6uop4IlaZ6HKfw3TRjG0vnJMmRd9ms01T+HSAU/0EZjTasNOOTeuo+JMYlrZo4EpgiIiW3N9M+l8JFPn9Sk5NkTIYYG5PXvs08sa0gZoUaYfPcv0TZJn03xArUfcvQZ6BbwV2O8o1ERk0BqMxMCBRU5LC6fz8rmyYX5/sOb2en0/EF13OCq482dv5QF564XNDkzglKaFEyBURiqi5kDfPQ6ODSArCA6w1NSSL2qRDTdMNSu6enZ4TLz4ey7q5FC8ozquXCBH0AdcW7uKXkcv/5+6/8OvG/f8ufenKuvhPlhIvCoVCoVAoFAqFQuEF2C9JBUZ4LSdyPzylObxRsZDklGpfKBG7RkLoZbD0R56UytIhOZFp5H+mujj1HiR2Fi9mESMTsjYWskm6WGNe+bBZUtDnOXWUp48x7/whEStMi3iRbOaen8ZDJi9x8+jO5LCbM5GDM3kfpJ/V9UhGgkefpOe3zsOIPHl/TWDf/4yXlxGRkz3fI+1WXJwFgLnuRSKAkZYLX9CbyX6p5cTuyzNeve9Pzawe6LqWRx84jsMLY0tU0kozX5DsekloGOkaXsnumEjaAdO8iFhSR22b29LWyRhSo2CMc3ov+x3RLPOc87X3hIeVmF+FkSsRKKdWytE6PtNFTJmfNM3X2nhUFLtYkfpnROrMNS4AU4LY54q8DX2Px1FEiz3s5pHaCJHiQ72PV/Eij99OCnvHoPO90znKWFfkeThnQSXEVJufCRPLc7XnOQhEDa0N3bdNnodGnw3fz6zHcf77wrfiB/H/H+/VbX9LnElBvkxn+IJ+psN4bbMI8afwi7EP4bwmfilD+2Dh96fDL2ksPzfs7+uPMcmVplyWLRQKhUKhUCgUCoUPg0dgTKlmTrnXV+Lt+/3zy4QKdZp2HULGYGQyMPiAlKK2XtWTWnO70wjichIAEnkbnLaSggTxXHftgwGty0yW6ggzIQvgVJh4Ih4mQnKaKQDyGWzpgiCpg/QVstzaC+J/JaiNGCeKMVtvJl7MKa1m2/v4uSEKsp/7mqd2RQtODUf0xVr3Qv/JbmS7zNXSNHUvCJ3HeDWW9fvT52mk9gEthaOnugWTyBJ9HseB2+2Wnsl5HETXz1oY2xDZOD3vK4/0HEkyRdKQ/N62hn3XYuMaOTPGkAgRFVn66Jc2yST7FVF2EvxwXpfZZle4ut77GKzEsbbd5lovIWLY2rI1kMUca+zqmRthzb5ngfM4LKXVan/5LM6XvNdN2OrUZI1ytDHGcLuY/c72f/kZ6c8AQW6FlMeX0Vj5JP5Quj+f4ybqxhoKIU8ESbNVm+61uR/c0UekyhIh2P7esI9p6m8SUOjDx/9hk8WHPIpCoVAoFAqFQqFQKBQKhcIvDPuT3yrB6KSakdzAyaX2nCLk+jv5QD+z9EJYfdGDOO1ekANBhjonOre7ErJCDjPmQrYqghjZ5hOypmdCeRUsct2GSNMTY24IWlXMF+7HAxylKqy/hTwm1kLYRMnudCJLmxadnsjTJA70nj2tr55HWH316jY3dXnOZxvLd0E8R2vtwmZmN6jHttYa0OiBqDNw7dH+bLoeHahLOBQ2i4/iPhNWojh21DOwiJDjOHAcR3i9e6chLszEMd0h2M84iQGucsQaaPpDBC8wLtrImNNbXaSQmppe98MTnu3rd5eCxCKIZFteRWBI6q0UYcOItTu17zseacnECybfY/lssD2c90mMz8YMXEXx2DjNhleCpXTbMAZh+Jod0xyzbTzKQ+eS991z3syrkHTptnvvXvt6mvQ8V7luritjnw7uoK7nXrZf6tPORSITVvXLQQAfYQ8VV2UIZqPVDiHO+r6dzqJ0/pCdn8/vLzu7znrPjyqRFL4HPj7apvBzYBWCC4VCoVAoFAqFQqFQ+ClwKWBk8jeYrOcJuPn+p722KWphx+eYSdDOA8fo830MWCHnp9LdSJSHN+xFr22sIV4EEdlGi5Icd9B7n7yQAaARgb2Ghwgkwxi46AZMQFfCtSnpTguxTEQyhGQHu19bCvKYmvYd8xLv/IHj1rUoNbkXde4jT3SKUMjvOP1MESHBKZKnzrIxWQTGTCSPwejHAHPHYLHh6COiTp7AGllDM/sZS02J1ZYEpTxn+9m2DQ8PD9i2bbJzFi9ut1tEDOhEkqSE5/bDczjNCSEKbKSFzzfyyAsh0BnHOHCMjt7n9Fv30kF9iIhxZauMpsWufZzbhn3f/b0hRKkQqWzdEhHGOAtdRA2NjGTPNtbIJ+Q0X5YiTX6szrPdtc7QBNerud9uNxzHDdu2+XrIwhY1AvMG5oGu0UL2Y2eBRWT4c/Xi3WHPl+AlkS7pE1ww9f7VZRse8ZAuUIFpsEmSsZnkCG1obXNb2DC2bUejBgzdFRxihRRCh0SuUJ9EUdjrfDhPr1P/Nked57O8KfO0Dryrj4pTKRQKhUKhUCgUCoVCoVAo/BKwmzf96lHskQhbi2gGwD3NnRdKmHO7ny9wstI9qVO6phPljNWJGVoOA+QevtcegZeEoZPdPNFk7nG83HdFBsfPiCgUytnqKf2p9ympTuYFboIGhQrA602YvcmZQwgArjzY8/jguenbJXl3cq12e1zxi6mCh8/5vFYQ0SCtLYIJIZsyiy1OVS5zuYp2sc/X9xENMttODDG3b0T7vu8TSW0ktJHSLgywCTTJYirOnf3YsS73RVhZvsprVtdRjmhoraFtcCHIoi+scHeMwYSia4LW9m5IAk+zwFfrfv0+R1/Y6ynywsj9YVEYqxd+jniwNiVVkry3cRAIFkkU8+RpoU6DhdWyCcFSRY5r60wvLTpgDAZhgKjp/tX1lgRWEzpM0DFbWc0VirCgu7gSYO9fC1w2SFeLjK77VntwWg15HLM9Nf0Xpec3oA8HGCosiB12v4bBaIMxKIvIqR8XIkKYXcXZHL0VkV535n93jtkaea7z5RUBUCi8HB9yZhUKhUKhUCgUCoVCofBDYefBONF7yhkRAMl51Jx8Ssy2EFoXzKATXk/+G1ckizZJF6vXN0X7OYLCvqXZy/xuL2RT0jnQLJYYEW6f3iNxJxFfjHNxAAAOL0lEQVQjT35qT141ZKIuCMVc3FYCFxbRaCLHY57ze3Jy8Dxrjnlq1+fomMsbV71o6tdoeidpEeshSOggpdc2Tj7WRE6AvoSYXEUMAFJgGNfPXuxrazCe65o+yto24n0VZ+5jHrMsIV6+t02SrzuP19IguSjQpOZF2yySJY/PohlUNAJ5LZizEeIXgbTuy0vmpvctkUZ5Dmv6qHxPRCckEWFah0agk4pzImBI0fIsVpht5toYFIto2j9uTUvnxZOOdbGGJFpn0wgDv2cwBoZ2J2IYAdPaWKNU8tkgARmcxIVz/zlq40XwppYNdYX1vFiaYZ9oFubi/Mt3uamzOM0A0EVwAqG1zVtmNiHKBIz4XO41sXAWM6x5WU8RzRNzf4mIQem6fPss9hUKhQ9HpY4qFAqFQqFQKBQKhcLPhX1wSn8SVJ/8cq5MRAxQ8po3cnxSFVK6mtyMk8hGQEYUhiZMCiJQiUOvP8BG6p8JLLbQhGciMNwrG6SCTBIJOH1PZ7Eg9yWpVoJEDo/h87g8gsVG4MWMF5ItfcRg8RwHPOpimotFixh5qoXMTUyQtC3m1cwukiwzcb/m9ctT2qVkciJMRbjjjpnA1ic8RYx4v5TuNTHMi4ifUxadRz7b7UxeL1entEFX4oURyVm8yMQymdCzrF+3VBpuCGnLvGl53nmEJgCluU0RGK68abqtLsQvj0SkAxOJHu1Gty6upGG4sPRE5MYyUCW8hWBea18AmNIrjTEAbmEK36qx4KMewryPpP2wo3nhb7peYm81zS5kq5lPpmbwNBdLQZXFhYgUiLmPAYA6TJzkZBOyDYE5LVVeR6Blvd+z7Yug40t1VmZC/rq905ONiUwfkh58JnSdmrSzPt0n9hnY2yY1WtSu8YxVHaHmURb2ORHQ2ipihP1C1GR/9nlCazRgDFMGmqf3nHDxlEf5XVG8vM8/CpfibXn0A/j1iQMvinotfG+sf7cXCoVCoVAoFAqFwqeKnScPcRMR4ApF8NCJLMukf3yqTNRCLnkDJlTEhyaMmJSRoxqMnJ4ypFD0EUIK31Ud5n9Um7uxqwWYJgCeiNR7hNbgAbOZDel+v5lQm5QKbU/vN7uZiOH3LzakqJXg14zoiwf0J8kT/u/eldWOyBuPPEmvp/uSsHMig5MAZUToid2HrRdLsTMzjHb5k/9YN47UCOx71yF4Wn+/pP3JUQOreGECzdORIYlInojva2L5LrmjxRtY2znXhGiwgufcWX4W8eJZ6HIjCnFjFdyevj/Wa+zaDY02NJoFDLlc7TgYQBSSN51RLk1REtOTGhijobUsvpn6Yc8evlZER0zrPx5Amv6qaOieHCZiLOOHptyyeRNr5EoCkVfm8O2SxDJ5PuPSxB9LRIVoszaa9+P6iu9cuTYu+5asmAgu7GZt6DyHqhK8bSliIs6Y1lgFjU1E1SFtDh5oxBiTkIpJGJ1tlJ/pLLycB0f3v/vNYxZCf5gWC4VAkemFQqFQKBQKhUKhUPi5sFsR5UzAGYTDVU9/GLk3UfGJwwvP19lDP34Ld6lkmRHdRqw6sb14+Xsaktxxzmq+kOo0Fy2enXQt+kOiSbwdMs/gRCDr9Uh9eA73+BQDwJLwZBIghHhUYcZvJLel2E9tx0I/dR5oTYQIL0hNyYY+ZwbQwQwpnJs8nKkFYW2/yaIBrMhwhNGc4XypEdCkxDJHdAiEvDRRwAqL52fCGrFiXvmrANKsmruJYndA6zgn4WC+z2xh6YfupTzKtS/yT2tt2Q93hDnjuO/ZMI1vTTWU25iFC1rGxzj6wHF0HEdP9icn8o1wj7Ufzz3vsUvjPTFmvzoJaI0I2xZ1WPKcJjGIpYbEGtUUzfL0mbUla2x4oW9ovQw7h4bWprDIrJNgYetuEgrT/O2sc9ENALXk6T8LbGzmS/vgyoQx/qbrZ4DRp+ed7fNk2jtdAx9PFD59X6TLWkS6dKiu/V++JkI/DqCNpLHG2de0hghRbBtmRh+k6d8GiIZGjc12Cs98dtFpGeLdea4C5ktQpGyhUCgUCoVCoVAoFAqFwi8T+1BvfUudc53Kx0SMIN4TpQsXKpw4Nc9m+y57xtp7IwJDGBDyMIhbI3c5p+FhCm/uiwlN5KCRZvpdQ9BgRC2GoNEBee7eI88EreoMqVUpVJxT8gQZFj3Kd8A0KBMJOPqL+hIDDUoAzhZLc5Vi4jwAZnLvfEnRYkWVjZzLZPwIQhYmTJG/M299E01c+FFC2jLGSJTAdkrLZM9BRKm5MHYIQaQC0iwSrVi9Pu9dP0d1WGF0ArWzeGHPKIsWUrMhiNNt29J7CpHCxwMfzzQyWgnzeaHOwSch5l3tOwZ8bEcXAcPQWpuejdlgEjFO1jzjpcQtEUltjma1Odq01qc94lEsZ1H0Xr/r+9ZMXNN1B0LfCEQDjQDCfdEhzgof/dyPbgEmSB0RrzOyjDHCVxZb3LeT20VFSasxlNMjPZVuJb6Pc26ayjrIrKfZWrsYc1weQgzbnvfUgOd7JrFtukIGN3oH9+FCla2TOCvkGfK0Rhldx2Ii0iqCUdpnuc8Uf3UK8rMzLE67p6SOjxEsnhf+flv41OZbyDj9feSiYq2LQqFQKBQKhUKhUCj8tNgBI5s13cs9XscEhtMFSgg7BeaZyJ0APxG0KQIjamiEd78Rf3opnI5SggyDnJ87UVQvJG5j9DDmLTyyrTnGEwRvqj0RVOACS8+VajEgxB/Wea/jSbQsLL4j13IAAUMGp6IFwENScDGGEoctRKk0Oh8j2xyyiMH+FVEajI9NiWsnQc81JbyuBDBFXhgRP8/0njSD05w/1EPaoi62bTuNj5nRexdh4Djmug13+ntJ/7N3vj3FTI5HeiqJkmnIESKZGDISt4+BbvYbPdne+rsWfXIkRuabPpR8yns325Ma+frIxLPZcJ3PS2F2HmPAUpQxJw9/AIO00DNdnUe5sfvzZTtoqPnrc0o1ulyaV8KZ/TYbCbsuY5Q0d+x1PXyPLO3MqePsdUrxx0ifp3Mz3QXVBu1coel6ClHSIncWdc0FkxTJcjJBEjZ5DI22CQG4mXjVKNUyyUedrCX5u0PqnVyJF9NNfPGkTePC5WOaL8o2+yBk6z7dU6FQKBQKhUKhUCgUCoVC4YfHDiDxO+y/TvmOTwKGeQmHt7BR7k7DJfFhim4wIpmC/BPBwkgsSH57EvfbE8ntPHHyrAYWbimTVckflxevcBcWyL2IfSard7mT0tGDE3yscQyJlMz9zGmlYC7T3qF7z6chu4dxFi9y38wYowODIDUFTMCgKZUTM808nKszMwHOyGy3fOfJw0wfouYEaFvI7QmJkLQIjCwQhOnTnFYxR9tdUzBd/Z67Zifat23Dvu/T+ssCxlV0yHrtvX7Wsc5RINO3Yr/0cIkatrahte0k8K0ph4aJGMxOOUtz5N792a5rJMZs8w8nYNcUXE/VEMkCxoowz31b5rEbhU72moGDWYSfZqnV0pw4BNF8NlzNOc61cb0PcGff3REv5pRbea9b1JpEPW3b5iT9mjYpbETnMZ3WX+xhng7duJamq9n3rctq3gW5eHFe71YDxA+A6Ajx/EXAgJ/rLZ3b1qM9QzvTJUpsnl+OhoppWlRaiCqTGWwt5kPOzzk36mTPl4DTn8C8pgqFQqFQKBQKhUKhUCgUCj8Ndskzn0nAixQn5pl7yqUPhBe9fW+sWPPvViLfxAJtOnfjJFpENWTxQimqpuQ7J79Yep6eilRVZ3GBKBNUcp2PhSUVUvaGntLX5E+ZZ6KLrmgvm9k4eZCTCQOYLDt9z0rqjsGpqHMQnjMhHilafG4wu1stjMlIAFRc4vn5xMyNlt2mW59LKZQ7iwgCwLz478HI8JeIF3ksV6mtAFymjlrbvRf1cS/N1exBDyeGI0oHsJoDkopp05+2PK+FxF/IcV/0Uz/3bXD1+kNwJTxOacKSUJWjClaCXq7PwtVLwL4eTZyQyAzZ/yKgNRcxcrqjnBZtFiwXEQJGrZ+N+pJ1dhWpYyKO2WF93VrzItiURLORRNwQjGM8OL22K2heb7GtlvNFzjGC2YpwruATY2UXWFchxXptOp/luTKD0YHRMFifj541s4Z1bRsgUhq6LUyvCcXD7pzaPS2t/HcW88UFL4XNuUSMQqFQKBQKhUKhUCgUCoWfEvuXX/wuEfQGSn9iZp0oU0VCsTdN5wKMxdl1JmedCPXUIkZ+BgnqaaRMwKBMsjHY8rUz0Lglki6RU08pGWwiQbskZ6WNAR4SCSLpkIyIHRiAj28VAIh5yvhCRk5LB5PdfDCT3Wdej8y+7WxDId+HRGB4uxz9Onl+TcZee4zrGNwOIUNNE9X5yLOPSAcjPYXM7hh8oI+B43bz52rChQlZkiLovjBwqqsxrZP7D3rf9+knY4yB2+2GPjrG/nDXk/7KQ/5qjPeiG+7d61EhjbClCIyrNo7jwLE/yHh7d0/3TSMhhtZBuer3aozPjW2FiUCkYsG+y7O2eyWCRYp29+PwNGGr+CK8dwidT4kYWdgCGojzutP5tIaW9vGpQeuKWlxj8x1JZNisvyxfPiFYvMCLP2o7ZGI/n51RaD6T8pJyLdLoIZ8x98bjQm4SkS+F5jQ+t1VzEeJKiNFXp7HLx3EuU1YYUh9ETfuAiM7Il1D6mTo+RURNPxgXf1fNbUTqrzGN6UNxupNFtPmhRAx/7ne+/5Be4ml9/HzvNvqBY/HbLwXg62s/Rl+9e3S96L8DfkA7eZP323xOQL7798v3GtHz+BArPPd3xY8JP0vTZ3JE8unzD8XT88pncDstm/PJNzU8j80dU+Z7Lk7Bu39ffwx+iuf2sQ4ShUKhUCgUCoVCofBS/P9Rfaf5UJJpgAAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCwFoaqYAJ3"
      },
      "source": [
        "# 💠 Text Analysis. Data Extraction and Creation database\n",
        "- Raw data from articles and Reports of Guidelines in  Citizen Science \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiJy2v45CWEB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT8LSVoWnpNd",
        "outputId": "cf172602-d4b7-4b5e-e834-698c77c0c510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 118 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 28.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 42.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 13.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 431 kB 60.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 49.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 38.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 300 kB 60.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 68.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 748 kB 57.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 273 kB 74.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 70.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.5 MB/s \n",
            "\u001b[?25h  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install txtai[pipeline] -qq # extract text and others functionalities\n",
        "# Get test data\n",
        "#!wget -N https://github.com/neuml/txtai/releases/download/v3.5.0/tests.tar.gz\n",
        "#!tar -xvzf tests.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAhEwdXUratr",
        "outputId": "9ab47435-4176-4652-b8c9-4c01ff7c6571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 6.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install texthero -qq # clean text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N46XXjLIntqF"
      },
      "outputs": [],
      "source": [
        "# Install NLTK\n",
        "# import nltk\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDfs4aiZfa09"
      },
      "source": [
        "# 🔷 Extracting the Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZblK5Vr9oJin"
      },
      "source": [
        "### Create a Textractor instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKEarY_1oKCe"
      },
      "outputs": [],
      "source": [
        "from txtai.pipeline import Textractor\n",
        "# Create textractor model\n",
        "textractor = Textractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knXaS0PeBTYj"
      },
      "source": [
        "### Text etxraction workflow in action!\n",
        "\n",
        "The following show a basic workflow in actions!. Reads a series of url,files in google drive, extract the text and create a list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjbIRvzqBF_8"
      },
      "outputs": [],
      "source": [
        "from txtai.workflow import Workflow, FileTask, UrlTask, Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_PrVvPRfa0-"
      },
      "outputs": [],
      "source": [
        "# workflow that extract the text for each link \n",
        "workflow1 = Workflow([Task(lambda x: textractor(x))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxfSHOBXUwGA"
      },
      "source": [
        "## Importing Files from google drive "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBYxayQph_ha",
        "outputId": "b6170080-9637-4a76-e4ad-8005485f8a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km-EkEmPiJVZ"
      },
      "outputs": [],
      "source": [
        "# you need to read files in folders\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqotG6ys9v2K",
        "outputId": "b684a6e6-d73e-473a-c96c-03dd8f91e811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/A_Framework_for_Articulating_and_Measuring_Individ.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/Skarlatidou-2019-What-do-volunteers-want-from-citize.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/d0ab7fefe655535c9a3a31d457ed27f3cb47.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/Design Principles of Online Learning Communities in Citizen Scien.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/Chapter 26.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/Designing_a_Platform_for_Ethical_Citizen_Science_A.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/Wiggins-2018-A-science-products-inventory-for-ci.pdf']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files_adrive = glob.glob(f\"/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/*.pdf\")\n",
        "\n",
        "files_adrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHTvP7T_f33",
        "outputId": "907bdf09-e83d-4f1a-9582-184d7308815e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/508_csqapphandbook_3_5_19_mmedits.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/9788794233590-cs4rl-skilling-v1-screen.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/2018-1.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/aphl-epa-citizenscience-qualityassurance-orientationguide.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/citizenscienceguide.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/Citizen science_guide for all.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/Citizen Science Manual March 2019 ed-without appendices.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/CoProductionResearch_Booklet_WebFinal.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/Co-production booklet for researchers.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/download.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/handbook_november 2021.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/Scivil Communication Guide.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/MotivationsforCSREPORTFINALMay2016.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/hp1114final_5_complete.pdf',\n",
              " '/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/Vickie Curtis PhD Thesis Oct 2014.pdf']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files_rdrive = glob.glob(f\"/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_reports_clean_pages/*.pdf\")\n",
        "\n",
        "files_rdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otw2gUkZVZit"
      },
      "outputs": [],
      "source": [
        "# workflow that extract the text for google drive files \n",
        "workflow = Workflow([FileTask(lambda x: textractor(x))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnyLWdpyh6M7",
        "outputId": "dfc1866b-4f60-4627-f842-b96d12ce9d02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-20 23:36:09,922 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "2022-01-20 23:36:14,676 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2022-01-20 23:36:15,863 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
          ]
        }
      ],
      "source": [
        "# text from articles\n",
        "text_adrive = list(workflow(files_adrive))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62zTFmYZ_po-"
      },
      "outputs": [],
      "source": [
        "# text from reports\n",
        "text_rdrive = list(workflow(files_rdrive))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9g58XmKiYsF",
        "outputId": "0c558a9e-0ad5-434f-8241-23b94975b5e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science RESEARCH PAPER A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Tina Phillips*, Norman Porticella†, Mark Constas† and Rick Bonney* Since first being introduced in the mid 1990s, the term “citizen science”—the intentional engagement of the public in scientific research—has seen phenomenal growth as measured by the number of projects developed, people involved, and articles published. In addition to contributing to scientific knowledge, many citizen science projects attempt to achieve learning outcomes among their participants, however, little guidance is available for practitioners regarding the types of learning that can be supported through citizen science or the measuring of learning outcomes. This study provides empirical data to understand how intended learning outcomes first described by the informal science education field have been employed and measured within the citizen science field. We also present a framework for describing learning outcomes that should help citizen science practitioners, researchers, and evaluators in designing projects and in studying and evaluating their impacts. This is a first step in building evaluation capacity across the field of citizen science. Keywords: learning outcomes; evaluation; informal science learning * Cornell Lab of Ornithology, US † Cornell University, US Corresponding author: Tina Phillips (tina.phillips@cornell.edu) Introduction Citizen science, defined here as public participation in scientific research, was originally conceived as a method for gathering large amounts of data across time and space (Bonney et al. 2009b). For decades or even centuries, citizen science has contributed to knowledge and understanding about far-ranging scientific topics, questions, and issues (Miller-Rushing et al. 2012). More recently, citizen science practitioners—those who conceive, develop, and implement citizen science projects—have sought not only to achieve science research outcomes but also to elicit learning and behavioral outcomes for participants ( Bonney et al. 2016; Phillips et al. 2014). Many proponents of citizen science argue that participat- ing directly in the scientific process via citizen science is an excellent way to increase science knowledge and literacy (Bonney et al. 2016; Fernandez-Gimenez et al. 2008; Jordan et al. 2011; Krasny and Bonney 2005); understand the process of science (Trautmann et al. 2012; Trumbull et al. 2000); and develop positive action on behalf of the environment (Cornwell and Campbell 2012; Cooper et al. 2007; Lewandowski and Oberhauser 2017; McKinley et al. 2016). While some projects have demonstrated achieve- ment of a few learning outcomes (see Bonney et al. 2016 for examples), most projects have yet to document robust outcomes such as increased interest in science or the environment, knowledge of science process, skills of science inquiry, or stewardship behaviors (Bela et al. 2016; Bonney et al. 2016; Jordan et al. 2012; Phillips et al. 2012). Several factors may account for the lack of demon- strated and measurable learning outcomes. First, the field of citizen science is still young. Few if any specific outcomes have been defined or described by the field, therefore, project designers may not have clear concepts of what types of learning they are attempting to foster. In addition, measuring learning requires dedicated time, resources, and expertise in conducting social science research or evaluations, which many citizen science pro- jects lack. As a result, citizen science suffers from a lack of quality project evaluations and cross-programmatic research (Phillips et al. 2012). The informal science learning community recently devel- oped guidance including tools and resources for evaluat- ing learning outcomes from participation or engagement in informal science education (ISE) activities (Friedman et al. 2008; National Research Council 2009). These tools and resources are relevant to the field of citizen science, because many citizen science projects operate in informal environments such as private residences, parks, science and nature centers, museums, community centers, after- school programs, or online. In addition, many citizen science projects are funded through ISE initiatives because the projects are expected to foster lifelong science learn- ing (Crain et al. 2014). Therefore, tools developed to meas- ure learning outcomes resulting from ISE can serve as logical starting points for evaluating outcomes of citizen science participation. Phillips, T, et al. 2018. A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science. Citizen Science: Theory and Practice, 3(2): 3, pp. 1–19, DOI: https://doi.org/10.5334/cstp.126 mailto:tina.phillips@cornell.edu https://doi.org/10.5334/cstp.126 Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 2 of 19 The objectives of the research presented in this paper were to determine and describe the types of learning outcomes that are intended by citizen science project developers, to examine the alignment of these outcomes with informal science learning frameworks and guide- lines, and to develop and present a new framework for articulating citizen science learning outcomes. We believe that the framework will help citizen science practitioners to design projects that achieve measurable learning. We also hope that the framework will facilitate cross-programmatic research to help the citizen science field show how its projects are impacting science and society. Our research further sought to determine the extent to which citizen science learning outcomes have been evaluated across the field, as a first step toward our overall goal of deepening evaluation capacity for the citi- zen science community. Citizen Science and Informal Science Learning The educational underpinnings of citizen science— particularly when involving adults—draw heavily from Informal Science Education (ISE), what Falk and Dierking (2003) refer to as “free-choice learning”—lifelong, self-directed learning that occurs outside K-16 classrooms. Two influential documents from the ISE field provided a starting point for our study. The Framework for Evaluating Impacts of Informal Science Education Projects (Friedman et al. 2008), supported by the National Science Founda- tion (NSF), was the first publication produced by the ISE field that described a “standard” set of learning outcomes (referred to as impact categories) that could be used to systematically measure project-level outcomes. (We will refer to this publication as the “ISE Framework” for the remainder of this paper.) A major goal of the framework was to facilitate cross-project and cross-technique comparisons of the impacts of ISE projects on public audiences. The five impact categories are: • Knowledge, awareness, or understanding of Science, Technology, Engineering, and Math (STEM) concepts, processes, or careers • Engagement or interest in STEM concepts or careers • Attitude toward STEM concepts, processes, or careers • Skills based on STEM concepts or processes • Behavior related to STEM concepts, processes, or careers A second document, Learning Science in Informal Environments: People, Places, and Pursuits (National Research Council 2009), focuses on characterizing the cognitive, affective, social, and developmental aspects of science learners. Termed the “LSIE strands,” these aspects of science participation include: • Interest and motivation to learn about the natural world • Application and understanding of science concepts • Acquisition of skills related to the practice of science • Reflecting on science as a way of knowing, participat- ing in, and communicating about science • Identifying oneself as someone capable of knowing, using, and contributing to science The authors of the LSIE strands noted that while the con- cepts originated in research, at the time of writing they had not yet been applied or analyzed in any systematic venue. The significant overlap between the LSIE strands and the ISE Framework’s impact categories is shown in Table 1. A third ISE document also contributed to framing this study. In 2009, an inquiry group sponsored by the Center for Advancement of Informal Science Education (CAISE) produced “Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education” (Bonney et al. 2009a), which was cre- ated as a “first step toward developing an organized methodology for comparing outcomes across a variety of Public Participation in Scientific Research (PPSR) projects” (p.20). This paper included a rubric of potential citizen science learning outcomes, based on the ISE Framework, and examined ten NSF-funded citizen science projects to assess whether they reported outcomes similar to those described in the ISE Framework. Figure 1: Participants in citizen science engage in a large number of activities such as designing studies, collecting and analyzing data, and disseminating project results. What do project designers hope that participants will learn from their participation? How are desired learning outcomes designed? How are they measured? Credit: No copyright. Pacific Southwest Region USFWS/ Flickr/Public Domain. https://www.flickr.com/photos/usfws_pacificsw/36220761671/ https://creativecommons.org/publicdomain/mark/1.0/ Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 3 of 19 One result of the CAISE report was a realization that citizen science practitioners were measuring project outcomes in varied ways, making it difficult for cross- programmatic research to study the collective impact of the field. Another result was the development of a project typology based on the level of participant involvement in the scientific process (Bonney et al. 2009a). This typology described “Contributory” projects that are researcher- driven, where participants primarily focus on data collection; “Collaborative” projects that are typically led by researchers, but may include input from participants in phases of the scientific process such as designing methods, analyzing data, and disseminating results; and “Co-created” projects that involve participants in all aspects of the scientific process, from defining a question to interpreting data to disseminating results (Figure 1). This typology allowed projects designed for different rea- sons and in different ways to be grouped to help research- ers understand common outcomes. The three documents described above served as foun- dations for articulating learning outcomes from citizen science participation, however, they lacked systematic empirical support. This study provides such support by ground truthing and applying the concepts within the ISE Framework, the LSIE strands, and the Bonney et al. (2009a) rubric to the field of citizen science. Methods and Results Our research used two sources of data—a structured review of citizen science project websites and an online survey of citizen science practitioners—to address the following three questions: 1) What are the learning outcomes that are intended or desired by citizen science practitioners, and to what extent do these outcomes align with those described by the field of informal science education? (Data Source: Website Review) 2) What is the status of evaluation of citizen science learning outcomes across the field? (Data Source: Online practitioner survey) 3) How are citizen science learning outcomes measured by different projects? (Data Source: Online practitioner survey) We also conducted a literature review to uncover definitions, descriptions, and elucidations of the learning outcomes that we identified through our research. We used the results of this review, along with our new under- standing of the outcomes desired and measured by citizen science practitioners, to develop a framework of common learning outcomes for the citizen science field. Intended Learning Outcomes To describe and understand the learning outcomes that are intended or desired by citizen science practitioners as they develop projects, we first identified individual projects by conducting a semi-structured search of the following citizen science portals: Citizen Science Central (citizenscience.org); InformalScience (informalscience. org); SciStarter (scistarter.com); Citizen Science Alliance (citizensciencealliance.org); and National Directory of Volunteer Monitoring Programs (yosemite.epa.gov/ water/volmon.nsf/). The last portal included 800 pro- jects, from which we sampled every fifth one. If a project Table 1: Comparison of NSF Framework and LSIE strands. NSF Framework Category LSIE Strands Knowledge, Awareness, Understanding: Measurable demonstration of assessment of, change in, or exercise of awareness, knowledge, understanding of a particular scientific topic, concept, phenomena, theory, or careers central to the project. Strand (2), Understanding: Come to generate, understand, remember, and use concepts, explanations, arguments, models, and facts related to science. Engagement, interest or motivation in science: Measurable demonstration of assessment of, change in, or exercise of engagement/interest in a particular scientific topic, concept, phenomena, theory, or careers central to the project. Strand (1), Interest and motivation: Experience excitement, interest and motivation to learn about phenomena in the natural and physical world. Skills related to science inquiry: Measurable demonstration of the development and/or reinforcement of skills, either entirely new ones or the reinforcement, even practice, of developing skills. Strand (3), Science Exploration: Manipulate, test, explore, predict, question, and make sense of the natural and physical world; and Strand (5): Participate in scientific activities and learning practices with others, using scientific language and tools Attitudes toward science: Measurable demonstration of assessment of, change in, or exercise of attitude toward a particular scientific topic, concept, phenomena, theory, or careers central to the project or one’s capabilities relative to these areas. Attitudes refer to changes in relatively stable, more intractable constructs such as empathy for animals and their habitats, appreciation for the role of scientists in society or attitudes toward stem cell research. Related to Strand (6), Identity: Think about themselves as science learners, and develop an identity as someone who knows about, uses, and sometimes contributes to science. Also, related to Strand (4), Reflection: Reflect on science as a way of knowing; on processes, concepts, and institutions of science; and on their own process of learning about phenomena. Behavior: Measurable demonstration of assessment of, change in, or exercise of behavior related to a STEM topic. Behavioral impacts are particularly relevant to projects that are environmental in nature since action is a desired outcome. Related to Strand (5), Skills: Participate in scientific activities and learning practices with others, using scientific language and tools. http://www.citizenscience.org/ https://informalscience.org https://informalscience.org https://scistarter.com/ https://www.citizensciencealliance.org/ https://yosemite.epa.gov/water/volmon.nsf/ https://yosemite.epa.gov/water/volmon.nsf/ Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 4 of 19 was listed on multiple portals, we included it only once. In total, 327 citizen science projects met our criteria for study inclusion: Being open to participation in the U.S. or Canada, having an online presence, and being opera- tional at the time of the search (2011). The complete list of databases and search terms used to locate citizen science projects is available in the supplemental material for this paper (Appendix A). From each of the 327 project websites, we gathered the following information: Project name, URL, contact information, general goal statements, learning objectives or desired outcomes (if any), and potential indicators of learning (if any). Nine percent of project websites did not describe intended learning outcomes (e.g., some projects stated their goals to be purely scientific in nature), but the remaining 92% of projects described at least one. We coded each goal statement and learning objective into one of the major categories outlined in the ISE Framework (knowledge, engagement, skills, attitude, behavior, other) and into sub-codes outlined in the assessment rubric by Bonney et al. (2009a). Several projects described multiple learning outcomes. In these cases, each distinct outcome was coded sepa- rately. For example, the Great Lakes Worm Watch states that its goal is “increasing scientific literacy and public understanding of the role of exotic species in ecosys- tems change.” Objectives are to “provide the tools and resources for citizens to actively contribute to the devel- opment of a database documenting the distributions of exotic earthworms and their impacts across the region as well as training and resources for educators to help build understanding of the methods and results of scientific research about exotic earthworms and forest ecosystems ecology.” The text from the goal statement and learning objectives (left) were coded into the outcomes categories on the right: • Increasing scientific literacy and public understanding \\uf0e0 content knowledge • Citizens actively contribute to the development of a database \\uf0e0 data collection and monitoring, data submission • Help build understanding of the methods and results of scientific research \\uf0e0 Nature of Science knowledge Results from our coding of project goals and objectives are presented in Table 2. They reveal that the number of aspirational learning outcomes for projects ranged from zero to as many as seven, with about 40% of projects includ- ing at least two. The majority of projects (59%) focus on influencing skills related to data collection and monitoring. Intended outcomes for these projects are often stated as “Volunteers gain data collection and reporting skills.” The second most frequently stated intended learning outcome (28% of projects) was understanding of content knowledge (e.g., “volunteers learn about macroinvertebrates and stream health”). The third most-common intended out- come, increased environmental stewardship—which typi- cally includes some type of behavior change (e.g., “engage watershed residents in protecting water quality”)—was specified by about 26% of projects. Other intended learning outcomes were mentioned much less frequently, Table 2: Count of specified learning outcomes as coded from 327 citizen-science project websites. Percentages represent the proportion of projects that described the stated outcome. Several projects stated more than one outcome. Stated Outcomes on project websites Count of projects stating outcome (N = 327) Percentage of projects stating outcome Data Collection and Monitoring 193 59% Content Knowledge 90 28% Environmental Stewardship 86 26% No Education Goal Specified 29 9% Attitude/Awareness 25 8% Nature of Science 20 6% Data Analysis 14 4% Interest in the Environment 13 4% Civic Action 12 4% Submitting Data 12 4% Interest in Science 10 3% Community Health 9 3% Communication Skills 7 2% Using Technology 6 2% Science Careers 4 1% Designing Studies 2 0.5% Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 5 of 19 including increases in knowledge of the nature of science, data analysis skills, interest in the environment, civic action, data submission, communication skills, use of technology, science careers, study design, and also shifts in attitude/awareness. Considering all projects for which intended learning outcomes were stated, each of the ISE Framework impact categories was represented, suggesting a strong alignment between learning outcomes desired for citizen science participants and those for participants in the informal science learning community more generally. Status of Citizen Science Project Evaluation To uncover the status of evaluation of citizen science learn- ing outcomes across the field, we conducted an online survey of citizen science practitioners in March 2011. Delivered via Survey Monkey™, the survey contained 25 questions, including 20 closed-ended questions with predetermined options including “other.” The remaining five questions were open-ended, providing text boxes for answers. Only one question, which asked respondents to classify their project according to the three-model typol- ogy of citizen science developed by Bonney et al. (2009a), required a response. Additional questions focused on the duration of the project, the approximate number of partic- ipants, and the type of training that participants received. Respondents also were asked if any type of evaluation had ever been conducted for their project; details about evaluations that were conducted; what learning outcomes described in the ISE Framework had been measured; and what other types of outcomes had been measured. The complete set of survey questions is available in the sup- plemental material (Appendix B). Following approval by the Cornell University Institutional Review Board (#1102002014), we sent an email invitation to potential respondents describing the goal of the survey and explaining that participation was voluntary and confidential. Two reminder emails were sent approximately two and four weeks following the initial invitation. An informed consent statement was included at the start of the survey. Potential respondents were recruited via the citizenscience.org listserv (citsci-dis- cussion-l), which anyone could join, and which at the time of recruitment had approximately 1,100 members. Not all members of the listserv were project leaders, and multiple list members likely represented a single project, making it difficult to know the actual number of projects repre- sented by listserv members. After the survey was closed, we made sure that all responding projects were included in the previously described website review, to obtain as much overlap between the two datasets as possible. The survey was completed by 199 respondents repre- senting 157 unique projects (some projects had multiple entries, in which case only the first entry was included; other respondents failed to include information about their project name, which was optional). All but ten of the 157 unique projects also were represented in the project web- site data. The remaining ten projects that responded to the online survey but were not in the website review were either no longer operational, not in the US or Canada, or did not have a web presence. The majority of projects (72 or 37%) had been operating from 1–5 years, and nearly half (49%) had fewer than 100 participants. Because most questions were optional, response rates varied for different survey items. Results revealed that of the 199 respondents, 114 or 57% had undertaken some type of project evaluation. More than half of the evaluations were administered by inter- nal project staff to measure project outcomes or impacts, mostly using data collected through surveys. About one third of respondents reported conducting post-only or pre-and posttest evaluation designs. Reasons for conduct- ing project evaluations included: Gauging participant learning; identifying project strengths and weaknesses; obtaining additional funding or support; promoting a project more broadly; and providing recommendations for project improvement. In addition to asking about pro- ject learning outcomes (described in the next section), the survey also asked what other aspects of the project had been evaluated. Two thirds of participants reported measuring satisfaction or enjoyment with the project, followed by motivation to participate (53%) and evalua- tion of project outputs such as numbers of participants, web hits, journal articles, and amount of data collected (44%). Other measured outcomes included scientific/con- servation (39%); effectiveness of workshops and trainings (38%); data quality (37%); community capacity building (23%); and social policy change (3%). Another open-ended question asked respondents “Please do your best to provide the name or description of any instrument (e.g., Views on Science and Technology Survey) used to collect evaluation data, even if you developed the instrument.” Of the 72 respondents to this question, only three had used a pre-existing, validated instrument. The majority of respondents had developed their own instruments in-house or had an external evalu- ator develop original instruments. A handful of respond- ents replied with “Survey Monkey” or some other data collection platform as opposed to describing an evalua- tion instrument. Some mentioned tools such as GPS units or calipers as instruments used by the project, while others stated that they did not understand the question. When asked about their overall satisfaction with their evalua- tions, more than half of respondents expressed agreement or strong agreement that evaluations were of high quality, that evaluation findings were informative to the project developers, that recommendations from the evaluation were implemented, that the project had improved as a result of evaluation, that they learned a lot about evalu- ation, and that they felt confident they could personally conduct an evaluation in the future. Survey respondents also were asked about aspects of the evaluation process for which they would like assistance. The highest priority was help with developing goals, objectives, and indicators, followed by creating or finding appropriate survey instruments, help with analyzing or interpreting data, and help with data collection. Participants also were asked what specific resources would be most helpful for conducting evaluations. The most common replies were a database of potential surveys and data collection instruments; sample evaluation reports from citizen science; examples of evaluation designs; and an entry-level guide for conducting evaluations. https://citizenscience.org Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 6 of 19 Finally, respondents were presented with a list of eight different online organizations that support or provide resources for evaluation and were asked how often they access them. Surprisingly, the majority of respondents had never heard of any of the resources or organizations. The only exception was citizenscience.org, which was used by 46% of respondents, but rarely (as opposed to frequently, sometimes, or never). These results show a range of evalu- ation efforts and a positive attitude toward evaluation and findings among citizen science practitioners, but also a need for more knowledge of and accessibility to evalua- tion tools and resources. Measurement of Learning Outcomes Respondents who reported having conducted evaluations (114 or 57%) were asked “For the most recent evalua- tion of your project, which broad categories of learning outcomes, if any, were evaluated?” Responses to this question were based on the ISE Framework broad impact categories. Aggregated results across all projects revealed that interest or engagement in science was the most com- monly measured outcome (46%), followed by knowledge of science content (43%). Behavior change resulting from participation and attitudes toward science process, con- tent, careers, and the environment accounted for 36% and 33%, respectively, of measured learning outcomes. Science inquiry skills (e.g., asking questions, designing studies, data collection, data analysis, and using technol- ogy) were the least commonly measured outcomes across all projects (28%). In an open-ended question about other types of learning outcomes, about 10% of respondents also described measuring motivation and self-efficacy or confidence to participate in science and environmental activities. Considering differences in categories of learning out- comes measured within project types, contributory projects (for which there were 69 respondents that had conducted evaluations) reported measuring interest in science most frequently (43%) and skills of science inquiry least frequently (18%). Two-thirds of all collabo- rative projects (N = 21) measured content knowledge, followed by interest (57%), behavior change (52%), and attitudes and skills (both 43%). Only nine survey respond- ents represented co-created projects that had conducted evaluations, and of these, skills of science inquiry were measured most often. Responses combined across pro- jects and separated among project types are summarized in Figure 2. Earlier in this paper we showed that a majority of citizen science project websites described intended learn- ing outcomes very similar to those in the ISE framework, although not always using the same language. Results from the online practitioner survey added to our “ground truthing” of the ISE Framework, as respondents described attempts to measure these same outcomes, albeit to vary- ing degrees. Open-ended responses highlighted the need to emphasize efficacy as an important learning outcome in citizen science. Survey respondents also made it clear that additional resources were needed to help formulate and measure learning outcomes. A Framework for Articulating and Measuring Common Learning Outcomes for Citizen Science In addition to synthesizing and comparing empirical results from our website review and practitioner survey to describe intended and measured learning outcomes, we used key word searches to conduct a review of more than 40 peer-reviewed articles focused on defining and measuring these learning outcomes. Our data and review facilitated a re-conceptualization or contextualization of several of the impact categories presented in the ISE Framework to make them relevant to citizen science, in Figure 2: Measured learning outcomes from online survey of citizen science practitioners who reported having conducted some sort of evaluation (n = 99). https://citizenscience.org Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 7 of 19 particular environmentally based projects. For example, some outcomes uncovered in our research, such as “skills of science inquiry,” map well to the categories and their definitions in the ISE framework. Other outcomes, such as “attitude,” required clarification. Our new framework is thus based on both empirical data and contributions from the literature and includes the following learning outcomes: Interest in Science and the Environment; Self- efficacy for Science and the Environment; Motivation for Science and the Environment; Knowledge of the Nature of Science; Skills of Science Inquiry; and Behavior and Stewardship (Figure 3). This framework should help citizen science practition- ers consider some of the more commonly desired and achievable learning outcomes when designing projects. However, we emphasize that no single project should try to achieve and/or measure all, or even most, of these out- comes, as doing so can set up unreasonable expectations for both the project and its evaluation. We also note that the framework is not exhaustive. Indeed, as citizen science continues to expand, new research will inevitably reveal other learning outcomes that are important to articulate and measure. Below we describe each outcome within the framework, highlighting how each has been explained in the broad educational field and also providing examples of how each has been used in published studies of citizen science. These outcomes are not hierarchical but, beginning with interest in science and the environment, build from and reinforce each other. Interest in Science and the Environment We define interest as the degree to which an individual assigns personal relevance to a science or environmental topic or endeavor. Within ISE, Hidi and Renninger (2006) treat interest as a multi-faceted construct encompassing cognitive (thinking), affective (feeling), and behavioral (doing) domains across four phases of adoption: triggered situational interest typically stimulated by a particu- lar event and requiring support by others; maintained situational interest, which is sustained through personally meaningful activities and experiences; emerging individual interest characterized by positive feelings and self-directed pursuit of re-engaging with certain activities; and well-developed individual interest leading to enduring participation and application of knowledge. Our defini- tion of interest is compatible with Hidi and Renninger’s (2006) later phases of interest development, which are characterized by positive feelings and an increasing investment in learning more about a particular topic. Interest in science is considered a key driver to pursuing science careers in youth (Maltese and Tai 2010; Tai et al. 2006) and sustained lifelong learning and engagement in adults (Falk et al. 2007; Hidi and Renninger 2006). Over time, this type of interest can lead to sustained engage- ment and motivation and can support identity develop- ment as a science learner ( Fenichel and Schweingruber 2010; National Research Council 2009). Further, interest is noted as an important precursor to deeper engage- ment in democratic decision-making processes regarding science and technology (Mejlgaard and Stares 2010). Figure 3: Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science. Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 8 of 19 Although interest is considered to be an attitudinal struc- ture (see Bauer et al. 2000; Fenichel and Schweingruber 2010; Sturgis and Allum 2004), equating interest with atti- tudes should be avoided because attitude is a very broad construct, encompassing related but distinct sub-constructs such as efficacy, interest, curiosity, appreciation, enjoyment, beliefs, values, perseverance, motivation, engagement, and identity (Osborne et al. 2003). Interest also has been used synonymously with engagement (Friedman et al. 2008), but as McCallie et al. (2009) point out, engagement has yet to be well defined and has multiple meanings within the literature, particularly in ISE. Citizen science projects, especially those for which repeated visits or experiences are the norm, can lend themselves to deeper and sustained interest in science and the environment, yet few studies have looked at inter- est as an outcome, and those that have find mixed results. Price and Lee (2013) reported increased interest in science among Citizen Sky observers, especially among partici- pants who engaged in online social activities. Crall et al. (2012) examined general interest in science as a reason for participation in citizen science and suggested that interest was not a driving force for joining a project. Interest in specific nature-based topics, i.e., butterflies, was seen as a driver for engagement and also as a motivator for adding increasingly more complex data protocols to the French Garden Butterflies Watch project (Cosquer et al. 2012). Other research has shown that interest in use of natural resources can be a very strong determinant for future and sustained involvement in the decision-making process about management of natural resources (Danielsen et al. 2009). From these studies, it appears that examining interest in science more broadly may be less effective than measuring specific science topics. However, an audience’s pre-existing interests in specific topics may not change significantly through participation. Self-efficacy for Science and the Environment Another important outcome for studying learning is self-efficacy, i.e., a person’s beliefs about his/her capabilities to learn specific content and to perform particular behav- iors (Bandura 1997). Research has found that self-efficacy affects an individual’s choice, effort, and persistence in activities (Bandura 1982, 2000; Schunk 1991). Individu- als who feel efficacious put more effort into their activi- ties and persist at them longer than those who doubt their abilities. Self-efficacy is sometimes referred to as “perceived competence” (in Self Determination Theory) and “perceived behavioral control” (in Ajzen’s Theory of Planned Behavior, Ajzen 1991). Berkowitz et al. (2005) treat self-efficacy as an essential component in environmental citizenship (along with motivation and awareness), which is dependent on an individual’s belief that they have sufficient skills, knowl- edge, and opportunity to bring about positive change in their personal lives or community. In the context of citizen science, self-efficacy is the extent to which a learner has confidence in his or her ability to participate in a science or environmental activity. In a study involving classrooms, middle school students participating in a horseshoe crab citizen science project showed greater gains in self-efficacy than a control group (Hiller 2012). In an online astronomy project, however, researchers found a significant decrease in efficacy toward science, possibly owing to a heightened awareness of how much participants did not know about the topic (Price and Lee 2013). Crall et al. (2011) determined that self- efficacy is not only important in carrying out the principal activities of a project but also in the potential for individu- als to carry out future activities related to environmental stewardship. Working in a participatory action project with Salal harvesters, Ballard and Belsky (2010) found that the process of co-developing and implementing different experiments increased workers’ self efficacy regarding their skills in scientific research. Although efficacy was not called out directly in the ISE Framework, it can be con- sidered part of the LSIE Strand 6, “identity as a learner” (National Research Council 2009). Self-efficacy also was mentioned by project leaders in our online survey and thus appears to be an important potential outcome from citizen science participation. Motivation for Science and the Environment Motivation is a multi-faceted and complex attitudinal con- struct that describes some form of goal setting to achieve a behavior or end result. The LSIE strands (National Research Council 2009) include motivation to sustain science learning over an individual’s lifetime as an impor- tant aspect of learning in informal environments. The literature on volunteerism frames motivation as an impor- tant factor in effective recruitment, accurate placement, and volunteer satisfaction and retention (Clary and Snyder 1999, Esmond et al. 2004). Of the dozens of theories on motivation, two perspectives seem especially relevant to volunteerism and citizen science. First, the Volunteer Functions Inventory (VFI), developed by Clary et al. (1998), examines how behaviors help individuals achieve personal and social goals. Clary et al.’s (1998) categories of motivation include values (importance of helping oth- ers); understanding (activities that fulfill a desire to learn); social (influence by significant others); career (exploring job opportunities or work advancement); esteem (improv- ing personal self-esteem); and protective (escaping from negative feelings). Wright et al. (2015) studied the motiva- tions of birders in South Africa using a modified version of the VFI and found five categories of motivation to be most important: Recreation/nature; values; personal growth; social interactions; and project organization. The second perspective comes from Self-Determination Theory (SDT), which treats motivation as an explana- tory variable for meeting basic psychological needs (i.e., competency, relatedness, and autonomy) and describes different types of motivations as falling on a contin- uum from intrinsic to extrinsic (Ryan and Deci 2000a, 2000b). According to SDT, individuals are likely to con- tinue pursuing a goal to the extent that they perceive intrinsic value in the pursuit of that goal (i.e., the extent to which they experience satisfaction in performing associ- ated behaviors themselves versus performing behaviors to comply with extrinsic goals such as conforming to social pressures, fear, or receiving rewards). Although SDT can Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 9 of 19 help practitioners better understand the psychological needs behind participation, few published studies have used SDT in the context of citizen science. One excep- tion is a paper by Nov et al. (2014), which used SDT with social movement participation models in an examination of three digital citizen science projects. These researchers found that intrinsic motivation was one of four drivers that influenced quantity of participation, but that it did not affect quality of participation. In the context of citizen science, motivation can serve as both an input and outcome, i.e., to understand the basis of motivation for ISE/citizen science experiences (input) and to sustain motivation to continue participating over long time periods (outcome). However, most studies have examined reasons for participation such as the desire to contribute (see Bell et al. 2008; Hobbs and White 2012; McCaffrey 2005; Raddick et al. 2010; Reed et al. 2013), rather than motivations, which describe the psychological underpinnings of behavior (e.g., “because it makes me feel good”). In an examination of motivation in online projects, Rotman et al. (2012) described a complex and changing framework for motivation that was influenced by partici- pant interest, recognition, and attribution. Although sev- eral studies have purported to examine motivation, it has not been defined nor studied uniformly throughout the field of citizen science. Nevertheless, the major consen- sus appears to be that motivation for citizen science, like other volunteer activities, is dynamic and complex. Content, Process, and Nature of Science Knowledge Included within the ISE Framework’s impact category of “awareness, knowledge, and understanding” are several subcategories such as knowledge and understanding of science content; knowledge and understanding of science processes; and knowledge of the Nature of Science. Knowl- edge of science content refers to understanding of subject matter, i.e., facts or concepts. Knowledge of the process of science refers to understanding the methodologies that scientists use to conduct research (for example, the hypo- thetico-deductive model or “scientific method”). Knowl- edge of the Nature of Science (NOS) refers to understanding the epistemological underpinnings of scientific knowledge and how it is generated, sometimes presented from a post- positivist perspective (Lederman 1992). NOS addresses tenets of science such as tentativeness; empiricism; sub- jectivity; creativity; social/cultural influence; observations and inferences; and theories and laws (see Lederman 1992, 1999, Lederman et al. 2001, 2002). For improving scientific literacy, understanding of NOS and the process of science are generally considered more important than understand- ing basic content or subject matter (American Association for the Advancement of Sciences 1993; National Research Council 1996; NGSS 2013), and knowledge of the pro- cess of science is a regular component of well-established assessments of science knowledge (National Science Board 2014). Despite this recognition, most attempts to measure science literacy within the ISE field fall back on content knowledge, i.e., rote memorization of facts, rather than knowledge of the nature or process of science (Bauer et al. 2000; Shamos 1995). Indeed, citizen science evaluations have typically empha- sized measuring gains in topical content knowledge as opposed to science process knowledge, with mixed results (Ballard and Huntsinger 2006; Bonney 2004; Braschler et al. 2010; Brewer 2002; Devictor et al. 2010; Evans et al. 2005; Fernandez-Gimenez et al. 2008; Jordan et al. 2011; Kountoupes and Oberhauser 2008; Krasny and Bonney 2005; Phillips et al. 2006; Sickler et al. 2014; Trumbull et al. 2000; Trumbull et al. 2005). Overdevest et al. (2004) did not find a significant increase in project participant knowledge about streams and water quality, probably because new volunteers were already highly knowledgea- ble about the subject matter. Price and Lee (2013) actually found a decrease in science content knowledge among project participants, likely owing to exaggerated notions of participants’ self-perceived content knowledge before starting the project and the realization of how much they did not know after participating in the project. However, a few studies have used measures of the process of science to assess impacts of citizen science pro- ject participation. Jordan et al. (2011) and Brossard et al. (2005) used adaptations of the science and engineering indicators and showed no gains in understanding of the process of science as a result of citizen science participa- tion. In contrast, Ballard et al. (2008) used interview data to show evidence that the Salal harvesting project “… increased local people’s understanding of the scientific process and of the ecosystem on which they were a part (p. 14)”. And significant increases in understanding of the process of science before and after participation in a stream water quality-monitoring project were reported by Cronin and Messemer (2013). However, this study had a very small sample size, which may limit generalizability of the results. Likewise, few citizen science projects have attempted to study understanding of the NOS. Jordan et al. (2011) found no evidence for change in knowledge of the NOS using pre-post scenario-based questions in an invasive species project. Price and Lee (2013) found little evidence that project participation influenced epistemological beliefs about NOS, owing to the fact that “epistemological beliefs are personal beliefs and thus harder to change after participating in only one citizen science project” (p. 793). These findings suggest that while citizen science can effec- tively demonstrate gains in content knowledge, it has a long way to go before it can positively establish increases in understanding of science process and the NOS. Skills of Science Inquiry Skills of science inquiry are observable practices that can be transferred to daily life, such as asking and answering questions; collecting data; developing and using models; planning and carrying out investigations; reasoning about, analyzing, and interpreting data; constructing explanations; communicating information; and using evidence in argumentation (National Academies of Science, Engineering, and Medicine 2016; NGSS Lead States 2013). The hands-on nature of many environmentally based citizen science projects makes them particu- larly well suited to influence the development and/or Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 10 of 19 reinforcement of certain science-inquiry skills including asking questions; designing studies; collecting, analyzing, and interpreting data; and discussing and disseminating results (Bonney et al. 2009a; Jordan et al. 2012; Phillips et al. 2012; Trautmann et al. 2012). Top priorities for many practitioners are helping participants learn to follow pro- tocols and exercise accurate data collection skills, because these practices directly influence data quality. The field- wide emphasis on data quality likely comes from the large percentage of contributory, scientist-driven projects, for which a key goal is gathering data of sufficient quality to add to the existing knowledge base through publication in peer-reviewed journals. Consequently, many citizen science projects most effectively influence skills that are related to data and sample/specimen collection, iden- tification of organisms, instrument use, and sampling techniques. Many projects also engage participants in the use of various technological tools such as GPS units, digital thermometers, water conductivity instruments, rain gauges, nets, and smartphones, to name just a few (Figure 4). A few researchers have begun to study skill acquisition in citizen science. Becker et al. (2013) showed an increase in the ability to estimate noise levels with increasing participa- tion in WideNoise, a soundscape project operated through mobile devices. Increases in youths’ self-reported science inquiry skills, such as their perceived ability to identify pond organisms and to develop testable hypotheses before and after participation in Driven to Discover, also have been reported (Meyer et al. 2014). Sullivan et al. (2009) describe the use of communication prompts and strategies to “steer birders toward providing more useful data” and essentially change the birding habits of eBird participants to increase data quality. Using the theory of legitimate peripheral par- ticipation, Mugar et al. (2014) used practice proxies, a form of virtual and trace ethnography, to increase accuracy of data annotation among new members. Additionally, some projects have successfully conducted small-scale studies that compare volunteer- collected data to those collected by experts, thereby creating a baseline metric for assessing their participants’ skills (see Crall et al. 2011; Jordan et al. 2011; Schmeller et al. 2009). Another hallmark of citizen science is the collection of large, publicly available data sets and rich, interac- tive data visualizations. Many projects that provide data visualizations may seek to enhance skills related to data interpretation, i.e., the ability to effectively comprehend information and meaning, often presented in graphical form (Devictor et al. 2010). In one of the few studies exam- ining data interpretation in citizen science, Thompson and Bonney (2007) showed that even the majority of “active users” of eBird did not properly use the extensive array of data- analysis tools. Numerous studies in educational research have shown that assessing the type of reason- ing skills needed for data interpretation requires asking a series of reflective questions to determine one’s justifica- tion underlying the reasoning (e.g., Ayala et al. 2002; Roth and Roychoudhury 1993). Other inquiry skills such as study design, communica- tion, critical thinking, decision making skills, and critically evaluating results are less studied within the citizen science literature. Crall et al. (2012) used open-ended questions to determine whether engaging in an invasive species project improved the abilities of participants to explain a scientific study, write a valid research question, and provide a valid sampling design. These researchers noted positive gains in all but the ability to explain a scientific study. Char et al. (2014) found an increase from pre-post training in the ability of COASST volunteers to correctly weigh evidence to determine whether it con- tained sufficient information for accurately identifying species. These few studies show the potential for studying citizen science participants to evaluate the development of complex science inquiry skills, but such studies are in their infancy. Behavior and Stewardship Behavior change and development of environmental stewardship are among the most sought-after outcomes in science and environmental education programs, both in and out of schools (Bodzin 2008; Heimlich et al. 2008; Kollmuss and Agyeman 2002; Stern 2000; Stern et al. 2008; Vining et al. 2002). Theories examining various determinants of environmental behavior include those espousing the links between knowledge, attitude, and behavior (Hungerford and Volk 1990; Kollmuss and Agyeman 2002; Osbaldiston and Schott 2012; Schultz 2011); attitudes and values (Ajzen 1985; Fishbein and Ajzen 1975); behavior modification and intervention (De Young 1993); and nature exposure (Kaplan 1995; Kellert and Wilson 1993; Ulrich 1993; Wilson 1984). We define behavior and stewardship as measurable actions resulting from engagement in citizen science, but external to the protocol activities and the specific project- based skills of the citizen science project. For example, collecting water quality data may be a new behavior for a project participant, but if the data collection is part of the project protocol it should be measured as a new skill rather than a new behavior. However, somebody decreas- ing their water usage as a result of participating in a water quality monitoring project would be an example of behav- ior change. Our literature review identified five categories of behavior and stewardship that are of interest to the citizen science field and for which we provide definitions below: Global stewardship behaviors; place-based behav- iors; new participation; community or civic action; and transformative lifestyle changes. Global stewardship refers to deliberate changes in behavior that minimize someone’s individual ecological footprint and which collectively can have global influence (e.g., installing low-flow shower heads, recycling, purchas- ing energy-efficient appliances). Place-based behaviors refer to observable actions to directly maintain, restore, improve, or educate about the health of an ecosystem beyond the activities of a citizen science project (e.g., removing invasive species; cleaning up trash; eliminating pesticide use; purchasing locally grown food; engaging in outreach to youth groups). New participation is defined as engagement in science or environmental activities, organizations, or projects spurred on by participation in Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 11 of 19 a citizen science project. Community or civic action refers to participation in civic, governmental, or cultural affairs to solve problems at the local, regional, or national level. Actions could include donating to environmental organi- zations, signing petitions, speaking out against harmful environmental practices, or recruiting others to partici- pate in environmental causes. Finally, transformative life- style changes are efforts that require a strong up-front cost or long-term commitment to maintain, such as investing in a hybrid vehicle, becoming a vegetarian, or pledging to use mass transit whenever possible. Citizen science projects, especially those dealing with environmental topics, are typically hands-on, occur in local environments, and require repeated monitor- ing and data gathering, making them natural conduits for affecting behavior change (Wells and Lekies 2012). However, research has been limited and results have been mixed regarding the actual influence of citizen science on behavior change. For example, in a study examining two different projects, one on pollinators and one on coyotes, Toomey and Domroese (2013) show that partici- pants engage in new activities and change their gardening practices, but otherwise did not take part in advocacy or change their environmental stewardship practices. Crall et al. (2012) found significant differences between current and planned behavior as a result of participating in an invasive species project using self-reported measures, but the actual behavior change was not well described. Using a case-study approach, Oberhauser and Prysby (2008) claim that participants of the Monarch Larva Monitoring Project “work to preserve habitat at many levels, from advocating a more environmentally friendly mowing regimen and insect-friendly pest control, to challenging parking lot, building, and road development projects that threaten monarch habitat (p. 104).” However, the source of these data or accompanying methodologies are not clearly described. Cornwell and Campbell (2012) also used a case study approach and were able to document advocacy and political action by volunteers which directly benefited sea turtle conservation. Evans et al. (2005) docu- mented locally, place-based stewardship in a bird breed- ing program, while other projects showed no change in place-based stewardship practices (Jordan et al. 2011). In a study of human health effects of industrial hog opera- tions, Wing et al. (2008) describe actions being taken by community groups to engage in decision-making that addresses local environmental injustices. Taken together, these examples provide some evidence that citizen science may influence behavior and stewardship, but more robust methodologies are needed to establish causation. Plenty of anecdotal data also highlight other examples of behav- ior change that have not been published or exist only in the gray literature. Discussion Results from research conducted through a systematic review of citizen science project websites and a sur- vey of practitioners who design and implement citizen science projects confirm the relevance and applicability of three ISE documents (Friedman et al. 2008; National Research Council 2009; Bonney et al. 2009a) in framing intended learning outcomes for citizen science partici- pants. Informed by this research along with a systematic literature review, we have modified and contextualized these documents to create a new framework that contains definitions and articulations of learning outcomes for the citizen science field. We believe that the framework provides a robust starting point for setting learning goals and objectives for citizen science projects and designing projects to meet those objectives. Our research has some limitations, however. First, both the co-created and collaborative project categories represented in the online practitioner survey have small sample sizes, so generalizing the types of learning out- comes intended by these project types is challenging. Also, it’s unclear whether the distribution across project types in the online survey reflects the actual distribu- tion of contributory, collaborative, and co-created pro- jects across the U.S. and Canada, or if a disproportionate number of contributory projects received and responded to the survey request. We made no additional effort to recruit additional collaborative or co-created project respondents, thus response bias may be an issue. Also, while we made an effort to ensure that projects which responded to the practitioner survey were included in our website review, project level data from the two sources were not examined together. Doing so may have shown convergence or divergence of intended versus measured outcomes, but was beyond the scope of this work and may have violated confidentiality conditions. Finally, this work is a descriptive study based largely on self-reports in the case of the practitioner survey and published desired outcomes in the case of the website review. More robust inferential studies that can examine Figure 4: Many citizen science project designers hope not only to collect important scientific information but also to help project participants gain skills such as scientific reasoning. Here, a team of volunteers with Public Lab, a non-profit environmental science community, launch a weather balloon. Data collected via the balloon will be used in 3-D mapping surveys, but figuring out how to measure just what participants are learning as they conduct this research is a challenge for the citizen science field. Credit: Alan Kotok/Flickr/CC BY-2.0. https://www.flickr.com/photos/runneralan/14372872424/ https://creativecommons.org/licenses/by/2.0/ Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 12 of 19 field-wide relationships and causal factors between pro- ject characteristics and observed learning outcomes would be a significant next step. Despite these limitations, our findings provide insights into the ways in which learning has been articulated, studied, and measured by citizen science projects. They also provide information about the status of citizen science project evaluation in general. For example, an overwhelming majority of survey respondents expressed positive attitudes toward the importance of evaluation and the evaluation process. However, they also expressed a need for additional support and resources to conduct evaluations. Nearly all respondents reported developing their own evaluation instruments, although most pro- jects measured very similar outcomes. And the fact that very few projects were aware of resources available for guidance in conducting evaluations and locating evalu- ation instruments suggests that work needs to be done to disseminate tools and resources to the citizen science professional community. The comparison of intended learning outcomes described on citizen science project websites and the outcomes actually measured by projects highlights some interesting disconnects. For example, fewer than 5% of project websites stated “increasing interest in science and/or the environment” as an intended outcome, yet interest in science was the most commonly measured out- come (46%) across all projects in the online survey. The frequent measurement of interest in science may result from the relative ease of obtaining instruments to measure this outcome or it may be a proxy for measuring interest in the specific topic addressed by the project (e.g., birds, butterflies, astronomy, weather). Further, despite these reported measurements, few studies have published data about changes in interest, perhaps because they have not actually tried to measure it or because the typical citizen science participant (Caucasian, older, highly educated) already demonstrates a high interest in science when joining a project, making it difficult to detect changes in interest over the course of project participation (Brossard et al. 2005; Thompson and Bonney 2007). However, ample opportunity exists for citizen science projects to increase interest in science and the environment by reaching individuals who are not already engaged, espe- cially underserved audiences for whom access to informal science programming may be limited (Bonney et al. 2016; Flagg 2016). Additionally, projects that reach youth audi- ences via K-12 settings can minimize self-selection bias and carry out quasi-experimental studies to determine whether interest in science is leveraged through citizen science participation (Bonney et al. 2016). As another example of a disconnect, self-efficacy was seldom stated as an intended outcome in the website review and did not emerge as a major category of desired outcomes via the online survey. However, approximately 10% of survey respondents mentioned the concepts of “agency,” “confidence,” or “efficacy” in open-ended com- ments. As stated earlier, self-perceptions of efficacy affect choices of activities that individuals pursue, how much effort they put toward them, and how long they persist in those pursuits (e.g., Bandura et al. 1977; Weinberg et al. 1979). Enhancing perceptions of efficacy may be the single most important outcome for many citizen science projects, thus we have included efficacy in our framework. Yet another disconnect relates to motivation. Few project websites mentioned motivation as an intended learning outcome, and our online survey showed that practition- ers measured motivation primarily to understand reasons for participation. Motivations change over time, however, and sustaining project participation requires an under- standing of changing roles for individuals within a project and motivations for continued participation. More work also is needed to understand how motivations connect to Self-Determination Theory and serve psychological needs within the context of citizen science. For example, the desire to contribute to a project may be associated with a psychological need for competence, and the desire to engage socially with others may serve the psychological need for relatedness. Studies that examine where motivations fall within the intrinsic-extrinsic motivation continuum are needed to understand how motivation might influence sustained participation over time. Our results also reiterate the inclination for practition- ers to expect and measure gains in science content knowl- edge, typically through context-specific instruments that measure mastery of project activities and program content rather than increased knowledge about the process of sci- ence or the Nature of Science. Although some projects have begun to demonstrate outcomes related to “think- ing scientifically” (Braschler et al. 2010; Kountoupes and Oberhauser 2008; Trumbull et al. 2000), a gap remains in our understanding of the potential for citizen science to influence deeper understanding of the process of science and the Nature of Science as well as the more complex facets of science inquiry (i.e., critical thinking, reflection, and reasoning). Future work should focus on the develop- ment of robust and contextually appropriate tools to bet- ter capture deep reflection and rich dialogue about NOS. In perhaps our most surprising finding, nearly 60% of project websites in our study listed data collection as an intended outcome, yet across all projects combined, our online survey showed that skills related to data collection were the least-measured outcome (28%). These findings may reflect the difficulty of measuring attributes such as the acquisition of skills and the relative ease of measur- ing other constructs such as knowledge, interest, and atti- tude. This disconnect also represents a potential tension that exists within the citizen science field, particularly among contributory projects: The need for high confi- dence in data quality versus the dearth of studies that have assessed data collection skills. While several studies demonstrate that volunteers are able to collect data of similar quality to experts, these tend to be isolated exam- ples (Crall et al. 2011; Danielsen et al. 2014). Although a multitude of ways to validate citizen-science data exist (see Kosmala et al. 2016), tools and techniques are needed that can assess changes in participant data collection skills over time. Additionally, the field needs to better understand whether citizen science participation can influence other important inquiry skills such as the ability to make Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 13 of 19 decisions regarding appropriate research methodologies, to use variables and control groups properly, and to evalu- ate evidence. And as attention is increased on the poten- tial for citizen science to democratize science, further work should examine the extent to which it can support devel- opment or reinforcement of critical thinking skills that inform decision making and help to create an informed citizenry. Also, in the new world of “Big Data,” citizen sci- ence is well poised not only to provide the public with large and robust data sets but also to develop support systems so that users can understand how to effectively use these dynamic resources. Such provisioning may facilitate new lines of research to better understand how participants engage with data sets and what meaning they hold for them. Finally, in our website review, environmental steward- ship was mentioned as an intended outcome by 25% of projects—second only to data collection—suggesting a strong desire for citizen science projects to influence individual behavior change. About one-third of survey respondents reported measuring behavior change, but based on several open-ended comments, some practi- tioners equated the act of participating in a project as a change in behavior, meaning that such change was indicated for all participants. Recall, however, that we define behavior change as change that goes beyond pro- ject activities. Further, tacit assumptions may exist about engagement in specific project activities leading to more global environmental behaviors (Kollmuss and Agyeman 2002; Vining et al. 2002) (e.g., the assumption that water- quality monitoring can lead to reducing carbon emissions, recycling, and conserving energy). Intended behavioral outcomes should be directly connected to project con- tent and activities, and the knowledge of how to per- form these targeted behaviors should be made explicit to participants (Phillips et al. 2012; Toomey and Domroese 2013). While citizen science can likely impact behavior change, the development of effective implementation strategies and measurement of those outcomes are still in their infancy. Conclusion Thousands of citizen science projects exist around the word, reaching potentially millions of people, particularly in the observation and monitoring of species and habitats (Theobald et al. 2015). Such projects have the potential not only to engage individuals in the process of science, but also to encourage them to take positive action on behalf of the environment (Cooper et al. 2007; McKinley et al. 2016). If such outcomes are to be achieved, project developers need to better understand how to design pro- jects so that activities and educational learning opportuni- ties support and align with feasible and realistic outcomes (Shirk et al. 2012). This study has resulted in a framework to support citi- zen science practitioners in articulating and measuring learning outcomes for participants in their projects. The framework also should help to build capacity for practi- tioners seeking to conduct evaluations of citizen science projects by helping them to develop their program theory, i.e., to identify underlying assumptions about how project activities affect expected outcomes (Bickman 2000; Chen 2005; Funnell 2000; Funnell and Rogers 2011). In this regard, most evaluators recommend starting with articu- lation of project outcomes, then working backward to determine not only what can be achieved and how, but also what can be reasonably measured (Center for the Advancement of Informal Science Education 2011). Toward that end, work proceeding in parallel to this research is developing generic, yet customizable, evalua- tion scales that are tested as valid and reliable in citizen sci- ence contexts and which align to the framework described above (see DEVISE scales: https://cornell.qualtrics.com/ jfe/form/SV_cGxLGl1AlyAD8FL). By adopting common learning outcomes and measures, the citizen science field can further evaluation capacity and begin to conduct cross-programmatic analyses of citizen science projects to provide funders, stakeholders, and the general public with evidence-based findings about the potential for citi- zen science to impact the lives of its volunteers. Such stud- ies also could provide critical information regarding why and how to achieve outcomes and under what conditions outcomes can be maximized. Future work should support continued development of consistent measures that can be used across studies, par- ticularly those that do not rely on self-reports ( Becker-Klein et al. 2016; Phillips et al. 2012; Wells and Lekies 2012). Continued professional development opportunities for citizen science practitioners to spearhead evaluations of projects will increase capacity for such endeavors, build a steady source of knowledge about impacts, and lead to improved project design, implementation, and sus- tainability for the field as a whole. Initiation of in-depth longitudinal studies that measure persistence of change over time would add understanding of the impacts of such experiences (Schneider and Cheslock 2003). To the extent possible, more effort should be placed on studies that include experimental designs, random assignment, and control groups. Such efforts will increase the field’s ability to provide evidence for causal connections between citi- zen science participation and learning outcomes. Additionally, continued research on learning outcomes should seek to incorporate social learning theories, which may be helpful in understanding how learning happens in citizen science and the mechanisms and processes that enable active learning. Social learning theories such as Cultural Historical Activity Theory (Vygotsky and Cole 1978); Activity Theory (Engeström 1999), Experiential Learning (Dewey 1938; Kolb 1984), Situated Learning Theory (Lave and Wenger 1991), and Communities of Practice (Wenger 1998) are ideally suited for examining learning in citizen science because they emphasize the roles that participation in socially organized activities play in influencing learning (Roth and Lee 2002; National Research Council 2009). Social learning theory may be particularly useful to consider when developing project activities and experiences. Practitioners interested in incorporating social learning theories into citizen science project design, research, and evaluation should refer to the following studies for guidance: Roth and Lee (2004); Brossard et al. (2005); Ballard et al. (2008); Raddick et al. (2009); and Jackson et al. (2014). https://cornell.qualtrics.com/jfe/form/SV_cGxLGl1AlyAD8FL https://cornell.qualtrics.com/jfe/form/SV_cGxLGl1AlyAD8FL Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 14 of 19 Finally, as citizen science continues to grow, it will be important for the field to take a reflective look at its rela- tive impact, and to evaluate whether appropriate ques- tions are being asked by qualified researchers working across projects that involve diverse audiences and issues. Such an analysis will be a first step in gathering critical evidence to demonstrate the potential of citizen science to truly democratize science. Additional Files The Additional Files for this article can be found as follows: • Appendix A. Databases and Search terms used to locate citizen science websites. DOI: https://doi. org/10.5334/cstp.126.s1 • Appendix B. Questions from Online Practitioner Survey. DOI: https://doi.org/10.5334/cstp.126.s1 Acknowledgements The work described in this paper is part of a larger study called DEVISE (Developing, Validating, and Implement- ing Situated Evaluation Instruments). DEVISE is based at the Cornell Lab of Ornithology, with Rick Bonney as Principal Investigator and Tina Phillips as Project Director. We thank the project’s co-PIs, Kirsten Ellenbogen and Candie Wilderman; project consultants Edward Deci, Cecilia Garibay, Drew Gitomer, Kate Haley Goldman, Joe Heimlich, Chris Niemiec, and Gil Noam; and project advi- sory board members Heidi Ballard, Rebecca Jordan, Bruce Lewenstein, and Karen Oberhauser. We also thank the many practitioners who took the time to respond to our survey and queries. Finally, we thank many Lab of Orni- thology staff who helped along the way including Jennifer Shirk, Matthew Minarchek, Marion Ferguson, and Holly Faulkner. DEVISE is supported by the National Science Foundation under Grant No. 1010744. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessar- ily reflect the views of the National Science Foundation. Competing Interests One of the authors of this paper, Rick Bonney, is editor-in- chief of this journal. He was not involved in the process of reviewing the paper other than responding to reviewers’ comments. References Ajzen, I. 1985. From intentions to actions: A theory of planned behavior. In: Kuhl, J and Beckmann, J (eds.), Action Control, From Cognition to Behavior. Berlin, Heidelberg: Springer Berlin Heidelberg. DOI: https:// doi.org/10.1007/978-3-642-69746-3_2 Ajzen, I. 1991. The theory of planned behavior. Organizational Behavior and Human Decision Processes, 50(2): 179–211. DOI: https://doi. org/10.1016/0749-5978(91)90020-T American Association for the Advancement of Science). (ed.) 1993. Benchmarks for science literacy. New York: Oxford University Press. Ayala, CC, Yin, Y, Shavelson, RJ and Vanides, J. 2002. Investigating the cognitive validity of science performance assessment with think alouds: Technical aspects. Presented at the Annual meeting of the American Educational Researcher Association. New Orleans, LA. Ballard, HL and Belsky, JM. 2010. Participatory action research and environmental learning: Implications for resilient forests and communities. Environmental Education Research, 16(5–6): 611–627. DOI: https:// doi.org/10.1080/13504622.2010.505440 Ballard, HL, Fernandez-Gimenez, ME and Sturtevant, VE. 2008. Integration of local ecological knowl- edge and conventional science: A study of seven community-based forestry organizations in the USA. Ecology and Society, 13(2): 37. http://hdl.handle. net/10535/2424. Ballard, HL and Huntsinger, L. 2006. Salal harvester local ecological knowledge, harvest practices and understory management on the Olympic Peninsula, Washington. Human Ecology, 34(4): 529–547. DOI: https://doi.org/10.1007/s10745-006-9048-7 Bandura, A. 1982. Self-efficacy mechanism in human agency. American Psychologist, 37(2): 122–147. DOI: https://doi.org/10.1037/0003-066X.37.2.122 Bandura, A. 1997. Self-efficacy: The exercise of control. 1st edition New York: Worth Publishers. Bandura, A. 2000. Exercise of human agency through collective efficacy. Current Directions in Psychological Science, 9(3): 75–78. DOI: https://doi. org/10.1111/1467-8721.00064 Bandura, A, Adams, NE and Beyer, J. 1977. Cognitive processes mediating behavioral change. Journal of Personality and Social Psychology, 35(3): 125–139. DOI: https://doi.org/10.1037/0022-3514.35.3.125 Bauer, MW, Petkova, K and Boyadjieva, P. 2000. Public knowledge of and attitudes to Science: Alternative measures that may end the “science war.” Science, Technology and Human Values, 25(1): 30–51. DOI: https://doi.org/10.1177/016224390002500102 Becker-Klein, R, Peterman, K and Stylinski, C. 2016. Embedded assessment as an essential method for understanding public engagement in citizen science. Citizen Science: Theory and Practice, 1(1). DOI: https:// doi.org/10.5334/cstp.15 Becker, M, Caminiti, S, Fiorella, D, Francis, L, Gravino, P, Haklay, M (Muki), Hotho, A, Loreto, V, Mueller, J, Ricchiuti, F, Servedio, VDP, Sîrbu, A and Tria, F. 2013. Awareness and learning in participatory noise sensing. PLOS ONE, 8(12): e81638. DOI: https://doi. org/10.1371/journal.pone.0081638 Bela, G, Peltola, T, Young, JC, Balázs, B, Arpin, I, Pataki, G, Hauck, J, Kelemen, E, Kopperionen, L, van Herzele, A, Keune, H, Hecker, S, Suškevičs, M, Roy, HE, Itkonen, P, Külvik, M, László, M, Basnou, C, Pino, J and Bonn, A. 2016. Learning and the trans- formative potential of citizen science: Lessons from the Study of Nature. Conservation Biology, 30(5): 990– 999. DOI: https://doi.org/10.1111/cobi.12762 https://doi.org/10.5334/cstp.126.s1 https://doi.org/10.5334/cstp.126.s1 https://doi.org/10.5334/cstp.126.s1 https://doi.org/10.1007/978-3-642-69746-3_2 https://doi.org/10.1007/978-3-642-69746-3_2 https://doi.org/10.1016/0749-5978(91)90020-T https://doi.org/10.1016/0749-5978(91)90020-T https://doi.org/10.1080/13504622.2010.505440 https://doi.org/10.1080/13504622.2010.505440 http://hdl.handle.net/10535/2424 http://hdl.handle.net/10535/2424 https://doi.org/10.1007/s10745-006-9048-7 https://doi.org/10.1037/0003-066X.37.2.122 https://doi.org/10.1111/1467-8721.00064 https://doi.org/10.1111/1467-8721.00064 https://doi.org/10.1037/0022-3514.35.3.125 https://doi.org/10.1177/016224390002500102 https://doi.org/10.5334/cstp.15 https://doi.org/10.5334/cstp.15 https://doi.org/10.1371/journal.pone.0081638 https://doi.org/10.1371/journal.pone.0081638 https://doi.org/10.1111/cobi.12762',\n",
              " 'What do volunteers want from citizen science technologies? A systematic literature review and best practice guidelines JCOM USER EXPERIENCE OF DIGITAL TECHNOLOGIES INCITIZEN SCIENCEWhat do volunteers want from citizen science technologies? A systematic literature review and best practice guidelines Artemis Skarlatidou, Alexandra Hamilton, Michalis Vitos and Muki Haklay Although hundreds of citizen science applications exist, there is lack of detailed analysis of volunteers’ needs and requirements, common usability mistakes and the kinds of user experiences that citizen science applications generate. Due to the limited number of studies that reflect on these issues, it is not always possible to develop interactions that are beneficial and enjoyable. In this paper we perform a systematic literature review to identify relevant articles which discuss user issues in environmental digital citizen science and we develop a set of design guidelines, which we evaluate using cooperative evaluation. The proposed research can assist scientists and practitioners with the design and development of easy to use citizen science applications and sets the basis to inform future Human-Computer Interaction research in the context of citizen science. Abstract Citizen science; Public engagement with science and technologyKeywords https://doi.org/10.22323/2.18010202DOI Submitted: 3rd April 2018 Accepted: 25th July 2018 Published: 17th January 2019 Introduction The ubiquity of technology and the Internet has dramatically changed the landscape of information availability, including the ways in which we interact with it to make better decisions and improve our quality of life. Technological innovations have resulted in changes not only in the economy and the workplace, but also in the ways people choose to live their lives, spend their free time, and interact with others. Such changes have led to social innovations, which have ushered in a new wave of social change. One such change took place within the scientific context, with the ongoing growth number of amateur volunteers, with the help of technology, now work together with scientists to explore and address scientific issues. This collaboration, or partnership, between professional scientists and amateur volunteers is known as citizen science (CS). In its simplest form it involves amateur Article Journal of Science Communication 18(01)(2019)A02 1 https://doi.org/10.22323/2.18010202 scientists collecting data; an activity which reduces the costs of addressing scientific questions that require the collection of massive amounts of data, and which further bridges the intellectual divide magnified by the professionalisation of science, and the scientific expertise that it may assume. Many CS scholars situate the activity over two centuries ago when amateur scientists, such as Charles Darwin, made significant contributions to science [Silvertown, 2009]. Currently, hundreds of CS projects engage thousands of volunteers across the world. A relatively recent analysis of 388 CS projects revealed that they engaged 1.3 million volunteers, contributing up to US$2.5 billion in-kind annually [Theobald et al., 2015]. The eBird project alone collects five million bird observations monthly, which has resulted in 90 research publications [Kobori et al., 2016]. Technological innovations such as the Internet, smartphone networked devices equipped with sophisticated sensors, and high resolution cameras have had a massive impact on science, especially the way in which CS is currently practiced. Volunteers who interact with these technologies come from different age groups and geographic locations; their cultural contexts and the languages they speak may vary; as do their skills, motivations and goals. A typical user might be an MSc marine biology student who uses a mobile CS application to identify invasive species, or equally an illiterate hunter-gatherer in the Congo basin who collects data to address the challenges of illegal logging and its impact on local resource management. Making sure that users can fully utilise the technology at-hand is fundamental. Within this context, Preece [2016] calls for a greater collaboration amongst “citizens, scientists and HCI specialists” (p. 586). Human-computer interaction (HCI), the discipline which studies how humans interact with computers and the ways to improve this interaction, has a long tradition in the design and development of technological artefacts, including aviation technology, websites, mobile devices, 3D environments and so on. Volunteers’ involvement in the use and design of CS applications is fundamental for effective data collection. Issues such as motivating users to remain active, ensuring that users can effectively use the applications, and guaranteeing satisfaction of use, should be central in the design and development of such applications; issues that according to Prestopnik and Crowston [2012] look beyond “building a simple interface to collect data” (p. 174). Existing studies which investigate user needs and requirements, usability, and user experience (UX) elements (such as having fun and joy) of CS applications provide interesting insights, usually in specific contexts of use. Nevertheless, there is a lack of reflection on the lessons learned from these studies. A more holistic overview is needed to explore the current state of HCI research within CS, and how this knowledge and preliminary empirical evidence can inform the design and development of ‘better’ CS applications. This paper attempts to address this gap. With the preliminary aim of identifying key design features, as well as other relevant interaction recommendations, a systematic literature review (SLR) was utilised to capture research studies which discuss user issues of CS applications that support data collection (either web or mobile based). This evidence and knowledge is summarised in a set of design guidelines, which were further evaluated in a cooperative evaluation (CE) study with 15 people. Before we explain how these methods were employed in this study in section 3, the next section provides their theoretical background. Section 4 briefly presents the https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 2 https://doi.org/10.22323/2.18010202 results, and section 5 discusses, in more detail, the studies that we analysed in the SLR, together with the findings of our CE. Conclusions are drawn in section 6, focusing on limitations of this research and suggestions for future research. Theoretical overview of methods used A systematic literature review (SLR) provides a standardised method for reviewing literature which is “replicable, transparent, objective, unbiased and rigorous” [Boell and Cecez-Kecmanovic, 2015, p. 161]. SLRs emerged in evidence-based medicine during the 1990s, but have since been found, in increasing numbers, within various fields including education, psychology and software engineering [Boell and Cecez-Kecmanovic, 2015]. SLRs have also been used in the context of HCI and CS. For example, Kullenberg and Kasperowski [2016] outline a scientometric meta-analysis, using datasets retrieved from the Web of Science, to investigate the concept of CS, its development over time, the research that it covers and its outputs. In their study they mention implications associated with the use of key terms (e.g. use of terms related to CS, such as crowdsourcing, participatory monitoring, public participation and acronyms such as PPGIS), but generally no other criticisms are mentioned. Zapata et al. [2015] use a SLR approach to review studies that perform usability evaluations of mHealth applications and their user characteristics on mobile devices. Due to the narrow focus, and the fact that the results were further analysed qualitatively, Zapata et al. [2015] only screen 22 papers (out of the 717 articles which were initially retrieved). Connolly et al. [2012] review the impacts of computer games on users aged above 14 years old. With an initial 7,391 articles retrieved from various databases, they applied a relevance indicator (from 1 least relevant to 9 most relevant) to each paper and analysed the papers that received a rank higher than 9, resulting in a final screening of 70 papers which discuss several dimensions of the detected impacts. Other studies investigate the accessibility and usability of ambient assisted living [Queirós et al., 2015]; studies which integrate agile development with user centred design [Salah, Paige and Cairns, 2014] to identify challenges, limitations, or other characteristics, thus requiring a qualitative analysis of the papers reviewed. Although the previously mentioned studies offer a limited insight into the limitations of SLRs, the most widely acknowledged limitation surrounds the weaknesses of search terminology, as effective keyword combinations cannot be always known a priori [Connolly et al., 2012; Boell and Cecez-Kecmanovic, 2015]. Likewise, the process’ internal validity, such as problems related to data extraction, the so-called recall-precision trade off, and a lack of evaluation, is also considered a limitation [Boell and Cecez-Kecmanovic, 2015]. To adjust for these limitations, this research evaluated the content of our meta-analysis in a user testing experiment. We employed cooperative evaluation method; a user testing method which allows for the observer to interact with the user subjects in order to direct their attention and record their opinion about specific design features while they work together as collaborators. Thus, the observer can answer questions and doesn’t have to sit and observe in silence while the user interacts with the application involved in the experiment [Monk et al., 1993]. Furthermore, this research focuses on the environmental CS context. As an increasing number of CS projects in various fields (e.g. astronomy, linguistics) https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 3 https://doi.org/10.22323/2.18010202 make use of digital technology to engage their volunteers in data collection activities, only some of these projects consider user issues in their design [e.g. Sprinks et al., 2017]. The interfaces, tasks, and therefore design issues or user needs and requirements vary across different contexts of use. For example, in astronomy-related CS applications image classification is a major task, whereas in conservation applications tasks such as adding an observation using a mapping interface are more common. In order to narrow down the focus, and also due to the authors’ interest in the way geographical interfaces assist volunteers in data collection, the term ‘environment’ has been included in all keyword searches. Although this might limit the scope and extent of retrieved studies, it should be noted that the term is already quite broad, since it may include applications from contexts where CS is widely used, including, but not limited to, ecology, conservation, environmental monitoring, etc. In the next section we review the methodological implementation and experimental design. Methodological procedure and experimental design A two-stage methodological approach was applied. In the first part, a systematic literature review (SLR) was conducted to review and evaluate relevant research studies; section 3.1 describes this process. In the second stage, discussed in section 3.2, 15 CE experiments were implemented to evaluate three online CS applications, which provided a context to discuss the preliminary guidelines with our participants. Systematic literature review The following databases were included in our search: Web of Science1 of Thomson Reuters, Scopus2 of Elsevier, and Google Scholar.3 The sources comprise access to multidisciplinary research studies (i.e. peer-reviewed and grey literature) from the fields of sciences, social sciences, and arts and humanities. We decided to employ all of them to obtain reliable, robust, and cross-checked data; including a reasonable amount of ‘grey literature’ via Google Scholar, so that we could include in our search technical reports and government-funded research studies which are not usually published by commercial publishers [Haddaway, 2015]. The searches took place in June (i.e. Google Scholar) and December 2017 (i.e. Web of Science and Thomson Reuters) following the same methodological protocol. In order to automate the otherwise time-consuming search process, we used the free software “Publish or Perish”, which allows importing a set of keyword combinations, executing a query on Google Scholar for each of them and collecting the results into a CSV file. The software was configured to execute 120 queries per hour. Our database search aimed at retrieving papers with a focus on user issues of CS technologies in the environmental context. The majority of key terminology (i.e. user, citizen science, technology) is not standardised; for example, in the literature alternative terms that are used for user requirements may include “user needs”, “volunteer needs” or “volunteer requirements”, which were all included in the SLR. Similarly, other terms related to citizen science such as “participatory science” and “crowdsourced science” we deemed relevant as several of these terms are used 1Web of Science: http://apps.webofknowledge.com/. 2Scopus: https://www.scopus.com/search/form.uri. 3Google Scholar: https://scholar.google.co.uk/. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 4 http://apps.webofknowledge.com/ https://www.scopus.com/search/form.uri https://scholar.google.co.uk/ https://doi.org/10.22323/2.18010202 interchangeably in the literature and the similarities in the way these are practiced especially concerning user issues of those digital technology-based implementations. Considering that the search for the right terminology in SLRs becomes a search “for certain ideas and concepts and not terms” [Boell and Cecez-Kecmanovic, 2015, p. 165] which are relevant to the particular topic, we decided to include the concepts/keywords that are shown in Table 1; with the keyword ‘environment’ consistently applied to all searches. Our extensive concept identification analysis resulted in 1,045 keyword combinations, which significantly increased the effort and time needed to gather and analyse the data. SLR searches “are optimised to return as many (presumably) relevant documents as possible thus leading to a high recall” [Boell and Cecez-Kecmanovic, 2015, p. 165], which may be at the expense of data accuracy. Although each keyword search combination was designed to return only 200 results, we were confident that by including such a broad range of concepts we would be able to effectively capture the broader state-of-the-art literature in the area of environmental digital CS with a focus on user issues or detailed user studies. Table 1. Concepts/Keywords used in the Systematic Literature review. Highlight in bold are the most popular terms in each category according to the number of articles retrieved. Themes User-focus Citizen Science - focus Technology - focus Keywords HCI, human-computer interac- tion, usability, evaluation, user experience, UX, user needs, user requirements, user research, human factors, UCD, user- centred design, human-centred design, accessibility, volunteer needs, volunteer requirements, user testing, volunteers’ needs, volunteers’ requirements Citizen science, particip- atory science, DIY science, grassroot science, civic science, crowdsourced science, community sci- ence, community-based participatory research, community-based monitor- ing, public participation in scientific research, PPSR Technologies, technology, digital, ICT, technology- mediated For each result the following information was retrieved: title, author(s), date, number of citations, publisher and the article URL. After duplicates were removed and any missing fields (i.e. mainly dates) were manually filled in, we applied our exclusion criteria. These included: studies published before 2000, as digital CS only gained momentum in the mid-2000s [Wald, Longo and Dobell, 2016; Silvertown, 2009]; studies in languages other than English, as it would not be possible to further analyse their content. The studies that were included in the analysis all discussed digital CS in the environmental context (i.e. either focusing entirely on an environmental CS application or including examples of such applications) and had a user focus (i.e. recommendations for design features based on user studies or on user feedback; insight into user issues and design suggestions based on broader experience; expert inspections for usability improvements, etc). To ensure that these themes were all covered by the remaining studies the following were separately assessed by two of the authors: title, followed by the abstract and, if necessary, the introduction and conclusions. The final list of SLR papers (i.e. 62 papers in total) were all read in-full to inform our preliminary set of best practice guidelines. More than one paper mentioned the same design issue in order for it to be included it in the preliminary list of design guidelines. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 5 https://doi.org/10.22323/2.18010202 Cooperative Evaluation (CE) The preliminary guidelines that resulted from the meta-analysis of the final articles were further evaluated, through the use of CE experiments, and extended upon so that users’ direct feedback is taken into account. For the purposes of this study, three applications were evaluated, which featured either a mobile or a web interface (or both). The three applications which were included in the CE experiments are: – iSpot;4 nature-themed CS application provided by Open University, with over 65,000 registered users (as in March 2018). The application provides users with a web-based interface, which people can use to upload geo-referenced pictures with various wildlife observations, explore and identify species, and connect with other enthusiasts worldwide (Figure 1a). It further supports gamification (via badges, which are popular in the engagement and retainment of online volunteers.). – iNaturalist;5 environmental CS application developed originally at UC Berkeley and now maintained by the California Academy of Sciences with over 0.5 million registered users (Figure 1b). It provides both a web-based interface and a mobile application to contribute biodiversity data in the form of photographs, sound recordings or visual sightings. The data are open access and available for scientific or other purposes. – Zooniverse;6 one of the most popular digital CS platforms, with over 1.6 million registered users6 was developed by Citizen Science Alliance. The platform includes several CS projects, of which the most popular is the Galaxy Zoo project. The platform was included in the evaluation for the assessment of overall interface design and assessment of such design features as tutorials and communication functionality and therefore we decided to investigate these features within as well as beyond the environmental context (i.e. projects of environmental interest). The project pages included in the evaluation are: Wildlife Kenya, Shakespeare’s World, the Elephant Project, Galaxy Zoo and Understanding Animal Faces. It should be noted that iSpot and iNaturalist applications were chosen on the basis of their popularity and strong environmental focus. Zooniverse was chosen due to its popularity and the significant number of users that it attracts to mainly focus on design features of the communication functionality category which extends to the broader CS context. Fifteen participants (seven male and eight female) were recruited, via opportunity sampling, to evaluate the applications and offer user feedback with respect to the SLR preliminary guidelines. Since all of the applications’ projects are designed to be inclusive towards a wide user audience we decided against restricting participation to specific conditions, which would in turn influence the usefulness of 4iSpot: https://www.ispotnature.org/ It should be noted that while in the past iSpot had a dedicated Android app, it was withdrawn in February 2015, and the web-based interface is used on mobile phones, too. 5iNaturalist: https://www.inaturalist.org. 6Zooniverse: https://www.zooniverse.org/. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 6 https://www.ispotnature.org/ https://www.inaturalist.org https://www.zooniverse.org/ https://doi.org/10.22323/2.18010202 Figure 1. Interfaces of applications included in the CE experiments; a. iSpot; b. iNatural- ist.org; c. Zooniverse. this study. Nonetheless, the majority of our participants (9/15) were between 18–25 years old (with a further two between 25–34; three between 45–54 and one 55+). Only six of the participants had prior knowledge or experience with CS projects, all had experience interacting with online/mobile mapping interfaces and, in terms of their technological skills, five rated themselves as ‘intermediate’, six as ‘advanced and four as ‘experts’. The applications were evaluated in a random order to minimise bias introduced by the learning effect. The experiments were carried out in July and August 2017. Each experiment was carried out over one hour and a set of tasks were provided to the users to guide their interaction with the application they interacted within the CE session. The tasks were designed to gain a better understanding of the design guidelines derived from the SLR (e.g. Find the forum page and read discussion about ‘adding photo to button’ on iSpot; check notifications for new messages/comments on Zooniverse; check news feed for updates from iNaturalist community). Of course more simple tasks that helped participants get a good understanding of the application’s context of use were also included (e.g. Explore photos for observations in your local area on iSpot; explore projects and then select a project to contribute an observation on Zooniverse). A total of 10 tasks per application were provided to guide interaction and participants were asked to think aloud. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 7 https://doi.org/10.22323/2.18010202 Results Systematic literature review The 1,045 keyword combinations returned 24,147 results from all three databases, dated from 1970 to 2017. In the three categories of keywords the most popular were: technology/technologies (13,781; followed by digital with 6,057 results); citizen science (8,793 results; followed by participatory science with 2,168 results) and evaluation (5,160; followed by usability with 3,038 and human computer interaction with 2,088 results). After removing duplicates, non-English results and any publications dated before 2000 we ended up with 844 results (Figure 2). Figure 3 summarises the top ten keyword combinations returning the majority of results in this stage. This dataset includes articles and other results from various disciplines, mainly healthcare, education, environmental studies and HCI. The terms ‘technology’ or ‘technology-mediated’ are especially popular in education literature and ‘community-based participatory research’ is popular amongst health scholars, yet the majority of these articles do not satisfy the rest of the SLR criteria. Moreover, the keyword ‘evaluation’ returned almost 5,160 results, the majority discussing different aspects of evaluations (e.g. project, methodological or result evaluations), with only a few focusing on evaluations of user issues. Figure 2. Systematic Literature Review procedure. The next step involved inspecting all n=844 results (i.e. examination of title, abstract, methodology and conclusions); a process undertaken by two of the authors of this paper. During this process irrelevant results (i.e. studies that do not satisfy the search criteria) were excluded together with: i. Results that we could not access (e.g. conference proceedings, other n=47); ii. Books (not available online or in a digital form) (n=3 entries), as it would be difficult to further process content in the meta-analysis study; iii. Slideshare, poster presentations and keynotes (n=18), as they did not provide enough insight to inform meta-analysis; iv. Studies that targeted only children (n=4), as the user audience of all other studies included in the SLR is much broader and including these articles would introduce a bias; v. Studies https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 8 https://doi.org/10.22323/2.18010202 Figure 3. Ten most popular keyword combinations (for n=848) based on number of articles returned. ‘Tech’ is used for technology; ‘CS’ for Citizen science; and ‘env’ for environment. discussing technologies at a conceptual level (i.e. applications do not yet exist) (n=4); and vi. Studies which describe sensors (non-phone based) and wearable technologies (n=2); these technologies have different characteristics that need to be considered for their effective design. Keywords such as “community-based monitoring” returned several results focusing on the development and user issues of Web Geographical Information Systems (GIS), which is commonly used in CS. We included only studies which described technologies to support data collection, and not studies on data visualisation for public consultation, another popular area in Public Participation GIS. Theoretical papers providing overviews of CS were included only when provided insight into technology-related user issues [e.g. Newman, Wiggins et al., 2012; Wiggins and Crowston, 2011]. Also we included papers on user design suggestions even if the focus was not entirely on a specific application [e.g. Jennett and Cox, 2014; Rotman et al., 2012]. Further application of the exclusion criteria resulted in 62 relevant articles suitable for meta-analysis. It should be noted that the keyword combinations at this stage vary with most popular being: “technology”, “environment”, “citizen science” and “usability” (n=7), followed by “user experience” (n=6), “human-computer interaction” (n=5) and user-centred design” (n=4). “Evaluation”, which was the most popular keyword of initial search results, was the least popular amongst the final list of articles (n=62). Nine results use the term “technology-mediated”, but “technology” was the most popular term in this category. Publication dates range from 2002 up to 2017, with the majority (n=55) unsurprisingly published after 2010. The results include three reports, one dissertation thesis, 28 journal papers, 27 conference proceedings, with most of them being peer-reviewed (e.g. CHI, CSCW, GI Forum), and three book chapters which are available online. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 9 https://doi.org/10.22323/2.18010202 The final list includes several studies (n=18) where users are directly involved in usability evaluations [e.g. R. Phillips et al., 2013; Kim, Mankoff and Paulos, 2013; Jennett, Cognetti et al., 2016; D’Hondt, Stevens and Jacobs, 2013], co-design [e.g. Fails et al., 2014; Bowser, Hansen, Preece et al., 2014] and user-centred design processes [e.g. Woods and Scanlon, 2012; Newman, Zimmerman et al., 2010; Michener et al., 2012; Fledderus, 2016], in contexts such as wildlife hazard management [e.g. Ferster et al., 2013]; water management [e.g. Kim, Mankoff and Paulos, 2013]; environment-focused CS games [e.g. Bowser, Hansen, He et al., 2013; Bowser, Hansen, Preece et al., 2014; Prestopnik and Tang, 2015; Prestopnik, Crowston and Wang, 2017]; species identification, reporting and classifications [e.g. Newman, Zimmerman et al., 2010; Sharma, 2016; Jay et al., 2016]; and noise and ecological monitoring [e.g. Woods and Scanlon, 2012; Jennett, Cognetti et al., 2016]. Some of these studies include an impressive number of user subjects; e.g. Luther et al. [2009], in their evaluation of Pathfinder — a platform that enables CS collaboration — include over 40 users and Bowser, Hansen, Preece et al. [2014] in a co-design study for the development of Floracaching evaluated the application with 58 users. Several other studies involve users through surveys and interviews [e.g. Idris et al., 2016; Wald, Longo and Dobell, 2016; Eveleigh et al., 2014; Wiggins, 2013]; or other forms of user feedback such as online forums and discussions [e.g. Wiggins, 2013; Sharples et al., 2015]. Methods used within the studies included personas and user scenarios to identify user needs and requirements [e.g. Dunlap, Tang and Greenberg, 2013], and heuristic evaluations to examine virtual CS applications [Wald, Longo and Dobell, 2016]. Other studies did not include users directly, but still provided insights on user requirements, needs and other design issues from the design and development of a specific application or by reviewing existing ones [e.g. Connors, Lei and Kelly, 2012; Ferreira et al., 2011; Johnson et al., 2015; Newman, Wiggins et al., 2012; Stevens et al., 2014; Kosmala et al., 2016]. Six categories were created to inform the meta-analysis. The categories, created in agreement by two of this paper’s authors who analysed the data and they were designed to effectively group key design features and which include: Basic features and design recommendations (e.g. homepage; registration); Design for Communication Functionality (i.e. functionality that supports communication between users such as activity updates, comments, likes, forums, links to social media etc); Design for Data Collection; Design for Data Processing and Visualisation; Gamification Features; User Privacy Issues (e.g. setting various levels of access in the collected data). In section 5 we present and discuss the guidelines in detail, as these were influenced by the CE. Cooperative Evaluation (CE) In the CE experiments, participants were given a set of tasks and explored features from the same six categories that we described in the previous section as discussed in section3.2. Here, we briefly present some general observations with respect to each one of the three applications. Additional findings that informed the guidelines are discussed more extensively in section 5 where we discuss the guidelines in more detail. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 10 https://doi.org/10.22323/2.18010202 Table 2. Summary of design features and content identified by participants in each of the three applications examined with the method of CE. iSpot website iNaturalist app Zooniverse website Most Useful/usable features Information content; external links; termin- ology; data collection formats Search by location; search autocomplete function; data collec- tion formats Projects page — cat- egorisation tabs Least useful/ least usable features Filtering on main page; lack of mobile app for data collection; locat- ing & filtering the map Search function; help page and tutorials News page hard to read; distracting colour and small font size; Controversial amongst participants features Social media login fea- ture — Opening pages in new/same window’; use of back button Suggestions Forums and help pages separate menu items; main page’s filtering should accommod- ate needs of global community; introduce ‘expert status’ Homepage of the mo- bile app to contain links to news and external articles; Tab to check notifica- tions; place search tab on top of the screen; ap- plication zooms to the new observation point added Newsfeed to resemble that of Facebook’s lay- out; ‘talk’ tab to be re- named to ‘forum’; skip tutorial option iSpot. Participants appreciated the fact that the main iSpot’s homepage is informative and contains links to news articles and educational material. Nonetheless, participants suggested that the main page should not be organised by ‘communities’, neither it should show content based on user’s ‘location’. They recommended that end users should have control of such filtering options so that it is more open towards its global community. Logging in using social media was quite controversial (“It’s a nice feature. . . I don’t need to remember another password”; “It depends on the permission settings, because I don’t like having to change all the permission settings on Facebook” user comments). Participants also liked features for entering data in various formats, and the concurrent use of both scientific and lay vocabulary to explain species identification. Problematic features included finding the interactive map, filtering the map results and the lack of a search by location function. CE participants were not interested in the badges award feature, although they expressed greater trust levels to data contributed by volunteers with badges. They suggested that an ‘expert status’ feature which takes into account skills and professional qualifications would be more effective in this respect. iNaturalist.org. Participants found the ‘search by location’ function particularly useful; tutorials and help pages and the search function were hard to locate and participants suggested that these should be located on the top of the screen. Users were not interested in communication features (“Everybody just uses Facebook messenger” user comment) and gamification features. Participants suggested that a feature similar to that provided by Facebook to check notifications would improve the user experience, as well as external links to news or relevant articles from the mobile app’s homepage. The fact there is no legend provided caused frustration. Participants further suggested that once a new data point is added the application should zoom in on this observation automatically. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 11 https://doi.org/10.22323/2.18010202 Zooniverse. Participants appreciated external links and the news stories, yet they found the page hard to read and suggested it should be reorganised to resemble that used of Facebook. They disliked the orange colour on the background of the headings and the small font size. They did not immediately understand the purpose of the ‘talk’ feature, and when they did, they suggested it should be renamed to ‘forum’. With respect to tutorials, participants suggested that pop-ups work best (“Nobody has the time to watch a video” participant comment) and that a skip option should be provided. Communication and gamification features were again not highly rated, yet one participant commented in his think aloud “what do I get with my points?”, and suggested that such a gamification feature would only make sense if users are truly awarded with something in return (e.g. zoo vouchers). Designing for citizen science: design guidelines Basic features and design recommendations General interface design should follow popular name and navigation conventions (e.g. ‘forum’ instead of ‘talk’). Both CE experiments and the SLR meta-analysis [e.g. see Teacher et al., 2013; Kim, Mankoff and Paulos, 2013] agree that the project’s main page should contain information about: project description; data collected; project outcomes; and links to news and external links for additional information. Providing a news section, according to articles reviewed, serves as a motivation incentive feature [Eveleigh et al., 2014] and its design should follow name conventions, as suggested by the majority of our participants and the SLR [e.g. McCarthy, 2011]. A forum should be provided as separate menu item to support volunteers collate and respond to feedback, offer their suggestions and support, as well as for social interaction purposes [Woods, McLeod and Ansine, 2015; Wald, Longo and Dobell, 2016]. Similarly, a help page should be provided as a separate menu item, a suggestion made by our participants, which is also in line with other guidelines [e.g. Skarlatidou, Cheng and Haklay, 2013]. Registration is a common feature that many studies discuss, with a growing number of applications providing the option to sign up using social media [e.g. Dunlap, Tang and Greenberg, 2013; Ellul, Francis and Haklay, 2011; Wald, Longo and Dobell, 2016]. Jay et al. [2016] demonstrated that “it is possible to increase contributions to online citizen science by more than 60%, by allowing people to participate in a project without obliging them to officially sign up” (p. 1830). Participants in our study expressed their support towards the social media login function. Nevertheless they were concerned with changes in permission settings in their social media accounts. We thus suggest that sign up using social media should not be the only option and that a registration page is also provided. Several of the articles in the SLR discuss that CS technologies should provide tutorials in various forms (e.g. textual, videos) [Yadav and Darlington, 2016; Dunlap, Tang and Greenberg, 2013; Stevens et al., 2014; Kim, Mankoff and Paulos, 2013; Prestopnik and Crowston, 2012, etc]. Our participants recommended that tutorials should be provided using pop-up functionality (which highlight relevant features) with the option to skip them and/or allow to return at some later point in time. Photographs in tutorials should be used to communicate scientific objects of interest [e.g. see Dunlap, Tang and Greenberg, 2013]. Overall interface design should also take into account cultural and environmental characteristics. There is, for example, a growing number of studies in CS which https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 12 https://doi.org/10.22323/2.18010202 investigate how such technologies should be designed to support data collection by illiterate or semi-literate people [Stevens et al., 2014; Liebenberg et al., 2017; Vitos et al., 2017; Idris et al., 2016; Jennett, Cognetti et al., 2016]. Studies from this context suggest early engagement with end users, placing the user at the centre of the design and development lifecycle, removing any form of text from user interfaces, and so on [Idris et al., 2016]. Also, several of these applications are designed to be used in remote areas and/or the outdoors (e.g. in the forest, parks or recreation areas). Therefore, the design of buttons should be usually larger than in standard application design and the screen lighting and battery life should be tested separately [Ferster et al., 2013; Stevens et al., 2014; Vitos et al., 2017]. Design for communication functionality The SLR articles emphasise the importance of providing (real-time) communication functionality [e.g. see Newman, Wiggins et al., 2012; Wald, Longo and Dobell, 2016; Fails et al., 2014; Woods, McLeod and Ansine, 2015] to support communication amongst volunteers, or between volunteers and the scientists, to retain volunteers, and to communicate further information about how the data are used or will be used. The latter has been discussed within the context of trust-building, for example, its potential for increasing confidence in the project and corresponding scientists [Ferster et al., 2013]. Map tagging and adding comments on map have also been suggested, together with chats and forums [Dunlap, Tang and Greenberg, 2013]. CE participants showed no preference towards the inclusion of a chat function but they all thought that a forum page adds value. Nevertheless, it should be acknowledged that the artificial experimental setting of CE is perhaps not appropriate to fully appreciate the benefits in the same way that actual volunteers experience this feature. Furthermore, communication functionality depends on the nature of the project, its geographical extent, and other attributes, and we therefore suggest that these are all taken into account when designing for communication in CS applications. Design for data collection Data collection is perhaps the most widely discussed feature. The SLR articles provide suggestions for: real-time data collection [e.g. Panchariya et al., 2015]; supporting de-anonymisation from sharing data with close friends and other groups [e.g. Fails et al., 2014; Maisonneuve, Stevens, Niessen et al., 2009]; enabling the collection of user’s GPS location data [e.g. Maisonneuve, Stevens and Ochab, 2010; Kim, Mankoff and Paulos, 2013], only if it is of high accuracy, otherwise users can become frustrated/confused [Bowser, Hansen, He et al., 2013]; the ability to collect various data types such as numbers, videos, photographs, text, coordinates [Ellul, Francis and Haklay, 2011]; the ability to combine various data types derived from various sources, which requires a good understanding of the types of data that are useful in the specific project [e.g. Wehn and Evers, 2014; Kim, Mankoff and Paulos, 2013]; and the ability to add qualitative data to observations [Maisonneuve, Stevens and Ochab, 2010], which is an attribute that it is very popular amongst the SLR articles. Adding a photograph to showcase the observation captured can be a significant trust cue (as noted previously, it may also support the development of data validation mechanisms) [e.g. see Kim, Mankoff and Paulos, 2013]. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 13 https://doi.org/10.22323/2.18010202 In terms of design, form design should be simple to improve accessibility [e.g. see Prestopnik and Crowston, 2012]; the use of images and drop-down lists can improve the time required to enter data and should be preferred over text [Idris et al., 2016], especially in mobile devices. When users are not required to fill in all data fields make sure they understand this, thus preventing them entering incorrect data [Woods and Scanlon, 2012]. Data collection supported by showing the location of the user (using GPS tracking) in the application (mapping interface), should use symbology that stands out from base map [e.g. Dunlap, Tang and Greenberg, 2013]. Moreover, the provision of additional reference points shown on the map help users validate the data they collect [Kim, Mankoff and Paulos, 2013]. When possible, allow for data tailoring and personalisation to improve motivation [Sullivan et al., 2009]. In some contexts, the submission of high quality images is absolutely essential (e.g. in species identification), however, the handsets that users are equipped with might not support zooming to the required level [Jennett, Cognetti et al., 2016]. In this case, we suggest that users should be made aware of their handset limitations well in advance, before they contribute any data. In conditions where there is limited (or no) Internet access, scholars emphasise the importance of collecting data offline, storing them and uploading them automatically once a connection is established [Stevens et al., 2014; Fails et al., 2014; Bonacic, Neyem and Vasquez, 2015; Kim, Robson et al., 2011]. We also suggest that, within specific contexts of use, developers should consider the benefit of providing effective data validation functionality when new data are being submitted [e.g. averaging data records, flag errors and provide feedback to the user to correct errors, ask users to inspect data to throw out outliers/check data accuracy, view redundant data collected by others and decide whether it is helpful etc., as in Dunlap, Tang and Greenberg, 2013], as it can improve the quality of data and user trust in the data. A feature explicitly mentioned in the CE sessions by several participants is the feedback provision to notify volunteers when a new observation has been submitted. Design for data processing and visualisation Features relevant to data processing and visualisation include: a data sharing and viewing website and mobile application to see the data collected instantly, preferably on a map but other visualisations are also suggested, e.g. tables [Kim, Mankoff and Paulos, 2013; Woods, McLeod and Ansine, 2015; Maisonneuve, Stevens, Niessen et al., 2009]; a search function with autocomplete capabilities [Yadav and Darlington, 2016; Ellul, Francis and Haklay, 2011]; search by location [e.g. Sullivan et al., 2009; Elwood, 2009]; and filter data on the map using different variables [Kim, Mankoff and Paulos, 2013]. Other features include: map zooming and panning tools [Higgins et al., 2016; Kar, 2015]; the ability to switch between different map backgrounds, especially in applications on mobile devices to save battery life and conserve on data consumption; and the ability to read details of data collected while browsing the map interfaces, e.g. using a hover text showing details for each data pin [e.g. in Kim, Mankoff and Paulos, 2013]. When observations on a map are linked to other information on other parts of the interface (e.g. textual information or graphs on the site) provide visual cues for the user to understand association [Fledderus, 2016]. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 14 https://doi.org/10.22323/2.18010202 Gamification features Gamification is popular in CS literature for its potential to motivate users and increase numbers of contributions. Therefore several studies explore the use of badges, awards or leaderboards to incentivise users and keep them active [e.g. Prestopnik and Crowston, 2012; Newman, Wiggins et al., 2012; Crowley et al., 2012; Panchariya et al., 2015; Bowser, Hansen, He et al., 2013; Bowser, Hansen, Preece et al., 2014; Wald, Longo and Dobell, 2016]. Nevertheless, studies acknowledge that gamification might not be such a significant motivation factor in terms of collecting scientific data [e.g. one of the participants in Bowser, Hansen, Preece et al., 2014, mentions that “contributing to science. . . that’s kind of motivating to me” p. 139] [Preece, 2016; Prestopnik, Crowston and Wang, 2017], which is in line with the results of our CE. Preliminary evidence suggests that leaderboards, rating systems and treasure hunting type of game features might improve user experience of CS applications [e.g. see Bowser, Hansen, Preece et al., 2014]. The option to skip the use of gamification features is also strongly recommended [Bowser, Hansen, Preece et al., 2014], so that volunteers do not have to deal with the competitive part of a gamified application if they don’t want to [Preece, 2016]. CE participants suggested that gamification would not make them contribute more data and that an option to opt out is essential. They also suggest that gamification features would be more effective if they lead to tangible outcomes (i.e. non-diegetic reward features, such as sponsorships from local organisations for vouchers to participate in CS activities and events). This is congruent with the wider gamification literature, where point systems and leaderboards, although once addictive features, are being replaced by experience-based gamification features [McCarthy, 2011]. Other features such as badges or ‘expert’ status, may also help to tackle trust concerns in the same way that reputation systems, seals of approval, and other trust cues are discussed in other contexts such as online shopping, education and even Web GIS [Dellarocas, 2010; Skarlatidou, Cheng and Haklay, 2013]. User privacy issues The articles of our meta-analysis make extensive reference to privacy issues and con- cerns [e.g. R. D. Phillips et al., 2014; Kim, Mankoff and Paulos, 2013; Preece, 2016]. Filtering, moderation, ensuring that the data stored do not support identification of indi- viduals and providing the option to collect data without sharing it are some of the options that are suggested in the SLR as potential solutions [e.g. Ferster et al., 2013; Leao, Ong and Krezel, 2014; Maisonneuve, Stevens, Niessen et al., 2009]. Anonymity (via the option to contribute without registration/signature) has been also suggested to address privacy concerns [e.g. Dale, 2005] and improve participation. Nevertheless, it is suggested by existing studies that applications should be designed with attribu- tion, which improves volunteers’ trust, and therefore anonymity should not always be encouraged [e.g. Luther et al., 2009]. In the CE experiments only five participants expressed privacy concerns and they suggested that complying with security standards and offering the ability to change privacy settings would definitely be beneficial. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 15 https://doi.org/10.22323/2.18010202 Conclusions and future work The research carried out demonstrates the current trends and main design considerations, as suggested in user-related research and user testing findings, within the environmental digital CS context. It resulted in design guidelines which developers in CS can follow to improve their designs. Likewise, they can also be used to inform future research, for example, addressing methodological implications of the methods employed herein, as well as extending this work further. The SLR was a time-consuming process given the extensive number of keywords, however it resulted in the identification of a satisfactory number of resources to include in the meta-analysis study. It should be noted that user research in the context of CS is in its infancy, and therefore many of the studies were making suggestions based on the authors’ experiences rather than on hard evidence derived from the involvement of actual users. Nevertheless, we were also impressed by the number of studies which did involve users in evaluation, UCD practices, or simply in capturing user needs and requirements. We acknowledge the fact that a significant number of studies (i.e. n = 7,169) were removed due to language barriers. Although this sample was not further processed to assess its relevance it still may poses a limitation and therefore a future study that aims at investigating non-English research studies could significantly contribute to the proposed design guidelines for digital CS applications. Overall we suggest that wider communication of findings, preferably in more than one languages, will inspire more CS practitioners and researchers to involve end users in the design process, thus significantly improving the usability and user experience of CS applications. Moreover, the SLR described focuses only on the environmental context. Thus, our preliminary findings can be extended by further analysing similar studies in other contexts of digital CS. The second limitation arises from the CE implementation, which involved 15 users. Although this sample provided us with enough insight into the proposed guidelines, increasing the number of applications tested and the number of users involved in CE could provide more in-depth insight to significantly improve the proposed guidelines. Introducing a set of recruitment criteria to inform user participation in any future studies may also extend the usefulness of our approach. For example, repeating the experiments with two groups of participants (i.e. users with some or significant experience in the use of CS applications and users who never used CS applications) can improve our understanding around user issues and corresponding design features that are important in terms of both attracting new users as well as retaining them. Similarly, participants of specific age groups, cultural backgrounds and especially those who speak languages other than English may help uncover more user issues which may be applicable to all or to specific design categories (e.g. gamification features and communication functionality) and contribute to the establishment of a more inclusive set of guidelines. Considering the current state of research in this area, we also suggest that any user testing focuses on additional user experience elements to understand not only usability or volunteer retainment, but also issues surrounding communication, privacy and improving trust via interface design. Finally, the proposed guidelines can inform the design and development of environmental design CS applications and several of the guidelines are in line with similar work within the broader digital CS context [e.g. Sturm et al., 2018]. It should be however acknowledged that guidelines cannot replace contextual and https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 16 https://doi.org/10.22323/2.18010202 environmental considerations. We therefore propose that user studies are fully-integrated in citizen science, especially with respect to its technological implementations, in terms of understanding user issues that are relevant to specific contexts of use. The wider CS community can further benefit from the communication of user-based research in terms of not only improving our knowledge and understanding of what users want, but also in terms of providing the methodological protocols that others can easily replicate for exploring user issues in various contexts of use. Acknowledgments This research project is funded by European Research Council’s project Extreme Citizen Science: analysis and Visualisation under Grant Agreement No 694767 and it has the support of the European Union’s COST Action CA15212 on Citizen Science to promote creativity, scientific literacy, and innovation throughout Europe. References Boell, S. K. and Cecez-Kecmanovic, D. (2015). ‘On being ‘systematic’ in literature reviews in IS’. Journal of Information Technology 30 (2), pp. 161–173. https://doi.org/10.1057/jit.2014.26. Bonacic, C., Neyem, A. and Vasquez, A. (2015). ‘Live ANDES: mobile-cloud shared workspace for citizen science and wildlife conservation’. In: 2015 IEEE 11th International Conference on e-Science (Munich, Germany, 31st August–4th September 2015), pp. 215–223. https://doi.org/10.1109/escience.2015.64. Bowser, A., Hansen, D., He, Y., Boston, C., Reid, M., Gunnell, L. and Preece, J. (2013). ‘Using gamification to inspire new citizen science volunteers’. In: Proceedings of the First International Conference on Gameful Design (Gamification ’13). ACM Press, pp. 18–25. https://doi.org/10.1145/2583008.2583011. Bowser, A., Hansen, D., Preece, J., He, Y., Boston, C. and Hammock, J. (2014). ‘Gamifying citizen science: a study of two user groups’. In: Proceedings of the companion publication of the 17th ACM conference on Computer supported cooperative work & social computing — CSCW Companion ’14 (Baltimore, MD, U.S.A. 15th–19th February 2014), pp. 137–140. https://doi.org/10.1145/2556420.2556502. Connolly, T. M., Boyle, E. A., MacArthur, E., Hainey, T. and Boyle, J. M. (2012). ‘A systematic literature review of empirical evidence on computer games and serious games’. Computers & Education 59 (2), pp. 661–686. https://doi.org/10.1016/j.compedu.2012.03.004. Connors, J. P., Lei, S. and Kelly, M. (2012). ‘Citizen science in the age of neogeography: utilizing volunteered geographic information for environmental monitoring’. Annals of the Association of American Geographers 102 (6), pp. 1267–1289. https://doi.org/10.1080/00045608.2011.627058. Crowley, D. N., Breslin, J. G., Corcoran, P. and Young, K. (2012). ‘Gamification of citizen sensing through mobile social reporting’. In: 2012 IEEE International Games Innovation Conference (Rochester, NY, U.S.A. 7th–9th September 2012). https://doi.org/10.1109/igic.2012.6329849. D’Hondt, E., Stevens, M. and Jacobs, A. (2013). ‘Participatory noise mapping works! An evaluation of participatory sensing as an alternative to standard techniques for environmental monitoring’. Pervasive and Mobile Computing 9 (5), pp. 681–694. https://doi.org/10.1016/j.pmcj.2012.09.002. https://doi.org/10.22323/2.18010202 JCOM 18(01)(2019)A02 17 https://doi.org/10.1057/jit.2014.26 https://doi.org/10.1109/escience.2015.64 https://doi.org/10.1145/2583008.2583011 https://doi.org/10.1145/2556420.2556502 https://doi.org/10.1016/j.compedu.2012.03.004 https://doi.org/10.1080/00045608.2011.627058 https://doi.org/10.1109/igic.2012.6329849 https://doi.org/10.1016/j.pmcj.2012.09.002 https://doi.org/10.22323/2.18010202',\n",
              " \"1424-8071-2-CE The Journal of Community Informatics ISSN: 1721-4441 Articles Toward a Sociocultural Learning Theory Framework to Designing Online learning Communities in Citizen Science How can sociocultural learning theory inform design principles for citizen science online learning communities to inspire local environmental action? The purpose of this article is to identify themes in sociocultural learning theory that could inform the use and development of highly collaborative online learning communities that utilize community informatics tools for citizen science to enable on-the-ground environmental actions. Applying previously established socio- cultural theories provides an opportunity to build on what’s already known about how people learn and collaborate. Finally, this article explains how communities of practice theory, knowledge building theory, and place-based education theory can be woven together to create the basis for development of a conceptual framework. !4 Ruth Kermish-Allen Maine Mathematics and Science Alliance, United States Corresponding Author. rkermishallen@gmail.com Kate Kastelein Maine Mathematics and Science Alliance, United States kkastelein@mmsa.org Kermish-Allen, R., Kastelein, K. (2017). Toward a Sociocultural Learning Theory Framework to Designing Online learning Communities in Citizen Science. The Journal of Community Informatics, 13(3), 4—19. Date submitted: 2017-11-02. Date accepted: 2017-12-15. Copyright (C), 2017 (the author as stated). Licensed under the Creative Commons Attribution- NonCommercial-ShareAlike 2.5. Available at: www.ci-journal.net/index.php/ciej/article/view/1424 http://www.ci-journal.net/index.php/ciej/article/view/1424 mailto:rkermishallen@gmail.com mailto:kkastelein@mmsa.org http://www.ci-journal.net/index.php/ciej/article/view/1424 The Journal of Community Informatics ISSN: 1721-4441 Introduction People want to learn with technology, with one another, in their own time, in their own place, and do things that matter (Fadel & Lemke, 2006). To meet this demand, new digital opportunities have quickly worked their way into educational contexts. People around the globe are using new tools within community informatics to understand their environments better and share those understandings as they exponentially amplify connections across the globe (Dickinson, Bonney, Fitzpatrick & Louv, 2012). For example, community activists in Peru are connecting with scientists in Pennsylvania via online communities to learn about new methods to determine the amount of lead and other chemicals in drinking water near resource gas extraction sites. Indigenous students in Alaska are connecting with climate scientists to document changes in the sea ice at villages where their families have lived for generations. Citizen science is a fast-growing sector of informal science education that is working hard to leverage global connectivity and improve the health and sustainability of communities. Citizen science originated as a way for the general public to assist scientists in collecting data for their research, as well as a vehicle to communicate aspects of science to the general public (Bonney, Ballard, Jordan, & McCallie, 2009). But what do the citizens get in return? How can the citizen and his or her community be seen more as a partner and beneficiary in citizen science projects? A project has the potential to go beyond learning about the monarch butterfly, for example. Instead it can bring people together to understand how their region relates to the butterfly’s migration routes and life cycle, as well as what they can do in their everyday lives to address the problems facing the monarch and the ecosystems upon which it depends. Learning theory plays a very important role in the realization of the vision for citizen science described above. Within the field of community informatics, applying existing theory to a new platform, such as an online citizen science community, plays a key role in providing greater understanding to what we already know about technology in online communities (Willams & Durrance, 2008). Timothy Kochmann (1996) coined the term computer-supported collaborative learning (CSCL), to define a new educational paradigm which focuses on the use of technology as a tool within collaborative methods of instruction. As stated by Dennen & Hoadley (2013, p. 392): “The design of CSCL is not to define a specific learning theory or content domain to be covered and the optimal way to cover it...instead CSCL instructional theories often specify roles, norms, values, or other process oriented aspects of the learning environment. The CSCL designer gives up control of many instructional choices that would be normal in the traditional design of non-collaborative environments - in exchange the designer can tap into powerful (if unpredictable) social processes to help drive learning.” In this spirit of adventure, the purpose of this article is to identify themes in educational theory (cognitive and instructional) that could inform the use and development of highly !5 The Journal of Community Informatics ISSN: 1721-4441 collaborative online learning communities for citizen science. This theoretical exploration will offer thoughtful research-based guidance to designing strategies that link the fields of community informatics with citizen science to inform the design of online communities with the goal of placing power into the hands of those who need it most, the citizen scientists and community members looking for answers to local questions. To accomplish this goal the article will first provide an exploration of the tenets of Vygotsky’s sociocultural learning theory as well as more contemporary perspectives. Next, the article delves into the applications of sociocultural theory in related instructional theories such as: Wenger’s Communities of Practice (CoP) (Lave & Wenger, 1991; Wenger, 2000b), Scardamalia and Beretier’s Knowledge Building Theory (Scardamalia & Bereiter, 2006), Gonzalez and Moll’s Funds of Knowledge (Gonzalez, Moll, & Amanti, 2005; Moll, Amanti, Neff, & Gonzalez, 1992), and Sobel’s Place-based education (Gruenewald & Smith, 2008; Sobel, 2005). !6 The Journal of Community Informatics ISSN: 1721-4441 Linking Sociocultural Education Theory with Citizen Science Collaborative and co-created variants (Bonney, Ballard, Jordan, McCallie, Phillips, Shirk & Wilderman, 2009) of citizen science align well with sociocultural learning theory due to the strong recognition and value of cultural and historical perspectives that individual participants can bring to the study. Vygotskian sociocultural approaches are based on the concept that human activities take place in cultural contexts and are mediated by tools, language, and other symbols that can be best understood when investigated in their cultural and historical settings (Kozulin, Gindis, Ageyev, & Miller, 2003). Sociocultural perspectives on learning have many common threads including: 1) the importance of tools, both socially and culturally constructed; 2) the need for a diverse social circle, including lesser and more experienced individuals; and 3) pedagogies and contexts that respect cultural and historical perspectives (John-Steiner & Mahn, 1996). Sociocultural learning theory moves away from the norm of learning as an individual enterprise and instead places emphasis on the social processes of the co-construction of knowledge (Tobin, 2014). From a sociocultural perspective, an individual is always closely related to the social spheres and groups within which he or she functions, thus the goals of an individual are closely related to the group’s motives and purpose (Tobin, 2012). Therefore, productive learning environments foster opportunities for individuals who not only act for themselves but also promote their own achievement to expand the learning of others (Tobin, 2014). This approach to learning favors a co-production model characterized by a redistribution of the traditional roles of participation in the production of scientific knowledge (Cook, 2015). Co-created and collaborative forms of citizen science build upon sociocultural learning theory as projects ask participants to not only gather data but also revise data collection protocols (in partnership with scientists) to fit within cultural norms. In some instances, projects provide the opportunity for participants to ask their own questions related to the local contexts in which participants live. When projects take this approach they are gaining the benefit of local social and historical contexts to make the overall project goal relevant to the learner and his or her community. For example, a sociocultural perspective would challenge individuals involved in study with the goal of increasing the frequency of the identification of new or invasive species in a key fishery’s ecosystem to work closely with the communities that depend on that fishery – valuing and respecting their cultural norms, incorporating their values and traditional knowledge into data collection methods, and asking them for assistance in the interpretation of findings and recommendations for increasing populations. Sociocultural theory also provides insight into the design and use of technology- mediated learning environments, such as online contexts. Using a socio-cultural approach to develop online learning communities provides a lens for investigating the interconnectedness between the individual and social spheres mediated by modern technology (Bencze & Alsop, 2014). Applying these understandings provides a platform to build on what we already know about how people learn and collaborate to guide the !7 The Journal of Community Informatics ISSN: 1721-4441 development of effective online citizen science communities. Empowering communities and individuals to ask their own scientific questions, using new tools to understand their environments better, sharing those understandings with a broad audience, and amplifying connections across the globe (Mueller, Tippins & Bryan, 2012) as those involved build healthier and more sustainable communities (Jenkins, 2011). This article explores four variants of sociocultural learning theories that provide solid ground upon which to build a new framework for collaborative online learning in citizen science. Communities of practice theory, knowledge building theory, place-based education theory, and funds of knowledge theory are all discussed in detail. This article will first introduce each theory and then describe theory-driven design principles for online communities, to provide design principles and integrate concepts from all four theories for the basis of a conceptual framework. Communities of Practice Theory A Community of Practice (CoP), as defined by Lave and Wenger (2000), is any group of individuals working in relation with each other and the world through a shared set of practices to accomplish a shared enterprise or goal (Lave & Wenger, 1991). The theory’s main assumption is that learning occurs through social participation (Wenger, 2000a). Participation in this sense refers not just to being engaged in local events with specific people, but to a more all-encompassing process of becoming active participants in the practices of social communities and construction of identities in relation to those communities (Wenger, 2000b). Kisiel (2010) expanded on these ideas with the introduction of intersecting CoPs that bring together various CoPs to develop new goals together utilizing a combination of each CoP’s original shared practices. There are many questions in science that cannot be answered by one CoP alone. Citizen science projects have the potential to leverage both the CoP and intersecting CoP models to advance scientific and educational goals. As the problems our communities face become more complex, the assumption that individuals can solve a problem alone, or the “expert as savior” mentality, has melted away. It is becoming clearer that we need CoPs that are continually building knowledge together to learn from the past mistakes of others to share all possible resources to combat the problems at hand. Utilizing an online based citizen science system can link members of communities that may not normally interact with each other. By building relationships between a diverse range of community members, a collective sense of community power may be established (Stoecker, 2005). Examples of this are often seen in understanding and mitigating the impacts of climate change on fishing communities, for instance. Scientists have realized that they need the traditional knowledge, expertise, and access that generational fisherman have to the populations and fishing grounds. At the same time, fishermen acknowledge that the marine ecosystem they know and love is changing and they need the partnership of the scientific community to understand these changes and develop their industry around sustainable practices that will ensure their livelihoods. !8 The Journal of Community Informatics ISSN: 1721-4441 Some basic design principles emerge from this exploration and critique of CoP that could be built upon to develop online learning communities – they are shared below: 1. Online learning communities (OLC) should connect individuals who have a shared repertoire – use the same resources (same tools, artifacts, experiences, definitions) to accomplish the shared goals of the community (Hoadley & Kilner, 2005). 2. The goals and/or requirements of the online learning community should be defined and negotiated informally by members of the OLC – an example of joint enterprise (Hoadley & Kilner, 2005). 3. The overlapping purpose or joint enterprise of the citizen science OLC should unite, motivate, and, in part, validate the activities of the OLC as significant. (Barab & Duffy, 2000). 4. Having a defined central purpose of the OLC can provide a starting ground from where members can begin to develop relationships and take on roles within the OLC (Hoadley & Kilner, 2005). 5. OLCs should provide tools and associated practices that the community needs to solve an authentic, real-world problem (Jonassen & Land, 2012). 6. Both novices and experts should be valued in the OLC (Barab & Duffy, 2000). 7. OLCs can provide opportunities for mutual engagement, referring to actions and especially interactions which members of the community share (Wenger, 2000b). 8. OLCs should provide a shared repository of information resources that are used by the community in its practices (Roschelle, Pea, Hoadley, Gordin, & Means, 2000). CoP as an instructional theory provides a magnificent basis upon which to build online learning communities for citizen science, but it does not go far enough. For example, CoP theory refers to learning as a linear process that moves along a continuum from novice to expert, as in the apprenticeship model from which CoP theory emerged. This is an important aspect of learning, but it does not value multi-directional learning in which the apprentice can also provide learning experiences for the mentor. A citizen science example of multi-directional learning could be when a local fisherman shares knowledge of where a specific species of interest can normally be found with a scientist known as an expert in the species. Furthermore, CoP theory only places emphasis on groups that already have a shared repertoire as they use the same resources (same tools, artifacts, experiences, definitions) to accomplish the shared goals of the community. CoP theory does not discuss the rich learning experiences that occur when groups of people that do not have a shared repertoire come together and work toward a common goal. The recognition of this gap is an outstanding opportunity to advance CoP theory. Researchers such as Kisiel (2010) are beginning to enter into this gap to offer alternatives such as intersecting CoPs which value bringing together multiple CoPs to solve a problem across a variety of shared repertoires. !9 The Journal of Community Informatics ISSN: 1721-4441 Lastly, in the CoP model, theorists state that most CoPs have a shared repository of the knowledge for that community. This component, of course, is necessary to a successful CoP as seen in indigenous or traditional cultures that use multiple methods to store and share traditional knowledge. This concept is not new and has been shared from generation to generation over millennia. Shared repositories are not static but ever changing as evolving members of the CoP critique, refine, and use the knowledge of the CoP in new and more productive ways. Through the technological capabilities we have available to us today we can modernize this concept and share these forms of knowledge with communities across the globe. These expansions of CoP theory provide the groundwork for a new framework which builds off of the strengths and potential new applications of CoPs. Knowledge building theory, which is a variant of CoPs, provides greater emphasis on the actual mechanics of how knowledge is created rather than how it is shared. Knowledge Building Theory Knowledge building - the creation of knowledge as a social product - is something that scientists, scholars, and employees of highly innovative companies do for a living (Bereiter, 2005). Knowledge Building (KB) theory, defined by Scardamalia and Bereiter (1994), is a particular kind of community of practice that has the explicit goal of developing individual and collective understanding (Hoadley & Kilner, 2005). KB should not be confused with knowledge dissemination, which is defined as the transfer of knowledge across settings. There is an area of overlap between knowledge dissemination and KB, but that distinction is not central to the purpose of this article and KB’s role. In short, knowledge dissemination is the process of sharing knowledge from an individual entity to others. KB is built upon: 1) a shared commitment of the community to generate new knowledge; 2) the importance of discourse; 3) the ability to build upon past knowledge, ideas, and artifacts; 4) shared responsibilities across the community for collaboration and decision-making; and 5) the importance of new and emerging sub-goals (Zhang, Scardamalia, Reeve, & Messina, 2009). Knowledge building represents an attempt to re-invent formal education to initiate students into a knowledge creating culture, involving learners not only developing knowledge building competencies but also coming to see themselves and their work as part of the civilization wide effort to advance knowledge (Scardamalia & Bereiter, 2006). Scardamalia and her colleagues (Hewitt & Scardamalia, 1998; Oshima, Scardamalia, & Bereiter, 1996; Zhang et al., 2009) have further defined four essential design principles, or opportunities for engagement, that must be present for knowledge building communities to function: 1. Collective Cognitive Responsibility requires taking responsibility for the state of public knowledge (Zhang et al., 2009), anticipating and identifying challenges and solving problems, and collectively defining knowledge goals as they emerge throughout the process. !10 The Journal of Community Informatics ISSN: 1721-4441 2. Awareness of Contributions implies that there is a collective responsibility to knowing the ‘from where’ and ‘from whom’ information, actions, and goals have emerged, as well as understanding the changing goals, situations, actions, and connections in a community (Zhang et al., 2009). 3. Complementary Contributions (Zhang, 2009) respond to and build upon one another’s ideas (Palincsar, Anderson, & David, 1993) and contribute non- redundant and important information that advances the pursuit of knowledge as a whole. 4. Distributed Engagement (Zhang et al, 2009) provides a framework for high- level operations such as community coordination, goal setting, and decision making to be completed across the entire community with minimal hierarchical control. Knowledge building has strong core principles defining how knowledge is built in a CoP and is rooted in the power and ability of learners (specifically students) to truly advance knowledge in society. But it does not explain the interactions between overlapping communities, such as exchanges among inter-generational or multiple stakeholder-based communities. This opens up the opportunity for a new form of knowledge building community, one that reaches outside the boundaries of the classroom to include members of the greater community in knowledge building. This essential theme of bringing groups together to build knowledge is paramount in the realm of citizen science. Citizen science hopes to partner the goals and research questions of the scientific community with the large numbers and interests of the general public. Additionally, citizen science aims to answer scientific questions that cannot be answered by one discipline alone, but depend upon the coming together of many areas of expertise and ways of knowing. Therefore, the question becomes, how can design principles for online communities in citizen science foster and value multiple stakeholder perspectives? To address this question, an interwoven conceptual framework should build upon learning theories that place particular emphasis on the expertise of the learners themselves and how their life experiences can be extremely valuable assets to a learning community. Funds of Knowledge Funds of Knowledge (FoK) theory places emphasis on historically accumulated and culturally developed bodies of knowledge and skills essential for survival, success, and well-being (Gonzalez et al., 2005; Moll et al., 1992). In Moll’s (1992) investigations into knowledge exchange in immigrant communities she found that each household held accumulated bodies of knowledge based on the family members’ life experiences, including agricultural, socio-political, and historical knowledge. The methods for knowledge transfer in the home and community setting was in stark contrast to the experiences of the community’s youth in the formal classroom. FoK theory investigates how this accumulated knowledge from life experiences can provide value and meaning for formal and informal learning experiences (Gonzalez et al., 2005). The findings of Moll and her colleagues identified strategies for how families develop social networks !11 The Journal of Community Informatics ISSN: 1721-4441 that interconnect them with their social environments and how those relationships share and build new knowledge, information and resources related to a households' ability to survive and thrive in sometimes very difficult situations (Moll et al., 1992). These same strategies could prove very useful in the exchange of information and knowledge building between diverse stakeholder groups, such as those in citizen science. In these settings, sharing individuals’ funds of knowledge can accomplish the shared goal of the group. Moll’s core concepts provide valuable insight for potential design elements in online learning communities for citizen science: 1. Place value on each individual’s historically accumulated and culturally developed body of knowledge and skills – fund of knowledge (Gonzalez et al., 2005). 2. Provide opportunities for community member to interact in adaptive, flexible, multi-dimensional ways to encourage multiple forms of relationships between individuals (Moll et al., 1992). 3. Provide opportunities for connections between members to become reciprocal and build trust over time instead of becoming buried at the end of an activity feed (Moll et al., 1992). Citizen science projects ask participants to share components of their individual fund of knowledge based on where they live and what they see around them every day. An example is India’s People’s Biodiversity Register that asks residents to share historical and current information on dwindling numbers of the Siberian crane or the Vital Signs project’s request for local gardeners to share their historical understanding and current knowledge of invasive plant species in their region. FoK theory can provide insight into design elements that illicit local traditional knowledge from individuals who may not necessarily see that know-how as important or beneficial to the scientific community, when in fact that information can be more powerful than any expensive monitoring device. In addition, the questions posed by citizen science projects are usually inter-disciplinary in nature. Therefore, citizen science communities will be comprised of individuals or stakeholder groups that may not naturally seek each other out. FoK theory can contribute greatly to the development of both design and instructional elements for citizen science projects. The marriage of ideas between citizen science and FoK could prove useful in accomplishing the goals of the citizen science field, especially locally driven initiatives. Place-based Education The application of funds of knowledge usually occur in a specific place, for example the knowledge built up over generations about how to respond to periods of drought in local farming regions. The construct of place is very important when contextualizing this type of knowledge. Place-based education theory provides guidance for how to structure learning experiences grounded in questions of place. Place-based Education (PBE) promotes interdisciplinary learning rooted in the local community to accomplish both academic- and civic- engagement goals, while at the same time providing learners with !12 The Journal of Community Informatics ISSN: 1721-4441 the experiences and confidence to believe that they can influence positive change in their communities (Gruenewald & Smith, 2008; Smith & Sobel, 2010; Sobel, 2005). Designers of successful classroom interventions must make sure they are engaging enough to seduce learners into the world of learning (Brown, 1992). PBE takes this advice to heart by engaging most actively in topics that are framed with a high level of personal relevance and authenticity (Sobel, 2005). PBE begins locally and answers questions that are relevant to that community. Learning environments present core concepts through a locally-framed lens, leading to high levels of ownership and engagement (Chawla & Cushing, 2007a). Accomplishing the civic and academic goals of PBE requires a great deal of skill development, specifically around problem solving, communication and collaboration. It is not enough for learners to learn beliefs and values about what they should do, they need opportunities to learn what they can do (Chawla & Cushing, 2007b). The key to PBE is that participants are learning about how they can influence their own community and see the change that they are capable of bringing about through partnerships with other groups in the community. PBE fits squarely within a sociocultural paradigm as it provides guidance for how to apply learning concepts in a locally relevant way. Duffin and colleagues (2008) reviewed educational literature and reported six core qualities essential to PBE that can serve as design principles for online learning communities in citizen science: 1. Focus on topics that are relevant to learners; 2. Involve experiential and hands-on activities; 3. Promote understanding of concepts; 4. Use the local environment as a context for learning; 5. Learners work individually and in groups; and 6. Incorporate project-based work; The focus on influencing locally relevant problems and questions fits perfectly with citizen science, since one of the goals of citizen science is to gain access to local data across a very large geographical span with the aid of locally trained volunteers. PBE by nature is small in scale and locally contained. An opportunity for growth within PBE lies in partnering the power of online environments with the power of local questions. Many of today’s environmental challenges have the potential to unite learners from around the world, if they have access to each other. Community informatics provides insights from online citizen science communities which may be utilized to improve a number of areas including sustainability, educational, and economic issues (Eagle, Hague, Keeble, & Loader, 2005) Studies of invasive species, for example, or farming in drought conditions, provide the opportunity to connect with others trying to find solutions to similar problems. However this is only possible if the online communication tools we have available to us today are brought to the forefront. Together individuals in online communities learn how to combat the issue in question through sharing experiences of their own place. In !13 The Journal of Community Informatics ISSN: 1721-4441 many situations, such as climate change, solutions to the problems being faced by local communities cannot be understood or solved without the complex coordination of many different communities (and places) sharing what they are experiencing and what their strategies for solutions are. Online communities for citizen science may provide a vehicle with which to meet this goal of starting with the local and reaching out across a much wider distributed population. In addition, incorporating PBE with online communities will provide new opportunities for learners to get out into their world and gain access to new places across the globe that they would not have experienced otherwise. Developing a Conceptual Framework Socio-cultural theory provides an established basis from which a conceptual framework can be built. Communities of Practice (CoP) focuses on how a group of individuals work and learn together. Integration of CoP theory helps inform how an online community might function. Place-based education (PBE) links the work of the community to the relevant interests and place of the participants. Knowledge Building (KB) guides the community with the intention to build new knowledge together related to the citizen science question mind. Funds of Knowledge (FoK) provide the framework with guidance for how to value diverse lived experiences and not just the “usually suspected” forms of expertise. This conceptual framework incorporates diverse participant groups, real-world investigations rooted in place (local contexts), valuing lived experience as essential to building new knowledge, a recognition that knowledge generation is not a top-down process but instead a dynamic multi-directional process between participants, and finally leveraging the power of a digital culture to build a knowledge-building community that transcends geographic limitations of traditional place-based education to answer questions people care about. Image 1 below summarizes the key ideas and design principles in each of the targeted sociocultural instructional theories explored. When looked at as a whole, we can begin to see core themes emerge. For example, the recognition of diverse participant groups is clearly present in both Funds of Knowledge (FoK) and Place-based Education (PBE) instructional theories. Have a shared a shared purpose of the learning community with user-defined goals or sub-goals is a strong theme across CoP, Knowledge Building (KB), and PBE. When looked at as a patchwork or interwoven quilt working together we see a a conceptual framework emerge – the Non-Hierarchical Online Learning Communities conceptual framework. “Non-hierarchical,” in this sense, is defined as a collaborative learning forum in which traditional experts, such as scientists are no longer perceived as the sole owners and creators of knowledge. Instead, all participants are generators of content and knowledge as well as active learners; the boundaries between scientist and citizen, young and old are blurred into one cohesive community of actively engaged learners. !14 The Journal of Community Informatics ISSN: 1721-4441 Image 1: Summary of the NHOLC Conceptual Framework ! This interwoven framework places emphasis on: 1. Bringing together diverse participant groups from widely differing areas of expertise to enable multi-directional learning opportunities in which everyone that joins the community has something they can offer and teach others within the community 2. Enabling participant-driven real-world investigations that are personally relevant to participants’ lives 3. Sharing project purpose and goals 4. Enabling communication structures to build relationships and roles amongst a diversity of participants 5. Sharing place-based data across geographic boundaries Conclusion Through this exploration of sociocultural learning and instructional theories the NHOLC framework has been developed in the hopes of providing learning theory-based guidance to the citizen science field in the design of online learning communities for citizen science. This framework, targeted at supporting collaboration is specifically !15 The Journal of Community Informatics ISSN: 1721-4441 designed to support more of the collaborative and co-created variants of citizen science gaining momentum in the world. As more and more individuals and communities throughout the world gain access to the digitally connected world, we can now leverage that connectivity to support change and action initiatives that matter to people where they live. We can bring together groups of people that would otherwise never have had the opportunity to learn from each other. A common goal may now bring together adversaries in a less-contentious online space to work together. There are still many unanswered questions, such as: does the NHOLC framework match up to existing examples of successful collaborative online citizen science communities; does one aspect of the NHOLC carry more weight than the others when working toward achieving collaboration; does the NHOLC framework match with the experiences of online citizen science community members? It is the hope and purpose of this theoretical exploration to offer thoughtful research-based guidance to find innovative strategies to link the field of community informatics with citizen science to inform the design of online communities with the goal of placing power into the hands of those who need it most: the citizen scientists and community members looking for answers to local questions. References Barab, S. A., & Duffy, T. (2000). From practice fields to communities of practice. In Theoretical Foundations of Learning Environments (pp. 25–55). Bencze, L., & Alsop, S. (2014). Activist Science and Technology Education (2014 edition). Springer. Bonney, R., Ballard, H. L., Jordan, R., & McCallie, E. (2009). Public Participation in Scientific Research: Defining the Field and assessing Its Potential for Informal Science Education. A CAISE Inquiry Group Report. Center for Advancement of Informal Science Education (CAISE). Brown, A. L. (1992). Design Experiments: Theoretical and Methodological Challenges in Creating Complex Interventions in Classroom Settings. The Journal of the Learning Sciences, 2(2), 141–178. Chawla, L., & Cushing, D. F. (2007a). Education for Strategic Environmental Behavior. Environmental Education Research, 13(4), 437–452. Chawla, L., & Cushing, D. F. (2007b). Education for strategic environmental behavior. Environmental Education Research, 13(4), 437–452. https://doi.org/ 10.1080/13504620701581539 Conover, S., Kermish-Allen, R., & Snyder, R. (2014). Communities for Rural Education, Stewardship, and Technology (CREST): A Rural Model for Teacher Professional Development. In J. MaKinster, N. Trautmann, & M. Barnett (Eds.), Teaching Science and Investigating Environmental Issues with Geospatial Technology (pp. 139–152). Springer Netherlands. Retrieved from http://link.springer.com/chapter/ 10.1007/978-90-481-3931-6_9 Cook, K. (2015). Democratic Participation with Scientists Through Socioscientific Inquiry. In M. P. Mueller & D. J. Tippins (Eds.), EcoJustice, Citizen Science and Youth Activism !16\",\n",
              " 'Design Principles of Online Learning Communities in Citizen Science ONLINE LEARNING COMMUNITIES MAINE POLICY REVIEW • Vol. 26, No. 2 • 2017 80 Design Principles of Online Learning Communities in Citizen Science by Ruth Kermish-Allen BUILDING BRIDGES BETWEEN COMMUNITY, SCIENCE, AND ACTION As we learn to use the connectivity available to today, the definition of community changes. Community is no longer limited to those organizations and individ- uals in our neighborhoods or specific locations. Online communities are another way to engage in community activities, from simple friendships to civic and political engagement (Lindros and Zolkos 2006). Our society retains a sense of community that is tied to place, while at the same time it is expanding to include a new global community (Maibach et al. 2011). Imagine the possibil- ities, not only for how quickly we can share, but for how quickly we can learn and create change. Maine is the perfect breeding ground for innova- tions using digital connectivity. Improved communica- tion in the form of expanding cellular and internet service has benefited Maine’s rural communities in many ways. Connecting isolated rural communities not only facilitates new opportunities for work and improved quality of life, but residents also see enormous opportunities for broadening the education and social experiences available to their children and for preparing them for the techno- logical innovations to come. These connections have also opened up the world of online communities to Mainers for a variety of purposes. In addition, digital connectivity has also opened up the world of citizen science to Mainers interested in participating in local and/or global scientific investigations. Citizen science projects have become a popular method for scien- tists to use global connectivity to collect data for their research as well as to communicate aspects of science to the general public (Bonney et al. 2009). But the level of citizen participation doesn’t need to stop there. The involvement of local people in all aspects of scientific inquiry through citizen science can lead to faster and more reliable data collection (Newman et al. 2010). This, in turn, can inform environmental decision making at a much faster rate than more traditional scientific approaches (Mueller and Tippins 2012). Citizen science can be more than just a service that the public provides for scientists. It can also be a tool for communities and individuals to ask their own scientific questions as they work toward building healthier and more sustainable communities. LEARNING FROM SUCCESS— A MULTIPLE CASE STUDY This paper explores three online citizen science communities that successfully leveraged digital connectivity and the power of citizen science to foster collaboration and environmental actions. In exploring how these online communities were designed and used by the participants, design principles for programmatic and technological features of successful online citizen science communities begin to emerge. Abstract Online communities for citizen science are expanding rapidly, giving participants the opportunity to take part in a wide range of activities, from monitoring invasive species to targeting pollution sources. These communities bring together the virtual and physical worlds in new ways that are egalitarian, collaborative, applied, localized and globalized to solve real environmental problems. Rural communities especially can leverage these learning and sharing spaces to take advantage of resources they would otherwise not be able to access. A small number of citizen science projects truly use an online commu- nity to connect, engage, and empower participants to make local change happen. This multiple case study looked at three online citizen communities that have successful- ly fostered online collaboration and on-the-ground environmental actions. The findings provide insight into potential design principles for online citizen science communities that support environmental actions in our backyards. ONLINE LEARNING COMMUNITIES MAINE POLICY REVIEW • Vol. 26, No. 2 • 2017 81 The three projects included in the study are the Gulf of Maine Research Institute’s Vital Signs project, the Maine Math and Science Alliance’s WeatherBlur project, and the international Public Lab project. Vital Signs links participants—ranging from students and teachers to master gardeners—from across the state via missions that provide a structure and connections with experts/scientists for identifying and documenting invasive species in the Northeast. WeatherBlur is a citizen science project that guides participants’ through the collaborative process to explore the local impacts of today’s shifting climate and weather trends from iden- tifying a common question to interpreting the data to inform local decision making. Public Lab is an interna- tional open online community where participants can learn how to investigate a wide range of environmental concerns using inexpensive DIY techniques, such as spectroscopes, air particulate sensors, water quality tests, and many others. Each of these projects resulted in online collaboration and local environmental actions. METHODS This two-part study attempts to understand what makes these kinds of online communities successful at transforming data collection into local action. In particular, the study focused on understanding the programmatic design elements and technological func- tions that support collaboration and environmental action in these projects. To tease out the components most essential for collaboration in these online communities, a Q-methodology or QSort (Stephenson 1935) was used to assess participants’ priorities about an issue. To under- stand each participant’s experience of the functions of the site and how it enabled or limited collaboration across the online community, a semistructured interview protocol and online observation tool was used. Initial findings were then shared with the focus group for refinement and reliability. The entirety of the study is grounded in sociocul- tural learning theory, specifically drawing upon the instructional theories covered by Communities of Practice, Place-based Education (Sobel 2005), Funds of Knowledge (Gonzalez, Moll, and Amanti 2005), and Knowledge Building (Scardamalia and Bereiter 2006). These sociocultural theories informed the development of the Non-Hierarchical Online Learning Community (NHOLC) conceptual framework (Figure 1) that iden- tifies some of the critical elements to creating an ideal online citizen science community committed to solving local and global environmental problems. All of the methods in this study looked specifically at how each project applies the core concepts of the NHOLC framework: • Bringing together diverse participant groups from widely differing areas of expertise to enable multidirectional learning opportunities in which everyone who joins the community has some- thing they can offer and teach others within the community. • Enabling participant-driven real-world investiga- tions that are personally relevant to participants’ lives. • Sharing project purpose and goals. • Enabling communication structures to build relationships and roles among a diversity of participants. • Sharing place-based data across geographic boundaries. The QSort asked participants to rank 49 statements based on their personal experiences of what made the online citizen science community that they participated in successful in fostering collaboration and supporting local environmental actions. The statements can be found in the appendix, which can be found on MPR’s Digital Commons site for this article. The findings reported here emerge from 15 QSorts and 20 interviews with individuals across the three proj- ects. Participants in this study represented the different types of groups that use each project, such as scientists or experts, project coordinators, and general citizen scientists including teachers and community advocates. FINDINGS Looking across the data, four themes emerge that seem to foster collaboration online to address local environmental issues. The key design principles (Figure 2) include (1) diverse groups with a wide range of exper- tise; (2) participant-driven real-world investigations that are relevant to participants’ lives; (3) access to tools and stories about past successes and failures; and (4) online activities combined with on-the-ground activities. ONLINE LEARNING COMMUNITIES MAINE POLICY REVIEW • Vol. 26, No. 2 • 2017 82 Diverse Participant Groups Participants across all of the projects agreed on a few statements. One of those statements was that “the different types of expertise present in the online learning community are a factor in making members feel like they are working toward the common goal of building knowledge together.” At the same time, community members across all projects also unanimously agreed, “the online learning community does not need to connect individuals who use similar resources for work (same language, tools, experiences, definitions).” Participants believed that projects are successful when they can connect with members who have expe- riences, information, or expertise that can help them reach the goals they have in mind. A Public Lab partic- ipant summed it up nicely saying, If it wasn’t for the site, I would never have known that there was a need for the expertise I have in these different contexts. I’d be off here in the middle of North Carolina, and I wouldn’t be connected with these people in Los Angeles, Peru, or India and places where they do fracking. Figure 1: The Original NHOLC Framework Knowledge Building (KB) Shared commitment to building new knowledge Knowledge is built through discussions among community members Awareness of the context of the community’s past knowledge Shared responsibilities for collaboration and decision making Build on past knowledge, ideas, and artifacts Emergent subgoals Communities of Practice (CoP) Learning within a community of shared practice Sharing tools and associated practices that the community needs to solve and authentic, real-world problem Experts teaching novices— linear learning Working together toward a shared goal Funds of Knowledge (FoK) Value each individual’s historically accumulated and culturally developed body of knowledge and skills Knowledge is based on what is needed for survival, success, and well-being in a given environment Flexible multidimensional inter- actions and communications to build relationships Place-based Education (PBE) Use the local environment as a context for learning Focus on topics that are relevant to learners Interdisciplinary learning rooted in the local community Opportunities to influence positive change local communities Work individually and in groups Incorporate project-based work Non-Hierarchical Online Learning Community (NHOLC) Conceptual Framework Communication to build relationships (FoK, KB, CoP) Real-world relevance (PBE, FoK, CoP) Shared purpose and user-defined goals (CoP,KB,PBE) Sharing place-based data (PBE, FoK) Diverse participant groups (FoK, PBE) ONLINE LEARNING COMMUNITIES MAINE POLICY REVIEW • Vol. 26, No. 2 • 2017 83 I wouldn’t have access to the questions they are all interested in, and I wouldn’t be able to contribute. Simply bringing together people with the same experiences and expertise will not create the type of rich, productive communities present in these projects. Access to Tools and Stories Across all projects, everyone agreed, “the online learning community needs to provide access to the tools and practices needed to solve authentic, real-world problems.” There are two key ideas built into that state- ment: first, access to tools and practices to do the work of the project and, second, solving authentic real-world problems. But, what do the terms tools and practices mean? In this case, they mean the methods of data collection, stories of local citizen science projects that share the lessons learned, methods of communication within the community, and information about how to do the work of the project. Everyone who participated in this study agreed that the online learning community needed to provide the opportunity for community members to share informa- tion with one another. Many of the participants in all three projects value a format that allows them to deter- mine quickly if material is relevant and usable. Whether that information is provided in narratives, databases, or maps, participants need to access the past knowledge of the online community to learn from it and apply it for their own purposes. In some cases, finding the information a member needs to advance her ideas can be difficult. To address this issue, the Public Lab and WeatherBlur use a recom- mendation list alert function. These online match func- tions connect individuals who can help each other meet their goals (for example, connect an expert in freshwater algae with someone trying to understand how algal blooms in a local lake are affecting fish). The function also highlights information related to each member’s interests that are hidden in the community and difficult to find otherwise (such as examples of how others gather data on algal blooms, what they found, and what they did about it). Interviewees from the other projects alluded to needing a function like this to foster more collaboration. In addition, all of the project participants agreed that an online community does not need to provide a variety of communication methods to connect members and build relationships. In fact, during the interviews, participants repeatedly mentioned that when there are too many options for communicating, it becomes over- whelming and actually hinders communications and relationship building. In the projects explored, it is clear that simpler is better. Providing a few targeted means of communication that are available to everyone is the best choice when designing for collaboration and action. In summary, to foster the types of collaboration and environmental action observed in the three proj- ects, the following technological tools and practices are important: • Provide access to knowledge from the commu- nity’s past experiences (for example, past studies, subprojects or investigations, data collection methods). • Present information in a format that allows members to quickly determine if what is presented is relevant and usable for them. • Connect members who have information or knowledge that others need. • Alert members to activities (in person and online) related to their interests and goals. • Offer a few accessible means of communication. Relevant and Participant-driven Real-world Investigations Relevance of the project to the community member emerges repeatedly in the data. As a Public Lab member stated, “People can work on things that are really important to them—it’s the people themselves who decided that it was important to them—and they are the ones working to figure it out.” The collaborations are driven by the participants’ knowledge that the project could result in improving life in someone’s backyard. A tool developer in Public Lab shared, Figure 2: Design Principles for Online Citizen Science Communities Design Principles Real-world Topics That Are Relevant to Participant’s Lives Diverse Participant Groups Tools and Stories of Success and Failure Bridge Online and Offline Activities ONLINE LEARNING COMMUNITIES MAINE POLICY REVIEW • Vol. 26, No. 2 • 2017 84 People can ask a question about their real-world environmental problem and other people, like me, suggest ways to deal with it. People post their new tool that measures some environmental variable and other people at the site can see that and say, “Oh, I could apply this to this particular environmental problem I have.” Members of Vital Signs highlighted the importance in collecting data that they knew was relevant and needed by scientists. This was a major driver in initial and continued participation that lead to new and exciting questions. As stated by a Vital Signs member, Once you’re going out into the field to learn about invasive species then that opens up a whole doorway of learning about what are the regulations around this species, why is this a problem, why are some invasive species desirable, what makes something invasive versus just intro- duced. So it’s a real-world problem that you’re introducing participants to, and they can have an impact on the issue at hand. On the other hand, when participants are uploading data but do not get any responses from experts to confirm or deny their findings, they quickly feel not valued. Many participants become discouraged when there are no comments or discussions related to their posts. How projects highlight the potential relevance of their work to community members vary, but they all use mapping, narrative, and discourse in various formats. Essentially, both visual and narrative stories are shared to help community members ascertain whether the infor- mation and resources provided are relevant to their interests and local real-world problems. Originally, the NHOLC framework assumed that the overall goals of the online learning community needed to be defined and refined by members. Instead, as seen in the findings from this study, there was consensus that it is not important for an online citizen science community to define and redefine its goals. To understand this better, the interview questions probed the contrast between individual goals and the project’s overall goals. It became clear that each participant joins an online citizen science community to accomplish a personal goal. While one’s personal goal aligns with the overall purpose of the project itself, the participants have specific outcomes in mind that they want to achieve. For example, an individual may join Public Lab because he wants to find new uses for a tool that he has designed, while another member joins to find a tool that can address the local environmental questions she is concerned about. In WeatherBlur, a research scientist may join the community to gain access to a population of individuals interested in topics related to her research, while a fisherman may join to connect with other fish- ermen. And in Vital Signs, a student joins because her class are taking part in a mission to find local invasive species, but a scientist may join to mobilize a network of individuals from across the state to look for a newly introduced species. The overall goal of the project might draw them into the community, but members need to be able to identify, share, and address their own subgoals or subprojects. When online communities provide exam- ples or stories of how members use the community’s resources to meet their own goals, new members report that they find it easier to understand how the commu- nity can help them meet their own personal goals. Online and On-the-Ground Activities One of the most intriguing findings from this research highlights the importance of balancing online activities and collaboration with on-the-ground activi- ties and relationships. As expressed by a WeatherBlur participant and echoed by participants across each of the projects, “We crafted our investigations offline with members of the local community, but we grew the investigations together with online community members from everywhere.” Relationships and connec- tions built in the online community cannot exist in isolation. In Public Lab, members often design and invite others online to attend in-person meetings to talk about an issue or learn a new skill. Successful projects found ways to use the online community to continue or deepen conversations that began in person or vice versa. It became clear that each participant joins an online citizen science community to accomplish a personal goal. ONLINE LEARNING COMMUNITIES MAINE POLICY REVIEW • Vol. 26, No. 2 • 2017 85 CONCLUSIONS As the digital world begins to connect the farthest reaches of the physical world, citizen science proj- ects designed with these research-based design princi- ples in mind can leverage that connectivity for greater impacts on local environmental activities. Applying these design principles leverages the power of online communities to gather, analyze, and share data that will shed light on ecological issues affecting commu- nities across the globe. In addition, these design prin- ciples can connect individuals across great distances to address those issues as they share stories of success and failure. In a rural state like Maine, the potential collective power of individuals using online citizen science communities is tremendous. Citizen scientists of all ages can learn, explore scientific investigations, gather and interpret data, and solve problems together to inform wide-ranging scientific studies as well as local environmental actions and decision making. The design principles discussed in this article summarize both the overarching design elements for developers of online citizen science projects and the needed tools and practices to realize this vision. This study adds to a growing body of literature focused on citizen science (Cronje et al. 2011; Druschke and Seltzer 2012; Newman et al. 2010). The design principles highlighted here serve as a starting point for others interested in designing engaging citizen science projects that build upon the power of both place and online collaboration to enable action in our own backyards. - REFERENCES Bonney, Rick, Heidi Ballard, Rebecca Jordan, Ellen McCallie, Tina Phillips, Jennifer Shirk, and Candie C. Wilderman. 2009. Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education. A CAISE Inquiry Group Report. Center for Advancement of Informal Science Education (CAISE), Washington, DC. Cronje, Ruth, Spencer Rohlinger, Alycia Crall, and Greg Newman. 2011. “Does Participation in Citizen Science Improve Scientific Literacy? A Study to Compare Assessment Methods.” Applied Environmental Education and Communication 10(3): 135–145. Druschke, Caroline G., and Carrie E. Seltzer. 2012. “Failures of Engagement: Lessons Learned from a Citizen Science Pilot Study.” Applied Environmental Education & Communication, 11(3–4): 178–188. https://doi.org /10.1080/1533015X.2012.777224 Gonzalez, Norma, Luis C. Moll, and Cathy Amanti (eds.). 2005. Funds of Knowledge: Theorizing Practices in Households, Communities, and Classrooms, 1st ed. Routledge, Mahwah, NJ. Lindros, T., and C. Zolkos. 2006. “Technology, Community, and Education in Neoliberal Society: A Review of Michael Bugeja’s Interpersonal Divide.” Student Affairs Online 7(2). Maibach, Edward W., Anthony Leiserowitz, Connie Roser- Renouf, and C.M. Mertz. 2011. “Identifying Like-Minded Audiences for Global Warming Public Engagement Campaigns: An Audience Segmentation Analysis and Tool Development.” PLoS ONE 6(3): e17571. https://doi.org/10.1371/journal.pone.0017571 Mueller, Michael P., and Deborah J. Tippins. 2012. “Citizen Science, Ecojustice, and Science Education: Rethinking an Education from Nowhere.” In Second International Handbook of Science Education, edited by B.J. Fraser, K. Tobin, and C.J. McRobbie, 865–882. Springer, Dordrecht. Newman, Greg, Alycia Crall, Melinda Laituri, Jim Graham, Tom Stohlgren, John C. Moore, Kris Kodrich, and Kristin A. Holfelder. 2010. “Teaching Citizen Science Skills Online: Implications for Invasive Species Training Programs.” Applied Environmental Education & Communication 9(4): 276–286. https://doi.org/10.1080 /1533015X.2010.530896 Scardamalia, Marlene, and Carl Bereiter. 2006. “Knowledge Building: Theory, Pedagogy, and Technology.” In Cambridge Handbook of the Learning Sciences, edited by K. Sawyer, 97–115. Cambridge University Press, New York. Sobel, David. 2005. Place-based Education: Connecting Classrooms and Communities. Orion Society, Great Barrington, MA. Stephenson, W. 1935. “Technique of Factor Analysis.” Nature 136: 297. Ruth Kermish-Allen is executive director of the Maine Math and Science Alliance. Her current research focuses on defining the essential design elements for online learning communities for use in citizen science projects, specifically those that foster online collabora- tion and local community actions.',\n",
              " 'Citizen Science 381 26 Learning and developing science capital through citizen science Richard Edwards1, Sarah Kirn2, Thomas Hillman3, Laure Kloetzer4, Katherine Mathieson5, Diarmuid McDonnell6 and Tina Phillips7 1 University of Stirling, UK 2 Gulf of Maine Research Institute, Portland, US 3 University of Gothenburg, Sweden 4 University of Neuchatel, Switzerland 5 British Science Association, London, UK 6 University of Stirling, UK 7 Cornell Lab of Ornithology, Ithaca, NY, US corresponding author email: r . g . edwards@stir . ac . uk In: Hecker, S., Haklay, M., Bowser, A., Makuch, Z., Vogel, J. & Bonn, A. 2018. Citizen Science: Innovation in Open Science, Society and Policy. UCL Press, London. https://doi.org/10.14324 /111.9781787352339 Highlights • Increased attention is focused on how to support and evaluate par- ticipation and learning through citizen science. • The dimensions of science capital provide a new framework through which to consider participation and learning. • The links between volunteers’ prior level of educational qualifica- tions and disciplines studied, and the learning they report from contributing to citizen science are not uniform across projects. • The levels and dimensions of volunteers’ engagement and learn- ing do not always reflect the intentions of citizen science project designers. Introduction Inclusiveness and learning are two concepts underpinning the principles of citizen science put forward by the European Citizen Science Associa- tion (ECSA). The learning of volunteers in citizen science and its educative https://doi.org/10.14324/111.9781787352339 https://doi.org/10.14324/111.9781787352339 CIT IZEN SCIENCE382 potential have been much discussed in recent years, as have the educational backgrounds and qualifications of those contributing to such projects (e.g., Bonney et al., ‘Citizen Science’, 2009; Garibay Group 2015; Haklay in this volume). There has also been growing exploration of how project design can affect the educational profiles of volunteers, the learning potential of projects (e.g., Phillips et al. 2014) and how projects may be designed to widen participation in citizen science (Novak et al. and Mazumdar et al., both in this volume). Overall, however, the educational impact of participating in citizen science has remained under-researched (see also Peltola & Arpin in this volume). Evidence is often anecdotal or based on evaluations rather than up-to-date learning theory and system- atic research (Falk et al. 2012). This is beginning to change and this chapter offers research evidence on learning through citizen science based on work that has been devel- oped in the United States and Europe. This is ongoing, so broad conclu- sions would be premature. Research on learning through citizen science is in its infancy and, while it can draw on wider research traditions in infor- mal science learning (e.g., Falk et al. 2012) and informal and experiential learning more generally, this is not yet fully the case. The chapter also seeks to make the case for considering learning through citizen science within a broader conceptual framework, that of science capital (Archer et al. 2015). The developing concept of science capital points to the itera- tive relationship between people’s dispositions towards science, partici- pation in science-related activities and science-related outcomes, including learning (DeWitt, Archer & Mau 2016). Basically, the more one is part of a culture of participation in science-related activities, the more one is likely to develop science learning outcomes and the disposition to participate further in science-related activities. In other words, developing science capital means developing a culture of participation in, and learning from, science-related activities, including citizen science. The concept of science capital is relatively new within research on science learning in general and at the periphery of research and practice in citizen science (Edwards et al. 2015). It provides a broader framework to consider issues of participation and learning in citizen science. It contrasts with many preliminary explorations of learning through citizen science, which focus primarily on what people learn and how best to evaluate learning outcomes while trying to draw connections to peoples’ motiva- tion to participate. These outcomes can be identified narrowly or broadly. Narrow science learning outcomes may embrace areas such as domain knowledge, for instance in relation to specific fauna or flora, or specific 383lEArning And dEVEloPing SciEncE cAPitAl tHrougH cit iZEn SciEncE scientific methods. Broader learning outcomes may embrace areas such as environmental stewardship and the development of science identities. While there has been a widening of ideas about what volunteers may learn from participation in citizen science, less attention has been given to how they learn; that is, the practices in which they participate that ena- ble learning when contributing to citizen science projects. Exploring how people learn focuses on the people and resources with which volunteers interact and how they engage with them to learn, if indeed they do learn. Better understanding how people learn can enable practitioners to better design projects or develop curriculum, training materials or professional development materials for teachers to enhance the educative potential of citizen science projects. Learning is not simply cognitive, but also social and cultural (Fenwick, Edwards & Sawchuk 2011), hence the interactions among volunteers or between volunteers and project coordinators, and facilitation thereof, should be carefully considered. If it is important to develop broad science-related outcomes, includ- ing learning, then exploring the social and cultural aspects of volunteer participation – the nature and extent of their science capital – and how these can be developed becomes important. Some citizen science practi- tioners are becoming interested, therefore, in how citizen science might enhance the building of science capital among volunteers – developing a wider culture of engagement in science-related activities – as well as their specific science learning through individual projects (e.g., Bailey 2016; Kirn 2016). This chapter suggests that an approach to developing citizen science projects that seeks to develop science capital could have positive benefits on the educational profiles of those who participate and enhance the edu- cative potential of citizen science. Science capital The concept of science capital has been developed from the work of the French sociologist Pierre Bourdieu. Science capital refers to the educa- tional qualifications, social networks, dispositions and behaviours among those working in, or engaged with, sciences (Archer et al. 2015). It is a subset of the social and cultural capital that accrues to individuals une- qually in society and results in the reproduction of those inequalities. In other words, inequality is not only economic, but is also social and cultural. CIT IZEN SCIENCE384 The existence of individuals and families with higher or lower lev- els of science capital, therefore, can be utilised to explain inequalities in participation in science-related activities and the unequal learning of science. It can also help to shape practitioner responses to this situation. Science capital can be seen as a resource to support the development of science learning and identities as part of a culture of engagement with science-related activities. Individuals and families may develop more or less science capital and the children of those families with most science capital are more likely to consider science education and a scientific career as options for their futures (Archer et al. 2015). Science capital helps explain the ways some people engage with and learn sciences, while oth- ers do not, and can also be considered an outcome of participation in science-related activities. In other words, the more one develops science capital, the more one is likely to participate in science-related activities, thus further enhancing one’s capital. Archer et al. (2015) identify eight dimensions of science capital: • scientific literacy; • scientific-related values; • knowledge about transferability of science in the labour market; • consumption of science-related media; • participation in out-of-school science learning contexts; • knowing someone who works in a science-related job; • parental science qualification; and • talking to others about science outside the classroom. Some of these dimensions may be used to design citizen science projects and develop pedagogical and other interventions that can build science capital and change current patterns of participation. Science capital also suggests that broadening participation in citizen science and enhancing its educative potential is not simply an educational issue, but also social and cultural. Citizen science projects may become a means to enhance volunteers’ science capital, but, at present, they seem to draw largely from populations with higher pre-existing levels of sci- ence capital. Refocusing attention on potential volunteers with lower science capital means addressing the wider cultural factors that influ- ence what and how people participate in science-related activities in society. Little research has yet been done to investigate how citizen science participation may increase science capital and the concept itself is still in development. More rigorous research drawing on the notion of science 385lEArning And dEVEloPing SciEncE cAPitAl tHrougH cit iZEn SciEncE capital is required before stronger claims can be made. This might involve new studies or the re-analysis of existing datasets. Some existing studies are discussed in the next section. Science capital and volunteer demographics Understanding who currently contributes to citizen science and their edu- cational and wider backgrounds is an area of concern. While there are significant attempts to widen participation and encourage diversity among volunteers contributing to citizen science projects, to date most surveys show that it is those that are older, more highly qualified and from higher socio-economic backgrounds who are most likely to participate (e.g., Gar- ibay Group 2015). In addition to these factors, gender and race are also significant in who volunteers in what types of citizen science project. In general, it is the already advantaged – those with the greatest social and cultural capital – who are most likely to volunteer. This is a pattern to be found in volunteering more broadly (European Foundation for the Improvement of Living and Working Conditions 2011). Determining the extent to which this is also related to higher levels of science capital, how- ever, means examining the specific scientific disciplines previously stud- ied by volunteers and the nature and level of their engagement in wider science-related activities. In two ornithology citizen science projects in the UK studied by Edwards, McDonnell and Simpson (2016), 83 per cent of respondents were male, 98 per cent were white and the largest proportion was in the 61–70 age range. As a proxy of their higher socio-economic status, 67 per cent of respondents had a university-level qualification. In other words, the majority of volunteers might be argued to have high social and cul- tural capital. Exploring further, the study found that large numbers of volunteers had gained either school and/or university-level qualifica- tions in the sciences. Therefore, a majority of volunteers could further be argued to have higher levels of science capital as the basis of their partici- pation in these citizen science projects than the wider population. To explore the impact of citizen science participation on the development of science capital, the study also explored volunteers’ enjoyment of partici- pation in a wider range of science-related activities, such as scientific hobbies or watching science television programmes as a result of partici- pation in the projects. Little overall affect was found. The building of sci- ence capital was not an explicit goal of the two projects studied, nor does it appear to be a significant implicit outcome. CIT IZEN SCIENCE386 Working with schoolchildren offers an opportunity to engage a pop- ulation with more diverse levels of science capital than would be the case through volunteer-based projects (see Makuch & Aczel; Harlin et al., both in this volume). In these cases, the student citizen science participants are not volunteering out of interest, but rather participating in a compulsory curriculum. Increasingly, citizen science programmes are designing expe- riences and curriculum that engage students in both practising the skills of science and interacting with the broader community of volunteers and scientists also participating in the project. For instance, the Gulf of Maine Research Institute’s Vital Signs programme (Kirn 2016) has developed novice-friendly protocols, standards-aligned curriculum, and professional development support and coaching for schools and teachers to facilitate the successful engagement of children in scientific investigations and pro- vide an opportunity for increasing science capital. Through Vital Signs, students practice scientific skills to explore their environments, collect rigorous observational data, conduct peer review of one another’s work, share data online, and engage in public discussion through the programme website with the scientists and natural resource managers using their data. Kirn notes how resources and protocols designed explicitly for novice volunteers, as well as interaction with experts, helps to encourage and sustain engagement and learning, contributing to an increase in science literacy. Additionally, these interactions between experts and novices give science novices the opportunity to get to know scientists and/or sci- ence enthusiasts with high science capital. Other providers of science-related activities, such as urban ecology centres and museums, link with citizen science projects to promote wider engagement and learning. Some citizen science projects, such as eBird at the Cornell Lab of Ornithology, have developed curriculum and profes- sional development materials for teachers to support engagement and learning. While valuable initiatives, the extent to which they continue to engage those with pre-existing higher levels of science capital rather than provide bridges for those with lower science capital remains unknown. How and what volunteers learn In addition to increasing knowledge in science content areas, some citizen science projects aim to increase science learning in the broader sense and include cognitive, affective, practical and behavioural outcomes (Bonney et al. 2016). Here, learning is not simply focused on the knowl- edge and skills relevant to the scientific goals of the specific project, but 387lEArning And dEVEloPing SciEncE cAPitAl tHrougH cit iZEn SciEncE extends to a wider engagement with science as a whole. For instance, learn- ing outcomes intertwined with environmental science knowledge include interest in science and the environment; efficacy to do and learn about sci- ence and engage in environmental activities; motivation to participate in science and environmental learning; understanding of the nature of sci- ence; acquisition of science enquiry skills such as data collection, analysis and interpretation; and involvement in environmental stewardship prac- tices outside of project activities (Phillips et al. 2014). It is in developing these broader learning outcomes and how they are enhanced that citizen science might be said to contribute to the building of science capital and a culture of engagement with science-related activities in society more generally. However, although many citizen science projects have successfully demonstrated an increase in participants’ understanding of science content and processes, fewer studies have examined wider outcomes. For instance, in their study of ornithology citizen science projects, Edwards, McDonnell & Simpson (2016) found that large percentages of volunteers identified themselves as learning something across a range of science-related out- comes. However, it was only in relation to ‘learning about the topic’ and ‘learning about data collection’ that volunteers identified themselves as learning a lot. Prior level of educational qualification, one marker of sci- ence capital, was significant here, as there were statistically significant dif- ferences between volunteers with or without a degree. Overall, the less qualified the volunteers, the more they evaluated themselves as learning across most of the outcomes. This suggests that those with a higher level of qualification are not being extended or are not extending themselves in contributing to projects – they are simply drawing on their existing levels of science capital. There are indications from this study that citizen science participation can enhance the learning of those with less science capital. However, existing research is not entirely consistent on this point. For instance, Kloetzer, Schneider & Jennett (2016) researched learning and creativity in nine online citizen science projects. Unlike Edwards, McDonnell & Simpson (2016), they found a very low correlation between level of education and self-reported learning. However, as with other stud- ies, they found also different degrees of participation among volunteers with a minority being more active than the majority. The degree of active participation was linked to the level of learning outcomes reported. The extent to which that participation was linked to prior levels of science cap- ital remains unknown. However, there are some indications that high engagement enhanced science capital as higher-order learning was related to active and social learning, and 37.5 per cent of the participants claimed CIT IZEN SCIENCE388 that participation in a citizen science project helped them discover a new field of interest. This shows the importance of examining not only who is participating in citizen science but also how they are participating and examining the impact of citizen science on volunteers within the frame- work of a wider culture of participation in science-related activities. Kloetzer, Schneider & Jennett (2016) identified a number of ways in which people learn through participation in citizen science: contributing to the project; using external resources; using project documentation; interacting with others and personal creations. These point to the rela- tional and material ways in which people learn, and the heterogeneity of learning outcomes and processes: people learn different things in differ- ent ways within the same project. In other words, how learning is designed into citizen science projects does not guarantee that volunteers will learn what is intended or in the ways planned. The mismatches between planned and actual outcomes is found elsewhere. Drawing on a study of six citi- zen science projects across a spectrum of contributory to co-created (see also Ballard, Phillips & Robinson; Novak et al., both in this volume, on these different types of participation), Phillips (2016) identified four different dimensions of engagement – behavioural, affective, effort and social – and various indicators of each. Significantly, levels of engage- ment were not directly related to the type of project as more co-created projects did not necessarily have a larger proportion of participants iden- tifying themselves as having higher levels of engagement. This suggests that what is planned and designed as learning does not necessarily result in the anticipated outcomes; volunteers engage with and make use of projects in unplanned ways and, as a result, learn different things. Evidencing participants’ existing expertise and how peer support occurs is another important element of citizen science practice. For instance, Hillman and Mäkitalo (2016) studied the learning of contribu- tors to Galaxy Zoo, an online international project to classify images of gal- axies (see also Haklay in this volume). They argue that online citizen science projects that focus on classification tasks tend to deliberately require relatively low skill levels since their goal is often to enrol as many volunteers as possible and render all their contributions equal in relation to the scientific protocol. Communities of volunteers were iden- tified as developing around classification tasks and it was activities in these communities that provided a rich source for learning. It was also in online discussion among these communities that the resources volun- teers drew on became visible through, for instance, moving from using URLs to newspaper articles or popular science websites to referring to more research-focused resources such as astronomical databases. Drawing on 389lEArning And dEVEloPing SciEncE cAPitAl tHrougH cit iZEn SciEncE a sociocultural conception of learning as the appropriation of cultural tools or resources, Hillman and Mäkitalo (2016) used changes in resource use in online forums as an indicator of learning and scientific literacy. In the discussion forums, those with less scientific literacy moved from the use of popular to more scientific resources, from curating content to for- mulating arguments, and from soliciting advice to providing guidance as contributors developed more familiarity and expertise. In particular, the authors argue that the appropriation of scientific resources is a strong indi- cator of scientific literacy and that progression along learning trajecto- ries is visible for new members of citizen science communities as they successively appropriate these resources. While such shifts are difficult to track in the more ephemeral interactions of face-to-face citizen science projects, the technologies of the internet often render them readily chart- able in relation to online citizen science projects. Tracing the activities of citizen science volunteers as they discuss online means that data produced can be argued to reflect trajectories in the building of science capital and reveal some of the means through which it can be built. Issues for the future It is clear from both the research and practice worlds that learning is occurring among volunteers in citizen science. Yet exploring how that occurs as well as what is learnt remains in its the early stages, and the picture emerging is complex, full of tensions and highly influenced by context. Prior qualifications, volunteer demographics, project design, par- ticipation and engagement are all significant. Examining these issues through a social and cultural framework and drawing on the dimensions of science capital could enable a better understanding of the dynamic interrelationship of these and other issues. At a broad strategic level, building a stronger international research base on learning in citizen science; relating it more clearly to wider educa- tional research; and engendering stronger relationships between research and practice are clear priorities for the future. More specifically, questions remain about the correlation between project design and learning out- comes; variations in the prior science capital of participants; and what and how resources are used in citizen science projects. Much research and eval- uation in citizen science to date relies on self-reported learning processes through surveys and less often interviews (see Kieslinger et al. in this vol- ume). There is a need for more refined, ethnographically informed studies to examine more closely how volunteers learn (see, for example, Peltola & CIT IZEN SCIENCE390 Arpin in this volume). Exploring the extent to which patterns emerge in relation to prior science capital, project design, participant recruitment, engagement and learning would be helpful for practitioners. The forms of project support for learning, and the possibilities for peer learning within the context of projects, are also of interest. The possible contribution of citi- zen science to enhancing science capital has also yet to be addressed fully, as has how best to support those contributing to projects with different levels of science capital. These are only a few of the issues emerging for research and practice as the field of citizen science expands. Conclusions At a policy level, the potential of citizen science to engage citizens in more informed debates on science and scientific issues as they relate to broader social, economic, environmental and cultural questions is becoming clear. In relation to education policy specifically, the continued growth of the links between citizen science projects and formal educational institutions is to be encouraged (see also Wyler & Haklay in this volume). Here citizen science can be rethought of as itself a form of pedagogy, and one with the capacity to increase learners’ science capital. The extent to which citizen science can build science capital and enable wider engagement with sci- ence-related issues, such as the impact of climate change, deserves fur- ther experimentation and investigation. In relation to the management of citizen science projects, a more explicit engagement with the issues of learning and science capital would be welcome when designing and resourcing projects. This entails more and greater systematising of relevant research, and developing more and better models of research-practice interactions. As the field of citizen sci- ence grows, there will no doubt be a related growth in the diversity of pro- jects and scientists seeking to engage participants or understand the dynamics of participation. Supporting that growth while enhancing the diversity of participants in citizen science and their learning remains a challenge, but one for which there is a growing evidence base. Acknowledgements Funding for the research projects reported in this chapter is gratefully acknowledged from the British Academy, European Union, National Sci- ence Foundation and Wellcome Trust.',\n",
              " 'Designing a Platform for Ethical Citizen Science: A Case Study of CitSci.org Lynn, SJ, et al. 2019. Designing a Platform for Ethical Citizen Science: A Case Study of CitSci.org. Citizen Science: Theory and Practice, 4(1): 14, pp. 1–15. DOI: https://doi.org/10.5334/cstp.227 CASE STUDIES Designing a Platform for Ethical Citizen Science: A Case Study of CitSci.org Stacy J. Lynn*, Nicole Kaplan†, Sarah Newman*, Russell Scarpino* and Greg Newman* Involving the public in scientific discovery offers opportunities for engagement, learning, participation, and action. Since its launch in 2007, the CitSci.org platform has supported hundreds of community-driven citizen science projects involving thousands of participants who have generated close to a million scientific measurements around the world. Members using CitSci.org follow their curiosities and concerns to develop, lead, or simply participate in research projects. While professional scientists are trained to make ethical determinations related to the collection of, access to, and use of information, citizen scientists and practitioners may be less aware of such issues and more likely to become involved in ethical dilemmas. In this era of big and open data, where data sharing is encouraged and open science is promoted, privacy and openness considerations can often be overlooked. Platforms that support the collection, use, and sharing of data and personal information need to consider their responsibility to protect the rights to and ownership of data, the provision of protection options for data and members, and at the same time provide options for openness. This requires critically considering both intended and unintended consequences of the use of platforms, data, and volunteer information. Here, we use our journey developing CitSci.org to argue that incorporating customization into platforms through flexible design options for project managers shifts the decision-making from top-down to bottom-up and allows project design to be more responsive to goals. To protect both people and data, we developed—and continue to improve—options that support various levels of “open” and “closed” access permissions for data and membership participation. These options support diverse governance styles that are responsive to data uses, traditional and indigenous knowledge sensitivities, intellectual property rights, personally identifiable information concerns, volunteer preferences, and sensitive data protections. We present a typology for citizen science openness choices, their ethical considerations, and strategies that we are actively putting into practice to expand privacy options and governance models based on the unique needs of individual projects using our platform. Keywords: citizen science platforms; governance; ethics; membership openness; data openness; personal privacy; openness typology * Colorado State University, Fort Collins, Colorado, US † United States Department of Agriculture, Fort Collins, Colorado, US Corresponding author: Stacy J. Lynn (Stacy.Lynn@colostate.edu) Introduction People have been collecting and interpreting observa- tions of the natural world for millennia (Miller-Rushing et al. 2012) and have had to decide who participates, what they observe, and how to manage the resulting informa- tion in ways that respect norms, access, sharing, privacy, and ownership (Bernholz and Ormond-Parker 2018). Tens of thousands of years ago, for example, aboriginal com- munities in what is now northern Australia developed systems related to managing information to help them survive. These systems delegated roles and responsi- bilities for information management to the people who possessed the skills necessary to perform them (Bernholz and Ormond-Parker 2018). Starting around 2010, in the small aboriginal town of Wadeye, community elders, local museum staff, and scholars began collecting this ancient and modern indigenous knowledge and making it accessi- ble via multiple digital media formats, all while encoding traditional rules of access (Bernholz and Ormond-Parker 2018). By codifying information with a metadata schema that enabled individuals to find only information to which they would traditionally have access, they enabled sharing information in ways that respected traditional norms, val- ues, and levels of comfort (Bernholz and Ormond-Parker 2018). Over the past few decades, technological develop- ments have facilitated significant growth in our ability to conduct and document observations through citizen science, bringing new challenges to information manage- ment and associated privacy (Bowser et al. 2017). We are http://CitSci.org https://doi.org/10.5334/cstp.227 http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org mailto:Stacy.Lynn@colostate.edu Lynn et al: Designing a Platform for Ethical Citizen ScienceArt. 14, page 2 of 15 witnessing exponential growth in community-generated data, information, knowledge, and wisdom arising from citizen science (Follett and Strezov 2015). To support this growth, our team at Colorado State University has been developing and maintaining a platform, CitSci.org, since 2007 to facilitate citizen science project creation and implementation. CitSci.org enables individuals and communities to cre- ate citizen-driven research programs to meet their needs and interests (Newman et al. 2011). The platform is unique among field-based systems by being transparent and cus- tomizable. Projects are created with a do-it-yourself (DIY) approach, and the platform supports heterogeneous data related to diverse topics. The entire research process, from asking questions through data collection and analysis, can often be managed entirely by the very people creat- ing projects. Project managers define what they wish to measure, document how to measure it, and build custom datasheets for project participants to collect data in real- time, online, using mobile applications no matter where they may be located. The design, development, and implementation of CitSci.org has been inspired by our previous research in (primarily) ecological systems (Crall et al. 2011; Crall et al. 2012; Crall et al. 2010; Gray et al. 2016; Newman et al. 2017; Newman et al. 2010a; Newman et al. 2012; Newman et al. 2010b); by published studies related to citi- zen science platform design (Edelson et al. 2018; Hoffman 2015; Sturm et al. 2017; Switzer et al. 2012; Yadav et al. 2015); by our understanding of citizen science volunteer needs (Longan 2007; Van Den Berg et al. 2009); and by engaging with our users to discuss CitSci.org features and functionality. Interactions with our users have included direct questions and communications via emails and phone calls, numerous surveys of our user base, two years of monthly Feature Friday sessions with our users and team to chart the path forward by responding to fea- ture requests and discussions, and our involvement in an NSF i-corps team that involved 120+ user discovery interviews in 2018 (unpublished data 2018). Since CitSci. org’s launch, hundreds of unique citizen science projects involving thousands of members have generated close to a million scientific measurements. As developers of this platform, we have witnessed peo- ple around the globe engaging in science, action, and policy based on their own interests in their communities and environment. Such engagement offers great poten- tial benefit for both science and society through learn- ing, participation, and action (Brossard et al. 2005; Crall et al. 2012; Frensley et al. 2017; Mathews 2014; Newman et al. 2017; Newman et al. 2012; Theobald et al. 2015). At the heart of the growing citizen science movement are deeply rooted and contextually appropriate values related to information sharing and use, as is evident from recent research (Bowser et al. 2017) and the increasing popularity of open science, open access, open source, and crowdsourcing movements. Yet, risks are also created by these new approaches and technologies as people, their actions, and their data become more visible and vulner- able (Bowser et al. 2017). Tensions arise between the value of information sharing and open access on one hand, and respect for the privacy and sensitivity of information on the other. Citizen science platform developers find themselves caught in the middle; they must negotiate these tensions efficiently and transparently by offering what they feel are appropriate options for both projects and participants, while at the same time communicating the nuances and implications of each option and the potential consequences of alternative choices. Project participants using these platforms – including community members, educators, scientists, members of the lay public, and other stakeholders – engage in projects in many capaci- ties and may serve various roles within them. These roles can involve setting the research agenda; articulating pro- ject governance structures; selecting protocols; collecting, analyzing, visualizing, interpreting, archiving and sharing data; informing decision makers; contributing code to applications; making instrumentation useful for projects (as in makerspaces); changing individual behaviors; and sharing results via social media, to name a few. Given this breadth of ways in which people partici- pate in citizen science, and the roles they can take on, our team set out on an adventure to develop CitSci.org to accommodate not only diverse questions and topics, but also multiple governance approaches and data access needs. Bernholz and Ormond-Parker (2018) describe four common values that help guide digital data use in the non-profit sector, including voluntary or permission-based participation, recognition of the private rights of individ- uals, a public benefit mission, and a pluralistic effort to engage diverse participants. Platform design facilitates our ability to achieve and operationalize these values as critical underpinnings of the citizen science agenda. We operate CitSci.org based on several core underlying ethical principles: Transparency, adaptability, humility, reflection, and what could be seen as our meta-principle of inclusiveness. Our users drive our platform develop- ment both directly via our interactions, and indirectly as our team meets to discuss and prioritize next steps based both on user feedback and on our grounding in the sci- ence and theory that informs citizen science practice. Over the course of a decade of continual and iterative platform development and improvement, we observed that the ability to customize membership structure and level of information privacy provides an opportunity to respond to both the goals of a project and its ethical needs and circumstances. Despite the current flexibility of our platform, and because we are continually challenged by our users and their needs, we continue to think strategi- cally about ways to accommodate varying needs related to data access, personal information, and privacy pro- tection. While our goal is to be able to serve as many of the unique needs of citizen science projects as possible, there are limits to what any given platform may be able to accommodate, especially given the costs of creating a flexible platform that attempts to meet the needs of hun- dreds or thousands of heterogeneous projects. Our pri- orities remain the development of platform options that best meet the most pressing needs of most projects and http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen Science Art. 14, page 3 of 15 maintenance of our platform’s long-term sustainability given what we know and have yet to uncover. As was famously said in 2002 by then-Secretary of Defense Donald Rumsfeld, “… as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don’t know we don’t know” (Davey Smith 2016). This quote highlights one of the challenges we face in citizen science platform develop- ment. There is a complex web of scientific, regulatory, and user/project-defined needs that must be responded to, in addition to the technical expertise and financial resources required to operationalize these needs within a platform. Users may not even be aware of some of these needs – the unknown unknowns – just as we were not aware of many of them when we began developing CitSci.org. The revela- tion of these unknown unknowns comes with experience. Platforms can expand the opportunity to digitally engage people who are interested in citizen science work, but who may not have the theoretical or scientific knowledge, technical expertise, time, or financial resources required to create a custom platform on their own. Indeed, citizen science should be accessible to the greatest number and diversity of projects and participants possible. Reducing barriers to access builds the social justice of citizen sci- ence opportunities and expands the question of “who can do science?” (Ottinger 2010) to include those who may be resource-limited or differently able. Platforms fundamen- tally determine who can participate and what they can do, providing various protections for people and information, but also raising some important ethical dilemmas. Based on our support and hosting of hundreds of projects using CitSci.org around the globe, this high- level view has revealed to us the impact of project-level governance models and possibilities, and the diversity of privacy and openness options that projects need and want to adopt. Like other platform developers, we have had to make design decisions that either constrain or bring flex- ibility to the projects on our platform. Because we strive to support diverse projects in many places on many topics, we have had to wrestle with the many different norms, values, and permissions that projects using our platform require or need to be made aware of, much in the same way that the Wadeye community has adapted and accom- modated protocols of information collection, access, use, and sharing over thousands of years. Goals While traditional academic research has a long and important history of addressing ethical concerns related to personnel and information management, some of these issues may not be at the forefront of citizen science project concerns at the time of a project’s design. In this paper, we use our decade of experience developing CitSci.org and engaging with the projects that we support to offer guidance related to project governance and privacy as seen through the lens of platform developers. We discuss the ethical challenges that we have faced during our ongoing platform design and development adventure, and our resulting thoughts on ethical platform design and use for citizen science. This paper covers the difficult intersection of theory and practice: How do we develop a platform that will succeed in both moving citizen science forward to meet underlying ethical requirements of all of our teams—and of science— while helping the greatest number of projects possible to do great science? What ethically must be protected and what must be made available? Who is in the driver’s seat? By contributing to the discourse of citizen science theory and practice, we hope to demonstrate our commitment to transparency by recognizing that our platform is not yet what we aspire it to be. We also want to acknowledge that being responsive to user needs is difficult: Even though we are aware of some of these needs given our experience in science, theory, and process, we have not always been able to meet them due to limited time and resources. We hope that this discussion will be useful for both other platform developers and project managers to help them assess the potential positive and negative trade-offs of opening or restricting information and participation, and to make decisions about who is granted power to make choices within their projects. More specifically, our goals are to reflect on our lived experience with our plat- form development and collective projects to: 1) Provide a conceptual framework for making decisions related to citizen science project governance; 2) Create a typology of citizen science project openness; 3) Discuss examples of how CitSci.org is working to address these scenarios; and 4) Offer recommendations for project managers regarding actions they can take during project design and implementation to create the most rigorous and ethical projects possible. We hope to contribute to answering the overarching question, “How can citizen science practition- ers balance their project’s unique aspirations and goals with contextual issues related to data governance, openness, and privacy, while acting ethically to protect information and people?” Citizen Science Governance Citizen science platforms are being created to host diverse project types being carried out by diverse leaders ranging from members of the lay public to highly trained research- ers. Each project needs to be able to justify its membership and data governance structure, because this is the structure that operationalizes ethical decisions. This structure frames options related to how members are identified, recruited, and allowed to participate; what scientific data are collected where and when; and what data (inclusive of metadata – the data about the data) are shared. Here, we list some of the many possible governance scenarios as examples: Scenario 1 A project may be created by a community group (for- mal or informal) that is concerned about a local natu- ral resource such as water quality in a local lake. The project members may choose a leader who acts as the project manager, and local NGOs may be invited by the community to participate or assist with establish- ing project and data collection guidelines based on http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen ScienceArt. 14, page 4 of 15 expertise that they may bring. All members may have equal access to all data. Scenario 2 A project may be created by a local community NGO in partnership with a researcher from a governmental organization such as a state natural resources depart- ment. The community organization that created the project may choose to have all data made available to all members of the project and the public, so long as these data are not sensitive. Scenario 3 A project may be created by a university researcher who recruits community members to participate in data collection that contributes to studying a re- search question related to a shared interest. The re- searcher may have primary access to all data collected by and about members. Alternatively, a project man- ager may have access to all research data, but not to the members’ personal private data unless individual members elect to share such information. The mem- bers themselves may have access to the subset of the data that they were involved in collecting, but not the full dataset, or they may be able to label a particular observation as sensitive (such as the location of an endangered species or sensitive human subjects data collected from project participants). Given the complexity of operationalizing these choices within a platform, the governance structure decision is higher-order than the individual choices themselves. We use these three scenarios to demonstrate the variability in citizen science project goals and participation that affect how choices related to data management can be pro- cessed and made. A Citizen Science Governance Framework From our experience developing CitSci.org, we believe that decisions related to governance –that is, the balance of decision-making power regarding who can make an information sharing choice about what information to share and when to share it—are usually best left to each project. Our platform thus offers flexible options. Here, we work through our development of approaches to Member Personal Privacy Choices, Project Membership Openness Choices, and Project Data Openness Choices based on a project’s Governance Framework. We find that choices related to project governance and the roles of dif- ferent project members generally occur within the realms of people-related and information-related decisions, and along a top-down versus bottom-up continuum. • People-related choices involve project member- ship (who can participate and how) and privacy of members’ personal data (whether project members’ personal information is visible or shared). • Information-related choices encompass privacy of both member personal information and scientific ob- servation-based data, as well as access to and ownership of these data (which information is shared and how). While member personal privacy choices can be classified as both people-related and information-related depend- ing on how personal data are being used (Figure 1), we will discuss member personal privacy choices as a distinct class of decisions, because personal privacy choices will not normally affect project structure and objectives, and are sensitive at the individual member level rather than at the project level. A top-down structure anchors the locus of project decision-making control in the hands of the platform developer, leaving fewer decisions to be determined by participating project managers and members. An exam- ple would be a platform that has a default global “open” setting; all projects that participate on the platform would be required to make their data available to the public, which is not appropriate for all citizen science projects and therefore would restrict participation on the platform. For a bottom-up approach, decisions about data sharing are placed in the hands of project manag- ers, providing options and flexibility for platform users to determine how they handle their own data sharing choices. Here we focus on three key choices that can be offered to platform users, within the context of the governance models that determine who can make them. Figure 1 portrays the three key choices that platforms can offer: 1) What member information is visible and to whom (henceforth member personal privacy); 2) Who can join a project (henceforth project membership openness); and 3) The visibility of project data (henceforth project data openness). As platforms are designed and devel- oped, considering with whom the locus of control over these choices lies is incredibly important, because—as we have learned firsthand—the flexibility to make such choices must be programmed from the outset into the platform itself, and it can be extremely challenging to retrofit choices into a platform with existing users. Through platform design, choices can be operational- ized for member personal privacy, project membership openness, and project data openness by using toggles at varying platform levels of operation, customizing the governance of each individual decision. Project structure may be more closed for some choices and more open for others, creating a hybrid model of openness. Location of these toggles determines who has control over each openness decision. Awareness of these three key decisions is important both for platform developers who design the underly- ing structure and user interface as well as for platform users. In addition to goal-related needs, contextual situ- ations that lie outside of an individual project’s control may inform or dictate choices (e.g., institutional regula- tory requirements, human subjects IRB review, and spon- sor/funder requirements). Evolving regulations such as the Global Data Protection Regulation (GDPR), data shar- ing policies of US agencies, and recent social media data breaches (Bloomberg 2018; Mele 2018; Rosenberg et al. 2018) have revealed new challenges requiring improved clarity and transparency. Each project group must assess both its goals and externally driven contexts to structure the project accordingly. The more flexible the options http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen Science Art. 14, page 5 of 15 available, the more nimble a project can be. Next, we pre- sent detailed considerations for each of the three classes of key decisions. Member personal privacy choices The first key choice relates to member personal privacy, demonstrating the common value of consent and permis- sion. Decisions related to what personal information is collected from project members may be determined at the platform level, the project level, or both. Assuming that legal and regulatory laws and policies are being fol- lowed, we suggest answering these questions during the project planning and design stages: 1) Which personal information is necessary to collect, 2) What should the default settings for sharing personal information be, and 3) What personal information sharing choices should be put into the hands of the volunteer? We find the most parsimonious solutions are those that focus on collecting only personal information necessary to ensure quality and integrity of the project and volunteer management; and on giving volunteers governance to choose how they are identified within the project and which data are shared, with whom, and when. Classes of potential viewers of this information include the project manager, fellow project members, other registered platform users, and the digitally connected global public. This diversity of questions may be overlooked by projects led by individu- als lacking experience with them, which may set up such projects for imbalance between project-related goals and the personal privacy needs or wishes of individual project members. Personal data can be documented either as personally identifiable contributions (those that display contributor true full names) or as anonymous contributions (those that obscure personal identifiers through the removal of the last name or creation of an anonymous user name). Personally identifiable information may be used for volunteer management when it is necessary or desirable to know specifically who project data contributors are, or when contributors would like recognition for their con- tributions. For example, being personally identifiable was likely a huge boost to Hanny, the Dutch school teacher who was recognized for her discovery of Hanny’s Voorwerp while volunteering with Galaxy Zoo (Clery 2011; Lintott et al. 2009). If she had hidden her identity and been com- pletely anonymous, she might not have been recognized and received credit. A hybrid approach would allow vol- unteers who are known by name within the confines of their project to be anonymized for the digitally connected global public. We believe that defaulting to an anonymized user name for public view—while offering the opportunity to display Figure 1: Citizen Science Project Governance Framework showing key decisions related to people and information that determine project membership openness, member personal privacy, and project data openness. Platform governance can present a more flexible bottom-up model (more choice) or a more rigid top-down (less choice) governance model. A platform’s governance model will determine its flexibility to accommodate projects with diverse needs by either dictating a single model or offering choices to participating projects and/or its members. Lynn et al: Designing a Platform for Ethical Citizen ScienceArt. 14, page 6 of 15 a full name—is a best practice for personal privacy pro- tection, reiterating a recommendation made by Bowser et al. (2017). Ultimately, the goal is to protect the pro- ject members and their personal privacy at a high level of protection or anonymity. There are valid arguments both for and against the collection and sharing of person- ally identifiable information, and each project needs to assess what is appropriate given its goals, objectives, and visibility. Many volunteers in the citizen science context, for example, seem more willing to share their personal information and are less concerned about privacy than in other contexts (Bowser et al. 2017). Factors influencing decisions about member personal privacy also relate to the sensitivity of information con- tained within a project’s volunteer database, or the tech- nologies used for data collection and the alignment of these technologies to the project mission (Bernholz and Ormond-Parker 2018). This is of great public concern given recent large data breaches and mishandling of personal data by organizations and third-party users (Bloomberg 2018; Mele 2018; Rosenberg et al. 2018), yet the public may be becoming desensitized to these risks (Vance et al. 2014). Not unlike other types of digital platform-based networks, citizen science projects may be at risk of per- sonal data breaches or data mishandling, underscoring the importance of establishing thoughtful and proactive member personal privacy protection policies. Guidance from platforms in raising questions related to the value of consent and permission may help project managers to make appropriate choices. Contextual requirements, regulations, laws, and poli- cies that guide platform developers, project managers, and others responsible for structuring or managing data- bases of participant information also must be taken into consideration. These requirements vary around the globe, and include sponsor-driven, organizational, national, and international policies. For example, projects with mem- bers living within the European Union (EU) must com- ply with General Data Protection Requirements (GDPR EU 2016/679). In addition to complying with contextual policies, project managers must consider the personal information privacy policies of third-party technologies used for project management and communication (e.g., document sharing platforms, social media tools, email campaign tools, and platforms such as CitSci.org). These technologies can present additional risks for revealing volunteers, which may conflict with a citizen science project’s mission. Thorough consideration of third-party toolkits’ data sharing policies is recommended prior to adopting their use (Bernholz and Ormond-Parker 2018). Project membership openness choices The second key choice for platform development is pro- ject membership openness, which relates to the degree to which participation is open to all members of the pub- lic. We use three general classes of openness in creating our typology. The most open and accessible projects are “crowdsourcing” projects that allow anyone, anywhere (with access to the project and interest in participating) to participate and contribute observations or perform citizen science tasks. The most restricted projects oper- ate on an “invitation-only” basis, with project managers targeting potential members who are desirable due to expertise, education, location, professional connection, or other criteria. Falling between these two extremes, other projects allow interested individuals to request to become members, with oversight over this decision and its criteria being in the hands of the project manager or other desig- nated individual or group. Project data openness choices Choices related to project data openness relate directly to data and metadata protection, privacy, access, and own- ership, as well as to decision-making governance. Citizen science project managers must grapple with the serious questions surrounding which and how much data col- lected by members should be viewable, and with whom these data should (or may) be shared. This concept relates to the need to recognize individuals’ control over their data and their associational and expressive rights. In field-based citizen science, many objects represent “what” data are being collected. These commonly include observations, locations, species, and individual measure- ments (including photos). Access to each of these objects can and should be considered individually and should respect various rules that dictate who can access them and for what purpose. In citizen science, there are compel- ling reasons to ensure that data collected by a particular individual should at the very least remain visible to that individual to “close the loop” between science and citizen scientist (Nov et al. 2011). Data protection is about securing data against unau- thorized use, whereas data privacy and access focus on who has data, who defines it, and who uses it. All protec- tions placed on data involve those who impose the protec- tions, and thus bring about issues of data ownership (and associated licensing, where applicable). More rigid and prescriptive top-down platforms make a predetermined choice of whether the platform, and therefore participat- ing projects’ data, are open or closed to public viewing and use, and do not allow projects to govern this choice due to platform inflexibility. More flexible platforms allow choices related to project data openness to be made by the project manager and/or participants, and possibly at multiple structural levels from the entire project to individual cells in a database. Such selections can involve opening or closing an entire project, opening up project metadata (information about the project) for public view- ing while keeping scientific data closed, giving the flex- ibility to close sensitive portions of a project (such as a data subset), or potentially allowing flexibility in project data openness to be set at the micro scale of individual columns or cells (individual data point) within a particular data set. We have come to believe that providing options to open or close data at the level of the individual data point brings an added benefit to a project, as data points that otherwise may have been left uncollected for fear of their being exposed, may instead be collected and pro- tected, leading to a more complete and representative data set for analysis and even reuse. http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen Science Art. 14, page 7 of 15 We have experienced arguments for the need to specify that all data at one project location must be kept private, and that other data within the project should remain fully open. Species observations often require special attention given various regulations and laws such as the Threatened and Endangered Species Act of 1967, which legislates pro- tection of species and, by extension, mandates protections for the whereabouts of these species of concern. For indi- vidual measurements such as photos and attribute types, there are circumstances where some measurements may need to be open access, while others need to be accessible only to project members, and still others accessible only to those contributing them. Human subjects informa- tion including personal identifiers such as name and data about the individuals who are subjects of research pro- jects (as opposed to project members) may be especially sensitive in cases of health-related or participatory citizen science projects (Kounadi and Resch 2018). The combina- tion of project membership openness and project data openness in citizen science drives the determination of how open or closed a project is to the public. At CitSci.org we recognize that the requirements for each project will be different according to its goals and context, and rec- ommend that projects select or build platforms that will accommodate their project’s needs. A Typology of Citizen Science Project Openness Here, we build on our discussions of citizen science par- ticipation and privacy to present a citizen science openness framework, a typology of project membership openness and project data openness for citizen science platforms and projects. We introduced the term “openness” to consider these two characteristics of projects as occurring along a gradient, and suggest that citizen science project manag- ers may want to evaluate where their project(s) fall along the membership openness and data openness gradients, and where their optimal placement would be based on project objectives, sensitivity, and other criteria. Choices related to membership and data openness will help to guide these decisions. These two dimensions of project governance form part of our core CitSci.org platform structure and functionality, a structure that accommo- dates projects from the most closed project type (both closed data and closed membership) to the most open (open data and open membership – the crowdsourced for- mat). Each of the dimensions also has intermediate levels of openness, nuances we continue to be pressed to work toward in future releases of CitSci.org. We created this typology as a conceptual representation of the requests by different projects to allow different lev- els of openness. Open membership and open data are com- mon terms in contemporary scientific inquiry, including citizen science. However, we argue that due to the diver- sity of citizen science project objectives and inherent sen- sitivities or project structures, the goal should not always be to become more open, despite recent trends leading in this direction. Projects may choose to keep data private or to keep data open for many reasons, and these rationales will be project-specific. We emphasize that the typology does not impose judgment on projects for where they may lie along these gradients. The important thing is that each project be designed to best meet its own needs. Where issues may occur is when a project that needs to be placed in the “closed” realm of the typology due to sensitive data or other contextual reasons is created on a platform that requires data to be “open” or where there is another simi- lar mismatch in project-specific needs versus available options/capacity. We encourage project managers to con- sider this typology when assessing their project needs so that they can either develop their own appropriate cus- tom tools or platform, or find a platform that is flexible enough to accommodate their needs. Typology descriptions The typology of citizen science openness is portrayed as a grid of “Project Membership Openness” on an x-axis vs. “Project Data Openness” on a y-axis (Figure 2). Each cell presents a unique combination of membership and data openness, labeled with a letter code that designates a spe- cific combination of “Open,” “Partially-open,” or “Closed” membership and data openness, leading to nine unique combinations. As project managers design and develop their projects, they will likely identify with one of these descriptive combinations as being most appropriate for their particular project. Having the ability to customize the openness of their membership and data allows them to develop their project to best meet their needs and goals. Typology operationalization in CitSci.org To operationalize a diverse array of project openness capa- bilities, we built on our conceptualizations of user govern- ance to place toggle switches (currently termed “privacy” for project data openness, and “membership” for project membership openness) for project managers to make openness choices (Figure 3). We developed a tooltip (an information button that displays additional help when hovered over or tapped) so that when project managers are faced with a project structure decision, they can learn about each choice and consider for themselves which option would be best for their particular needs. Our project membership openness options include the full breadth from Closed Projects (Invitation-Only), cur- rently implemented using the member-based selection combined with an “Invite Members” tool; Member-Based Projects that sit in an intermediate realm and require project manager approval to join; and Open Projects that follow a crowdsourcing model and which are our newest project type. CitSci.org’s project data openness choices are more complex, and full choice selection is still in development. Our most open privacy setting is the Public Project set- ting, which makes project data accessible for viewing, querying, and other platform-based exploration by any- one, including members of the lay public and those not registered on CitSci.org, while restricting data downloads and formal data use to those who are registered CitSci.org users. Our intermediate openness setting for data privacy is the Private Project setting, which allows viewing, que- rying, downloading, and other access options for project http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen ScienceArt. 14, page 8 of 15 Figure 2: Typology of citizen science project openness is determined by a combination of Project Membership Open- ness on the x-axis and Project Data Openness on the y-axis, creating a 3 × 3 grid of blocks, each with their own openness classification. “C” represents “closed,” “P” represents “partially open,” and “O” represents “open” status for both membership and data. Each cell is a unique combination of these three classifications for the two axes. The most open projects would be classified as “Open, Open,” or “O-O” and the most closed as “Closed, Closed,” or “C-C.” Figure 3: Toggle switches offer a selection of choices related to project membership openness, project data openness (project “privacy” in CitSci.org), personal information privacy, and an option to contribute to SciStarter, a partner citizen science platform. http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen Science Art. 14, page 9 of 15 members only, hiding data from all non-project CitSci. org members and the public. We are currently developing a Fully Restricted Project data privacy setting, which will limit data access to only those project members who col- lected the data and project managers or scientists who are leading the project, while excluding other project mem- bers and the public from access. Although this choice may seem uncommon in environmental field-based scenar- ios, it may be necessary for sensitive data such as health reports or personal information that could compromise the privacy, safety, or security of research subjects. These protections will be necessary if projects are to go through the human subjects institutional review board process, as anonymity is a foundation of human subject protection. Any combination of these project membership and data openness choices can be applied to customize projects to be more closed where privacy is required, or more open where visibility, access, sharing, and broad collaboration are desired. Thus, rather than requiring blanket selec- tions for our entire platform, we made it possible to mix and match open and closed settings as necessary. This approach has been critical to enable CitSci.org to meet the needs of our evolving client user base and their diverse project needs and specifications. By putting the selec- tion of membership and data openness into the hands of our users, we are both creating a platform that meets the needs of a diverse project base and giving governance over those decisions to our users. In addition to these existing settings, we are currently developing tools to expand our sub-choices within the realm of partially open by providing “open-close” toggles at different levels of the platform and at different levels of data representation. For example, we are developing an option that will allow project managers to specify whether access to all data submitted using a specific datasheet is to be open, closed, or of intermediate openness. This will give project managers the ability to close some data sets while leaving access to others open. We also are developing an option to select the open- ness of access to data submitted for specific species. This option allows project managers to protect the data related to sensitive species specifically, hiding all data related to the species so that the data cannot be exploited and the species potentially harmed by revealing observed loca- tions. Finally, we are also developing options to allow both project managers and members to choose whether specific observations (a single column in a database, e.g., all obser- vations of water temperature), specific locations (a single row in a database, e.g., all observations made at a specific study site), or individual measurements for an observa- tion (a single cell in a database, e.g., a single measurement of water temperature) should be open/closed/partially open. A few of these existing and envisioned future set- tings are illustrated in Figure 4, which illustrates global project openness choices as well as future capabilities to devolve data openness decisions to individual project participants. This choice may be desirable when revealing a location publicly would reveal the location of sensitive data such as a threatened and endangered species or a pri- vate residence. Our ultimate goal at CitSci.org is to make citizen science— good and well-thought-out citizen science—accessible to the greatest number and the greatest diversity of potential citizen scientists and their projects. We want people to be inspired to take part in science, and to have tools and guid- ance available that will help them to make progress toward their vision. By creating a platform that engages the great- est diversity of people and projects possible in both the use of the platform and in its user-driven design, we as hosts support the fourth of Bernholz and Ormond-Parker’s (2018) common principles for digital data use, pluralism. Illustrative Project Typology Examples from CitSci.org CitSci.org hosts hundreds of projects. Here we present four projects that fall at different locations on the Citizen Science Project Openness Typology (Figure 2) to illustrate the choices they made, the rationale for these choices, and how the projects used CitSci.org to operationalize them. These projects serve as demonstrative examples of our project openness typology, and their teams have agreed to share them for the benefit of the citizen science com- munity. Stream Tracker Stream Tracker is a citizen science project funded by the National Aeronautics and Space Administration’s (NASA) Citizen Science for Earth Systems Program. Stream Tracker studies intermittent streams—i.e., streams that do not flow all the time. Such streams are important for fore- casting water supplies, mapping critical aquatic habitat, and understanding how streamflow conditions change over time. Stream Tracker uses the CitSci.org platform to collect data on these previously overlooked streams by crowdsourcing volunteer observations of when and where intermittent streams are flowing. This project has chosen an open, crowdsourcing-style membership and open data structure, thus placing it in the “O-O” (Open Membership, Open Data) block in Figure 2. These program structure openness choices were made by the project science team, including the lead principal investigator and the volunteer project manager at the proposal stage, and were accommodated by our new crowdsourcing capability. Membership and data are open to all who are interested. For this project, open member- ship and public access to open data are critical given the goals, objectives, lack of privacy concerns, and context of the project. This format has allowed the project to grow rapidly from its original focus on a single watershed in Colorado to 29 states—growth that was not anticipated when the project was originally conceived to advance understanding of intermittent stream flow within a single watershed. Off the Roof The Off the Roof project developers came to CitSci.org seeking support to help organize and centralize data sub- mitted by volunteers pertaining to the quality of water collected from their roofs using rain barrel runoff collec- tion systems. Increasing demands on diminishing water http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen ScienceArt. 14, page 10 of 15 supplies and the movement for urban areas to use more local water supplies has intensified interest in alterna- tive water sources. However, lack of data on potential human health risks of these alternative water sources and treatment required to meet water quality standards has impeded use of these sources for both potable and non- potable applications. Data on pathogens in roof runoff is limited due to the need for a rigorous sampling campaign encompassing a large number of roofs in multiple regions and the complexities associated with measuring human pathogens. Our scientists facilitated the team’s project design development, which enabled them to have volun- teers gather data on the circumstances surrounding water collection events, while also allowing the project team to append data to these observations once pathogen analy- sis results were received from laboratories. This ability to add information to observations after laboratory analysis is critical to the project. Figure 4: Existing and envisioned platform capabilities as seen on the Stream Tracker Project Profile page as an example of operationalizing decisions on membership and information privacy choices. This is a fully open project, as seen by the “Open” icon and open padlock “Public” icon to the right of the project summary statistics. The Stream Tracker project manager chose Open Project Membership and Open Project Data for maximum participation and usability of project data. This project’s decisions were operationalized via an existing series of two toggle switches presented to the project manager during project creation. Also portrayed are future envisioned features (open and closed padlocks along with “Request to see” links denoted by a location marker icon) shown for two observation locations entitled “Plot 2” and “Plot 3” that we plan on implementing in the future. These envisioned features will allow project manag- ers and citizen scientists to choose accessibility of specific observations at specific locations within the project. For example, the project manager for this project may set a project-wide setting indicating that all observations are to be made publicly available (again, as evident by the open padlock to the right of the project statistics at the top of the profile), but in this case we can see two observations that have been selected to be kept private by those making the two observations. We also can see that one observation is visible, or open, to the person logged in (User B; see top right) because they made this observation (Plot 3), but is private to others. A second observation (Plot 2) that another user made can also be seen, however this location was kept private such that the specific location coordinates are not viewable by User B. If restrictions on visibility were actually put into practice for this project (they have not been), then this project’s Project Data Openness would become “Partially open,” and the project would move from an “O-O” position in the openness typology to “O-P.” Lynn et al: Designing a Platform for Ethical Citizen Science Art. 14, page 11 of 15 In this example, decisions about who can participate and why were made by the project research team and were guided by the unique research questions related to how different roof materials might affect pathogens in collected water. In this case, a broad recruitment strategy was used to identify anyone in target cities who may be interested in participating, but the research team then used filters and criteria based on reported roof types and proximity to target city universities to select members. For membership, this project chose the closed member-based structure because they needed to carefully vet participants based on the project’s strict criteria. The project also chose to make all project data private and accessible only to project members, thus placing it in Typology Block “C-C” (Closed Membership, Closed Data) in Figure 2. The Off the Roof team chose to create separate projects for each of their target cities to further restrict member data access only to the city project to which they contribute. This project encountered a privacy-related situation because data were being collected at individual member households. The project team initially decided to create predefined locations using the addresses of participants as data collection location names, unintentionally disclosing individual member household locations to other project members. When the project design team later discovered that project members did not wish their home addresses to be disclosed (see related discussions and examples in Bowser et al. 2017), they decided to code their household names (e.g., “Household 1”) while holding the address private. They also reduced the precision of the latitude/ longitude coordinates to avoid disclosing actual house- hold locations via location coordinates and mapped points. Instead, they decided to keep an offline key of the exact coordinates of the household location for the project manager and data analysis. Finally, when sharing project results with members, the team has chosen to share sum- mary statistics by city only to keep individual household location data private. Our experiences with this project have motivated the development of new features that will allow individual project members to choose whether they wish their individual default locations (possibly house- hold) to be shared with other project members—a choice that will be able to be layered on top of the choice made by project managers regarding openness of project data project-wide. Mountain Goat Molt Project The Mountain Goat Molt Project is aimed at studying the effects of climate change on the phenology (timing) of mountain goat winter coat molt (shedding). This project encourages people to submit photos of mountain goats (cold-adapted, alpine species) and to report on the degree that goats have shed their coats to help scientists study the effects of climate warming on the coat molt phenology. Membership is set to be open, and data are publicly accessi- ble. This project falls within Block “O-P” (Open Membership, Partially-open Data) of our Openness Typology ( Figure 2). However, the project would benefit if features were avail- able that allow individual photographers to preserve the copyright of their personal photographs of goats, while at the same time placing additional attribute data related to an individual photograph (such as the degree that the coat has been shed) under a more open-access Creative Com- mons license. If this feature existed, we would allow profes- sional photographers to participate in this project without fear of photo copyright violations. Thus, the copyrights to photographs submitted would remain with the contribu- tor of the photographs, rather than being transferred to the platform or the project. Other attribute data such as coat molt estimates would remain open data usable by others, copyright-free. We plan to develop several of these options for platform-wide availability in 2019, which will allow us to better support partially-open project data struc- tures, as well as make improvements to CitSci.org broadly to better support the needs of this unique project. This will include an integration with the Zooniverse platform for more streamlined image classification in parallel with image submission and associated data entry. Front Range Pika Project The Front Range Pika Project (FRPP) is designed to col- lect data about the American pika (Ochotona princeps) across the Front Range of Colorado. The project was cre- ated and is managed through a collaboration between Rocky Mountain Wild and Denver Zoo, with assistance from pika researchers at the University of Colorado, the Natural Resource Ecology Laboratory at Colorado State University, Colorado Parks and Wildlife, and Rocky Moun- tain Biological Laboratory. The FRPP partners are working with other regional citizen science projects to collect con- sistent, rigorous, and usable data on pika across Colorado. This project chose semi-open membership where anyone can request to join but must be approved by project man- agers. Approval is based on attending a required training program that consists of an in-class training session and a field training session to ensure data quality that meets rigorous scientific standards. This project closely aligns with Block “P-O” (Partially-open Membership, Open Data) ( Figure 2). By choosing to have data be fully publicly accessible, the project has benefitted by attracting greater collaboration than was initially anticipated. Once started, other similar organizations took note and replicated key aspects of project openness (e.g., the Cascades Pika Watch Project) and protocol (e.g., PikaNet—a project led by the Mountain Studies Institute). This shows the power of open access data and open science as related to sharing not only data, but also governance choices and data col- lection protocols, in an open platform context. Recommended Key Questions Given the complexities associated with choices related to project membership and data openness, it can seem daunting for project managers to design and implement projects. Here we provide a framework to aid in the design and use of platforms to best support project needs related to governance and openness, and associated questions to ask early in project development, reiterating that the moral benefit of one choice versus another is not being promoted or challenged here. Rather, each project must make decisions based on its project-specific needs. Note http://CitSci.org Lynn et al: Designing a Platform for Ethical Citizen ScienceArt. 14, page 12 of 15 also that situations change, so initial decisions may need to be modified based on changes to the answers, possibly necessitating platform structural changes. Choosing or developing a platform that will allow for customization is important, if platform default settings are not appropriate for the project. We summarize our paper with four key questions, and associated sub-questions, which all citizen science pro- jects should ask when setting up a project and choosing a platform. The same questions may be asked from a devel- oper’s perspective when envisioning the user audience’s potential needs (Figure 5). 1. Project Membership Openness: Who can join the pro- ject and why? The answer to this decision will depend on both plat- form structure and project needs. Some platforms default to either open or closed membership, and project needs must be considered when selecting or designing an online platform for the project. When a project is looking for as many participants as possi- ble with no restrictions, an open platform is desired. When there are criteria of location or participant qualifications, then the ability to partially or fully close membership is more appropriate. 2. Project Data Openness: Who can see or use project data? The answer to this decision will depend on some sub- questions, such as: a. Will members be collecting data about sensitive species? b. Will they be collecting data related to human sub- jects? c. Will members be collecting data on private lands? d. Would public sharing of elements of the data have the potential to put members or species at risk? e. Does the project have permission to share the data with the public? f. Are there appropriate and adequate member per- sonal privacy policies in place? g. Has the project sufficiently informed members— and possibly human subjects—of these policies? h. Will data be shared as individual observations or data points, or can data be shared in an aggregate format, such as a density map or in reports con- taining only summary statistics as results? When sensitive data are anticipated, project man- agers will want to ensure that their platform will accommodate the needs of their project to hide data from the view of the public. If the project’s or spon- sor’s aim is to contribute open and accessible data to global networks for broad use and re-use, then it will be important to structure databases to be maximally Figure 5: Project openness and governance decision trees, detailing the four key questions that platform developers and platform users need to ask to ensure that platform structures meet the needs of the project and why. Within each decision the options from left to right indicate increasing openness, devolution of governance, or potential fineness of the level of the decision. “C” denotes “Closed” projects, “P” denotes “Partially open” projects, and “O” denotes “Open” projects. Lynn et al: Designing a Platform for Ethical Citizen Science Art. 14, page 13 of 15 open, while working with the sponsor and platform to protect data too sensitive to be revealed in raw form. 3. Project Governance: Who can make project openness decisions? The answer to this decision will depend upon the structure of the project’s platform. Currently, many platforms have a default governance structure that dictates who can make choices related to Questions 1 and 2, so that the platform makes the decision, or only the project manager can make the deci- sion. If there are research sponsor requirements, project manager concerns, or even potential project member decision governance concerns over project openness choices, the platform needs to be able to be either custom designed or flexible enough to accommodate these choices. 4. Project Data Sharing Decision Levels: At what struc- tural level within the platform should the decision be made? The answer to this decision may be hardwired into a platform’s structure so that all data are open or all data are closed, with no option to make choices at different project structural levels. Alternatively, as in the case of CitSci.org, project managers may be able to assess their needs based upon whether they anticipate having no sensitive data, sensitive data occurring throughout the project’s structure and observations, or the potential for unknown sensitive observations to be recorded. Our general recommendations are to structure projects to be conservative with the sharing of information per- taining to project participants (see Bowser et al. 2017), and to allow the needs and goals of the project to guide decisions related to project membership and data open- ness that make the most sense to the scientific endeavor. Putting the choice in the hands of individual participants allows the locus of decision making to sit with the individ- ual. Providing information on the benefits and drawbacks of individual choices that are offered allows participants to make informed choices based on their comfort levels. These collective choices and actions contribute to the eth- ical conduct of science by trained and untrained citizen scientists alike. Conclusions We are in an era of growing citizen science data genera- tion. It is important that platform designs build on tradi- tional wisdom and information curation systems like that of the Wadeye, with structures in place to meet the needs of twenty-first century science and twenty-first century technology. CitSci.org has worked to operationalize gov- ernance and openness by incorporating options into the platform that allow project managers to customize their project governance and protections according to the needs of their project, volunteers, study subjects, and greater contexts. We have done this in an adaptive and iterative fashion, developing CitSci.org as we face new requests from users, and as our own knowledge and engagement in the theory and practice of citizen science, as well as our concern for meeting the needs of our users and identify- ing what those needs actually are, continues to grow. Our work to meet our own high standards continues, and will continue as we are faced with new capabilities, new plat- form and project scenarios, and developing needs of our new and existing CitSci.org users. We are grateful to all who continue to push boundaries to make us all better. Citizen science project managers are change makers, regardless of the top-down or bottom-up governance deci- sions that they make for their projects. If decisions are made thoughtfully, with full consideration of a project’s unique goals and needs, then projects will be poised to have greater impact in the world. It is this full consideration of a project’s unique goals and needs that ultimately will lead to the most ethical process and the greatest success and impact by harnessing the power of citizen scientists who want to participate in science and contribute to change. Acknowledgements This material is based in part upon work supported by the National Science Foundation under Grant Nos. 1339707, 1550463, and 1817612. The CitSci.org team would like to thank Colorado State University and the Natural Resource Ecology Laboratory for supporting CitSci.org. Our users are our greatest contributors to platform design and to our learning about on-the-ground technical needs of citi- zen science projects. Dani Lin Hunter, Ellen Eisenbeis, and Danielle Backman conducted and analyzed user surveys, and Alycia Crall and Jim Graham contributed to early Cit- Sci.org development. Two anonymous reviewers and jour- nal editors Rick Bonney and Lisa Rasmussen provided con- structive feedback that led to significant improvements to this manuscript. To all we are grateful. Competing Interests The authors have no competing interests to declare. References Bernholz, L and Ormond-Parker, L. 2018. The Ethics of Designing Digital Infrastructure. Stanford Social Innovation Review, 16. 3 ed. Santa Clara, California: Stanford University. Bloomberg. 2018. Uber Data Breach Exposed Personal Information of 20 Million Users [Online]. Fortune. Available: http://fortune.com/2018/04/12/ uber-data-breach-security/ [Accessed October 24, 2018]. Bowser, A, Shilton, K, Preece, J and Warrick, EE. 2017. Accounting for Privacy in Citizen Science: Ethical Research in a Context of Openness. CSCW. DOI: https://doi.org/10.1145/2998181.2998305 Brossard, D, Lewenstein, B and Bonney, R. 2005. Scientific knowledge and attitude change: The impact of a citizen science project. International Journal of Science Education, 27: 1099–1121. DOI: https://doi. org/10.1080/09500690500069483 Clery, D. 2011. Galaxy Zoo Volunteers Share Pain and Glory of Research. Science, 333: 173–175. DOI: https:// doi.org/10.1126/science.333.6039.173 http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://CitSci.org http://fortune.com/2018/04/12/uber-data-breach-security/ http://fortune.com/2018/04/12/uber-data-breach-security/ https://doi.org/10.1145/2998181.2998305 https://doi.org/10.1080/09500690500069483 https://doi.org/10.1080/09500690500069483 https://doi.org/10.1126/science.333.6039.173 https://doi.org/10.1126/science.333.6039.173',\n",
              " 'Forum 436 BioScience • June 2018 / Vol. 68 No. 6 https://academic.oup.com/bioscience BioScience 68: 436–444. © The Author(s) 2018. Published by Oxford University Press on behalf of the American Institute of Biological Sciences. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com doi:10.1093/biosci/biy028 Advance Access publication 28 March 2018 A Science Products Inventory for Citizen-Science Planning and Evaluation ANDREA WIGGINS, RICK BONNEY, GRETCHEN LEBUHN, JULIA K. PARRISH, AND JAKE F. WELTZIN Citizen science involves a range of practices involving public participation in scientific knowledge production, but outcomes evaluation is complicated by the diversity of the goals and forms of citizen science. Publications and citations are not adequate metrics to describe citizen- science productivity. We address this gap by contributing a science products inventory (SPI) tool, iteratively developed through an expert panel and case studies, intended to support general-purpose planning and evaluation of citizen-science projects with respect to science productivity. The SPI includes a collection of items for tracking the production of science outputs and data practices, which are described and illustrated with examples. Several opportunities for further development of the initial inventory are highlighted, as well as potential for using the inventory as a tool to guide project management, funding, and research on citizen science. Keywords: citizen science, evaluation, science products inventory, altmetrics, science policy The term citizen science is used to describe a wide variety of projects involving nonprofessionals in pro- ducing scientific knowledge (Bonney et al. 2009). Citizen science is often considered a research strategy or methodol- ogy that involves actively engaging members of the public in one or more core steps of scientific inquiry. Citizen science has proven useful for answering questions about plant and animal distributions across landscapes, which can result in advances to basic research and conservation actions (Cooper et al. 2014, McKinley et al. 2015, Ries and Oberhauser 2015, Sullivan et al. 2016). It is also valuable for research that requires the human processing and interpretation of large amounts of data (Swanson et al. 2016), such as the projects hosted on the Zooniverse platform, which focus on image classification and transcription. When projects are carefully designed and well managed, they can produce science not achievable through other means (Bonney et al. 2009). Defining metrics for assessing the full range of citizen- science outputs will help describe the breadth and value of this emerging field. Such metrics can also help project designers understand the relationships among project prod- ucts and data practices, as well as set informed expectations. Although research is demonstrating learning outcomes from citizen science (Masters et al. 2016) and evaluating participant outcomes (e.g., Phillips et al. 2014), metrics for evaluating science productivity and conservation outcomes are too simplistic or limited in applicability. Science productivity in citizen science is not readily assessable through traditional counts of papers and cita- tions (Dickinson et al. 2012, Wiggins and Crowston 2015). Current reliance on bibliometrics, or analyses that use publication and citation data to evaluate scholarship, “are based on assumptions, whether implicit or explicit, about how and why the authors of one work cite other works” (Borgman and Furner 2002, p. 57). Such analyses also assume that scholarly publications are a primary goal, which is not always true in citizen science. Scientometrics considers other quantitative measures of science activities and science policy impacts (Hood and Wilson 2001), focused on institu- tional rankings, faculty productivity and tenure standards, and productivity assessments, but still relies primarily on publication and citation counts. The Internet created new opportunities for evaluating impact: Article commenting, Wikipedia mentions, blogging, online video, and open data repositories provide new measures, known as altmetrics, to complement citation statistics (Priem et al. 2012). In citizen science, altmetrics may reflect goals for education, policy, and conservation, as well as evidence of scientific impact. Citizen-science project productivity has been evaluated through measures such as publication rate, completeness D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 Forum https://academic.oup.com/bioscience June 2018 / Vol. 68 No. 6 • BioScience 437 of analysis, resource savings, and effort distribution, but without evaluating “alternate” products such as data sets and secondary publications (Cox et al. 2015). Theobald and colleagues (2015) found that only about 12% of citizen-sci- ence projects surveyed showed evidence of turning data into peer-reviewed publications but did not evaluate other types of science outputs. Other potentially relevant indicators of productivity include data-set generation (Lagoze 2014), conservation actions (Sullivan at al. 2016), environmental- justice outcomes (Haklay 2013), education and community outcomes (Jordan et al. 2012), and policy impacts (McKinley et al. 2015). Although some of these measures may be gen- eralizable, many cannot be used outside of a specific project or technology platform. The lack of standard assessments for science productiv- ity underlies a barrier to the acceptance of citizen science as a research strategy: uncertainty among peer reviewers about how to evaluate the scientific merits of citizen-science projects, particularly for funding decisions (Shirk et al. 2012, Bonney et al. 2014). Carefully considered criteria by which to evaluate the the science outputs of citizen-science proj- ects will advance the research community’s ability to review citizen-science proposals and can support the work of federal agencies to adopt and develop citizen-science activi- ties (Holdren 2015). To meet these needs, we developed a comprehensive inventory of outputs based on observed characteristics of established successes, the science products inventory (SPI). Methods To generate an initial inventory of science productivity indi- cators, we adopted a variation of the Delphi method (Dalkey and Helmer 1963, Linstone and Turoff 2002). This method involves iterative, structured idea generation and ranking by a panel of experts until convergence is achieved. Because the Delphi method relies on anonymity not feasible for our project, we combined elements of the Delphi method with the nominal group technique processes for idea generation, voting, and ranking (Potter et al. 2004). Our panel included the members of the DataONE Public Participation in Scientific Research Working Group and invited guests (table 1). The members of the panel were selected to maximize the diversity of perspectives, which required recruiting individuals with extensive experience in citizen science. The group’s joint expertise involved direct experience with more than two dozen projects rang- ing from local to global in scale, developing and delivering citizen-science data products and conducting research and evaluation on citizen science. We also drew from across sectors—academia, nonprofits, and federal agencies—and across ranks, from PhD students to full professors and sea- soned federal scientists. Of the 20 participating individuals, Table 1. Expert panel members. Name Affiliation (at time of panel involvement) Project affiliation and/or expertise area Rick Bonney Cornell Lab of Ornithology Evaluation Anne Bowser University of Maryland Participatory design Eric Graham University of California, Los Angeles App development, What’s Invasive! Sandra Henderson NEON Education, Project BudBurst Megan Hines Wildlife Data Integration Network Data management Gretchen LeBuhn San Francisco State University The Great Sunflower Project Kelly Lotts University of Idaho Butterflies and Moths of North America William Michener University of New Mexico Data management Abe Miller-Rushing National Park Service Project design and management Greg Newman Colorado State University IT development, CitSci.org Karen Oberhauser University of Minnesota Monarch Larva Monitoring Project Julia K. Parrish University of Washington Project leadership, COASST Alyssa Rosemartin USA National Phenology Network Data management, Nature’s Notebook Eric Russell National Geographic Society IT development, FieldScope Jennifer Shirk Cornell Lab of Ornithology Project development Arfon Smith Zooniverse Community management, Galaxy Zoo Robert D. Stevenson University of Massachusetts Boston Project design, PRCWA Julian Turner Colorado State University IT management, Community Collaborative Rain, Hail, and Snow Network Jake Weltzin US Geological Survey Project leadership, Nature’s Notebook Andrea Wiggins DataONE Project and IT design Bruce Wilson Oakridge National Labs Data management D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 Forum 438 BioScience • June 2018 / Vol. 68 No. 6 https://academic.oup.com/bioscience 14 had graduate training in ecology and related disciplines, 2 in information science and technology, 1 in astrophys- ics, and 1 in computer science. In addition, of the 20 par- ticipants, 18 were professional scientists, 2 were professional software developers supporting citizen-science projects, and all were considered practitioners because of the nature of their involvement in citizen science as project staff and project participants. Our hybrid method was enacted through two intensive 3-day workshops held 6 months apart, with facilitated brain- storming, categorization, and prioritization of evidence and context for describing science productivity in citizen science. Every item in the inventory was vetted through in-depth discussions, with only two items added after pilot testing. During the first meeting, brainstorming, clustering, and consensus processes generated initial lists of science outputs, descriptive characteristics of projects, and potential mea- sures. The primary activities for this meeting included an agenda-setting overview, open discussion of the goals and desired outcomes for the process, structured brainstorming exercises to identify science products and potential metrics for evaluating them (concept generation, categorization, and voting), initial testing of the metrics for a few projects, discussion of strategy for data collection, and refinement of sampling criteria. The resulting metrics for measuring science products were aggregated into an inventory spreadsheet for testing. The participants “piloted” the inventory by independently completing inventories for projects with which they were familiar while recording feedback for improvement. Based on the pilot feedback, the second meeting further refined item definitions, finalized the recommended metrics for each item, and selected and initiated documentation of case studies. The panel completed eight case studies that represent a variety of participation models, scientific contexts, and types of success (box 1; table 2). Case studies were selected for diversity in scientific discipline (several topics within ecol- ogy, astronomy, and precipitation), geographic scale (local to global and aspatial), participation scale (40 to 325,000 contributors), and project goals including basic science and decision support for resource management and disaster prevention. Results The SPI includes multiple potential outputs that indicate sci- entific progress or products (table 3) and contextual details on data practices (table 4). The following sections describe these outputs in more detail, with examples from our case studies. The supplemental material includes templates of the SPI, as well as examples of the SPI for two representative case studies. Science products. Science products include varied forms of dissemination for multiple audiences and purposes (table 3), via a variety of formats and venues, such as publications, videos, and social media, which describe or discuss the proj- ect’s research design, progress, and results. Science products are subdivided into four categories: written, data, manage- ment and policy, and communication. Written. Citizen-science projects present their research to multiple audiences through a variety of formal and informal products designed for multiple stakeholders and purposes (table 3). We included four written product types, most of which involve peer review: scholarly publications, disserta- tions and theses, reports, and competitive grant awards. Scholarly peer-reviewed publications are typically assessed with a count of papers selected for inclusion on the basis of disciplinary conventions. Although simple to specify, this accounting can become complicated by the surpris- ingly broad variety of uses of citizen-science data (Lagoze 2014). Full publication counts are often underrepresented by indexes of journal databases, so staff for several case- study projects kept manual records informed by automated citation alerts and correspondence with external data users. Although several case-study project leaders wished to sub- divide “scholarly publications” into subcategories such as papers about a citizen-science project and papers based on the data from a citizen-science project, they found it unfeasible to implement these subcategories retrospectively. Dissertations and theses are more easily categorized, and although typically “unpublished,” they may lead to subse- quent scholarly publications and promote scientific inquiry more broadly. Formal and informal reports produced from citizen-science data can include white papers, technical reports, environmental assessments, species status reports, and policy advisory memos. Reports can be strikingly simi- lar to scholarly papers focused on supporting management, conservation, and policy goals, and many are routinely peer reviewed. Aside from relevant reports curated by project organizers, however, these types of publications can be dif- ficult to discover and track systematically because of non- standardized or inconsistent use of keywords (e.g., Cooper et al. 2014). Finally, initial implementation of the inventory with science teams at a government research facility identi- fied competitive grant awards as a strong indicator of project success and primary criterion for evaluation. This item can be measured through number of awards received or mon- etary value and could be further subdivided by award type or funder. An example of a project with all four types of written products is eBird. The project team regularly produces peer- reviewed scholarly publications (e.g., Sullivan et al. 2016) and also keeps account of publications by others resulting from access to eBird data. To date, eBird has identified more than 150 scholarly papers that either studied the project or used its data, and it has served as a case study for disserta- tions studying citizen science. The data have been used to study such topics as bird biology, natural-resource man- agement, and machine learning. The data are also applied extensively in “gray literature,” including technical reports D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 Forum https://academic.oup.com/bioscience June 2018 / Vol. 68 No. 6 • BioScience 439 for decision support and popular media such as magazines. Finally, the project is supported in part through competitive grant awards, cumulatively in the millions of US dollars. Data. The second category of science outputs includes raw data and value-added data products created and distributed for use by others (table 3). Theobald and colleagues (2015) observed that some of the most successful projects—at least in terms of peer-reviewed publications—are those that make their data readily available. Indicators that project data are being supplied for external users include whether the project has application programming interfaces for auto- mated data exchange (APIs), data packages, and metadata. The availability of data visualizations can also provide an indicator of data production as a type of preliminary result (Snyder 2017). Finally, demand for project data measured in number of data requests or data volume transferred is a good indicator of data distribution. These metrics require more substantial infrastructure. Tracking data transfer vol- umes (bandwidth consumption) requires having data access options already in place; such infrastructure is typically developed after demand for data access is established via increasing frequency of data requests that become burden- some to manage through other means. The volume of data requests is often tracked through email messages or Web form submissions when access is mediated by humans or else via server logs for self-serve database access. The Community Collaborative Rain, Hail, and Snow Network (CoCoRaHS) is experienced in measuring data product usage. The project provides multiple APIs plus reports, descriptive metadata, and staff support. Estimates of recent demand for data averaged more than 9 gigabytes of data served daily to satisfy a total of around 14,000 requests per month. CoCoRaHS’s staff can break down these statistics into more specific activities and uses on the basis of close relationships with data consumers, such Box 1. Summary descriptions of case-study projects. eBird has collected more than 400 million observations of bird abundance and distribution from around the world since 2005, rep- resenting a collective investment of nearly 30 million hours in the field. The data have been used for numerous conservation and management applications, many public-interest publications, hundreds of public talks and presentations, and scholarly publications across a diverse range of disciplines. Galaxy Zoo enlisted volunteers from 2007 to 2008 in classifying the morphology of galaxies in more than a million photographs from the Hubble telescope. At the project’s conclusion, all images had been classified multiple times and shared through the Sloan Digital Sky Survey, with several unexpected discoveries. Galaxy Zoo data appear in scholarly papers and the Zooniverse software platform has supported dozens of additional projects. The Coastal Observation and Seabird Survey Team (COASST) participants collect standardized, effort-controlled data on deceased birds and marine debris. Since 1999, monthly data from more than 450 beaches along the Pacific coastline contribute to a unique data set with extensive details documenting bird carcasses for 186 species. COASST data have been used in scientific papers, news media, and dozens of reports for regulatory action and decision support. Monarch Larva Monitoring Project (MLMP) volunteers have collected data on monarch butterflies’ egg and larvae distribution and abundance at more than 1000 sites across North America since 1996 and raised over 15,000 larvae to examine survival and parasit- ism rates. The data were valuable to a recent petition to list monarchs as a threatened species under the US Endangered Species Act, scholarly publications, and a field guide to milkweeds. The Great Sunflower Project (GSP) launched in 2008 to collect information about pollinator service for the United States on a continental level and to evaluate and improve pollinator habitat, collecting a unique data set on pollinator presence and absence at around 8500 sites. GSP data have been used in scholarly publications across multiple fields and more than 30 talks and presentations to scientific audiences. The Community Collaborative Rain, Hail and Snow (CoCoRaHS) Network started in 1998 to collect precipitation data for better weather forecasting and disaster preparedness, with 12 active observation protocols including daily precipitation, significant weather, and hail. CoCoRaHS data for the United States, Canada, and Caribbean islands are used extensively in weather forecasting and report- ing and feature in several scholarly publications. Nature’s Notebook has accumulated records of the life cycles (phenology) of plants and animals in over 18,000 US locations since 2009. Operated by the USA National Phenology Network and supported by the US Geological Survey, the project also provides multi- taxon national scale phenology protocols and a software platform that supports other groups. Nature’s Notebook data have contributed to scholarly publications and reports for decision support and natural-resource management. The Parker River Clean Water Association (PRCWA) ran its tidal restrictions study from 1996 to 1997 to generate decision-support data for saltmarsh restoration in Massachusetts’ Great Marsh, identifying significant tidal restrictions at half of the sites surveyed. The data were used to prioritize over a dozen restoration projects by state agencies and other authorities. Results were presented in community meetings and a technical report, with methods published as a stand-alone guide to volunteer-based assessments of tidal restrictions. D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 Forum 440 BioScience • June 2018 / Vol. 68 No. 6 https://academic.oup.com/bioscience as the National Weather Service. The drawback of making CoCoRaHS data so readily accessible was that staff had dif- ficulty tracking research publications. By contrast, access to comprehensive eBird data packages and downloads requires a Web form submission, which allows project staff to moni- tor publications by users. Because CoCoRaHS’s goals are to generate data for decision support and emergency prepared- ness, data request volume is likely a stronger indicator of achieving their targets than the number of peer-reviewed publications. Management and policy. A third category, management and policy products, identifies direct actions, decision-support products, and policy impacts from citizen-science projects (table 3). These items are among the least straightforward to evaluate because awareness of them is often limited and they can take many forms. The case-study projects were able to report on only the management and policy impacts with which they were directly involved. Decisions, policies, and actions can lead to conservation outcomes that are typically evaluated separately but also follow from the science. The inventory focuses on use of project data as an input to policy and management decisions, regardless of the subsequent outcomes. Management and policy impacts are likely best measured through internal tracking or other forms of direct monitoring by the parties involved in translating science to decisions and policy, who are best informed about what counts as a meaningful management or policy outcome. Three types of management and policy outputs identified as indicators of science productivity are regulatory action (e.g., enforcement or investigation by an authority), decision support (e.g., land management or conservation actions), and forecasting or models (often used for management and decision support). The Parker River Clean Water Association (PRCWA) was a small, short-term project that achieved substantive management impacts. Data collected over 1 year influenced the prioritization of major natural-resource management investments in multiple restoration projects. In addition, the monitoring methodology had further impact when several other coastal monitoring projects adopted it to support management decisions and restoration actions. Communication. Public discourse and science communi- cation products offer evidence of scientific productivity. Communications specifically targeted at public audiences require additional capacity and effort beyond the tradi- tional research team and can help advance project goals (table 3). These indicators include written communications (e.g., blogs, newsletters, and social media); multimedia con- tent (e.g., videos); and discursive events (e.g., public talks and presentations). Although science communication is easily consigned to an “outreach” category, it is critical for volunteer recruitment and retention, in which ongoing communica- tion can provide evidence that science is progressing prior to the availability of formal products such as reports (Snyder 2017). Several types of science communications, configured as binary presence or absence indicators, can be extended with counts to document annual project activity levels. The Coastal Observation and Seabird Survey Team (COASST) demonstrates strong science communications with regular e-newsletters of project progress and skills Table 2. Case-study projects and features as of 2016. Project (see box 1 for additional info) Sponsoring organizations Science focus Years active Total volunteers Paid staff Peer-reviewed papers Data points Coastal Observation and Seabird Survey Team (COASST) University of Washington Seabird mortality; marine debris 1999–current 1K 6 FT and PT, 20–25 interns 20+ 100K Community Collaborative Rain, Hail, and Snow Network (CoCoRaHS) Colorado Climate Center Precipitation 1998–current 37.5K 6 FT 30+ 32M eBird Cornell Lab of Ornithology Bird abundance and distribution 2005–current 325K 20 FT and PT 150+ 400M Great Sunflower Project San Francisco State University Pollinator service 2008–current 120K 1 PT 10 125K Galaxy Zoo Oxford University and Johns Hopkins University Galaxy morphology 2007–2008 100K 3 FT and PT 50+ 40M Monarch Larva Monitoring Project (MLMP) University of Minnesota, Chicago Botanic Garden Monarch butterfly abundance and distribution 1996–current 700 Varies, PT 18 24K Nature’s Notebook USA National Phenology Network, US Geological Survey Plant and animal life cycles 2009–current 7.6K 12 FT and PT, interns 24 8M Tidal Restrictions Survey (PRCWA) Parker River Clean Water Association Saltmarsh tidal restrictions 1996–1997 40 1 PT 0 1.4K Abbreviations: FT, full-time employees; PT, part-time employees; K, thousands; M, millions. D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 Forum https://academic.oup.com/bioscience June 2018 / Vol. 68 No. 6 • BioScience 441 practice; Web-based interactive data visualizations high- lighting trends in time, space, taxonomy, and conservation; an interim results blog with frequent use of data visualiza- tions and graphic representations; and a Facebook page and Twitter feed driving participants to these products. Galaxy Zoo similarly established blogs and social media as standard communication tools for the globally distributed contribu- tors to Zooniverse projects. Data practices. Data management and sharing practices max- imize the value of volunteers’ contributions. To achieve this impact, data must be available, discoverable, and well docu- mented. As for science products, data practices and tools can take both simple and complex forms, ranging from basic downloads of plain text files to direct database access with accompanying schema documents (table 4). The key compo- nents identified by our expert panel mirrored the FAIR Data Principles: findable, accessible, interoperable, and reusable (Wilkinson et al. 2016). Findable. Findability, or discoverability, is evaluated with binary items (yes or no) because of the wide array of specific con- figurations that may be appropriate for each individual project. Metrics to assess findability include the availability of data directly from the project, typically via its website, and the availability of project data via research data repositories or registries. Accessible. Once found, data must be relatively straightfor- ward to access, worth the trouble of doing so, and delivered in a usable format. The inventory includes multiple ways to assess data availability, with the expectation that not all met- rics will be applicable to every project. These items include the availability of data file downloads and database querying tools. In addition, documentation is important for acces- sibility and is evaluated through the presence or absence of explicit data licensing, descriptive metadata (i.e., a data dictionary with specifics of the data), and API documenta- tion where applicable. For example, eBird custom query downloads are delivered with terms of use, recommended citations, metadata descriptors, and extensive documenta- tion (Sullivan et al. 2014). Interoperable. Accessibility and interoperability are closely linked, because interoperability supports accessibility. The inventory includes one item for interoperability, identifying whether metadata employ appropriate structural data standards, such as EML or FGDC, for machine-enabled data discovery. Table 3. Science products. Category Product Definition Written Dissertations, theses (#) Number of theses and dissertations using data from or reporting on the project Written Scholarly publications (#) Number of published peer-reviewed science papers that report on the project or apply its data Written Reports (#) Number of formal reports reporting results, such as white papers, technical, and other reports Written Grants awarded (#, $) Existence (or total monetary value) of competitive funding awards from private or public funders Data APIs (Y/N) Existence of technologies for automated data exchange between computers Data Data packages (#) Number of curated exports of data and related documentation, usually as a downloadable zip file Data Metadata (Y/N) Existence of documentation describing data structure, formats, and contents Data Visualizations (Y/N) Existence of visual representations of data, such as graphs, maps, and animations Data Specimens/samples (#) Number of material data points in the form of physical specimens or samples Data Requests (# requests, transfer volume) Number of individuals or technical systems requesting data, or volume of transferred data Management and Policy Regulatory action (Y/N) Existence of legal rulings or regulation enforcement based on project data and findings Management and Policy Decision support (Y/N) Existence of decisions based on project data and findings (e.g., for policy or management) Management and Policy Forecasting/models (Y/N) Existence of models based on project data that simulate or predict complex phenomena Communication Blogs (Y/N) Existence of online informal written communications about project processes and findings Communication Newsletters (Y/N) Existence of structured publications for project stakeholders, produced in hard copy or digitally Communication Videos (Y/N) Existence of publicly available digital videos on project content, activities, and findings Communication Presentations (Y/N) Existence (or number) of oral presentations at conferences or public events Communication Website (Y/N) Existence of dedicated website for the project D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 Forum 442 BioScience • June 2018 / Vol. 68 No. 6 https://academic.oup.com/bioscience Reusable. Reuse, or use of data by a third party, is contingent on data management practices plus the inherent value of the data and evidence of its quality and rigor. Mainstream scientific communities often require precise metadata on the location, date and time, and effort-control of data, in addition to information on verifiability (Burgess et al. 2017). The inventory assesses the “unique” qualities of the data descriptively (i.e., as free text). Potential uses of the data are frequently determined by their spatial and temporal extent (Theobald et al. 2015), included as categorical items. The total number of data points can also influence potential uses, although the unit of observation or measurement is highly variable (table 2). Reuse depends on careful documentation, summarized with binary items for the presence of documentation on known errors, quality-assurance or quality-control pro- cesses, questionable data points, and data provenance or audit trails for changes to data after initial ingestion. For example, data on the MLMP project website are provided unedited, so downloads of basic monarch density data come with a warning that there may be errors in the data, and more detailed data sets are provided with recommended cleaning criteria. Reuse also applies to data infrastructures, such as the provision of freely available reusable software (e.g., the Zooniverse code base, available through GitHub) and low- or no-cost hosting platforms (e.g., Nature’s Notebook). Software and related technical infrastructure, platforms, and services are a critical element in many citizen-science proj- ects, so reuse of existing systems can contribute substantially to the field. For projects offering technical infrastructure, the number of known groups adopting the platform or using the code base may be a good measure of impact. Among our case studies, both Zooniverse and Nature’s Notebook offered no-cost platforms. eBird has informally been used to similar effect, with a fee-based service for customized portals. Discussion The SPI is a tool for documenting science products and data practices as indicators of the science contributions of indi- vidual citizen-science projects. It has applications for objec- tive project evaluations both internally (by project leaders) and externally (e.g., by external evaluators). Projects interested in planning and self-evaluation can apply the SPI internally to assess actual and potential science productivity or to adjust activities or resource allocations. Table 4. Data practices. Category Practice Definitions Findable Data available from project website (Y/N) Availability of data from the project’s own website in a downloadable or queryable format Findable Data available from repositories or registries (Y/N) Availability of data in a research data repository or via a data clearinghouse or registry Accessible Downloadable data file(s) available (Y/N) Existence of download data files via project website, repository, or third party Accessible Tools for data exploration (Y/N) Existence of tools for visualizing, summarizing, or querying project data via an app or website Accessible Data licensing specified (Y/N) Existence of text specifying terms and conditions for data use Accessible Metadata available (Y/N) Existence of documents with descriptive metadata such as known problems and data cleaning tips Accessible API documentation (Y/N) Existence of documentation to support users of an API, where applicable Interoperable Data recorded in standard formats for discipline (Y/N) Application of disciplinary standards for structural metadata and data formatting Reusable Uniqueness of data (describe) Description of the unique contributions and features of the project’s data Reusable Time scale of data (# yrs) Number of years of records in the data set; may include historical data Reusable Spatial scale of data (describe) Description of the geographic range for project data, such as continent, country, state, city, or watershed Reusable How much data (# data points, describe) Description of data volume in terms relevant to the data collected, such as number of data points Reusable Errors documented (Y/N) Existence of documentation for known errors in the data set Reusable Quality assurance or quality control documented (Y/N) Existence of documentation for quality-assurance and quality-control processes Reusable Changes documented (Y/N) Existence of documentation for data edited after initial receipt Reusable Questionable data flagged (Y/N) Existence of documentation for data considered questionable or problematic Reusable Software or platform development (Y/N) Existence of software or hosted technologies (platforms) that support external projects D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 Forum https://academic.oup.com/bioscience June 2018 / Vol. 68 No. 6 • BioScience 443 This requires identifying the applicable inventory items, which may involve adapting some measures for project- specific needs, eliminating others that are inapplicable, and collecting data from a variety of sources, including project staff and external records. Additional considerations include potential for weighting specific inventory items to reflect project priorities. The SPI could be similarly applied by funding or hosting organizations to guide assessments of current or potential future projects and inform resource allocations. The inven- tory was designed with this likely application in mind. Although the inventory can provide initial guidelines for project evaluation and comparison, it is open ended and descriptive enough to require informed judgment. Similarly, it is flexible and customizable, such as via weighting of items aligned with funder objectives or organizational goals. The SPI also lays a foundation for meaningful research on the relationships between data practices and science products and can be used in combination with other citizen-science evaluation tools for more comprehensive project assessment. By advancing the use of inventory-based evaluation tools, such as this one, the field can support evidence-based decision-making for science funding, tar- geted development of resources to strengthen citizen-science projects, and transparency in support of many objectives, such as more equitable and informed peer review of citizen- science products. These potential applications of the SPI also raise a point of concern for practitioners and reviewers: Any such inventory must be used judiciously to prevent unintended apples-and- oranges comparisons between different projects, and, as in other settings, productivity measures should be interpreted with care, because their value varies by context and purpose. Organizational features may affect the resources devoted to the project and the expected project outputs; for example, well-established projects such as eBird and Galaxy Zoo are more likely to have more completed science products (table 3) and more sophisticated data practices (table 4) than recently established projects and those that were never intended to produce scholarly publications (e.g., PRCWA). The parameters that make projects more or less directly comparable are a matter of judgment and should consider the purpose of comparison and availability of supporting descriptive information about projects. Conclusions As an evaluation framework for science productivity in citi- zen-science projects, the SPI presented in this paper is based primarily on expert consensus and exemplars. It should be applied with care and consideration because not all citizen- science projects are necessarily designed to a singular pur- pose such as science products. This initial version of the SPI provides a structured and relatively objective inventory for evaluating the science products of citizen science to address multiple needs across practical, policy, and research contexts. We note that many of the items in this inventory are equally applicable to evaluation of science productivity more broadly. This study is limited by its methods, which relied on an expert panel to develop and prioritize items and used case studies to identify and illustrate science productivity. In addition, it reflects the priorities of scientists rather than of the participants, who may place different value on these and other science products. To address these limitations, future work could further develop and refine this tool and evaluate its utility for reflecting citizen-science participants’ interests. For example, a number of inventory items would benefit from establishing categorical values through empirical study. Alternate items may also emerge through usage and to assess different types of products, resources, or practices. Several items should be revised as technologies evolve, and additional work is needed to identify generalizable indicators of excel- lence in citizen-science projects more broadly. The SPI offers an initial framework for guiding both citizen-science project management and research on the science of citizen science. Acknowledgments This work was supported in part by US Geological Survey Cooperative Agreement no. G16AC00267 and National Science Foundation (NSF) grant no 0830944. JKP was partially supported by NSF grants no. 1114734 and no. 1322820. Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the US government. The authors thank Anne Bowser, Eric Graham, Sandra Henderson, Megan Hines, Kelly Lotts, William Michener, Abe Miller-Rushing, Greg Newman, Karen Oberhauser, Alyssa Rosemartin, Eric Russell, Jennifer Shirk, Brian Sullivan, Arfon Smith, Robert D. Stevenson, Julian Turner, and Bruce Wilson for contributing to the inventory and case studies and Holly Faulkner and Fiona Jardine for editing assistance. Supplemental material Supplementary data are available at BIOSCI online. References cited Bonney R, Cooper CB, Dickinson J, Kelling S, Phillips TB, Rosenberg KV, Shirk J. 2009. Citizen science: A developing tool for expanding science knowledge and scientific literacy. BioScience 59: 977–984. Bonney R, Shirk JL, Phillips TB, Wiggins A, Ballard HL, Miller Rushing AJ, Parrish JK. 2014. Next steps for citizen science. Science 343: 1436–1437. Borgman CL, Furner J. 2002. Scholarly communication and bibliomet- rics. Annual Review in Information Science and Technology 36: 2–72. Burgess H, DeBey LB, Froehlich H, Schmidt N, Theobald EJ, Ettinger AK, HilleRisLambers J, Tewksbury J, Parrish JK 2017. The science of citizen science: Exploring barriers to use as a primary research tool. Biological Conservation 208: 113–120. Cooper CB, Shirk JL, Zuckerberg B. 2014. The invisible prevalence of citizen science in global research: Migratory birds and climate change. PLOS ONE 9 (art. e106508). Cox J, et al. 2015. Defining and measuring success in online citizen sci- ence: A case study of Zooniverse projects. Computing in Science and Engineering 17: 28–41. D ow nloaded from https://academ ic.oup.com /bioscience/article/68/6/436/5004320 by guest on 09 January 2022 https://academic.oup.com/bioscience/article-lookup/doi:10.1093/biosci/biy028#supplementary-data']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_adrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8-402VB_84t",
        "outputId": "1b9503d5-69eb-4238-dda6-3a5d312f9003"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Handbook for Citizen Science Quality Assurance and Documentation Handbook for Citizen Science Quality Assurance and Documentation – Version 1 This document is not intended for projects funded by EPA. If funded by EPA, Quality Assurance (QA) documentation is required by federal regulations (for example, 2 CFR 1500.11 for grants and 48 CFR 46.202 for contracts). QA Documentation must be completed and approved prior to conducting data collection activities. EPA’s Quality Directives (provided in references) must be consulted for QA documentation. Data used in regulatory and policy decision making and standard setting often must be collected using approved methods, which may include acceptance testing to demonstrate equivalence to these methods. Applicable Part(s) of Title 40, Protection of Environment, of the Code of Federal Regulations (CFR) should be consulted to ensure the study meets sampling, siting, quality assurance and all other requirements. How to Use This Document This Handbook should be used with the two companion documents – the Templates and the Compendium of Examples. This Handbook explains the purpose of each of the templates. The Templates provide instructions, tables and questions that should be filled in or responded to, and the Compendium provides specific examples of quality assurance documentation. Together, these documents will help organizations complete a QAPP and provide information for data users to evaluate the quality of data collected by citizen scientists. The Templates are recommended but not required. Federal, state, local, tribal, or other organizations may also be contacted for more assistance or guidance. The following are steps to consider when writing a QAPP. 1. Frame the Project’s Purpose Citizen science and crowdsourcing are terms describing diverse activities involving a range of organizations, uses and outcomes. Projects can span different environmental media (e.g. water, air, or biota) and different levels of participant engagement and responsibility. Some citizen science projects are designed for educational or community engagement purposes only, whereas others are designed to scientifically evaluate environmental exposure, to perform legally- defensible measurements, or to affect policy. Table 1 lists data uses by EPA and other organizations, organized by broad categories and specific project purposes (NACEPT, 2016). We recommend that your organization design a QAPP that matches the intended purpose of the project, as listed in Table 1. Determining where a project fits within the spectrum of project purposes can help characterize the necessary level of quality assurance documentation. Page 6 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Table 1. Categories of data use associated with project purposes. Categories of Data Use Intended Project Purpose Increasing public understanding Community engagement Education Scientific studies and research Environmental condition indicators (screening, exposure) Studies and research Legal and policy action Regulatory decisions 2. Define the Level of Quality Assurance and Documentation Once you have identified the project’s intended purpose, you should consider the activities you should perform to meet the needs and expectations of the data user. These activities are the basis of Quality Assurance (EPA, 2002) and may include: designing a plan to sample at representative locations; identifying the volume of air needed to meet appropriate levels of detection; or creating Standard Operating Procedures (SOPs) to ensure that volunteers record data uniformly. Quality assurance planning ensures that participants agree to roles and responsibilities, and that data can be used to answer the questions posed by the project, with a defined level of confidence to meet the intended use of the data. A citizen science organization should consider the project purpose and use of data as it selects the appropriate level of quality assurance and documentation. With this graded approach, data collected for legal and policy action would require more extensive quality assurance and documentation than data collected for increasing public understanding. Below, we provide additional information on these broad categories so that your organization can better determine the activities required to meet the appropriate level of quality assurance and documentation. Increasing Public Understanding (light shade) Projects in this category include studies on basic phenomena or issues, and often have as a primary or supporting objective to engage communities in environmental monitoring. Common objectives may center around educating citizens about their environments, scientific processes, and STEM (science, technology, engineering and mathematics) activities. These projects may result in more qualitative, or descriptive outcomes, such as presence or absence of specific species. In these projects, sampling locations may be more dependent on availability of volunteers, rather than based on a rigorous sampling scheme. Scientific Studies and Research (medium shade) Projects in this category are aimed at providing data useful for research, feasibility studies, or to identify baseline conditions or trends in exposure from water or air pollutants. Many projects in this category determine the effectiveness of environmental decisions, such as evaluating the number of fish traveling upstream after a dam has been removed. Many organizations in this category conduct screening studies for ecological or public health assessments and utilize risk assessment tools for prioritizing community-based actions. Page 7 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Legal and Policy Action (dark shade) Projects requiring the most rigorous level of quality assurance and documentation fall into this category. The purposes of these projects may include regulatory decision-making at a local, state or national level, and often use approved federal methods, or acceptance testing to demonstrate equivalence to these methods. The overall project purpose may include specific objectives for individuals, communities, and institutions (such as government agencies or universities). Also, the purpose of a project may evolve, or fall into more than one category of data use. A project may start as a community-based educational project but evolve into a more rigorous scientific study that evaluates baseline conditions. For example, an effort to engage local communities in measuring water quality may produce information indicating a need to curb pollution from specific sources. In these situations, more stringent quality assurance and documentation may be needed and pursuing the highest level of quality assurance that will meet a project’s intended purpose is recommended. A QAPP is not a static document and should be updated whenever an aspect of the project changes. 3. Factors to Consider How do you determine the right level of quality assurance and documentation? One factor is whether the data collected are qualitative or quantitative. In general, quantitative projects (i.e. how much?), as well as projects using statistical hypothesis tests, require a higher level of quality assurance than qualitative projects (Table 2). Table 2. Level of detail of quality assurance and documentation for different project purposes. The darker the shading, the more rigorous the quality assurance level of detail. Categories of Data Use Intended Project Purpose Quantitative Qualitative Level of Detail Increasing public understanding Community engagement Education Scientific studies and research Environmental condition indicators (screening, exposure) Studies and research Legal and policy action Regulatory decisions Projects whose primary purpose is to engage the public might be collecting qualitative information. The environmental question might address presence or absence of specific classes of plants or animals at targeted geographic locations or in a watershed. Or, citizen scientists might collect quantitative data on abundance of certain species, but summarize the data using descriptive measures, such as “low”, “medium” or “high”. In contrast, projects that provide information for measuring exposure, or for regulatory decision-making, typically require a quantitative estimation of an important condition indicator. For these projects, the study question often includes a statistic, such as the mean or median, and a measure of variability, estimated from the collected data, which can be visually displayed on a graph or map. These projects are often conducted in a well-defined study area that Page 8 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 represents a potentially impacted population (EPA, 2006). Suppose that citizen scientists are interested in the effectiveness of a pollution control device at a nearby smelter. The lead project scientist should establish a statistical hypothesis test to measure, estimate, and compare the median concentrations of air toxicants at specific locations (e.g. upwind and downwind) before and after a new pollution control device has been installed. Again, you should pursue the highest level of quality assurance and documentation that will meet your project’s intended purpose. 4. Link the Intended Use of Data to Recommended Quality Assurance Templates1 Like all scientific projects, citizen science projects employ specific strategies, or activities, to improve the credibility of data. These activities are grouped into 24 distinct elements found in standard EPA guidance (EPA, 2001; EPA, 2002). For this Handbook, however, we have consolidated these into 19 key elements shown in Table 3 (and Figure 1). The companion document, Templates for Citizen Science Quality Assurance and Documentation, helps organizations complete these elements. Your organization should fill out the templates based on how the data are intended to be used and on guidance received from a state and/or EPA regional office, if applicable. For each template, it is important that you also choose the right level of quality assurance and documentation to match the project purpose. So, projects addressing regulatory decision-making would require more quality control (QC) activities, such as more frequent instrument calibrations. Similarly, all projects require some level of training of volunteers, but projects to address regulatory decision-making would require more intensive training. 5. Just Getting Started? The guidance and concepts in the Handbook and Templates can be applied to citizen science projects at any scale, but we recognize that some groups may be in the early stages of learning how to document data quality or may be new to data collection entirely. Even at an early stage, EPA recommends projects provide some level of data quality documentation to help you make use of your data. As a coordinator of a citizen science project, you may be involved in many aspects of project planning, sample collection, laboratory analysis, data review, and data assessment and data management. Therefore, it is important to consider quality assurance (e.g. planning activities you perform to manage the project and collect, assess, and review data) and quality control (e.g. technical activities you conduct to limit error from instruments or in measurements) in every stage of your project. But, this doesn’t need to be overwhelming. You can address the essential elements of quality assurance and documentation by answering these key questions: 1) What is the purpose of the project, and the question you want to answer? 2) How and where are you planning to collect samples, data, or other information? 3) How are you training the volunteers to collect samples, data or other information? 4) How will you control for errors in the field, in the laboratory, or during data analysis? 5) How will you check your data and determine if it is useful? 6) Where do the data go and who will look at the data? 1 The distribution of quality assurance documentation for each of the data uses follows the alignment from tiered guidance of the EPA Office of Air Quality Planning and Standards, 2017. Page 9 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Science is great! Collecting data for a big project is fun and valuable but to make conclusions from the data, you need to carefully document these activities. We think that as you answer these questions by filling out the templates in this Handbook, you will be able to better communicate the quality and utility of your project’s data. Page 10 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Table 3. EPA QAPP elements and quality assurance templates recommended for citizen science projects. The templates are organized into four major Quality Assurance Project Plan element listed in EPA guidance documents. Template Increase Public Understanding Science/Research Legal/Policy A. Managing the Project 1. Title and Preparer Page X X X 2. Table of Contents X X 3. Problem Definition, Background and Project Description X X X 4. Data Quality Objectives and Indicators X X X 5. Project Schedule X X 6. Training and Specialized Experience X X X 7. Documents and Records X X X B. Collecting the Data 8. Existing Data X X 9. Sampling Design and Data Collection Methods X X X 10. Sample Handling and Custody X X 11. Equipment/Instrument Maintenance, Testing Inspection and Calibration X X 12. Analytical Methods X X X 13. Field and Laboratory Quality Control X X X 14. Data Management X X C. Assessing the Data 15. Reporting, Oversight and Assessments X X X D. Reviewing the Data 16. Data Review and Usability X X X Managing the Project (continued) 17. Organization Chart X X 18. Project/Task Organization X X 19. Project Distribution List X X Page 11 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Figure 1. Quality assurance and documentation templates recommended for citizen science projects. Increasing Public Understanding Community Engagement, Education Use of Templates 1,3,4,6,7,9,12,13,15,16 is recommended. Scientific Studies and Research Environmental Conditions Indicators, Screening, Studies and Research Use of Templates 1 - 19 is recommended. Legal and Policy Actions Regulatory Decisions Use of Templates 1 through 19 is STRONGLY recommended. This document does not define, or otherwise limit, the purpose to which citizen scientists may seek to apply their data or information. EPA retains the discretion to determine the extent it will use data or information produced or resulting from use of this document. Please note, however, that data used in regulatory and policy decision making often must be collected using approved methods, which may include acceptance testing to demonstrate equivalence to these methods. To ensure these studies meet the sampling, siting, quality assurance and all other requirements, consult the target data end users for your project. 6. In Summary This Handbook is meant to convey common expectations for quality assurance process and documentation, and best management practices for organizations that train and use volunteers in the collection of environmental data. This Handbook and its companion Templates and Examples will help the citizen science organization select the appropriate level of quality assurance and documentation to fit the intended use of the data. For over twenty years, consistent with national consensus standards for collection of environmental data, EPA has encouraged organizations to use the QAPP format to document the transparency of the project’s scientific methods, and the quality and usefulness of the collected data or information. The process of developing the QAPP is vital to the success of the project and often just as important as the final written QAPP document. The QAPP gives structure to the development of a project and helps when writing the conclusions reached at the end of the project. In addition, evaluation of the project always involves comparing what was actually done with the requirements in the QAPP. Page 12 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Instructions for Citizen Science Quality Assurance and Documentation Templates To develop the QAPP using the templates, complete each of the recommended templates applicable to your project. Templates may also be combined if the information is clearly distinguishable. Page 13 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Corresponding Template #1: Title and Preparer Page This page provides a title, an effective date of the plan, and confirms that key parties agree with the plan. Corresponding Template #2: Table of Contents For QAPPs longer than a few pages, the Table of Contents helps ensure all the information has been included and is easy to find. The table of contents should include a list (as appendices) of Standard Operating Procedures (SOPs), Instrument Manuals, and any checklists or forms, as applicable. Corresponding Template #3: Problem Definition, Background and Project Description A. Problem Definition This section describes the environmental problem, question or threat to be addressed, explains why this work needs to be done, and provides a framework for determining the project purpose, the use of the data, and the project objectives (see below). As described earlier, the QAPP development process is iterative and the problem definition can be revised as you gather new information (e.g. limitations of available methods, or results from complementary studies). B. Background This is an opportunity to describe the history of the project (or environmental problem), relevant previous studies, and how this project fills in a data gap (including from existing data) or complements existing information. If the project is related to other on-going projects, then it is important to show how and why it is related. The more specific you can be, the more favorably it will be received when being considered as an addition to your (or other) datasets. If similar work is being done by others, then collaboration could possibly lead to even stronger results. Working within the scope of existing projects may enable you to build on or possibly use an existing QAPP. A reference to papers or studies that inspired your project are also useful as it may demonstrate that you have researched and identified appropriate methods for collecting data that apply to the environmental problem. C. Project Description This section is an opportunity to succinctly describe work to be performed, the data you plan to collect, the technologies or methods used to collect the data, and the decisions you plan to make with the data. For example, you may state that volunteers for your project will visually record the number of fish passing through a fish ladder in two segments of a river during the spring over a three-year period, to determine whether fish passage increases after the removal of a dam. Additional project information as listed below will help your organization plan quality assurance activities to meet the project objectives. First, you need to identify the project objectives. These address the problem or answer the environmental questions and link data results with possible actions. One way to answer this is using “If…then…” statements, such as “If the result for parameter x in this area is above the regulatory standard, then we will…”. These objectives form the foundation for Page 14 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 the entire study. The more precise you are in defining the problem and the project objectives, the more likely it is that the project will successfully meet those objectives. This section should also include a brief description of the project site or study area, and locations as they relate to the environmental questions to be addressed. The definition of a target study area or population ensures that the samples taken are representative of the intended population, and that the locations are selected based on the project objectives. It is also essential to identify the time period for data collection, because your project objectives may apply only to a specific temporal window. For example, a project recording migratory fish through an estuary depends on the timing of the life cycle of the species of interest. Finally, you should include information on the data users. These are the individuals, groups, or agencies who are interested in, or who will make decisions based on, the data and information. Corresponding Template #4: Data Quality Objectives and Data Quality Indicators These are key elements of a QAPP and they translate project objectives into specific quality assurance and quality control activities. A. Data Quality Objectives Data quality objectives (DQOs) are quantitative or qualitative statements describing the degree of the data’s acceptability (i.e. performance criteria) for making decisions described in the project objective. For example, if you plan to compare results from a continuous air monitor for ozone to the national 8-hour standard, you can state that the data gathered must be able to measure ozone on an hourly basis, and that variability and uncertainty are minimized (and reduce the likelihood of “decision errors”) to determine statistically whether the national standard is being met. (Please note that ozone data collected by a citizen science organization would most likely be used for screening purposes and not for regulatory decision-making.) Qualitative DQOs, however, do not need to make a statistical statement. For example, a community engagement project may provide a DQO that plant species in the study area’s impacted wetlands are accurately identified by the volunteers. The process for setting DQOs can be very involved and relies on the information you provide in Templates 3 and 4. It can be summarized in the flowchart in Figure 2. Page 15 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Figure 2. The DQO Planning Process. State the problem Identify the goal Identify information inputs Define the boundaries Develop the analytical approach Specify performance criteria Develop the plan For more information on DQO planning, refer to EPA 2006 as found in the References at the back of the Handbook. B. Data Quality Indicators To determine whether the data quality objectives are being met, you should evaluate Data Quality Indicators (DQI) for each parameter measured. These DQIs are more typical for projects where quantitative measurements of parameters, such as bacteria or particulate matter in air, or abundance of species, are collected. DQIs are attributes of the data being collected, specifically related to minimizing the uncertainty for each measurement or set of measurements. They are listed below: Precision is the ability of a measurement to consistently be reproduced. Repeated measurements are usually used to determine precision. In the case of repeated measurements, one would see how close those measurements agree. Precision is often measured as the relative percent difference or the relative standard deviation. Bias is any influence in the project that might sway or skew the data in a particular direction. Taking samples from one location where a problem is known to exist, instead of taking samples evenly distributed over a wide area, is one example of how data can be biased. Bias can result from a non-representative sampling design, calibration errors, unaccounted-for interferences and chronic sample contamination. Accuracy is a degree of confidence in a measurement. The smaller the difference between the measurement of a parameter and its \"true\" or expected value, the more accurate the measurement. Also, the more precise or reproducible the result, the more reliable or accurate the result. Accuracy can be determined by comparing an analysis of a chemical standard to its actual value. Representativeness is how well the collected data depict the true system. Comparability is the extent to which data from one data set can be compared directly to another data set. The data sets should have enough common ground, equivalence or similarity to permit a meaningful analysis. Completeness is the amount of data that must be collected to achieve the goals and objectives stated for the project. It is determined by comparing the amount of valid, or usable, data you collected to what you originally planned to collect. Sensitivity is essentially the lowest detection limit of a method, instrument or process for each of the measurement parameters of interest. Measurement range is the range of reliable readings of an instrument or measuring device, or a laboratory method, as specified by the manufacturer or the laboratory. Each DQI has an associated activity, such as a Quality Control (QC) check. For example, volunteers recording pH in water using an electronic pH meter typically perform a calibration of the instrument before and after a sampling event with known standards. Or, an analytical instrument has a defined sensitivity, or detection limit. And, all DQIs have a quantitative goal. For an instrument measuring a specific analyte, a rule of thumb is that the detection limit should be at least three times less than the regulatory action level for that analyte. Page 16 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 If the project has a single use, or is limited in scope, then the data quality objectives will apply to just your project. If, however, data from your project contributes to a team or network of similar projects, then you should take into account the data quality objectives for the other projects as well. More information on selecting and calculating these indicators are available in many of the documents listed in the references including: EPA, 1996 (The Volunteer Monitor\\'s Guide to Quality Assurance Project Plans); EPA, 2002a (Guidance for Quality Assurance Project Plans. EPA QA/G-5); National Water Quality Monitoring Council (NWQMC), 2006; and Williams, et al., 2015 (Citizen Science Air Monitor (CSAM) Quality Assurance Guidelines). Corresponding Template #5: Project Schedule A project schedule helps identify timeframes necessary for planning sampling events, including preparing or procuring equipment, informing participants about timing of data collection, and expectations for distribution of data for review and reporting. Signing off on the QAPP implies that the participants agree to the planned schedule. Corresponding Template #6: Training and Specialized Experience The training of citizen data collectors is a very important part of a project. Training ensures consistency and should be recorded and documented. Note that records of training can be as simple as an email summarizing what was taught, by whom and for whom. Training records are essential to evaluate whether procedures are performed correctly and to document the qualifications of the people involved. Some projects rely on professional scientists or trainers with specialized experience that greatly assist the project. Corresponding Template #7: Documents and Records It is critical that the key project personnel are aware of the location and status of important documents (e.g. QAPPs, Standard Operating Procedures (SOPs), field data sheets and records (e.g. databases, quality control (QC) checklist). By attaching copies of procedures and checklists to the QAPP, you can provide consistency for the whole project. This also assists in training, data analysis and reporting. Corresponding Template #8: Existing Data and Data from Other Sources It is often valuable (or necessary) for projects to use existing data. Existing data can include sampling and testing data collected during previous investigations; historical data, background information; interviews; modeling estimates; photographs; aerial photographs; topographic maps; and published literature, including from Federal and State agencies. The project team should determine whether the quality of the data collected (even by reliable sources or reported in a peer reviewed journal article) are acceptable for the objectives of your project. You should consider the source of the data, the time period during which the data were collected, data collection methods, potential sources of uncertainty, and the type of supporting documentation available including quality assurance documentation such as precision, bias, representativeness, comparability, and completeness. Some of this information may not be available, and you will need to make a judgement on the limitations of the use of the data. Note that there are many sources of Page 17 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 existing data that are recognized as reliable, such as National Weather Service data, or other national monitoring networks (e.g. United States Geological Survey). You should complete this template if your project will be using existing data. If there are no existing data being used, state “no existing data is being used”. Corresponding Template #9: Sampling Design and Data Collection Methods A. Sampling Design You need to design a sampling regime that meets the project objectives. For example, you might consider a statistical, or probability-based design to make inferences over a large geographic area, a watershed, or a community. Or you may want to target specific areas using a judgmental design to identify “hot-spots” and contrast “reference” versus “impacted” populations. And, you need to consider the sampling methods or technologies you will employ, e.g. discrete sampling, in situ sensors, continuous monitoring, or vegetation transects. Details about the sampling design and quality control activities should also be documented so that if other organizations repeat the project, they would generate similar results. Some of this information may occur elsewhere in the QAPP and can be referenced here. This template should also list the quantity and type of quality control (QC) samples collected during the project. QC samples are needed to evaluate whether your data quality objectives and indicators are met (see Template #4). For example, contamination is a common source of error in both sampling and analytical procedures, so blank samples are collected to identify when and how contamination might occur. An organization should always document that blanks are consistently free of contamination. A general rule is that 10 to 20% of field collected samples should be QC samples. The laboratory must also run its own QC samples (such a positive and negative controls for microbiological analysis) because they also need to document that their operations do not cause increases in error. For a new monitoring project or for a new analytical procedure, it is a good idea to increase the number of QC samples (up to 20%) until you have full confidence in the procedures you are using. Types of QC samples are described in Table 4. Projects that collect qualitative information, such as general abundance of specific species, can evaluate quality by having more than one individual make the same assessment. Vegetation surveys that rely on correct identification of species often send voucher specimens to regional experts. Note that most of these QC samples are designed primarily for discrete water samples and other types of samples may be required for air or water continuous monitoring. For air monitoring projects, the organization should co-locate sensors at or near sites where reference instruments have been deployed by regulatory agencies. If you are mapping vegetation, or conducting a faunal survey, it is good practice to perform replicate transects to evaluate field variability or have more than one person evaluate the same transect. To assess accuracy of your taxonomic identification, some scientists recommend use of voucher specimens. A voucher specimen is any specimen that serves as a basis of study and is retained as a reference. It should be in a publicly accessible scientific reference collection. Page 18 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Table 4. Typical QC sample types, descriptions and uses for discrete sampling QC Sample Type Description Useful for (examples) Field Blank A “clean” sample, produced in the field, used to detect or document contamination during the whole process (sampling, transport, and lab analysis). Examples include clean sampling containers, blank filters, etc. that are treated the same as field samples, except no sample is collected in/on them. Water, sediment and soil sampling; air sampling onto filters Equipment This type of blank is used to evaluate if there is carryover contamination from Water, sediment and or rinse reuse of the same sampling equipment. A sample of distilled water (or other soil sampling blank solvent, per the method) is collected in a sample container using regular collection equipment and analyzed as a sample. equipment; filter air sampling equipment Split Sample that is divided equally into two or more sample containers and then Water, sediment, soil sample analyzed by different analysts or laboratories. Used as a measure of precision or to measure the variability in results between laboratories or samples independently analyzing the same original sample. and fish tissue sampling Co-located Applicable to air and water sampling. For air sampling, two or more sample Water, sediment and samples collection devices, located together in space and operated simultaneously, to supply a series of duplicate or replicate samples for estimating precision of the total measurement system/process and verification with established methods. Co-located samples must be exposed to the same environment and exposed to the same air. For water sampling, discrete sampling could be applied to compare with continuous monitoring stations. soil Replicate Obtained when two or more samples are taken from the same site, at the same Water, sediment and Samples time, using the same method, and independently analyzed in the same manner. When only two samples are taken, they are sometimes referred to as duplicate samples. These types of samples are representative of the same environmental condition. Replicates (or duplicates) can be used to detect both the natural variability in the environment and that caused by field sampling methods. soil Spiked Samples to which a known concentration of the analyte of interest has been Water, sediment and Samples added. Spiked samples are used to measure accuracy. If this is done in the field (which is unusual), the results reflect the effects of matrix, preservation, shipping, laboratory preparation, and analysis. If done in the laboratory, they reflect the effects of the analysis from the point when the compound is added, e.g. just prior to the measurement step. Percent recovery of the spike material is used to calculate analytical accuracy. soil B. Data Collection Methods A successful project relies on a consistent protocol for sample collection. Organizations should be able to train their volunteers in these methods so that data collection is feasible and repeatable. Information on recommended sample Page 19 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 collection methods, specialized containers, or technologies for ambient water and air, drinking water, and vegetation sampling, may be found in the references and at the following resources: \\uf0d8 Stream, estuary and lake monitoring: https://www.epa.gov/nps/nonpoint-source-volunteer-monitoring \\uf0d8 Drinking water monitoring: https://www.epa.gov/sites/production/files/2015- 11/documents/drinking_water_sample_collection.pdf \\uf0d8 New England States Drinking Water Monitoring: https://www.epa.gov/sites/production/files/2015- 06/documents/NE-States-Sample-Collection-Manual.pdf \\uf0d8 Air Sensor Monitoring: https://www.epa.gov/air-sensor-toolbox \\uf0d8 Continuous water quality monitoring: https://pubs.usgs.gov/tm/2006/tm1D3/pdf/TM1D3.pdf Additional resources are available at the following resources targeted for citizen scientists: \\uf0d8 https://www.citizenscience.gov/toolkit/resource-library/ \\uf0d8 http://citizenscience.org/ If there are Standard Operating Procedures (SOPs) used, either attach them to the QAPP or cite the publication where they can be found. When possible, obtain the exact position (using GPS or smartphone applications) of where the sample came from, including sample depth, height, and air or water temperature (if appropriate). If during data review, the reported value appears to be a potential outlier, it may be possible to resample exactly at that point where the initial sample was taken. It could be that it really was an anomaly, but it could be that something else was interfering with the physical conditions at that particular point, for example, an unexpected “hot spot” that should be considered differently from the other values. Corresponding Template #10: Sample Handling and Custody To ensure that the sample is not altered after collection or meets the laboratory holding time requirements, this template describes your efforts to have each collected sample retain its original physical form and chemical composition through collection to final disposal. It also identifies maintenance of custody (or possession) of the sample. These principles also apply to continuous monitoring measurements. Chain-of-custody procedures are especially critical for projects where the data may be used in court as evidence or for making regulatory decisions. A chain-of-custody log (which may be composed of shipping manifests or receipts, as long as a person is identified at the point of shipping and receipt at the lab) may be vital to defend the integrity of the data in the case of suspected tampering. Some methods include seals or other configurations to detect tampering. Corresponding Template #11: Equipment List, Instrument Maintenance, Testing, Inspection and Calibration Many citizen scientists use electronic (in situ) sensors in water and air matrices and these instruments need to be calibrated. All calibrations for a project should be planned for and documented. Calibration records should be kept on calibration data sheets specific to each piece of equipment and include date, time, name of individual doing calibration, and the calibration results themselves. Acceptance criteria for calibration checks should also be included Page 20 of 31 https://www.epa.gov/nps/nonpoint-source-volunteer-monitoring https://www.epa.gov/sites/production/files/2015-11/documents/drinking_water_sample_collection.pdf https://www.epa.gov/sites/production/files/2015-11/documents/drinking_water_sample_collection.pdf https://www.epa.gov/sites/production/files/2015-06/documents/NE-States-Sample-Collection-Manual.pdf https://www.epa.gov/sites/production/files/2015-06/documents/NE-States-Sample-Collection-Manual.pdf https://www.epa.gov/air-sensor-toolbox https://pubs.usgs.gov/tm/2006/tm1D3/pdf/TM1D3.pdf https://www.citizenscience.gov/toolkit/resource-library/ http://citizenscience.org/ Handbook for Citizen Science Quality Assurance and Documentation – Version 1 on the data sheets. Acceptance criteria are usually quantitative measures and goals that evaluate whether the results can be used. For example, you may check your instrument after sampling many sites against a calibration standard. This is a QC check and called verification (see Template #16). If the difference is within the acceptance criteria (or DQI goals, see Template #4), usually expressed as a percentage, you can accept the results. Calibration is important because it helps prevent bias. Depending on the specifications of the equipment manufacturers, some equipment needs more attention than others and this template helps in making sure equipment does not ‘drift’ over time. This applies especially to continuous monitoring instruments in water and air. Water sensors are especially prone to drift caused by biofouling and is often distinct from sensor drift. Analytical and field instruments differ in precision and sensitivity (i.e. lower limit of detection). To compare or merge datasets, or if instruments are changed during the course of a project, it is critical to document these specifications. Corresponding Template #12: Analytical Methods This template lists in one place all the specifications for the laboratory or field measurement method organized by each parameter, or analytical group (such as metals, or nutrients). It also helps establish the criteria, such as reporting limits, needed for the project, based on the data quality objectives identified in Template #4. Often, the most important specifications here are the actual sample volume the laboratory needs to make the appropriate analysis and the holding time for the sample. To complete this template, an organization should consult with the analytical laboratory, or field method manufacturer or authorized representative to ensure that the amount of material, water, or air sampled is sufficient to meet the target detection (or reporting) limits. Most information regarding limits and sample volumes for methods can be found online, for both laboratories using standard methods and for field measurement equipment. Corresponding Template #13: Field and Analytical Laboratory Quality Control Summary The tables in this template summarize information from Template #s 4 and 9 on the types of quality control (QC) samples that will be collected in the field and by the laboratory. Page 21 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Corresponding Template #14: Data Management As was done for sample handling, the information in this template traces the path of the data, from field collection and laboratory analysis to final use or storage. This ensures that project personnel use data of known status or of a defined stage of review or validation. You will need to describe record-keeping procedures, your document control system, and the approach used for data storage and retrieval on electronic media. Any forms or checklists should also be included as attachments. Corresponding Template #15: Reporting, Oversight and Assessments Assessments and project oversight include reviews (if possible, by an outside reviewer) to identify shortcomings or departures from the QAPP, corrective actions taken, and limitations of the use of the data. Reporting of results is critical for ensuring that any deviations are corrected if possible, and if none are found, to document the successful implementation of procedures. It is also important to report results and interpretations so that project participants are engaged, and understand the project’s progress and results, and limitations or setbacks, if found. Data and interpretive reports might be distributed to project partners and government agencies, but also posted to web sites for public access. Corresponding Template #16: Data Review and Usability Use this section to describe how your organization will review, verify, and validate data to determine whether your project objectives were met, and that your data are fit for use. The level of detail and frequency for performing data review depends on the intended use of the data. You should describe how project personnel (such as the project coordinator and the Quality Assurance Manager, if available) will review data as they are obtained and documented (e.g., in checklists, logbooks, emails, and QA reports). Although data verification and validation are typically conducted sequentially, it may be beneficial (and more cost effective) for smaller projects to combine steps. For example, the personnel conducting the verification could also conduct the first step of the validation process concurrently. When multiple people are involved in a project, the initial data collector typically conducts the first verification at the time of collection and later, if a QA Manager is available (see Template #17), he or she should review the QC checks recorded on the data sheets. The first step in review is verification. Verification is determining whether an activity conforms to the stated requirements, (such as acceptance criteria) for that activity. For example, if you are using a hand-held multi-meter to determine pH and conductivity at certain points in a stream, you should have established calibration procedures and QC checks (associated with data quality indicators) for those two parameters. While you are collecting your data, you would record the results (e.g. on a datasheet or in a notebook) and then review them to ensure that QC checks such as blanks and calibration checks are within the goals (or acceptance criteria) that you have set (see Template #s 4, 9 and 11). This verification process indicates that your multi-meter is operating correctly. Verification also applies if your data are qualitative, such as checking to see if a fish or plant species identification was performed correctly. The next step in review is validation. Validation is determining whether the activities conform to the user needs for the overall project. This step is a higher-level activity that introduces additional (mostly quality assurance) activities such as reviewing sampling locations, training, chain-of-custody, documentation, and appropriate methods. These activities tie Page 22 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 back to the Data Quality Objectives to determine whether the data can be used. (See Figure 3 below for the distinction between verification and validation.) Regarding the example used above, if you were measuring conductivity in a stream to evaluate the impacts of road salt, you might consider the following questions: 1. Did we collect the right types of data? Can we make a decision based on pH and conductivity alone, or should we have also taken other measurements such as temperature, nitrate, or dissolved oxygen? 2. Did we collect data from the right locations in the stream(s)? If we had planned to collect data from all the appropriate locations, did we meet our completeness goal? Can we compare our results statistically to a regulatory standard? 3. Did all the datasets meet 100% of the QC criteria? If not, what is the impact to the overall study? Are data qualifiers needed to describe certain specific QC issues that did not meet their planned QC acceptance criteria but do not necessitate complete rejection of the dataset? All data issues, such as outliers, or batches of samples that do not meet acceptance criteria, should be discussed with the project lead or QA Manager (if available) and explained in project reports. In some cases, data qualifiers may be needed to “flag” data that do not meet QC acceptance criteria, but do not require rejection of the full dataset.2 Figure 3. The distinction between verification and validation. 2 Data qualifiers (sometimes informally called “flags”) are a set of codes that are applied to your data to describe various aspects of an analysis that did not meet the QC goals (or acceptance criteria) described in the QAPP or SOPs. Typical flags are for high blanks, low check standards, or measurements below the detection limit. For additional information about common types of data qualifiers, see Appendix C of EPA QA/G-8 Guidance (EPA, 2002b) on Environmental Data Verification and Data Validation. Page 23 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Finally, you should ensure that your review will evaluate whether you have met the project objectives, and the data are usable, or whether you have identified and documented limitations in the use of the data. You may identify significant departures from the QAPP, or incorrect assumptions established in the planning phase of data collection. You may have uncovered unique qualities of the sample matrix; use of non-standard analytical methods; or a sampling design that is not representative or does not allow for planned statistical comparisons. Any potential limitations of the data should be documented in records associated with these data (i.e., metadata) and included in the final project report. Corresponding Template #17: Project Organization Chart The organization chart shows the lines of communication and reporting for the project, like a chain of command. The chart should represent the personnel responsible for ensuring data quality, such as the project lead, trainers, and field data collectors. Some projects require a team of people with differing responsibilities and titles; others may require the services of a single person on a part-time basis. In general, organization charts are not needed for small projects. We recommend that you identify one person responsible for the overall integrity of the project, sometimes referred to as the Quality Assurance Manager (QAM). To ensure the inadvertent introduction of bias, this person should be independent of the staff working on the project and should have sufficient knowledge of the methodologies of the project, and the confidence to ask questions. The role of the QAM is to evaluate the project activities, methods and results and determine whether the data collected are meeting the project objectives. Many small projects may not have the capability to employ an independent person, so a qualified, experienced, and trained individual can fill this role. Corresponding Template #18: Project Organization This shows everybody’s role in the development of the project. The responsibilities section provides an outline of the work that will be done for the project. Project-specific details are addressed in many of the other templates. Corresponding Template #19: Project Distribution List The distribution list ensures everyone involved with the project receives a copy of the QAPP or other documents and is aware about the work being conducted. It also provides the contact information for those involved with the project, so everyone is aware if changes to the project are made. Page 24 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Definitions3 Acceptance criteria. Quantitative measures and goals that evaluate whether the results can be used. Accuracy. A data quality indicator, accuracy is the extent of agreement between an observed value (sampling result) and the accepted, or true, value of the parameter being measured. High accuracy can be defined as a combination of high precision and low bias. Analyte. Within a medium, such as water, an analyte is a property or substance to be measured. Examples of analytes would include pH, dissolved oxygen, bacteria, and heavy metals. Assessment. The evaluation process used to measure the performance or effectiveness of a system and its elements. As used here, assessment is an all-inclusive term used to denote any of the following: audit, performance evaluation, management systems review, peer review, inspection, or surveillance. Audit (quality). A systematic and independent examination to determine whether quality activities and related results comply with planned arrangements and whether these arrangements are implemented effectively and are suitable to achieve objectives. Bias. Often used as a data quality indicator, bias is the degree of systematic error present in the assessment or analysis process. When bias is present, the sampling result value will differ from the accepted, or true, value of the parameter being assessed. Blind sample. A type of sample used for quality control purposes, a blind sample is a sample submitted to an analyst without their knowledge of its identity or composition. Blind samples are used to test the analyst’s or laboratory’s expertise in performing the sample analysis. Calibration. Comparison of a measurement standard, instrument, or item with a standard or instrument of higher accuracy to detect and quantify inaccuracies and to report or eliminate those inaccuracies by adjustments. Chain-of-custody. An unbroken trail of accountability that ensures the physical security of samples, data, and records. Citizen science. When the public participates voluntarily in the scientific process, addressing real-world problems in ways that may include formulating research questions, conducting scientific experiments, collecting and analyzing data, and interpreting results. Contractor. Any organization or individual that contracts to furnish services or items or perform work; a supplier in a contractual situation. 3 EPA, 2001 - EPA Requirements for Quality Assurance Project Plans, March 2001: QA/R-5. EPA/240/B-01/003. Office of Environmental Information. U.S. Environmental Protection Agency. Washington, DC 20460. https://www.epa.gov/sites/production/files/2016-06/documents/r5-final_0.pdf Page 25 of 31 https://www.epa.gov/sites/production/files/2016-06/documents/r5-final_0.pdf Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Comparability. A data quality indicator, comparability is the degree to which different methods, data sets, and/or decisions agree or are similar. Completeness. A data quality indicator that is generally expressed as a percentage, completeness is the amount of valid data obtained compared to the amount of data planned. Data quality assessment. A statistical and scientific evaluation of the data set to determine the validity and performance of the data collection design and statistical test, and to determine the adequacy of the data set for its intended use. Data quality indicators. Attributes of the data being collected, specifically related to minimizing the uncertainty for each measurement or set of measurements. These typically include precision, bias, accuracy, representativeness, comparability, completeness, sensitivity and measurement range. Data quality objectives (DQOs). Data quality objectives are quantitative and qualitative statements describing the degree of the data’s acceptability or utility to the data user(s). They include indicators such as accuracy, precision, representativeness, comparability, and completeness. DQOs specify the quality of the data needed to meet the monitoring project\\'s goals. The planning process for ensuring environmental data are of the type, quality, and quantity needed for decision making is called the DQO process. Data usability. The process of ensuring or determining whether the quality of the data produced meets the intended use of the data. Data users. The group(s) that will be applying the data results for some purpose. Data users can include the monitors themselves as well as government agencies, schools, universities, businesses, watershed organizations, and community groups. Detection limit. Applied to both methods and equipment, detection limits are the lowest concentration of a target analyte that a given method or piece of equipment can reliably ascertain and report as greater than zero. Duplicate sample. Used for quality control purposes, duplicate samples are two samples taken at the same time from, and representative of, the same site that are carried through all assessment and analytical procedures in an identical manner. Duplicate samples are used to measure natural variability as well as the precision of a method, monitor, and/or analyst. More than two duplicate samples are referred to as replicate samples. Environmental conditions. The description of a physical medium (e.g., air, water, soil, sediment) or biological system expressed in terms of its physical, chemical, radiological, or biological characteristics. Environmental data. Any measurements or information that describe environmental processes, location, or conditions; ecological or health effects and consequences; or the performance of environmental technology. Environmental data also includes information collected directly from measurements, produced from models, and compiled from other sources such as data bases or the literature. Page 26 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Environmental sample. An environmental sample is a specimen of any material collected from an environmental source, such as water or macroinvertebrates collected from a stream, lake, or estuary. Equipment or rinse blank. Used for quality control purposes, equipment or rinse blanks are types of field blanks used to check specifically for carryover contamination from reuse of the same sampling equipment (see field blank). Field blank. Used for quality control purposes, a field blank is a “clean” sample (e.g., distilled water) that is otherwise treated the same as other samples taken from the field. Field blanks are submitted to the analyst along with all other samples and are used to detect any contaminants that may be introduced during sample collection, storage, analysis, and transport. Instrument detection limit. The instrument detection limit is the lowest concentration of a given substance or analyte that can be reliably detected by analytical equipment or instruments (see detection limit). Matrix. A matrix is a specific type of medium, such as surface water or sediment, in which the analyte of interest may be contained. Measurement range. The measurement range is the extent of reliable readings of an instrument or measuring device, as specified by the manufacturer. Metadata.4 The simplest definition of metadata is “structured data about data.” Metadata is structured information that describes, explains, locates, or otherwise makes it easier to retrieve, understand, use or manage an information resource. Method detection limit (MDL). The MDL is the lowest concentration of a given substance or analyte that can be reliably detected by an analytical procedure (see detection limit). Performance evaluation (PE) samples. Used for quality control purposes, a PE sample is a type of blind sample. The composition of PE samples is unknown to the analyst. PE samples are provided to evaluate the ability of the analyst or laboratory to produce analytical results within specified limits. Precision. A data quality indicator, precision measures the level of agreement or variability among a set of repeated measurements, obtained under similar conditions. Precision is usually expressed as a standard deviation in absolute or relative terms. Protocols. Protocols are detailed, written, standardized procedures for field and/or laboratory operations. Quality assurance (QA). QA is an integrated management system designed to ensure that a product or service meets defined standards of quality with a stated level of confidence. QA activities involve planning quality control, quality assessment, reporting, and quality improvement. 4 EPA Classification No.: CIO 2135-S-01.0 - Enterprise Information Management (EIM) Minimum Metadata Standards Page 27 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Quality Assurance Manager (QAM). The individual designated as the principal manager within the organization having management oversight and responsibilities for planning, documenting, coordinating, and assessing the effectiveness of the quality system for the organization. Quality assurance project plan (QAPP). A QAPP is a formal written document describing the detailed quality control procedures that will be used to achieve a specific project’s data quality requirements. Quality control (QC). QC is the overall system of technical activities designed to measure quality and limit error in a product or service. A QC program manages quality so that data meets the needs of the user as expressed in a quality assurance project plan. Relative standard deviation (RSD). RSD is the standard deviation of a parameter expressed as a percentage and is used in the evaluation of precision. Relative percent difference (RPD). RPD is an alternative to standard deviation, expressed as a percentage and used to determine precision when only two measurement values are available. Replicate samples. See duplicate samples. Representativeness. A data quality indicator, representativeness is the degree to which data accurately and precisely portray the actual or true environmental condition measured. Sensitivity. Related to detection limits, sensitivity refers to the capability of a method or instrument to discriminate between measurement responses representing different levels of a variable of interest. The more sensitive a method is, the better able it is to detect lower concentrations of a variable. Spiked samples. Used for quality control purposes, a spiked sample is a sample to which a known concentration of the target analyte has been added. When analyzed, the difference between an environmental sample and the analyte’s concentration in a spiked sample should be equivalent to the amount added to the spiked sample. Split sample. Used for quality control purposes, a split sample is one that has been equally divided into two or more subsamples. Splits are submitted to different analysts or laboratories and are used to measure the precision of the analytical methods. Standard reference materials (SRM). An SRM is a certified material or substance with an established, known and accepted value for the analyte or property of interest. Employed in the determination of bias, SRMs are used as a gauge to correctly calibrate instruments or assess measurement methods. SRMs are produced by the U. S. National Institute of Standards and Technology (NIST) and characterized for absolute content independent of any analytical method. Standard deviation(s). Used in the determination of precision, standard deviation is the most common calculation used to measure the range of variation among repeated measurements. The standard deviation of a set of measurements is expressed by the positive square root of the variance of the measurements. Page 28 of 31 Handbook for Citizen Science Quality Assurance and Documentation – Version 1 Standard operating procedures (SOPs). An SOP is a written document detailing the prescribed and established methods used for performing project operations, analyses, or actions. True value. In the determination of accuracy, observed measurement values are often compared to true, or standard, values. A true value is one that has been sufficiently well established to be used for the calibration of instruments, evaluation of assessment methods or the assignment of values to materials. Validation. Confirmation by examination and provision of objective evidence that the particular requirements for a specific intended use are fulfilled. In design and development, validation concerns the process of examining a product or result to determine conformance to user needs. Variance. A statistical term used in the calculation of standard deviation, variance is the sum of the squares of the difference between the individual values of a set and the arithmetic mean of the set, divided by one less than the numbers in the set. Verification. Confirmation by examination and provision of objective evidence that specified requirements have been fulfilled. In design and development, verification concerns the process of examining a result of a given activity to determine conformance to the stated requirements for that activity. Page 29 of 31',\n",
              " 'Citizen Science Skilling for Library Staff, Researchers, and the Public 2 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries SECTION 1: INTRODUCTION Citizen science as a scientific discipline is inevitably linked to the creation of data: identifying which data may answer your questions by using citizen science, attracting citizens and other stakeholders interested in the data, collecting data, telling the story of the data, and repurposing data. Citizen science can increase scientific literacy by use of data. In this section, you will learn which skills can support a citizen science project across its life cycle and facilitate its success. These skills relate to project management, communication, management of research data and integrating scientific literacy into the project. Identifying persons and resources to the different tasks of the project already from the beginning may be a daunting task. However, realising and incorporating skills is a huge step towards creating a project that brings quality to not only the data, but also an experience of quality participation to the citizen scientist. It is important to note that citizen science belongs to the open science domain, and is therefore perceived as a method, where research data are shared at large with open access to publications and full transparency of data availability. However, data use has to comply with ethical and legal obligations, such as GDPR, and with the expectations of the citizens. Accordingly, the FAIR principles do also have a role to play for citizen science data. The principles can help navigate the open science expectations and engagement of the citizens with the actual possibilities for sharing and reuse. By Jitka Stilund Hansen, Technical University of Denmark, ORCID iD: 0000-0002-5888-1221 e-mail: jstha@dtu.dk Article DOI: 10.25815/rr9w-cw53 How to obtain good quality citizen science data is not addressed in this section, but it is inevitably linked to the possibility of reusing the data. A task of the research librarian is also to https://orcid.org/0000-0002-5888-1221 mailto:jstha%40dtu.dk?subject= https://doi.org/10.25815/RR9W-CW53 3 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries This section will help you: convey to the researcher or project holder, why project management and good research data management practices are important: Quality data emerges from good management. If the citizen experiences that the project produces quality data fit for reuse and creating impact, this could empower the citizen and is a strong motivation factor. The skills highlighted in the section may due to their diversity not all be embedded in the research library initially. Therefore, the intention is to help clarify which support is already present and which skills should be developed or sought elsewhere. Hopefully, this guide can create momentum for the development of library services directed at citizen science. Academic researchers and project managers should be able to extract useful knowledge about management of citizen science projects and their data, and where to obtain more information. Images: https://blush.design/collections/humaaans Blush license https://blush.design/license If the citizen experiences that the project produces quality data…, this empowers the citizen and is a strong motivation factor. · Learn how citizens and other stakeholders have a role to play at many points during the project life cycle: · how they are involved in project management and co-creation, · how communication with them could be handled, and · what are the obligations pertaining to the data and knowledge provided. · Get practical advice on project management and communication planning. · Identify elements of FAIR data that require particular attention in citizen science projects. · Understand how scientific literacy can be used for co-creation and education in citizen science. https://blush.design/collections/humaaans https://blush.design/license 4 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries This project plan is a generic guide on how to run the practical side of a citizen science project with regards to planning and governance. It is based on the condition that a researcher or Principal Investigator (PI) leads the project. Assisting the PI is a Project Manager (PM), who is responsible for practicalities in close dialogue with the PI. The role of the PM is sometimes defined as Community Manager, based on the scope and communication of the project. Library staff in many ways are suited for the role as PM or project coordinator. BEFORE Step 1: Definition of goal(s) The overall outcome of the project is defined. This should provide a clear picture of what the researcher wants to investigate, who has an interest in participating and the goal(s) of the project. At this point, common ground is laid between the PI and PM in a dialogue with indispensable stakeholders, optimally including citizens. Which research objective or perhaps question is covered? The overall goals could be e.g. · What is the research objective? How will it be investigated or expanded? · The central elements with regards to public engagement and data collection: What is the motivation and outcome for citizens and potential partners. Already It’s important that this generic plan is supplemented with e.g., a GANNT chart for organisational and time managing purposes, as well as other helpful tools such as a stakeholder analysis, communication plan and risk analysis. In some cases, the project plan cannot be outlined until funding is applied for and awarded. This guide does not cover that. Furthermore, if you are venturing in a multi-partner or EU project, this guide is hardly sufficient. It does also not take into account citizen-led or -initiated projects. The project plan Work on a citizen science project can roughly be divided into three phases. The BEFORE (step 1-5) where the project is initiated and planning is carried out. The DURING (step 6-8) where the project goes public. The AFTER (step 9) where evaluation, analysis, scientific outcome, and communication to and with participants is carried out. By Line Laursen, University of Southern Denmark, e-mail: linel@bib.sdu.dk and Thomas Kaarsted, University of Southern Denmark, ORCID iD: 0000-0001-6796-5753 e-mail: thk@bib.sdu.dk Article DOI: 10.25815/r3nj-fd31. The Project Planning guide stems from the work at SDU and the templates and tools we offer. PROJECT PLANNING: A STEP-BY-STEP GUIDE https://orcid.org/0000-0001-6796-5753 https://doi.org/10.25815/R3NJ-FD31 mailto:linel@bib.sdu.dk mailto:thk@bib.sdu.dk 5 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries at this point consider: What’s in it for the citizens? This means that the scholar should explain how his/her research relates to citizens’ preoccupations or areas of interest. An alternative approach would be to involve citizens in the definition of the research question. · Internal goal setting: Aligning the goals and roles between PI, PM, administration, possible partners, and citizens. Step 2: Stakeholder analysis In order to operationalize the objective and goals, a stakeholder analysis could be carried out. We suggest this template in order to identify stakeholders and possible partners. Stakeholders can be very varied depending on the project and cover institutions, organizations, groups, or individuals that could have an interest or stake in the outcome or goals of the project. Stakeholders could also be researchers, peers, students or pupils (in which case the project could have a learning component). After identifying stakeholders, a dialogue is established and stakeholders are onboarded. They should help form the project and outline their role. In this phase, ‘must be’ stakeholders who are critical for the outcome of the project are identified. Citizens, of course, are at the core of every citizen science project. The steps above focus on organized groups or institutions that represent other citizens. This is not to marginalize the individual, but to suggest a structured approach. Individuals are also reached through the communication plan. Step 3: Project goals and milestones Once the outcome and stakeholders are onboard, the project timeline is defined. We recommend that the PI and PM spend a fair amount of time on a realistic and detailed plan that covers the main deliveries and assess the needed resources. A definition of the tasks and milestones (main deliveries) enables the PM to work based on a critical overview. A detailed timeline should include target dates and identifying risks. As mentioned above, a GANNT chart could be recommended. Also, data management is addressed. See Management of Citizen Science Data onwards. Quite often a research library offers specialists within the field. Step 4: Assess resources and costs Once the timeline is complete it’s possible to create an overview of the resources and costs needed. Almost all projects come to fruition Example of resources needed In the SDU citizen science project ‘Find a Lake’ (see also communication plan) citizens including public schools and high schools) contribute to assistant professor Sara Egemose’s research into water quality by providing water samples and other data. From a resource standpoint the project relies on: · The human resources include a main researcher (PI), a research assistant (community manager), students that do advocacy, a former teacher for didactic and communication purposes, an associated professor in charge of evaluation (science literacy), an IT-consultant in charge of the app, and a project manager, coordinator and web manager who also does visualization of data. · The monetary costs focus mainly on citizen science kits for education and data collection purposes. The target group is kids and their families. Costs are also allocated for pop up events, camps and other equipment. 6 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries after a potential lengthy application and budget process. Otherwise the budget is outlined by the PI and PM in unison. By resources we mean both human and monetary. · Human: Researchers, research assistants, student help, communication, data management, visualization of data, it- infrastructure, and administration. · Monetary: The cost of recruiting citizens, conducting workshops, town hall meetings, building apps, boosting via social media, producing video, buying equipment, tools, as well as travel costs etc. With regards to human resources are the right competences present? Who does what? Occasionally, communication and community management are underestimated, leaving motivation and thus potentially data and outcome in jeopardy. This is one of the things to be addressed in the risk assessment analysis (step 5). At this point, it’s important to address which target group or groups the project aims at as this is critical for assessing risk vs. outcome with regards to resources. Communication also comes as a premium. No later than this step, a communication plan is devised. A data management plan to overview resources needed for handling data and possible ethical and legal advice should optimally be drafted. You might also explore if the project could be evaluated by learning outcome or science literacy in case pupils or students participate. Step 5: Risk analysis checklist Risk analysis can help manage the potential problems and uncertainties in a citizen science project, including, where the a lack of skills or resources could become a problem. The analysis should be done in each phase of the project. There are a vast number of risk analysis matrices, templates and models readily available online. Very few, however, address research projects let alone citizen science projects. Note: Some citizen science projects e.g. within health science have serious GDPR and ethic considerations. This should be addressed before the project is outlined as a very first step. Below you’ll find some observations and tips that can help structure your thinking. Risk, in this sense, is not meant as danger. It does not e.g. address unsupervised kids collecting data in a particular environment (although this is certainly a risk). The risk analyses should optimally be carried by the PI and PM together with staff with knowledge and insights in their specialist areas. 1. Brainstorm on potential troublesome areas. Every area should be included. From research outcome to data collection, to communication, retention and recruitment of citizens, to ethics, GDPR etc. Ask yourself: What could go wrong in this particular area? 2. What are the potential negative consequences? When the areas are identified, rate them on a scale from 1-5, where 5 is a risk that potentially can shut down the project. 3. How likely are the risks to happen? Again, rated from 1-5, where 5 is inevitable. 4. Multiply the numbers and you’ll end up with a list of potential risks rated from 1-25. Again, a score of 25 might close the project. 5. Make a prioritized list. Which risks should be on the project’s risk list? 6. How do you handle the risks? Some can be prevented. Then, they go off the list. Others can be remedied, and a Plan B can be ready at hand. 7. Make a ‘go’ or ‘no go’ decision, the PI and PM in unison. This abovementioned list is meant for inspiration. If you work with e.g., IT-heavy projects or have data collection with potentially huge GDPR or legal ramifications, your university might be able to advise. Images: All cartoons by Frits Ahlefeldt, CC BY-NC-ND 4.0, https://fritsahlefeldt.com/ https://fritsahlefeldt.com/ 7 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries The risk assessment might cause a revision of the project plan (aim, goals, partners, roles and timeline). This is also discussed in step 6. DURING Step 6: Recruitment and retainment For the research and public engagement side of the project to run as smoothly as possible, support from stakeholders and participants are critical. There are several ways to obtain that, but include both groups early in the conversation and co-creation by inviting to: · Workshops and dialogue meetings · Social media communities · Info meetings · Kickoff meetings · Collect suggestions and discuss terminology and research objectives Again, dialogue is critical which is also at the core of the communication plan. Get ideas, feedback, do post-its sessions, identify possible barriers and potential boosters. Be open and transparent on how you can communicate and include participants and stakeholders in activities, data collection and discussing the results. And where you cannot. The actors in a citizen science project very often have insights, experiences and connections into local communities, which are of high value. Step 7: Follow up, momentum and communication When the project partners agree on the project plan, it is distributed to all stakeholders and communicated to participants. In this phase, the communication plan kicks in. Including the ongoing collection, communication and sharing of data. An example of recruitment and retainment In the SDU citizen science project ‘Our History’, high school students interview elderly citizens on their life experiences with a focus on the changes in the family as an institution, work/ life values and socio-economic themes. The project was originally conceived by Professor Klaus Petersen and three students from the SDU Talent Programme. Very early the project reached out to high schools in the region to establish partners. During various steps, a plan to embed the project in the curriculum was conceived. Similarly, the project (via the university library) built a digital learning platform that teachers and students envision will make them capable of conducting interviews in a semi-structured scientific way. Finally, besides the interviews themselves, the students will participate in a poster session where they reflect on the results. The point: It took several steps or loops to design the project with the aim of recruiting and retaining participants. In the end, all interviews are available online. Image: SCANPIX https://www.sdu.dk/en/forskning/forskningsformidling/citizenscience/vores-historie 8 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Depending on the type of project and the complexity concerning partners and stakeholders, status meetings can be held. They should as a minimum include the PI, PM, and key members of the project team. If a steering committee of stakeholders are formed, they are included as well. Status meetings are for knowledge sharing, for boosting outreach and data collection, and potentially for discussing the milestones and revising the goals. These meetings should provide clear answers to whether the project is progressing as planned. An idea is to take stock of deliverables: · What is planned for the next 2-4 weeks? · Are there any critical deliverables? · Identify possible delays or non- deliverables due to lack of resources. Step 8: Revision of the project plan A logical consequence of the follow-up meetings may be revision of the project plan. Just as the partners and participants are visited and revisited, the idea is to establish a cycle where the plan is evaluated. Working with citizen science can be highly motivating and rewarding for both the PI, PM and the project team (and citizens as well). Nevertheless, a potentially large number of partners and elements of co-creation might distort the focus, timeline and goals, thus causing unpredictability. This can potentially cause stress, missed deadlines and demotivation in the team as well as lack of communication with partners and citizens. AFTER Step 9: Completion of the project When the project period has ended or if the goals are met, the PI would want to end the project. In some projects, the co-analysis with participants are discussed or shared with decision makers within the field. In other projects, the PI and the research team analyse the data. In some cases, this might take months, but data and results should still be communicated with participants and data shared as openly as possible. Stakeholders and participants joined the project for a reason. Has the project created some kind of change? Could results influence a change in policy? Did it provide citizens and the research community with new insights? Upon completion, there might also be a financial wrap and a report to various grant providers. An evaluation based on reach or learning outcomes e.g., science literacy would be completed. In conclusion, the collection of materials, kits and communication materials are done. Some citizen science projects are ongoing and the materials recycled, therefore, resources to secure documentation of the project and its data should be allocated. Since projects might be repeated (or converged into new projects), it’s recommended to do a memory log with members of the project teams. This can be quite simple, e.g. as a brainstorm. Name five successes or things to repeat. Name five failures or changes if the project were to be conducted again. STAKEHOLDER MATRIX When engaging in a citizen science project, collaboration and dialogue with citizens is at a premium. But which citizens to engage? Filling out a stakeholder matrix could be a useful structured approach to answer this question. The key is that citizens can be reached or included in groups or via institutions from civil society, private sector, government or the education system. Once stakeholders are identified, they might go into the communication plan as target groups. Identify your own stakeholders from these examples 9 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries STAKEHOLDER MATRIX When engaging in a citizen science project, collaboration and dialogue with citizens is at a premium. But which citizens to engage? Filling out a stakeholder matrix could be a useful structured approach to answer this question. The key is that citizens can be reached or included in groups or via institutions from civil society, private sector, government or the education system. Once stakeholders are identified, they might go into the communication plan as target groups. Identify your own stakeholders from these examples CIVIL SOCIETY GOVERNMENT/POLICY NGO’s International level Libraries National level Associations Regional level Foundations Municipalities Cooperatives Agencies Trade unions Authorities Think tanks Regulators Clubs Churches Public service media Bloggers Online communities EDUCATION PRIVATE SECTOR Public schools Businesses High schools Industry Universities Privately owned media Night schools Innovation hubs Informal learning organizations Crowdfunding hubs or platforms 10 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries In this section, we focus on communication with citizens. If you want to engage media or journalists this guide might be useful. Below we suggest two practical tools for operations: · A communication plan · A communication log Communication to and with citizens In citizen science projects, the citizens are more than respondents. They are participants who are invited into a dialogue regarding research. This dialogue can be conducted before, during and after your project. This is in order to create motivation, empowerment and potentially more or better data. Think reciprocity: the citizens donate data, time and energy. They should receive something in return. When designing your communication plan, focus on: · Engage in an ongoing dialogue with the citizens involved. This can be via social media, your own channels, website, newsletters etc. and also by inviting media and journalists to join the dialogue. At the outset, communicate to citizens about the scope and goal of the project. During the project, inform and give feedback on the volume of data and be open about the analysis of this data, which they have helped provide. Create an authentic dialogue. If the data or analysis provide new or even curious insights (if not final), these stories might be shared and/or posted. Try to communicate in ongoing loops to demonstrate that their data and participation is useful. · Enable an ongoing dialogue between the citizens involved. Very often citizen science projects create dialogue to collect data and enable co-creation. The knowledge and experiences of participants might be valuable. Therefore, it might make sense not only to think of communication as a transaction between the researcher and the citizen. Try to enable a dialogue between the participants. This might improve motivation, boost the collection of data and their possible interpretation. Think about communities. Virtual and non-virtual. Consider which communication platform would be most appropriate. Social media always plays a vital role today. Again, the more traditional media channels and journalists could have a stake. COMMUNICATION In citizen science, data is communication. Do not only regard data as the subject matter for analysis but expand it into communicable potential for dissemination. Citizens are curious and want to know how their data is used. Therefore, when designing a project, it is imperative you integrate communication as a tool for dialogue from beginning to end. By Lotte Thing Rasmussen, University of Southern Denmark. ORCID iD: 0000-0002-5549-7208 e-mail: ltr@bib.sdu.dk Article DOI: 10.25815/4834-zs30. This plan stems from the templates used at SDU for our projects. Images: Undraw.co https://www.publicengagement.ac.uk/do-engagement/choose-method/media-engagement https://orcid.org/0000-0002-5549-7208 https://doi.org/10.25815/4834-zs30 mailto:ltr@bib.sdu.dk https://undraw.co/ 11 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries Target groups Communications channels and theme Name Primary focus/message Primary Secondary (if present) Tertiary (if present) External channels Target groups Themes and messages External channels meaning channels of communication that you can edit and post on yourself as well as channels where you might share content – or get editors or moderators to share it for you. Normally, projects benefit from establishing their own digital channels in order to reach relevant target groups. But channels can also be in-person encounters ranging from anything from workshops to town hall meetings. Consider creating information and contact materials. Both digitally across platforms like videos and photos but also in print like e.g. handouts, flyers or even booklets. Make sure to have clear visual design. Citizens (participants) want an answer to the question: “What’s in it for me?” Don’t necessarily explain every aspect of the project. Begin with the target group and their primary interests. The operative word is dialogue. Meet the target group where they are. Create targeted content. The purpose and the ‘why’ needs to be clear, when you are engaging citizens: “What is it we want?” A careful thematization with regards to the identified target groups including subgroups and messages can reveal opportunities for direct communication. Different spheres of interest would need different messages. All of the time consider what the citizen needs or is interested in — and what is irrelevant. Web, including citizen science platforms. condary (if present) Social media Newsletters Other NB: The project might benefit from a communication plan for internal channels in order to target research communities, management, and groups of colleagues, etc. Communication Plan (Citizens) The plan can be implemented in the accompanying templates. Depending on the scope of the project other elements can be added. When identifying your target group(s), the stakeholder matrix might be a first stop. See the next Project Highlight for an example. Templates for the Communication plan 12 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Communication log In order to maintain overview and which methods and dialogue that work, we recommend doing a log of communication activities. What is communicated to whom and on which channels or platforms. The log will illustrate a progression or lack thereof and can be coupled to the collected data if the project wishes to include that. In the communication plan you write themes and certain angles that can be useful. In the communication log, you have the specific content. PROJECT HIGHLIGHT: FIND A LAKE A Communication plan “Find a Lake” is a citizen science project at SDU, Denmark, led by associate professor Sara Egemose. The project aims at involving kids in science in their free time. The goal of the project is for the researcher to recruit and educate citizens in collecting data of water quality and insect life to create a dialogue on future research questions. The project employs a range of citizen science components — kits, camps, pop up events, and has an app for data collection. Date Channel/media Content/message In charge + status Date/ week/ month Primary content: What has been sent out or what is being planned, posted, published etc. Supplementing content: Photos, illustrations, datasets, models, diagrams, videos etc. Responsible person Follow-up person Status on results https://www.sdu.dk/en/forskning/forskningsformidling/citizenscience/soer-i-fritiden/find-en-so 13 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries Description Primary focus/message Kids and their families You can become a citizen scientist Girls and boy scouts You can become a citizen scientist After school offers You can become a citizen scientist Nature guides and schools This is an important and fun event Public libraries Let’s do joint events and help us collect data Think tank Denmark Help us engage citizens and collect data Local Newspaper Help us engage citizens and collect data. Share data Environmental agencies We share data for decision making External channels Target groups Themes and messages Web All How to participate, information, booking, data collection, visualization of data Social media Kids and their families Girls and boy scouts Nature guides and schools Public libraries You can become a citizen scientist This is important and fun Promotion of events, camps, Pop Up, Citizen Science Kits App All Data collection How to participate Youtube Channel (videos) Kids and their families Girls and boy scouts Nature guides and schools Public libraries What is citizen science? How to participate Flyers, booklets and roll ups After school offers This is an important and fun How to participate Citizen Science Kits Kids and their families Girls and boy scouts Nature guides and schools After Schools offers Data collection How to participate Target groups for “Find a Lake” Communication channels and themes for “Find a Lake” 14 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Findability is addressed in Section 2 of the guide about infrastructures. Use of Data Policies in Citizen Science Projects describes obligations related to Access and Reuse. You will learn about issues related to creating Interoperable data in Citizen Science Data and Standards. The Acknowledgment of Citizen Scientists on Research Outputs is very important for Reuse conditions. Lastly, you will find more resources and links in Planning and Securing Resources — The Data Management Plan. MANAGEMENT OF CITIZEN SCIENCE DATA In the following parts of the guide, we address subjects that will aid create FAIR data, but with an emphasis on challenges particular for citizen science. RESEARCH DATA MANAGEMENT: QUICK START GUIDE Introduction to research data management, the FAIR principles and writing a data management plan Watch three short e-learning modules videos on Research Data Management, the FAIR principles, and Data Management Plans. You will get a general introduction to the concepts. · Module 1: Introduction; · Module 2: FAIR principles; · Module 3: Data Management Plans. (eLearning course) Holmstrand, K.F., S.P.A. den Boer, E. Vlachos, P.M. Martínez- Lavanchy, K.K. Hansen, A.V. Larsen, S. Zurcher, et al. “Research Data Management (ELearning Course),” 2019. https://doi.org/10.11581/DTU:00000047. https://doi.org/10.11581/DTU:00000047 https://doi.org/10.11581/DTU:00000047 https://doi.org/10.11581/DTU:00000047 https://doi.org/10.11581/DTU:00000047 15 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries USE OF DATA POLICIES IN CITIZEN SCIENCE PROJECTS: A STEP-BY-STEP GUIDE Librarians servicing an academic institution may be aware of legal and ethical conditions pertaining to research projects, but how will these conditions apply to citizen science? It is inevitable that project managers have to consider a myriad of issues about access to and protection of data produced or collected by citizen scientists. The checklist below may help sustain the engagement and trust of participants by adhering to ethical and legal obligations emerging in citizen science projects. Clarify and review ethical issues Evaluation by ethical committees are important for clarifying issues pertaining to health reporting and perhaps collection of biological material in projects, where citizens contribute with such data. Projects based outside an academic institution may experience difficulties receiving an ethical review depending on the regulation and possibilities in individual countries. Consider how participants are protected, their risk evaluated and how accidental finding disclosure will be handled. Protect studied objects and populations Sharing information about endangered species or particular populations requires special attention. Engaging specific populations in citizen science should be followed by clarifying their cultural needs during data collection and any resistance towards openly sharing (traditional) knowledge. It is the responsibility of the project manager to assess the consequences of data sharing and discuss this with the involved participants. Such issues may take time to investigate and should be planned for. Manage personal information according to current legislation and responsible practices If and how data can be accessed depends on private and sensitive information being embedded in the data. In citizen science projects, personal information (name, contact information etc.) of the volunteers and often location sharing must be protected and handled according to current laws. In the EU, the GDPR applies to all handling of personal data and includes data that can identify a person, but also sensitive data such as information on health, ethnicity or religion. Not all countries outside Europe have laws protecting privacy or sensitive information of participants in citizen science projects, so follow responsible practices. Step 1 Step 2 Step 3 By Jitka Stilund Hansen, Technical University of Denmark, ORCID iD: 0000-0002-5888-1221 e-mail: jstha@dtu.dk Article DOI: 10.25815/byq6-7095 https://orcid.org/0000-0002-5888-1221 https://doi.org/10.25815/BYQ6-7095 mailto:jstha@dtu.dk 16 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Manage intellectual property rights of the citizen scientist Citizen scientists may produce photographs, writings, and creative selections or arrangements of scientific data. In contrast to the undisputable regulations in many countries of employees’ inventions, citizen scientists retain the intellectual property rights (IPR) to any copyrightable work they produce. Because the citizen scientists possess the right to exclude the project in using an invention they have produced, it is recommended to make transparent IPR agreements that are regularly updated with the participants. Also, the project holder should aim at sharing IPR, education or monetary value with the volunteers. Step 5 Specify data access and license Data without a license is not FAIR and the project manager should early in the project consider how access to data can be aligned with a usage license. Citizen science data may have broad applicability and such reuse is facilitated by choosing legally interoperable licenses for datasets. Step 6 Create a data policy and specify the terms of participation Aggregate the above information and requirements in a Data policy and Terms of Participation. The information should be evaluated by relevant stakeholders of the projects (participants, organisations, institutions) before it is used. Participants must be informed about the Terms of Participation in clear and accessible language before they agree to engage in the citizen science activities. Summary The research librarian may readily assist in the practicalities of storing, sharing, publishing and licensing research data, and therefore also data of citizen science origin. However, bodies outside the research library often deliver legal advice and ethical evaluation of relevance for citizen science. Therefore, an important service from the library is to develop a framework to clarify ethical and legal conditions particular for projects relying on co-creation and involvement of citizen scientists. Step 7 Determine insurance coverage National or institutional frameworks often insure and protect participants of conventional academic projects. This may not be the case for participants of citizen science. Determine if extended insurance coverage is necessary and how to inform citizens of risks related to their contribution. Step 4 Content of this section is modified from: Hansen, Jitka Stilund, Signe Gadegaard, Karsten Kryger Hansen, Asger Væring Larsen, Søren Møller, Gertrud Stougård Thomsen, and Katrine Flindt Holmstrand. “Research Data Management Challenges in Citizen Science Projects and Recommendations for Library Support Services. A Scoping Review and Case Study.” Data Science Journal 20, no. 1 (August 18, 2021): 25. https://doi.org/10.5334/dsj-2021-025. Also, refer to: Bowser, Anne, Andrea Wiggins, and Robert D. Stevenson. “Data Policies for Public Participation in Scientific Research: A Primer.” Albuquerque, NM: DataONE, August 2013. https://old.dataone.org/sites/all/documents/DataPolicyGuide.pdf. Image: Undraw.co http://undraw.co https://doi.org/10.5334/dsj-2021-025 https://old.dataone.org/sites/all/documents/DataPolicyGuide.pdf 17 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries Three recommendations to cope with inherent diversity Before anything else, the first recommendation is to avoid assumptions about any citizen science activity. It cannot be assumed that a citizen science activity aims at or would benefit from applying a particular standard, or that the participants are interested in data being re-used by others. Instead, the participants of any existing or new project need to be consulted to clarify their intentions and needs. Which (kind of) data will be collected or analysed by the project? For which purpose? What are requirements in terms of data accuracy, spatial temporal coverage? Who should have access to the data collected by the project? Are there any privacy concerns and which agreements need to be put in place to protect those contributing? Regardless of the standard(s) that would be used in the project, it is essential to provide extensive instructions about their use, and to make sure standards are really understood and adopted by the different participants. Homogeneous data collection and representation are key for the overall scientific quality of the activity, so adequate training of participants will be utterly important. You can include information about standardisation in your communication strategy. The second recommendation is to apply those few standards that indeed hold across any (citizen science) activity — and especially to the data it might create — those cover topics, such as: CITIZEN SCIENCE DATA AND STANDARDS Applying standards to citizen science data is important for several reasons. First, the use of standard methods and tools to gather data helps to ensure the fit for purpose property, i.e., that the collected data meets the quality criteria of an intended use. Second, they can ensure that the data collected, validated or analysed in citizen science activities are provided under the appropriate access and use conditions — which hold to the participants but also to others (i.e., any third party that might be interested in re-using or replicating the work). Hence, standardization is essential for data to be reusable in other contexts. Third, the use of (domain-specific) data standards helps to ensure that the citizen science activity covers all important elements (attributes) of the phenomenon under investigation. When it comes to specific (data) standards, there are many to choose from. In the context of citizen science projects, this can be considered an asset, because citizen science approaches are very rich and diverse, so that there cannot be a one-size-fits-all solution. However, a few things should be said about standards and citizen science data, and a few recommendations might help in advising practitioners and managers of citizen science standardisation initiatives. By Sven Schade and Chrisa Tsinaraki, European Commission – Joint Research Centre Sven Schade, ORCID iD: 0000-0001-5677-5209 e-mail: s.schade@ec.europa.eu Chrisa Tsinaraki, ORCID iD: 0000-0002-6012-0835 e-mail: chrysi.tsinaraki@ec.europa.eu Article DOI: 10.25815/jqjc-qp38 Image: Undraw.co https://orcid.org/0000-0001-5677-5209 https://orcid.org/0000-0002-6012-0835 https://doi.org/10.25815/jqjc-qp38 http://undraw.co mailto:s.schade@ec.europa.eu mailto:chrysi.tsinaraki@ec.europa.eu 18 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries · Assessment of the need for an ethical review of data gathering and treatment methodologies, including the protection of personal data. Existing forms (for example, the dedicated online guide of the European Commission) might provide valuable insights. If required, ethical reviews might take particular attention, for example to control if the people involved are informed in the clearest way possible (i.e., not only being directed to long and complicated legal texts). · A careful assessment of the inclusiveness of the foreseen engagement methods and the way those are communicated. Here, it is important to clarify and clearly inform about conditions for participation but also which barriers this might create to particular communities. · The explicit use of standards and, if applicable, of machine-readable data licenses (for example, Creative Commons might be used). · Generic elements for describing data sets (metadata, such as Dublin Core) might be used in addition to any topic specific schemes. A first set of elements that are tailored to citizen science are under development by the community. · For data exchange and access, it might be assessed if the use of de facto standards for the machine-based exchange of data over the web (e.g., JSON and APIs) would add value to the project. This should include an assessment of the available technical capabilities of the team. In any case, it is also important to ensure that data is provided in ways that are appropriate for the targeted participants. Any additional advice on the use of dedicated standards for data itself has to be put into context. Standards for citizen science data are highly topic specific, for example, different approaches have to be taken when using air quality sensors (see e.g., the SamenMeten infrastructure — soon also in English), observing birds (see e.g., the European Bird Indexes), or collecting and reporting data about litter on beaches (see e.g., Marine Litter Watch). The odour pollution case description [link below] provides some more details for one selected example, but generally it is advisable (third recommendation) to engage with research institutions and/or public administrations that are parts of the dedicated thematic communities, to learn about their standards and if and how those might apply to a given citizen science project. The best entity to engage with will depend not only on the topic, but also on the intended outcome/purpose of the citizen science project. Public institutions, for example, should be engaged if there is an ambition that the created knowledge will affect policy-making – and the right level of administration depends on the intended outreach (local, regional, national, European or global). References Schade, Sven, Chrisa Tsinaraki, and Elena Roglia. “Scientific Data from and for the Citizen.” First Monday, July 31, 2017. https://doi.org/10.5210/fm.v22i8.7842. Turbé, Anne, Jorge Barba, Maite Pelacho, Shailendra Mugdal, Lucy D. Robinson, Fermin Serrano-Sanz, Francisco Sanz, Chrysa Tsinaraki, Jose-Miguel Rubio, and Sven Schade. “Understanding the Citizen Science Landscape for European Environmental Policy: An Assessment and Recommendations.” Citizen Science: Theory and Practice 4, no. 1 (December 2, 2019): 34. https://doi.org/10.5334/cstp.239. Hecker, Susanne, Mordechai Haklay, Anne Bowser, Zen Makuch, Johannes Vogel, and Aletta Bonn. Citizen Science Innovation in Open Science, Society and Policy. London: UCL Press, 2018: 321-336. https://www.uclpress.co.uk/collections/ science/products/107613. Standards support interoperability among systems The university library can be a hub of knowledge for working with metadata and data standards. It aids the academic disciplines to identify and introduce existing domain and community dependent metadata standards. Find links and further resources in the DMP part. FAIRsharing.org collects policies, standards and ontologies from different disciplines that may be useful also for citizen science. https://ec.europa.eu/research/participants/docs/h2020-funding-guide/cross-cutting-issues/ethics_en.htm https://ec.europa.eu/research/participants/docs/h2020-funding-guide/cross-cutting-issues/ethics_en.htm https://creativecommons.org/ https://core.citizenscience.org/ https://www.samenmetenaanluchtkwaliteit.nl/ https://www.samenmetenaanluchtkwaliteit.nl/ https://pecbms.info/european-wild-bird-indicators-2020-update/ https://www.eea.europa.eu/themes/water/europes-seas-and-coasts/assessments/marine-litterwatch http://FAIRsharing.org https://doi.org/10.5210/fm.v22i8.7842 https://doi.org/10.5334/cstp.239 https://www.uclpress.co.uk/collections/ 19 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries ODOUR POLLUTION A GROWING SOCIETAL CONCERN HIGHLIGHTS ● Odour nuisances, being the second cause of environmental complaints after noise, lead to a significant decline in our quality of life and must be urgently addressed. ● Odour regulations across Europe and within countries differ significantly from each other. In many places they are even completely lacking. There is a need for bottom-up, multi-level governance in Europe in order to protect its citizens. ● Odorous gases are commonly measured at the source (emission). The level of the odours in surrounding residential areas (immission) is more complex to determine, but also much more relevant to measuring the impact on residents. ● The Distributed Network for Odour Sensing, Empowerment and Sustainability (D-NOSES) project will reverse the way in which odour pollution is commonly tackled, through a co-creative citizen science approach. MAIN ODOUR ISSUES IN EUROPE The sources that generate odours in European communities are numerous and diverse; in many cases the same community is exposed to more than one odour source. Industrial activities, waste management and agriculture/livestock represent the main challenges regarding odour emissions within Europe. HOW TO CITE D-NOSES consortium (2019) Odour Pollution - A growing societal concern. D-NOSES Policy Brief #1 Authors: Simone Rüfenacht (ECSA), Clarisse Guiral (ECSA), Alaa Abou Daher (MIO-ECSDE), Anastasia Roniotes (MIO-ECSDE), Jose Uribe (ISWA), Nora Salas Seoane (IBERCIVIS), Rosa Arias (IBERCIVIS). Coordinated by: Funded by the Horizon 2020 programme of the European Union Grant Agreement No 789315 dnoses. eu @dNOSES_ EU dNOSES.E U #dNosesEU#OdourObservator y This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 789315 dnoses.eu @dNOSES_EU dNOSES.EU#dNosesEU#OdourObservatory This policy brief was facilitated by the lead authors (ECSA) through open interaction and discussion with the D-NOSES consortium. While this was carried out as part of H2020 D-NOSES Coordination and Support Action project, the views expressed in it do not reflect the consensus opinion of D-NOSES partners. The D-NOSES project applies citizen science and co-creation approaches to set new standards for odour pollution — globally. · Not all citizen science projects follow the same funding model. Available funds and related timelines do constrain the possibilities to learn to use existing or even to develop new standards. PROJECT HIGHLIGHT: DEFINING NEW DATA STANDARDS WITH CITIZEN SCIENCE The EU-funded project Distributed Network for Odour Sensing, Empowerment and Sustainability (D-NOSES) is an excellent example of the diversity of citizen science and data standards. It addresses odour pollution with the strong aim to influence policies — within countries, the EU and across the globe. Accounting for around 30% of the environmental complaints globally, odour pollution is an unregulated issue in many countries. D-NOSES aims to create scientific references and replicability guidelines for defining new regulatory frameworks. Among others, the project reviewed odour pollution and measurement techniques and compiled a list of good practices in handling odour pollution. A related Massive Open Online Course (MOOC) was developed to support capacity building. The project highlights that citizen science can be an integral part in developing standards, especially in domains where those standards have yet to be defined. However, a few particularities should be noted: · In most thematic fields, measurement standards for data gathering and exchange already exist. Therefore, citizen science projects have different starting points; · Regulatory impacts are not always the main driver for a citizen science project. Thus, different standards might be needed to address different ambitions; and Funded by the Horizon 2020 programme of the European Union GGrraanntt AAggrreeeemmeenntt NNoo 778899331155 OdourCollect is part of the D-NOSES project Are you experiencing any problem? odourcollect.eu @Odourcollect odourcollect Smell and share! dnoses.eu User guide of the OdourCollect App We are always looking for ways to improve the App. If you have any comments or difficulties while reporting odour observations, please let us know. You can contact us via email at odourcollect@ibercivis.es Become a super citizen scientist! Be part of a pilot case study Interact with other users Look up the odour map Images: User guide of the OdourCollect App; Policy Brief. © 2018 D-NOSES. See: https://dnoses.eu/downloads/ https://dnoses.eu/ https://dnoses.eu/ https://dnoses.eu/ https://dnoses.eu/d2-1_review-on-odour-pollution-measurement-abatement_v3-2-pdf/ https://dnoses.eu/d2-1_review-on-odour-pollution-measurement-abatement_v3-2-pdf/ https://dnoses.eu/wp-content/uploads/2021/06/D2.3_Good-practices_v3.1.pdf https://dnoses.eu/wp-content/uploads/2021/06/D2.3_Good-practices_v3.1.pdf https://dnoses.eu/wp-content/uploads/2021/06/D7.4.-MOOC-on-Odour-Pollution.pdf mailto:odourcollect@ibercivis.es https://dnoses.eu/downloads/ 20 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Providing credit where it’s due, and how to achieve the ‘Reuse’ principle with information on data provenance. researchers should strive always to credit citizen scientists in appropriate ways. There are myriad benefits to official acknowledgment, some of which may only apply to the citizen science groups (e.g., unique funding opportunities; support to create formal structures around the citizen science group). In a collaborative project, citizen scientists/groups can be credited via the official acknowledgements, or via authorship on manuscripts/datasets. The latter often requires justification that citizen scientists have met the International Committee of Medical Journal Editors (ICMJE) standards which warrant academic authorship. Citizen science does not always lend itself to meeting these rigorous academic criteria. The case study below outlines a common situation in this respect and the diversity of considerations (cultural, legal, ethical) when deciding the best form of acknowledgement for citizen science collaborators. ACKNOWLEDGMENT OF CITIZEN SCIENTISTS ON RESEARCH OUTPUTS The ‘Reuse’ component of the FAIR principles dictates that data should maintain its initial complexity and have clear provenance information on how and with whom the data was formed. This, coupled with discipline-specific data and metadata standards, allows for data reuse in the future. Citizen science projects present unique challenges for accommodating the Reuse principle. This is because there is currently no single or standardised way for crediting citizen science collaborators on publications or datasets, citing collaborative works or providing Intellectual Property (IP) rights to the citizen scientists involved. Depending on the methodology used by the authors (or imposed by research journals), data provenance can thus be a subjective and sometimes mis-represented aspect of research involving citizen scientists. Official acknowledgment for contribution to research is a cornerstone of academia and By Georgia Ward-Fear, Macquarie University, ORCID iD: 0000-0002-4808-1933, e-mail: georgia.ward-fear@mq.edu.au Article DOI: 10.25815/rkzw-z561 Image: The Balanggarra Rangers from the East Kimberley region of Northern Australia. Image courtesy of Kimberley Land Council. https://orcid.org/0000-0002-4808-1933 https://doi.org/10.25815/rkzw-z561 mailto:georgia.ward-fear@mq.edu.au 21 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries PROJECT HIGHLIGHT: LIZARD CONSERVATION WITH THE BALANGGARRA RANGERS IN AUSTRALIA Image: Herbert and Wesley Alberts with Georgia Ward- Fear. Photo by Georgia Ward-Fear. lacked academic affiliation. Even those that did, reduced the author name to ‘B. Rangers’ in citations, an unintended but nevertheless culturally inappropriate practice. This experience motivated the researchers to petition for more inclusive academic authorship protocols that keep pace with the changing socio-political nature of research. ‘Group co-authorship’ is an option for including citizen science groups as authors, providing strong public acknowledgment and Intellectual Property rights. It also avoids some of the ethical pitfalls for adding individuals by name, who have not met ICMJE guidelines (which can be viewed as academic fraudulence). A summary of the suggested qualifiers for citizen science group authorship can be found in the infobox. To mitigate the impact of an invasive toad on a native apex predator in tropical Australia, a collaboration between conservation researchers and indigenous Traditional Owners (the Balanngarra Rangers, of the Balanggarra people) was formed. Each group brought their unique skills, experience, and knowledge to the project; an excellent example of the synergy between ‘Western science’ and Traditional Ecological Knowledge and skills. The research was highly successful and culminated in the development of a new national and international conservation strategy. The Balanggarra Rangers were pivotal to the success of this project in unique ways, yet crediting the Rangers was not simple. The group consisted of many individuals with varying degrees of input, and the group wanted to be identified collectively by their cultural name. However, adding the ‘Balanggarra Rangers’ to the author byline was rejected by some scientific publishing outlets due to editorial or ethical protocols, or because the Rangers’ 22 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Research programs that engage with citizen scientists should identify the most appropriate method for credit at the outset. Not only is this ethical, but also helps to explore and achieve legal interoperability early on, i.e., the ability of organisations with different legal frameworks to work together. This may be especially pertinent in citizen science projects where disparate organisations, industry sectors or subsets of the community come together in a research setting. When assessing the best way to credit citizen scientists in a project, these are some of the questions to consider: 1. What are the wishes of the citizen scientists involved in the program? 2. Is the citizen science group readily identifiable by a collective name or could they create one? 3. Is the dataset in use ‘static’ (i.e., finished, whole and attributable to one group), is this a ‘living’ dataset and/or is only a portion of it being used? 4. Is it ethical or appropriate to identify individual citizen scientists by name? 5. Which form of citizen scientist identification will best support the ‘Reuse’ principle? References Ward-Fear, Georgia, Balanggarra Rangers, David Pearson, Melissa Bruton, and Rick Shine. “Sharper Eyes See Shyer Lizards: Collaboration with Indigenous Peoples Can Alter the Outcomes of Conservation Research.” Conservation Letters 12, no. 4 (July 2019). https://doi.org/10.1111/conl.12643. Ward-Fear, Georgia, Gregory B. Pauly, Jann E. Vendetti, and Richard Shine. “Authorship Protocols Must Change to Credit Citizen Scientists.” Trends in Ecology & Evolution 35, no. 3 (March 2020): 187–90. https://doi.org/10.1016/j.tree.2019.10.007. Hunter, Jane, and Chih-Hsiang Hsu. “Formal Acknowledgement of Citizen Scientists’ Contributions via Dynamic Data Citations.” In Digital Libraries: Providing Quality Information, edited by Robert B. Allen, Jane Hunter, and Marcia L. Zeng, 9469:64–75. Lecture Notes in Computer Science. Cham: Springer International Publishing, 2015. https://doi.org/10.1007/978-3-319-27974-9_7. Group Co-authorship for Citizen Scientists: Recommendations for Using, Listing, and Citing Group co-authorship should be used: · When the group in question expresses a desire for authorship. · When groups cannot meet ICMJE or journal specific standards but their contribution was deemed essential to the success of the project. For citizen science projects, this should include ‘data acquisition’, which would not normally warrant authorship alone. · Only for established groups (e.g., the ‘Balanggarra Rangers’) not for amorphous groups who engage with generic surveys or medical studies. Such groups are best recognised in the Acknowledgment section. Group co-author names should: · Be as short as possible, and · be listed in full at all times (e.g., ‘Balanggarra Rangers’ not ‘B. Rangers’) i.e., both in the author byline and also when citing and indexing in references. This is also the responsibility of the publisher and indexing programs/institutions. All authors collectively decide: · On the best form of acknowledgment and whether it meets the ‘Reuse’ principle. · The most appropriate order of authorship (groups co-authors can be anywhere in that order). · Whether to list individual citizen scientist names elsewhere in the text (e.g., supplementary materials or appendices), remembering that some citizen scientists may be eligible for individual authorship independent of the group. Group co-authorship doesn’t replace this option. Adapted from Ward-Fear et al. 2020 https://doi.org/10.1111/conl.12643 https://doi.org/10.1016/j.tree.2019.10.007 https://doi.org/10.1007/978-3-319-27974-9_7 23 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries PLANNING AND SECURING RESOURCES — THE DATA MANAGEMENT PLAN A data management plan (DMP) can support citizen science data in becoming FAIR. It should be created early in the research process and updated regularly to prepare for data deposit, sharing and reuse. University libraries already have knowledge of FAIR data. So in this final part about the DMP, we emphasize resources useful for citizen science projects and in which sections of the guide, you can find related information. A common understanding of how data will be managed is particularly important in collaborative projects that involve researchers, institutions and groups with different ways of working and expectations. This guidance follows the six Science Europe core requirements for DMPs (Science Europe 2021). Also, refer to Wiggins et al. (2013) for writing a DMP for citizen science projects. Find a curated collection of Horizon 2020 DMPs where several address citizen science projects. If your research library does not provide a DMP tool, use free online tools for writing DMPs such as ARGOS or DMPOnline. By Iryna Kuchma, Electronic Information for Libraries, ORCID iD: 0000-0002-2064-3439 e-mail: iryna.kuchma@eifl.net Article DOI: 10.20389/tpap-md16 Image: eLearning course about the importance of good research data management (RDM). https://doi.org/10.11581/ DTU:00000047 https://phaidra.univie.ac.at/search#?page=1&pagesize=10&collection=o:1140797 https://argos.openaire.eu/ https://dmponline.dcc.ac.uk/ https://orcid.org/0000-0002-2064-3439 https://doi.org/10.20389/tpap-md16 mailto:iryna.kuchma@eifl.net https://doi.org/10.11581/ 24 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries 1. Data description and collection or re-use of existing data The data description is the core part that you build upon to make decisions on data management. In this section you include information on the type of data that will be gathered. To increase the value of citizen science data from the perspective of the general public (community interoperability) or regulatory authorities, interoperable data should be planned for. This is important for integration with existing data or when using existing technologies for data collection. Learn more about Citizen Science Data and Standards and about open science. 2. Documentation and data quality Describing why and how data was collected is important for documenting the data quality. The data handling skills of the participants may be unknown, therefore, describe methods used to collect and treat data, data provenance, and quality-assurance steps taken. Explain how you will guarantee consistency within your dataset. Metadata standards from relevant disciplines (if existing) are key for data interoperability and reuse. Ventures in new technology should aim at following community standards and being open source so later users can implement and further develop the tools to their needs. Learn more about Citizen Science Data and Standards. Search FAIRsharing for standards, ontologies and policies. 3. Storage and backup during the research process It is good practice to store data in at least one non-proprietary format. Project managers often use their personal data storage and the library role could be to help with the institutional storage provisions or identifying infrastructures fit for citizen science data. Seek storage solutions, which offer flexibility and protection for sensitive data or data with disclosure risk. Best practice is to store data without direct identifiers and replace personal identifiers with a randomly assigned identifier (ask researchers to create a separate file, to be kept apart from the rest of the data, which provides the linking relationship between any personal identifiers and the randomly assigned unique identifiers). Where possible, select a storage solution that allows an easy way to maintain version control. Learn more about infrastructures for Citizen Science in Section 2 of this guide. 4. Legal and ethical requirements, codes of conduct Citizen science projects might not have access to legal and ethical advice, and may need help to establish approval mechanisms for sharing data (via consent, regulation, institutional agreements and other systematic data governance mechanisms, including restricted access conditions and https://fairsharing.org/ 25 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries embargoes if required). Acknowledge data provenance in metadata and any limitations or obligations in secondary use, inclusive of issues of consent. Learn more in Use of Data Policies in Citizen Science Projects and Acknowledgment of Citizen Scientists on Research Outputs. 5. Data sharing and long-term preservation Where possible, advice to provide immediate open access to citizen science data and recommend Creative Commons Attribution 4.0 International License (CC BY 4.0), a Creative Commons Public Domain Dedication or equivalent. Also, clarify whether any project funder has specific data access requirements. Know the needs of the participants before you share any data and determine methods for sharing. If immediate open access is not possible, consider creating a metadata record in a repository where a persistent identifier and license can be assigned. If data cannot be open, indicate how they can be made accessible, and under which conditions. Describe which measures you will take to enable long-term preservation. Remember to consult your participants during the project planning and with regard to their expectations of data sharing (see Citizen Science Data and Standards). Get more information about Use of Data Policies in Citizen Science Projects and about infrastructures for sharing data in Section 2. 6. Data management responsibilities and resources Describe who (for example role, position, and institution) will be responsible for data management. What resources (for example financial and time) will be dedicated to data management and ensuring that data will be FAIR. Apps or technologies for data collection and participant interaction may require regular maintenance and updates – and therefore, funding for long-term support. By planning early, costs can be significantly reduced. Identify and assess RDM costs and include them in the project planning. References Science Europe. Practical Guide to the International Alignment of Research Data Management. (Extended Edition). Brussels: Science Europe, 2021. https://www.scienceeurope.org/ media/4brkxxe5/se_rdm_practical_guide_extended_final.pdf. Wiggins, A, Bonney, R, Graham, E, Henderson, S, Kelling, S, Littauer, R, Lebuhn, G, Lotts, G, Michener, W, Newman, G, Russel, E, Stevenson, R, Weltzin, J. Data Management Guide for Public Participation in Scientific Research. DataOne, 2013 http://safmc.net/wp-content/ uploads/2016/06/Wigginsetal2013_DataManagementGuidePPSR.pdf. https://www.openaire.eu/how-to-comply-to-h2020-mandates-rdm-costs http://safmc.net/wp-content/uploads/2016/06/Wigginsetal2013_DataManagementGuidePPSR.pdf https://www.scienceeurope.org/media/4brkxxe5/se_rdm_practical_guide_extended_final.pdf 26 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Web: https://inos-project.eu/ content of personal data, the database is not shared openly but via a metadata record (Skov 2021). References Holmstrand, Katrine Flindt, Asger Væring Larsen, Signe Gadegaard, Jitka Stilund Hansen, Karsten Kryger Hansen, and Gertrud Stougård Thomsen. “FAIR Data in a Citizen Science Project ‘Fangstjournalen,’” 2020. https://doi.org/10.11581/ DTU:00000092. Skov, Christian. “Database from Citizen Science Project ‘Fangstjournalen.’” Technical University of Denmark, 2021. https://doi.org/10.11583/ DTU.13795928. PROJECT HIGHLIGHT: FAIR DATA IN A CITIZEN SCIENCE PROJECT Fangstjournalen is a citizen science project highlighting several of the points from this section: it demonstrates how good communication, project and data management create value to citizen scientists and also to scientific data. Learn how the project manager makes his data FAIR and share data with the citizens in this short video (Holmstrand et al. 2020). The collected data are relevant for reuse in projects about biodiversity, behavior and recreation, but also for national fishery regulation and policy development. Due to the Image: Fangstjournalen – registration of catches by recreational anglers. Photo by Christian Skov, ORCID iD: 0000-0002-8547-6520 CCBY- SA 2.0. Article DOI: 10.25815/tnrh-zg50 https://orcid.org/0000-0002-8547-6520 https://doi.org/10.25815/TNRH-ZG50 https://doi.org/10.11581/ https://doi.org/10.11583/ 27 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries Article DOI: 10.25815/aayw-w097 Towards a Roadmap on Capacity Building on Open Science and Citizen Science for Research Libraries — Co-creating a vision Building up on the project’s overall intellectual outputs and activities, LIBER is leading activities on engagement, awareness raising and fostering policy change. In this framework, LIBER has recently organised two highly interactive vision-building workshops that led to the publication of a report on co-creating a vision for citizen science in higher education. This activity, together with the previous OKAs, are the starting point for a workshop dedicated to research libraries, aiming to co-create a Roadmap on Capacity Building on Open Science and Citizen Science for Research Libraries. The project will wrap up its activities with a report the the final Stakeholders’ Consultation of the project and its final Vision and Policy Recommendations for Higher Education. Web: https://inos-project.eu/ PROJECT HIGHLIGHT: THE INOS PROJECT The INOS project is funded under the Erasmus+ KA2 Strategic Partnerships program and aims at integrating open science and citizen science into active learning approaches in Higher Education. INOS is a partnership of four universities (Aalborg University, Tallinn University, University of Oulu, University of Bordeaux), an SME (Web2Learn) and LIBER the Association of European Research Libraries. Upskilling through open knowledge activities In its three year duration and among its other goals and activities, INOS aims to expose academic and library staff and students to participatory methods in fostering open science, and consequently upskill them, so that they reflect on updating pedagogical models in Higher Education through citizen science. It does so by encouraging universities and university libraries to co-create and participate in Open Knowledge and Open Innovation activities. In this context LIBER co-organised four Open Knowledge Activities (OKAs) with five LIBER participant libraries and with the support of the LIBER Citizen Science Working Group and the LIBER Copyright and Legal Matters Working Group. The LIBER OKAs aimed at co-creating and debating on citizen science concepts, thus upskilling the participants in open and citizen science. https://doi.org/10.25815/aayw-w097 https://inos-project.eu/ 28 Section 1 / Citizen Science Skilling for Library Staff, Researchers, and the Public Citizen Science for Research Libraries Citizen science has the potential to sustain the citizen in developing scientific skills and to increase public understanding of science. Thus, scientific literacy can be an integral part of citizen science projects. The University Library of Southern Denmark is a partner of citizen science projects and several of these involve pupils from elementary schools and high schools. In one project, “A Healthier Southern Denmark”, high school classes were involved and learned about health science and the library held a course in critical source reading as a supplement to the students’ curriculum. Another example from the university library is the project “Find a lake”, where elementary school pupils test the quality of the water. The latter had a goal to educate the pupils to be responsible citizens through insight on humans’ impact on nature. The pupils were highly motivated and the purpose of the projects was clear and relevant to them. Both projects and the educational material were a co-creation between the schools, the researchers, and the university library. The library acted as a link between the schools and the researchers. INCREASING SCIENTIFIC LITERACY WITH CITIZEN SCIENCE Scientific literacy and citizen science Citizens who are scientific literate can contribute and collect data in a useful and qualified way in e.g., citizen science projects and can to a greater extent use scientific methods and make informed decisions. Some citizen science projects can even increase the participants’ scientific literacy, if projects may contain elements of empowerment, inclusion, and motivation. For citizens to acquire knowledge and thus, become more literate, the citizen science projects must be relevant and preferably related to the citizens’ local environment. Personal involvement and interests makes people more motivated, and thus more likely to seek information. Effects of citizen science projects on scientific literacy could be better comprehension of scientific information and scientific processes and a change in attitudes towards science. The citizens may engage more in science or even consider science as a career. Research libraries and scientific literacy The libraries’ mission to promote and provide tools and resources to master scientific information and information literacy, even scientific literacy, matches the citizen science projects, where citizens acquire scientific skills in observing, deriving, predicting, and making sense of collected data and observations. By connecting scientific researchers with scientific literate citizens, the library could become a channel for citizen science projects, as well as an intellectual hub. The library will be a place to access and use scientific information as well as to create and engage in scientific endeavours. By Berit Elisabeth Alving, University of Southern Denmark, ORCID iD: 0000-0002-4708-5000 e-mail: balving@bib.sdu.dk Article DOI: 10.20389/dnnk-tv30 Definition of Scientific Literacy Scientific literacy is knowledge and understanding of scientific concepts, processes, and methods, giving you the ability to discuss and evaluate the origin and quality of scientific results and thus seek answers to scientific questions. The scientific literate citizen can distinguish science from pseudoscience. https://orcid.org/0000-0002-4708-5000 https://doi.org/10.20389/dnnk-tv30 mailto:balving@bib.sdu.dk 29 Ligue des Bibliothèques Européennes de Recherche Association of European Research Libraries Examples on measuring scientific literacy Examples reprinted with permission from publisher and author: Gormally, Cara, Peggy Brickman, and Mary Lutz. “Developing a Test of Scientific Literacy Skills (TOSLS): Measuring Undergraduates’ Evaluation of Scientific Information and Arguments.” Edited by Elisa Stone. CBE—Life Sciences Education 11, no. 4 (December 2012): 364–77. https://doi.org/10.1187/cbe.12- 03-0026. To detect the citizens’ level of scientific literacy, it is useful to conduct a test at the beginning and end of a project, and thereby be able to evaluate the progression in scientific literacy. These tests are specifically useful when schools or educational institutions are involved. The citizen science projects can be part of or a supplement of their curriculum. Below are examples from the Test of Scientific Literacy Skills (TOSLS) used for undergraduate students. In this test, they must organise, analyse, and interpret quantitative data and scientific information. 1. Which of the following is a valid scientific argument? a. Measurements of sea level on the Gulf Coast taken this year are lower than normal; the average monthly measurements were almost 0.1 cm lower than normal in some areas. These facts prove that sea level rise is not a problem. b. A strain of mice was genetically engineered to lack a certain gene, and the mice were unable to reproduce. Introduction of the gene back into the mutant mice restored their ability to reproduce. These facts indicate that the gene is essential for mouse reproduction. c. A poll revealed that 34% of Americans believe that dinosaurs and early humans co-existed because fossil footprints of each species were found in the same location. This widespread belief is appropriate evidence to support the claim that humans did not evolve from ape ancestors. d. This winter, the northeastern US received record amounts of snowfall, and the average monthly temperatures were more than 2°F lower than normal in some areas. These facts indicate that climate change is occurring. 2. Researchers found that chronically stressed individuals have significantly higher blood pressure compared to individuals with little stress. Which graph would be most appropriate for displaying the mean (average) blood pressure scores for high- stress and low-stress groups of people? References Golumbic, Yaela N., Barak Fishbain, and Ayelet Baram-Tsabari. “Science Literacy in Action: Understanding Scientific Data Presented in a Citizen Science Platform by Non-Expert Adults.” International Journal of Science Education, Part B 10, no. 3 (July 2, 2020): 232–47. https://doi.org/10.1080/21548455.20 20.1769877. Shaffer, Justin F., Julie Ferguson, and Kameryn Denaro. “Use of the Test of Scientific Literacy Skills Reveals That Fundamental Literacy Is an Important Contributor to Scientific Literacy.” Edited by Peggy Brickman. CBE—Life Sciences Education 18, no. 3 (September 2019): ar31. https://doi.org/10.1187/cbe.18-12-0238. https://doi.org/10.1187/cbe.12-03-0026 https://doi.org/10.1187/cbe.12-03-0026 https://doi.org/10.1080/21548455.20 https://doi.org/10.1187/cbe.18-12-0238',\n",
              " \"www.bscs.org · info@bscs.org 5 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 2 Premises of This Work While both scientific1 and educational outcomes are touted as important benefits of implementing citizen science, there are two widespread misconceptions about the relationship between scientific and educational objectives in citizen science. The first is that if you design a project to achieve one of these objectives, you get the other automatically. That is, if you set up a citizen science project to contribute to scientific understanding, it will naturally lead to valuable learning for the participants. Or, if you set up an educational citizen science project, it will also contribute to science. The second misconception is at the opposite extreme. It holds that scientific and educational objectives are incompatible—that a project must choose between focusing on either scientific outcomes or educational outcomes, because it is not possible to achieve one without compromising the other. This work is premised on the belief that both of these views are wrong and, furthermore, that we can provide guidance to designers of citizen science initiators on how to pursue both outcomes simultaneously. So, the first premise of this work is that it is possible and practical to implement citizen science initiatives that simultaneously achieve scientific and educational objectives. That is because the scientific and educational goals that most designers hold for citizen science are not incompatible with each other. They may be in tension because they are in competition for resources, but being in tension is very different from being incompatible. Scientific and educational objectives can be in tension in the implementation of citizen science initiatives because they can be different enough that they require separate design strategies (Zoellick, Nelson, and Schauffler, 2012), which means that pursuing both requires more effort and resources than pursuing one. Therefore, a goal of this work is to help designers of citizen science initiatives to see science and education as competing priorities, rather than conflicting goals, and to help them navigate design tradeoffs between them on that basis. We believe that when designers know more about how to design for each outcome, they will be able to identify better options and weigh the tradeoffs among them more effectively. Our hope is that we will reach a point where the expertise and resources available to designers will make it possible for many more designers to create initiatives that have both scientific and educational benefits. It is worth noting that just because one can design a program for both science learning and scientific outcomes, addressing both is not always appropriate or desirable. There are times and places to select one and focus. Participants in projects that are focused on learning may move on to projects focused on scientific outcomes and vice versa. Every citizen science project need not be all things to all people and, arguably, shouldn’t try. The collective output of our workshop participants will be of value in any case. The second premise of this work is that software platforms can contribute to the practicality of simultaneously achieving scientific and educational objectives through citizen science initiatives. While the idea of software platforms is relatively new to the citizen science landscape, they have an important role to play in improving many aspects of citizen science and broadening access to its benefits. Historically, citizen science projects have had to develop their own software, making ¹ We use scientific outcomes to refer to both scientific research outcomes and applied science outcomes. Thus, monitoring the environment for practical purposes, while not scientific research as conventionally defined, falls under the category of a scientific outcome for the purposes of this report. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 6 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. organizational resources and technical know-how limiting factors in citizen science initiatives (Newman, Graham, Crall, & Laituri, 2011). Large, well-funded organizations with substantial technical capacity have been able to develop sophisticated, purpose-built software for their projects (e.g., Cornell Lab of Ornithology, GLOBE program, Smithsonian Institution). However, most citizen science projects are started by individuals or small organizations with minimal capacity for software development (Newman et al., 2011). They typically settle for adapting or customizing free or inexpensive, general-purpose software. More recently, though, organizations with software development capacity have developed a first generation of platforms designed to support citizen science (e.g., CitSci.org, FieldScope, MyObservatory, Vital Signs, Zooniverse). These platforms provide tools that enable project organizers to launch and maintain their citizen science projects with little or no specialized technical expertise. Such citizen science platforms address important pragmatic needs. They make sophisticated features and functionality available to citizen science projects that do not have the resources or capacity to develop their own software. In doing so, they also enable funders to spread their investment in a platform over multiple projects. However, financial and technological economies of scale are not the only payoffs of citizen science platforms. They also can make it possible to leverage solutions to design challenges across multiple projects simultaneously. In the context of this work, this means that platforms offer the opportunity to bring strategies for achieving both scientific and educational objectives to numerous projects simultaneously. The third premise of this work is that sharing design ideas and design cases can be an effective strategy for advancing a field. The method behind this work has a long tradition in the field of design, dating back to design’s origin as a craft, not a science. The method amounts to collecting insights and examples gleaned from practical experience and sharing them with a broader community to inform their practice. This approach may seem to be out of step in an era that prizes rigorous, evidence- based decision-making, but it reflects the fact that we are in the early days of citizen science design and that important design insights can be gained through individual experiences2 (e.g., case studies on CSA blog). It is hard to argue that at this time in the development of knowledge about the design and implementation of citizen science projects that we can justify the time and expense of conducting experiments around specific design decisions. Rather, it is more appropriate for the rapid advancement of knowledge that we develop a structured forum for designers to share their diverse experiences so that others can consider the lessons that they may hold for their goals and contexts and so that patterns may be revealed that would justify more systematic approaches to hypothesis testing in the future. Thus, there are two audiences for this work. The primary audience is individuals and organizations who seek to create citizen science projects. The work is designed to make them aware of the opportunities and challenges for citizen science that might not be apparent to someone who lacks prior experience designing and implementing a citizen science project. In particular, our work is organized to make them aware of opportunities and challenges for achieving both scientific and educational goals through a citizen science initiative. In addition, the goal of this work is to provide ² Case studies are available on the Citizen Science Association website at http://citizenscience.org/category/case-studies/. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ http://citizenscience.org/category/case-studies/ http://citizenscience.org/category/case-studies/ http://citizenscience.org/category/case-studies/ www.bscs.org · info@bscs.org 7 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. citizen science project organizers with strategies for achieving the opportunities and overcoming the challenges presented here. The second audience for this work is developers of platforms for citizen science projects. The information about opportunities, challenges, and strategies, together with the examples, that we provide in this report is intended to enable platform developers to build in supports for the design strategies. These supports will, in turn, enable project organizers to implement these strategies when building their citizen science projects on these platforms. This work is intended to contribute to a line of existing work in citizen science with the goal of supporting citizen science project organizers with guidance on project design. This work includes a white paper by Shirk and Bonney (2015) that offers a high-level guide to thinking about and designing citizen science projects, particularly those that engage citizens in data collection. Shirk and Bonney follow the program design process from asking whether citizen science is the right method for the question the designers want to answer through considering how programs may be adaptively managed to address emergent opportunities and issues. Similarly, the Citizen Science Association’s Education Working Group’s 10 questions to prompt reflection on practice (Kirn et al., 2016) advise citizen science practitioners on how to support both learning and science outcomes. 3 Design Objectives The first key activity of the workshop was identifying what we decided to call design objectives. It is necessary to develop a vocabulary for characterizing the scientific and educational objectives that citizen science projects may be designed to achieve. We selected the phrase design objectives to describe the specific objectives that a citizen science project is being designed to achieve. Design objectives are specific scientific goals and educational goals. They reflect the challenges that must be addressed and opportunities that must be exploited in order to achieve these scientific or educational benefits. These design objectives play two important roles in this report. First, they provide a taxonomy of scientific and educational objectives that have been pursued by citizen science initiatives. Second, they provide an organizational framework for discussing design strategies. All the design strategies that are discussed in this report are associated with one or more of these objectives. These objectives are used to organize the discussion of the design strategies in Section 4. Because we view this work as an initial step in a larger initiative, we have not set out to construct an exhaustive list of design objectives. Rather, we developed this list as a starting point. Our goal is to demonstrate the usefulness of this approach at the current time and lay the groundwork for a more comprehensive effort to follow. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ http://citizenscience.org/2016/09/01/learning-through-citizen-science-an-aspirational-vision-and-ten-questions-to-prompt-reflection-on-practice/ www.bscs.org · info@bscs.org 8 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 3.1 Design Objectives for Scientific Outcomes The design objectives for scientific outcomes reflect the opportunities that citizen science presents as a methodology for advancing scientific understanding. Some of them also reflect the challenges of implementing citizen science successfully to achieve those objectives. In this report, we discuss four core design objectives that apply to networked field studies, the subset of citizen science that was the focus of our workshop. They are 1. scale, 2. access, 3. community empowerment, and 4. data quality and credibility. 1. Scale. Scale is one of the most widely cited benefits of citizen science. It refers to the fact that by enlisting volunteer participants from the general public, a citizen science project can obtain a quantity of observations or analyses that would not be possible or affordable using a traditional scientific approach. In a networked field study, that scale may be measured in terms of numbers of observations, but it may also be measured in diversity of geography or some other attribute. The specific design objective associated with scale is • to achieve a larger number or broader diversity of observations or analyses than would otherwise be possi- ble as a result of enlisting an appropriately large number of participants and/or getting each participant to conduct a large number of observations or analyses. 2. Access. Access is another widely touted benefit of citizen science. The two categories of access are: access to locations and access to expertise. By enlisting participants from the general public, a citizen science project can gain access to locations or expertise that would not be possible using a traditional scientific approach. Access to specific locations is closely related to scale as measured by geographic distribution; but it is not about quantity of locations or magnitude of the area covered, it is about specific characteristics of those geographic locations. In many cases, networked field studies rely on recruitment of participants in specific locations (e.g., state parks) or who have access to locations with specific characteristics (e.g., high elevations, vernal pools, or private lands). Access to expertise refers to specific, rare expertise. One such form of expertise is familiarity with a specific area or phenomenon as a result of experience. Another is indigenous knowledge as a result of upbringing or education in an indigenous community. The specific design objective associated with access is • to obtain access to specific locations or expertise for data collection or analysis by engaging participants who have access to those locations or possess that expertise. 3. Community empowerment. Community empowerment is a third potential benefit of citizen science. It is less widely cited or achieved than scale or access, but many advocates for citizen science view it as the most valuable from a societal perspective. Community empowerment refers to the benefit that can be obtained by a group (the “community”) being able to influence or control a citizen science project to advance shared goals of the community. It contrasts with the model of science in which science is conducted by individuals who have an intellectual interest, but not a vested interest, in the outcome of the work. It aligns with movements to democratize science—to make science accessible to everyone as a tool and process for investigating and deepening knowledge of natural phenomena. The specific design objective associated with community empowerment is • to enable a community that may lack credentialed scientific experts to launch or shape a citizen science project that focuses on a phenomenon of concern or interest to the members of the community. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 9 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 4. Data quality and credibility. Data quality and credibility are recognized as challenges for citizen science, but studies of citizen science projects have shown that these challenges can be successfully managed (e.g., Bonney et al., 2009; Crall et al., 2011; Miller-Rushing, Primack, & Bonney, 2012; Newman et al., 2010; Shirk & Bonney, 2015; Wiggins, Stevenson, Newman, & Crowston, 2011). The large and growing number of publications in peer-reviewed scientific journals is testament to the validity of citizen-collected data3. The benefits associated with scale, access, and community ownership may be undermined if the quality of the data collected is not adequate for the purposes for which they were intended or if the data are not analyzed with sufficient rigor to address the research question. Therefore, the specific design objective associated with data quality and credibility is • to enable participants in a citizen science project to collect or analyze data of sufficient quality to serve the scientific purposes of the project. 3.2 Design Objectives for Learning Outcomes The design objectives for learning outcomes reflect the fact that citizen science offers an opportunity to engage participants in authentic science, which, under the right conditions, can lead to a wide variety of cognitive and noncognitive learning outcomes. Thus, two categories of design objectives for educational outcomes correspond to learning objectives and affective objectives. We have labeled a third category of design objectives instrumental objectives. This category captures intermediate goals that can be instrumental to achieving the objectives in the other two categories. The educational design objectives identified by our group of experts are: Cognitive objectives • Mastery of scientific practices • Understanding of nature of science • Understanding of science concepts Affective objectives • Self-efficacy • Stewardship attitude and behavior • Interest in pursuing science Instrumental objectives • Structuring participant activity to support learning • Supporting facilitation by teachers or other educators Because we did not want to be constrained by prior work, these objectives were identified through a brainstorming session at the workshop. Interestingly, though, they have considerable overlap with the outcomes identified by the Cornell Lab of Ornithology’s DEVISE project (Phillips, Ferguson, Minarcheck, Porticella, & Bonney, 2014) for use in evaluating citizen science projects. ³ For example, a search of the Web of Science for the keywords citizen science, public participation in scientific research, participatory action research, and volunteer monitoring found 280 publications published in 2010 alone. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 10 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Cognitive objectives. Cognitive objectives include the development of new understanding, skills, and abilities. Citizen science presents opportunities to achieve three kinds of cognitive objectives: • Mastery of scientific practices: to enable participants to develop enhanced abilities to plan and carry out investigations, analyze and interpret data, and use mathematics and think computationally by engaging in these practices as part of an authentic investigation (National Research Council, 2012). • Understanding of the nature of science: to enable participants to learn about the nature of science through firsthand experience. • Understanding of science concepts: to enable participants to develop conceptual understanding in the context of legitimate scientific activities that draw on or build such an understanding. Affective objectives. Affective objectives are changes in attitudes and dispositions. Citizen science presents the opportunity to achieve three important kinds of affective objective: • Self-efficacy: to help participants develop a greater sense of their ability to act effectively in the world. • Stewardship attitude and behavior: to foster a feeling of responsibility in participants to care for their com- munity and environment and an inclination to act on that feeling. • Interest in pursuing science: to cultivate a desire to engage in scientific activities in the future. Instrumental objectives. In contrast to cognitive and affective objectives, instrumental objectives do not lead to learning outcomes themselves. Instead, they represent important intermediate objectives that, once achieved, support the achievement of the other two categories of learning objectives. They include: • Structuring participant activity to support learning: to structure the activities of participants in a way that creates opportunities for achieving cognitive and affective outcomes for participants. • Supporting facilitation by teachers or other educators: to provide support for an educator to guide and monitor the activities of participants toward the educator’s learning and affective objectives. DEVISE Learning Objectives Workshop-identified Objectives Cognitive Cognitive Skills of science inquiry Mastery of scientific practices Knowledge of the nature of science Understanding of nature of science Understanding of science concepts Affective Affective Motivation Self-efficacy Self-efficacy Stewardship attitude and behavior Interest in science & the environment Interest in pursuing science Behavioral Instrumental Behavior & stewardship Structuring participant activity to support learning Supporting facilitation by teachers or other educators http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 11 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 4 Design Strategies Design strategies are specific techniques that citizen science project organizers can use to achieve design objectives. The design strategies included in this report were identified by the workshop participants as approaches they have observed being used to effectively achieve design objectives in the context of one or more project. The rationale for identifying these design strategies is to encourage developers of citizen science platforms to provide support in their software to enable project organizers to implement them in their projects. It is important to note that there is not a one-to-one correspondence between design strategies and design objectives. One design strategy may serve more than one design objective (see table in Appendix III). Likewise, one design objective can be supported by more than one design strategy (see table in Appendix III). In the sections that follow, the design strategies are organized by objective. However, they are cross-referenced to indicate when strategies support multiple objectives. Workshop participants developed a standard template for describing design strategies. In the sections that follow, design strategies are presented using this standard template. The template consists of the following fields: • Strategy name and description • Activity phase • Technology support • Examples • Other objectives The strategy name and description fields need no explanation. The activity phase field is used to describe when the strategy is employed in the course of designing and implementing a citizen science project. The activity phases used in this report are detailed in the following table: Preparation Protocol development Participant recruitment Participant preparation Field Study Participant retention Data collection Data entry Data Work Participant retention Coding Analysis Dissemination http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 12 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. The technology support field is used to describe the role that technology can play in the implementation of the strategy. The examples field describes how the strategy has been implemented in citizen science projects and platforms. The other objectives field highlights additional scientific and learning objectives that may also be supported by the strategy. Within the section for each design objective, the strategies are sequenced according to the activity phase where they apply. 4.1 Design Strategies for Scientific Outcomes In this section, we present design strategies identified by participants in the workshop for achieving valued scientific outcomes. 4.1.1 Scale The objective for scale is to achieve a larger number or broader diversity of observations or analyses than would otherwise be possible as a result of enlisting an appropriately large number of participants and/or getting each participant to conduct a large number of observations or analyses. Workshop participants identified six design strategies for this objective: Minimize barriers to entry Standardize protocols across projects Promote participation through publicity Support social interaction among participants Use and incorporate existing data sets Hold community events to reduce discomfort in nature Strategy Minimize barriers to entry Description Make the data collection protocol as easy to understand and implement as possible. Activity phase Protocol development Technology support NA Examples • In iNaturalist, the protocol only requires that the participant take a photo of an organism using the smart phone app. The app automatically uploads the image with location and time information to the iNaturalist database. • Snap-a-Striper asks participants to fill out a simple data card and include it in pictures of striped bass they catch. • In Project BudBurst, participants identify a tree or shrub using provided resources; record its location on a map; and describe its current state of budding, leafing, or flowering using a taxonomy of states and transitions that the project provides. Other objectives Successful participation can also contribute to self-efficacy. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 13 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Strategy Standardize protocols across projects Description Enable aggregation of data across projects and enable participants to contribute to multiple projects by adopting shared data protocols. Activity phase Protocol development Technology support Centralized databases of measurement protocols for project designers Examples • CitSci.org has a page with information about protocols being used on existing projects to encourage projects to use common protocols. • The iNaturalist database uses standards that enable the platform to share research-grade data with the Global Biological Information Facility (GBIF). Strategy Promote participation through publicity Description Seek publicity through traditional and social media in order to recruit more participants. Activity phase Participant recruitment Technology support Provide media kit (logo files, descriptions, contact name, etc.) on-site Examples • Project BudBurst received national coverage through NPR and had a big growth in interest immediately following. • eBird is actively promoted through the traditional and social media outreach mechanisms of Audubon and Cornell Lab of Ornithology. Strategy Support social interaction among participants Description Encourage retention by providing social interaction with other participants, scientists, and organizers. Activity phase Participant retention Technology support Online platform that supports social interaction around contributions. Examples • In Vital Signs, participants give and receive comments, and species experts confirm or question identifications. • In iNaturalist, participants receive comments and species identifications from others, including experts. • Zooniverse Talk supports discussions and learning among project participants. Other objectives Since quality of data increases with experience, retaining participants also improves data quality and credibility. Strategy Use and incorporate existing data sets Description Identify preexisting data sets that can supplement participants’ data. Activity phase Data analysis; protocol development Technology support NA Examples • eBird draws on historical data from the Christmas Bird Count. • The Tamarisk Coalition combines data from citizen science projects with data collected by scientists. • Vital Signs integrates data sets from other investigations. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 14 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Strategy Hold community events to reduce discomfort in nature Description Overcome the anxiety that certain populations have in nature by holding an event where they can be with others. Activity phase Data collection Technology support NA Example • National Park Service BioBlitzes are held as events where people can participate as part of a group led by a scientific expert. Other objectives Reducing discomfort can also support community empowerment. 4.1.2 Access The objective for access is to obtain access to specific locations or expertise for data collection or analysis by engaging participants who have access to those locations or possess that expertise. Workshop participants identified two strategies: Gain access to locations through partnerships Gain access to expertise Strategy Gain access to locations through partnerships Description To gain access to key locations, work with partners who can connect to participants in those locations. Activity phase Participant recruitment Technology support NA Examples • GLOBE Program works with partner organizations around the world who recruit, train, and support schools in their parts of the world. • FrogWatch USA works with a network of zoos, aquariums, and nature centers that train and oversee participants who are identified as members of their chapters. • Snap-a-Striper recruits fishing guides in areas of particular interest. Other objectives Partnering with organizations and their members can increase project scale. Strategy Gain access to expertise Description Recruit those with expertise to complete desired tasks. Activity phase Participant recruitment Technology support NA Examples • Vital Signs works with educators who have the expertise and access to youth to engage them in science activities. • Snap-a-Striper recruits fishermen and fishing guides who have expertise and interest in catching striped bass. • Front Range Pika Project recruits hikers and mountaineers with expertise to safely reach pika habitat and trains them in science skills required to make and report observations. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 15 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 4.1.3 Community empowerment The objective for community empowerment is to enable a community that may lack individuals with scientific credentials to launch or shape a citizen science project focused on a phenomenon of concern or interest to the community. Workshop participants identified three strategies: Support community creation of projects Accept suggestions from participants Offer examples of interesting research questions Strategy Support community creation of projects Description Provide technological tools that will allow anyone to create their own project. Activity phase Protocol development; Participant retention Technology support Project authoring tools. Examples • CitSci.org and FieldScope provide tools that enable people with minimal technical or scientific expertise to create a networked field study by specifying the observations to be recorded. These tools also help to manage the study. • iNaturalist provides tools to allow people to set up a focused species distribution study by specifying a geographic region, taxa of interest, and/or an interval of time. • Vital Signs invites anyone to define new Field Missions around specific research questions. • Zooniverse has a Project Builder tool to help anyone create a new Zooniverse project. Other objectives Enabling people to create projects can enhance their self-efficacy. Strategy Accept suggestions from participants Description Enable participants to influence the design of a study by providing them with functionality to suggest new protocols and data collection campaigns. Activity phase Protocol development; Data collection Technology support Suggestion functionality Examples • The GLOBE Program’s functionality for community generation of campaign ideas resulted in a mosquito larvae protocol and campaign and an El Niño/Southern Oscillation (ENSO) campaign. • iNaturalist expands functionality in response to user suggestions and has a “Feedback” link on the bottom of every page. • CitSci.org offers project forum pages and customizable feedback forms where project participants can offer suggestions to their project manager(s). Other objectives Allowing participants to shape a project can enhance their self-efficacy. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 16 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Strategy Offer examples of interesting research questions Description To encourage students to conduct their own investigations, provide them with examples of interesting questions they can pursue. Activity phase Participant recruitment; Participant preparation; Participant retention; Data collection; Coding; Analysis; Dissemination Technology support Extensible, searchable library of research projects Examples • Project Budburst provides instructional materials with example questions for investigation. • FrogWatch USA annotates maps that users can browse through with questions that can be investigated and instructions on how to conduct those investigations. • Vital Signs has 32 Field Missions for participants to choose from, or they can create their own. • iNaturalist has a wide variety of projects on local to global scales; participants can also create their own. 4.1.4 Data Quality and Credibility The objective for data quality and credibility is to enable participants in a citizen science project to collect or analyze data of sufficient quality to achieve the scientific goals of the project. Workshop participants identified six strategies: Train and/or test participants on data collection protocol Monitor protocol adherence Require documentation in support of an observation Constrain data entry Verify data or classification Employ reputation- or contribution-based data verification Strategy Train and/or test participants on data collection protocol Description Offer training for participants on how to implement the data collection in accordance with the protocol and/or require them to succeed on an assessment of their ability to implement the protocol. Activity phase Participant preparation; Participant retention Technology support Online training tools, tasks, or other resources http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 17 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Examples • FrogWatch USA offers both in-person and online training for volunteers. • The Citizen Science Academy is an online, asynchronous course for educators on how to use citizen science in their classrooms, using Project Budburst as a case study. • Front Range Pika Project provides in-person training that prospective participants are required to complete in order to participate. • Vital Signs has online trainings for educators, curriculum for developing required fieldwork skills, and illustrated How-to Guides for new participants (educators, students, and others). • Zooniverse projects have a compulsory training module before users can participate; users are periodically given test tasks to calibrate their performance. • GLOBE Program requires teachers to complete training on each protocol before submitting data for that protocol. Other objectives Increasing participants’ skill at implementing a data collection protocol can increase mastery of scientific practices and enhance self-efficacy. Strategy Monitor protocol adherence Description Ask participants to describe their data collection process to determine if it was consistent with established protocol. Activity phase Data collection Technology support NA Examples • FrogWatch USA asks participants to report how many minutes they listened—a key aspect of the protocol—as an indicator of adherence. • Various CitSci.org projects leverage this strategy. For example, the Front Range Pika Project asks volunteers to report search time for pika evidence of presence. Other objectives Asking participants to monitor protocol adherence can enhance their understanding of the nature of science. Strategy Require documentation in support of an observation Description For each observation, require the participant to provide an image or other recording that allows the participant’s observation to be verified by others. Activity phase Protocol development; Data collection; Data entry Technology support Technology may be used to automatically capture an image or other recording at the time the observation is being made; require defined fields to be filled in before observations are accepted. Examples • The Vital Signs project requires users to upload images, descriptions, and location data before observations may be accepted. • The iNaturalist project asks participants to submit photos and sound recordings of the organisms they observe to enable other participants to review and verify species identifications. • The CitSci.org platform asks participants to submit photos for observations and allows multiple photos for species observations to support further review by experts. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 18 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Other objectives Asking participants to provide documentation of observations can enhance their understanding of the nature of science. Strategy Constrain data entry Description The data entry mechanism constrains users to only entering data that conform to the protocol. Activity phase Data entry Technology support Allow definition of ranges for data fields Examples • The data entry forms for CitSci.org and GLOBE do not permit input of data that fall outside a prespecified allowable range for a given measurement. • Vital Signs data entry form has required and optional fields; acceptable ranges are defined for certain fields (e.g., temperature, pH, salinity in marine and freshwater environments). • iNaturalist project definition includes setting required and optional fields and setting input ranges if desired. Strategy Verify data or classification Description Verify data by having multiple, independent reviewers or by requiring that an initial classification be supported by other classifiers. Activity phase Coding Technology support Systems for presenting the same observation to multiple individuals for coding or for obtaining verification for all initial codes; alert systems to notify data classifiers of new submissions needing verification. Examples • In iNaturalist species identification must be confirmed by at least one other participant for that observation to be classified as “Research grade” and forwarded to the Global Biodiversity Information Facility (GBIF). • Vital Signs species identifications are confirmed or questioned by experts. • Zooniverse projects put the same task in front of multiple participants. • Zooniverse functionality intersperses test tasks to calibrate a participant’s contributions. Other objectives Transparently engaging participants in verification of data and classifications can support participants’ growing mastery of scientific practices and support their sense of self-efficacy. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 19 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Strategy Employ reputation- or contribution-based data verification Description Assess quality of data based on a participant’s credentials and history. Activity phase Data entry; data coding Technology support Track activities in order to weigh future contributions based on the history of a participant’s data being used and/or verification by respected users; algorithm to apply calibration factor to individual participants’ data contributions Examples • Zooniverse checks a participant’s performance on test tasks and uses the results to weight participants’ contributions. • iNaturalist tracks participants’ performance at identifying species as measured by how often their identifications are confirmed or challenged. 4.2 Design Strategies for Educational Outcomes In this section, we present the design strategies that can enable networked field studies to achieve valued educational outcomes. 4.2.1 Mastery of scientific practices The objective of mastery of scientific practices is to enable students to develop enhanced abilities to plan and carry out investigations, analyze and interpret data, and use mathematics and think computationally. Workshop participants identified one strategy: Data visualization and analysis Strategy Data visualization and analysis Description Provide participants with the ability to analyze and interpret data by creating visualizations and conducting analyses themselves. Activity phase Data analysis; Dissemination Technology support Visualization and analysis tools Examples • The GLOBE Program offers a sophisticated suite of tools for mapping, graphing, filtering, and exporting data. • The FieldScope platform includes a set of tools for creating maps and plots and for conducting geospatial analysis of data. • Vital Signs offers an interactive map and searchable, downloadable database. • CitSci.org offers dynamic real-time interactive charts that allow participants to make comparisons, visualize trends over time, and/or view relationships between variables being measured by a given project. Other objectives Creating data visuals and conducting analysis of data may increase participants’ interest in pursuing science. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 20 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 4.2.2 Understanding of the nature of science The objective of understanding of the nature of science is to enable students to learn about the nature of science through firsthand experience. Workshop participants identified two strategies: Provide a visual representation of inquiry process Require a peer review or self-check of work Strategy Provide a visual representation of inquiry process Description Help students understand the inquiry process by providing them with a visual representation of the process that they can use to guide and track their work. Activity phase Data collection; Data entry; Coding; Analysis; Dissemination Technology support NA Example • Driven to Discover offers a diagram of the process of science to help participants situate their participation within the process (University of Minnesota Extension, n.d.). Strategy Require a peer review or self-check of work Description Help students to understand how and why scientists review each other’s work to identify errors, evidence of bias, and weaknesses in reasoning. Activity phase Data collection; Data entry; Coding; Analysis Technology support • Feedback system • Embedded prompts for peer review; fields to track reviews Examples • Vital Signs has a review system in which students check each other’s identifications of organisms and review each other’s arguments for the quality of evidence and reasoning; the reviewing team’s name is added to the observation they review, making visible their responsibility for quality. Other objectives Asking for peer review or self-check can also enhance data quality and credibility. Since review is an important scientific practice, this also can enhance mastery of scientific practices. 4.2.3 Understanding of science concepts The objective of understanding of science concepts is to enable students to develop conceptual understanding in the context of legitimate scientific activities that draw on or build that understanding. Workshop participants identified two strategies: http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 21 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Support interaction among science experts and project participants Overlay an educational task on protocol Strategy Support interaction among science experts and project participants Description Enable learners to ask questions or receive instruction on science content and otherwise interact with scientists and science experts. Activity phase Participant preparation through Dissemination Technology support • Libraries of text or media recordings of scientists • Real-time or asynchronous discussion and commenting functionality • Databases of scientists • Matchmaking services Examples • In FrogWatch USA, participants receive training from an expert in amphibians from a local zoo or informal science institution, and they have ongoing access to them via email or phone. • iNaturalist has a community of novice and expert naturalists and biological scientists who freely interact with one another. • Vital Signs serves a diverse community of scientists and resource managers with observational data on species of interest and concern; these experts can interact with youth contributing data via public comments, species identification verification, and Field Missions. • CitSci.org supports a role for experts, allowing them to make comments and/or review data submitted by project contributors. Other objectives Exposure to and interaction with experts can help participants learn and grow their mastery of scientific practices and self-efficacy. Strategy Overlay an educational task on protocol Description Provide educational users with tasks that ask them to apply a science concept as part of conducting the data collection protocol. Activity phase Data collection Technology support NA Examples • The Great Nature Project provided “learning missions” that asked young people to take pictures of multiple organisms that exemplify a relationship and describe the relationship (e.g., multiple organisms in a food chain and describe the food chain). • Driven to Discover builds curriculum around existing citizen science projects such as the Monarch Larva Monitoring Project and eBird. 4.2.4 Self-efficacy The objective of self-efficacy is to help students develop a greater sense of their ability to act effectively in the world. Workshop participants identified three strategies: Provide multiple entry points to participation, multiple ways to contribute Publicly recognize participants’ contributions http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 22 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Provide participants with a sense of ownership of their contributions Strategy Provide multiple entry points to participation, multiple ways to contribute Description Provide multiple ways that learners can participate so that they can begin from a place of confidence and grow from there. Activity phase Participant recruitment; Participant preparation; Data collection; Data entry; Coding; Analysis; Dissemination Technology support NA Examples • Vital Signs observations include text, images, sketches, field notes, and measurements and are typically made by teams of students; individual students can contribute by lending their peer leadership, organization, attention to detail, art, mathematics, writing, and observational skills. • Participants contribute to iNaturalist by sharing observations and/or by identifying species in posted observations. Other objectives Giving participants many ways to contribute can encourage participation and can increase motivation and interest, thereby increasing project scale and participants’ interest in pursuing science. Strategy Publicly recognize participants’ contributions Description Provide participants with positive feedback in the form of recognition for their work. Activity phase Participant retention; Coding; Analysis; Dissemination Technology support • The ability to associate contributions with individuals or groups • Visible profiles Examples • iNaturalist and Vital Signs have publicly visible profiles that link to an individual’s contributions. • iNaturalist maintains leaderboards of top observers and top identifiers of each species. • iNaturalist selects interesting data contributions to present as an observation of the week to the community of participants. • Vital Signs has a Best Of section highlighting outstanding observations, images, sketches, etc. • Zooniverse cites all participants in relevant research publications. • The GLOBE Program displays the number of observations contributed by each school, as well as interesting extreme weather measurements. • CitSci.org maintains leaderboards of top observers. Other objectives Recognizing participants’ contributions can contribute to retention, which also enhances scale and data quality and credibility. Strategy Provide participants with a sense of ownership of their contributions Description By providing participants with private recognition of their contributions and a way to access them, projects give participants a sense of ownership that can increase their sense of self-efficacy. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ http://vitalsignsme.org/best-vs www.bscs.org · info@bscs.org 23 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Activity phase Data entry, Database design; Dissemination; Participant retention Technology support Provide participants with individual accounts that maintain connection to their contributions Examples • CitSci.org provides users with a My Profile page that shows statistics for their contributions. It also highlights recent observations by members on the home page. • eBird provides an environment that allows birdwatchers to maintain their personal lists of birds observed, while also sharing their observations as data with scientific researchers. • After submitting an observation to eBird, the participant gets an animation indicating success. • CitSci.org offers a My Observations tab on the My Profile page for each participant to help track their contributions. • GLOBE displays electronic badges on schools’ MyPage for their efforts. Other objectives Providing a sense of ownership can also increase retention of participants, which can contribute to scale and data quality and credibility. 4.2.5 Stewardship attitude and behavior The objective of stewardship attitude and behavior is to foster a feeling of responsibility in participants to care for their community and environment and an inclination to act on that feeling. Workshop participants identified one strategy: Provide information about and/or connection to stewardship actions Strategy Provide information about and/or connection to stewardship actions Description Provide information to participants about the actions they can take in response to the environmental problems they are investigating. Activity phase Participant recruitment; Participant preparation; Participant retention Technology support Support/provide extensible library of resources that includes information about stewardship actions Examples • Monarch Larva Monitoring Project provides participants with information about planting milkweed to provide food and habitat for monarch larvae. • Yard Map provides participants with information about how to improve the habitat for birds in the location they are monitoring. • Vital Signs provides a Managing Invasive Species curriculum extension to enable participants to plan and implement management projects. Other objectives To the extent that the project helps learners to see a relationship between stewardship and science, providing information about stewardship actions can also enhance interest in pursuing science. 4.2.6 Interest in pursuing science The objective of interest in pursuing science is to cultivate a desire to engage in scientific activities in the future. Workshop participants identified two strategies: http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 24 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Show peers participating in citizen science Connect to participants’ existing interests Strategy Show peers participating in citizen science Description Show examples of people doing the required tasks with whom they can identify to participants. Activity phase Participant preparation; Participant retention Technology support NA Examples • A research study found that girls who watched episodes of SciGirls featuring girls like them engaged in citizen science got more out of participation in FrogWatch USA than their peers who did not watch SciGirls (Flagg, 2016). • Vital Signs has a data quality hunt activity that gets participants to look at and critically evaluate species observations posted by their peers. • CitSci.org maintains a blog that highlights peers in featured projects participating in citizen science. • GLOBE has an alumni network that encourages alumni to engage with K-12 students in the program. Other objectives By attracting additional participants, showing peers participating in science can increase scale. Strategy Connect to participants’ existing interests Description Meet participants in issues of concern or interest, design project to build from there. Activity phase Protocol development; Participant recruitment; Participant retention; Analysis; Dissemination Technology support NA Examples • eBird gives active birders who are already recording what birds they see where and when a place to record these lists digitally, and share them with scientists researching birds. • iNaturalist gives people who are already taking pictures of organisms a place to share them with peers and scientists. • Monarch Larva Monitoring Project explicitly recruits people with an existing interest in monarch butterflies. • Front Range Pika Project takes advantage of participants’ passion for high elevation hiking and asks them to record observations of pika as they hike. • Snap-a-Striper takes advantage of the fact that people are already fishing and enjoy taking and sharing pictures of the fish they catch. • Vital Signs lets participants choose which habitats to explore and which species to investigate. Other objectives Tapping into potential participants’ existing interests can motivate participation and contribute to scale and access. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 25 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 4.2.7 Structuring participant activity to support learning The objective of structuring participant activity to support learning is to structure the activities of participants in a way that creates opportunities for achieving cognitive and affective outcomes for participants. Workshop participants identified two strategies: Scaffold inquiry steps Offer student guides and learning resources Strategy Scaffold inquiry steps Description Provide participants with guides that step them through the inquiry process. Activity phase Participant preparation through Dissemination Technology support Online activity guides; educator professional development resources Examples • Vital Signs provides Field Missions with research questions, special protocols, and rationale for why the question is important to scientists and to Mainers. It also provides curriculum units that support educators in guiding youth through a round of scientific inquiry. • Driven to Discover provides curriculum that helps educators guide students to generate and pursue questions of their own through a step-by-step structure. • The CitSci.org platform provides a template for each project that offers tabs that guide participants through steps of research from submitting data to viewing data to performing analyses and visualizations, getting feedback for program evaluation, and sharing results Other objectives Providing an explicit guide through the inquiry process can also enhance understanding of the nature of science. Strategy Offer student guides and learning resources Description Provide materials that guide learners through participation in a way that creates opportunities for learning and learning resources that can support knowledge building during participation. Activity phase Participant preparation; Data collection; Data entry; Coding; Analysis Technology support Online libraries of student materials Examples • Driven to Discover offers student guides and materials (e.g., field journal). • Vital Signs offers student guides for the activities that participants conduct. • GLOBE has materials available to teachers and students that guide student research and offers opportunities for students to share their findings with other students. Other objectives By supporting learning, offering student guides can help to achieve the cognitive objectives of mastery of science practices, understanding of the nature of science and understanding of science concepts, and the affective objectives of self-efficacy and interest in pursuing science. Because teachers can use student guides and learning resources, they are also supporting facilitation by teachers or other educators. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 26 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. 4.2.8 Supporting facilitation by teachers or other educators The objective of supporting facilitation is to enable an educator to guide and monitor the activities of learners toward the educator’s learning and affective objectives. Workshop participants identified two strategies: Provide teacher professional development Provide facilitation guides to teachers and educators Strategy Provide teacher professional development Description Provide educators with professional development experience that prepares them for leading learners through participation in the project. Activity phase Participant recruitment; Participant preparation Technology support Discussion forums to support peer-to-peer educator learning; extensible library of professional development offerings Examples • Vital Signs offers an online train-yourself guide for educators’ advanced training on special topics from data skills to biodiversity investigations. • Prior to the 2016 National Park Service BioBlitz series, teachers attended a weekend workshop in the park where the BioBlitz would take place. They receive instruction on how to prepare their students for the event, and they participate in a species inventory. • Through a network of partners, GLOBE offers professional development opportunities for teachers who want to be involved in the program. Other objectives Professional development for educators can positively impact project scale and access, improve data quality and credibility, and support deeper understanding of the nature of science and understanding of scientific concepts. Strategy Provide facilitation guides to teachers and educators Description Provide resources (e.g., teacher guides, lesson plans) to help educators facilitate learning around their students’ participation in citizen science. Activity phase Participant preparation; Data collection; Data entry; Coding; Analysis; Dissemination Technology support NA Examples • Driven to Discover offers both a Facilitator’s Guide and an Investigator’s Field Journal for each citizen science project they support. • Vital Signs offers Educator Tools, curriculum units and individual lesson plans, and illustrated How-to Guides. • iNaturalist offers a Teacher’s Guide with resources, advice, and examples. • GLOBE offers a Teacher’s Guide in multiple languages. Other objectives Providing a guide to educators can greatly increase the likelihood that educators will implement projects, thus favorably impacting project scale. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ http://vitalsignsme.org/vs-classrooms http://vitalsignsme.org/share-curriculum-resources/Units%20of%20study http://vitalsignsme.org/guides https://www.inaturalist.org/pages/teacher's+guide https://www.globe.gov/do-globe/globe-teachers-guide www.bscs.org · info@bscs.org 27 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. Discussion This report identifies valued outcomes of citizen science projects for science and education and presents strategies that existing projects have used to achieve them. While the report is intended to provide practical support for individuals or organizations who are setting out to design citizen science projects and platforms, it should be viewed as a framework for approaching design with some strategies to consider rather than a complete set of recommendations to be followed. While the strategies presented represent the experiences of the convened group of professionals working in citizen science and education, participants acknowledge that neither their effectiveness nor appropriateness across contexts has been systematically studied. Nevertheless, the participants believed that sharing these strategies would contribute to the field by promoting conversations about design objectives and strategies and might encourage researchers to begin developing an evidence base for the effectiveness of design strategies. Toward those ends, we make three suggestions to advance the agenda behind this work: The citizen science community should develop a shared taxonomy for describing valued outcomes of citizen science projects. This taxonomy would provide a common vocabulary to facilitate dialogue about how to design for improved outcomes. It would also support the development of common metrics for evaluating project outcomes and the implementation of research that could build toward the accumulation of findings based on common terminology and metrics. This report is designed to be a step toward this goal, but the effort to develop this taxonomy could build on the work of the DEVISE project (Phillips et al., 2014) for learning outcomes and best science method practices for scientific outcomes. To maximize its impact, such a taxonomy should be vetted through a public review process and should be promulgated by a membership organization, such as the Citizen Science Association. The citizen science community should develop and maintain a repository of strategies that have been identified for achieving valued outcomes of citizen science. This repository of design strategies would support citizen science project and platform developers in creating or improving projects. The repository should reference examples for each strategy that would enable designers to understand the strategies more completely and provide them with models they could follow in implementing the strategies. Again, the strategies and examples presented in this report could serve as a model or a starting point for a community-wide repository. The citizen science community should cultivate a body of evidence describing which strategies achieve which valued outcomes. The effectiveness of design strategies in achieving specific outcomes in human systems is inevitably contingent on specific circumstances. The evidence should not be oriented at simple “what works” conclusions in these cases, but should be contextualized in terms of the conditions under which certain strategies are effective and for which populations. The development of such a research base requires both collective and individual action. Collective action across a community is necessary to establish priorities, to advocate for resources to support the research, and to accumulate evidence centrally. Individual action is required to carry out the research, to maintain a healthy dialogue about the quality of evidence, and to apply the findings to project design and development. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ www.bscs.org · info@bscs.org 28 © 2018 by BSCS Science Learning. Designing Citizen Science for Both Science and Education: A Workshop Report is licensed under a Creative Commons Attribution 4.0 International License. In conclusion, the objective of this workshop and report will be achieved if it serves as a stepping stone to a community-wide, collaborative effort to inform the efforts of project organizers and project and platform designers. While citizen science has made enormous strides forward during the brief life of the internet, networked data collection and analysis are still in their infancy. To the extent that the citizen science community can join together to articulate common goals, develop and share design strategies, and accumulate research on the effectiveness of these strategies under different conditions with different populations, the progress of the citizen science initiative can be maintained and accelerated. The recent establishment of organizations such as the Citizen Science Association, the European Citizen Science Association, and the Australian Citizen Science Association are testament to the viability and value of collaborative efforts to support the evolution and improvement of citizen science research and practice. The work presented here is intended to serve as a step in that direction. Acknowledgments The design ideas presented in this report were generated by participants in the workshop. The objectives and strategies in this report, which was written by Daniel Edelson with assistance from Sarah Kirn, are largely a synthesis of those ideas. Ben Loh and Aleigh Raffelson made other important contributions. Ben Loh helped develop the design framework and contributed to workshop planning and facilitation. Aleigh Raffelson managed the administrative aspects of the workshop and the project. The workshop and the preparation of this report were supported by a grant from the Pisces Foundation. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Pisces Foundation. References Bonney, R., Ballard, H. L., Jordan, R. C., McCallie, E., Phillips, T. B., Shirk, J., & Wilderman, C. C. (2009). Public Participation in Scientific Research: Defining the Field and Science Education. A CAISE Inquiry Group Report. Washington, DC: Center for Advancement of Informal Science Education (CAISE). Crall, A., Newman, G., Waller, D. M., Stohlgren, T. J., Holfelder, K., & Graham, J. (2011). Assessing Citizen Science Data Quality: An Invasive Species Case Study. Conservation Letters, 4:433-442. Flagg, B. N. (2016). Contribution of Multimedia to Girls’ Experience of Citizen Science. Citizen Science: Theory and Practice. 1(2), p.11. DOI: http://doi.org/10.5334/cstp.51 Kirn, S. L., Ford, M. E., Collay, R., Henderson, S., Jolly, E., Trautmann, N. M., Arndt, L., & Lorek Strauss, A. (2016). Learning through Citizen Science: An Aspirational Vision and Ten Questions to Prompt Reflection on Practice. Retrieved from http:// citizenscience.org/2016/09/01/learning-through-citizen-science-an-aspirational-vision-and-ten-questions-to-prompt- reflection-on-practice/ Miller-Rushing, A., Primack, R., & Bonney, R. (2012). The history of public participation in ecological research. Frontiers in Ecology and the Environment 10:285-290. National Research Council. (2012). A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas. Washington, DC: The National Academies Press. https://doi.org/10.17226/13165. http://bscs.org mailto:info%40bscs.org?subject=TLSYN%20request https://creativecommons.org/licenses/by/4.0/ http://doi.org/10.5334/cstp.51 http://citizenscience.org/2016/09/01/learning-through-citizen-science-an-aspirational-vision-and-ten-questions-to-prompt-reflection-on-practice/ http://citizenscience.org/2016/09/01/learning-through-citizen-science-an-aspirational-vision-and-ten-questions-to-prompt-reflection-on-practice/ http://citizenscience.org/2016/09/01/learning-through-citizen-science-an-aspirational-vision-and-ten-questions-to-prompt-reflection-on-practice/ https://doi.org/10.17226/13165\",\n",
              " \"Working Together to Improve Citizen Science Data Quality: A Guide for Government Agencies | Citizen Science Quality Assurance Toolkit 3 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit INTRODUCTION The Importance of Collaboration and Quality Assurance in Citizen Science Citizen science—also known as community science, volunteer monitoring and public participation in scientific research, among other terms—uses the collective strength and knowledge of the public to gather and analyze data to answer environmental and public health questions. Answering these questions has long been the responsibility of federal, state, local and tribal agencies and laboratories, but citizen science provides a gateway for the public to do this independently, or in collaboration with these and other orga- nizations. Meanwhile, government agencies are recognizing that citizen science can help to maximize resources and com- munity knowledge while expanding public engagement and scientific knowledge. Collaborative citizen science projects can be initiated by either the agency or citizen science groups. For citizen-generated data to have a meaningful impact, qual- ity assurance during data gathering is critical. Citizen science groups need to systematically plan, carry out and document their project through project plans. The US Environmental Protection Agency (EPA) created the Quality Assurance Handbook and Guidance Documents for Citizen Science Projects (EPA QA Handbook) to help meet this need. The EPA QA Handbook provides a framework for citizen science groups to follow, but the project is best positioned to reach its goals when it is conducted in collaboration with environmental and scientific professionals. When citizen science groups develop and follow a project plan in collaboration with government and other professional scientific organizations, the chances are increased that the data will be of known quality and the projects will meet their goal of informing environmental edu- cation, research and policy in their community. About This Guide This orientation guide specifically speaks to state, local and tribal environmental and public health agencies and labo- ratories, illustrating how your agencies can be a valuable resource to citizen science groups so they can be a valuable asset to you and the communities you serve. This guide is part of a larger set of resources created by the Association of Public Health Laboratories (APHL) in collaboration with EPA, called the “Citizen Science Quality Assurance Toolkit.” See the “Resources for Facilitating Citizen Science” section of this guide to learn more about these tools. Government agencies can use this guide to foster mutually beneficial partnerships with citizen science groups. INSIDE THE EPA QA HANDBOOK The EPA created the Quality Assurance Handbook and Guidance Documents for Citizen Science Projects (EPA QA Handbook) to help citizen science groups improve their data by developing project plans. Benefits of Creating a Project Plan Project plans serve two purposes: 1. To help citizen science groups systematically organize and plan their project and; 2. To help potential data users, regardless of the data use category (Table 2), more easily evaluate and understand the project’s data quality. Project Plan Development Resources The EPA QA Handbook provides an easy-to-follow, step-by-step project development process based upon three types of citizen science goals (See EPA QA Handbook, page 7): Education: Increase public awareness Science: Support scientific studies and research Policy: Support legal action or policy making The EPA QA Handbook is supplemented with templates for and examples of citizen science project plan quality assurance and documentation. The templates are available in an editable format and provide instructions, tables and questions to answer while users prepare their project plan. Use the EPA QA Handbook to Create Usable Data Together, these documents can help citizen science groups successfully complete a project plan, providing agencies, laboratories and the community the information needed to evaluate the data’s quality. Look for this hyperlinked icon to the EPA QA Handbook throughout the guide! http://www.epa.gov http://www.epa.gov https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects http://www.aphl.org http://www.aphl.org https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects 4 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit AGENCIES AND CITIZEN SCIENCE GROUPS: A MUTUALLY BENEFICIAL PARTNERSHIP Benefits for Agencies Government agencies and laboratories can benefit greatly from involvement with citizen science projects. The decentralized nature of citizen science offers an opportunity to expand your agency’s environmental monitoring work through increased sample collection and observations. This broader range of data may give you greater study robustness and confidence that your agency’s monitoring plans are effectively responding to community environmental protection and public health needs. Citizen scientists may also have knowledge of the area’s current or historical uses and have an inside perspective on community concerns. Results collected through citizen monitoring could help your agency pinpoint areas for further study, prioritize resource allocation and justify samples needed for regulatory or legal purposes. EXPAND YOUR SCOPE Citizen science groups can help extend your agency’s reach by providing flexibility in: Sample Collection • Atypical or more frequent collection times • Remote or under-studied locations • Environmental matrices not typically analyzed by your agency Topic or Project Type • Unexpected environmental events or hazards • Community environmental justice issues • Exploratory projects or pilot studies Benefits for Citizen Science Groups When citizen science groups partner with government agencies, they may be able to greatly expand the scope and impact of their project. By engaging with and supporting citizen science groups from a project’s onset, agencies and laboratories can provide resources and expertise to help these groups plan and implement their project to produce valuable, high- quality data with a purpose. Areas where government agencies can help include: 1) access to monitoring equipment, 2) an understanding of governmental environmental and public health priorities, and 3) the scientific expertise to ensure the group’s data can be used for its intended purpose. “Citizen science embraces a suite of inno- vative tools to enable the public to apply their curiosity and contribute their talents to science and technology. Citizen scientists can provide information that would not other- wise be available due to time, geographic or resource constraints.” —US EPA 5 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit CITIZEN SCIENCE CAN PRODUCE VALUABLE INFORMATION Recent studies have shown that, with proper guidance and planning, citizen science projects can collect reliable and actionable information: • Analysis of long-term data collected by the Texas Stream Team citizen science program from 1992–2016 found strong data agreement between citizen- and professionally-collected samples, even with multiple sampling techniques, numerous citizen scientists, many years and a large geographic area.1 • Examination of 83 citizen science case studies found that consistent and continuous training appeared to be the key contributor to enhancing citizen science performance.2 • Comparison of citizen- and professionally-collected monitoring data in Southern California found sufficient guidance and supervision, combined with a rigorous sampling scheme, facilitated comparable professional and citizen scientist data.3 Besides these studies, Table 1 outlines citizen science projects that achieved high impact because of collaborations that supported strong quality assurance. A particularly important role agencies can play to ensure this impact is to help write the project plan. A list of additional projects with these characteristics is also available. 1 Albus, KH, Thompson, R, Mitchell, F, Kennedy, J, and Ponette-González, AG. (2020). Accuracy of long-term volunteer water monitoring data: A multiscale analysis from a statewide citizen science program. Plos One, 15(1). doi:10.1371/journal.pone.0227540 2 Aceves-Bueno, E, Adeleye, AS, Bradley, D, et al. (2015). Citizen Science as an Approach for Overcoming Insufficient Monitoring and Inadequate Stakeholder Buy-in in Adaptive Management: Criteria and Evidence. Ecosystems. 18. doi:10.1007/s10021-015-9842-4 3 Gillett DJ, Pondella DJ, Freiwald J, et al. (2012). Comparing volunteer and professionally collected monitoring data from the rocky subtidal reefs of Southern California, USA. Environ Monit Assess. 184(5):3239-3257. doi:10.1007/s10661-011-2185-5 Table 1. Citizen science projects that achieved high impact through strong collaboration and quality assurance. Project Type Project Name Project Goal Project Quality Assurance Measures & Impact Education Missouri Department of Natural Resources (DNR) Volunteer Water Quality Monitoring Program The Missouri DNR uses citizen science data to inform and educate Missouri residents and identify water quality problems. The program’s goal is to foster cooperation between the depart- ment, watershed management commit- tees and highly-trained citizen scientists to perform sampling projects. Highly-trained citizen scientists collect data using quality assurance project plans in accor- dance with DNR’s quality management plan. These data educate Missourians and help the agency establish baseline data on rarely-sam- pled streams. Scientific Studies & Research New York Community Air Screen Program (CAS) The CAS program partners with the Department of Environmental Conser- vation (DEC) to assist communities to conduct air quality surveillance. Toxic air pollutant identification helps determine emission sources and supports toxin-re- duction solutions. DEC's laboratory participates in US EPA's National Air Toxics Trends System and follows required QA/quality control procedures. In 2018, community members helped collect 38 samples in ten New York counties that identified 268 toxic pollutants. This informa- tion was used to determine the effectiveness of air toxin emitter regulation. Legal and Policy Action Save the Sound: Unified Water Study (UWS) UWS supports local monitoring groups to collect standard data across 40 different sampling areas. These data are used to inform and support community and policy actions to preserve and protect the Sound. UWS collaborates with science advisors and regulatory agencies to provide local moni- toring groups equipment, training, standard operating procedures, a project plan and other resources. UWS has many legal and legislative victories, including helping to secure $2.5 billion for water infrastructure investments. Learn more about projects with strong quality assurance and a big impact! A Texas Stream Team member tests water conductivity at White Rock Lake in Dallas, TX. (Photo: Alexander Neal) https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0227540 https://www.aphl.org/programs/environmental_health/Documents/Citizen-Science-Project-Examples.xlsx?Web=1 https://dnr.mo.gov/env/wpp/VWQM.htm https://dnr.mo.gov/env/wpp/VWQM.htm https://dnr.mo.gov/env/wpp/VWQM.htm https://dnr.mo.gov/env/wpp/VWQM.htm https://dnr.mo.gov/env/hwp/qapp/ http://www.dec.ny.gov/public/81629.html http://www.dec.ny.gov/public/81629.html https://www.savethesound.org/water-monitoring-ecological-health#:~:text=Unified%20Water%20Study%20%28UWS%29%20Launched%20by%20Save%20the,compare%20water%20quality%20conditions%20in%20the%20Sound%20 https://www.savethesound.org/water-monitoring-ecological-health#:~:text=Unified%20Water%20Study%20%28UWS%29%20Launched%20by%20Save%20the,compare%20water%20quality%20conditions%20in%20the%20Sound%20 https://www.savethesound.org/water-monitoring-ecological-health#:~:text=Unified%20Water%20Study%20%28UWS%29%20Launched%20by%20Save%20the,compare%20water%20quality%20conditions%20in%20the%20Sound%20 https://www.savethesound.org/about-us/major-victories/ https://www.aphl.org/programs/environmental_health/Documents/Citizen-Science-Project-Examples.xlsx?Web=1 6 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit SOLUTIONS TO COMMON CITIZEN SCIENCE CHALLENGES The following are some of the most commonly seen challenges in citizen science projects. Solutions to these challenges can be relatively straightforward given time, persistence and collaborative efforts between citizen science groups, agencies and laboratories. Adequately addressing these challenges can lead to usable data. Table 2. Common challenges in citizen science and their solutions Challenge Solution Example Defining research goals Work with partners to frame the question(s) your project will answer. Template #3 in the EPA QA Handbook template document helps define the problem, organize background information and solidify the project description. Pages 6-10 of the EPA QA Handbook provide more detail on framing the project’s purpose. In Alaska, the United States Geological Survey (USGS), University of Alaska-Fairbanks and the Indigenous Observation Network collaboratively defined research goals to set up a long-term Yukon River water quality project. Clear goals enabled the partners to turn their focus to continuous quality improvement, ensuring the correct parameters are measured and the data can be used as indicators of watershed health. Implementing adequate quality assurance measures Create a quality assurance project plan. By writing and applying a project plan, an organization builds data quality procedures, methodologies and data use plans into the project from the beginning. The organization will be more confident the data will meet the specific project needs. Page 9 of the EPA QA Handbook provides more detail on crafting a project plan. Michigan Department of Environmental Quality provided an example project plan for Michigan’s Inland Lake Water Quality Assessment. It is a compre- hensive documentation of the program’s planning, implementation and assessment; it includes the elements of program management, data generation and acquisition, assessment and oversight, and data validation and usability. Utilizing quality assurance expertise Work closely with professionals with quality assurance experience. Agencies and laboratories have experienced professionals that could help citizen science groups on project plan development. This helps ensure all key sections are included and the detail is sufficient based upon project type. In Pennsylvania, the Alliance for Aquatic Resource Monitoring (ALLARM) provides needed technical assistance and advice to citizen science groups that conduct monitoring. ALLARM’s tools and technical resources are available online. Assigning responsibilities Map out participants’ roles. Project organization charts help clearly outline the project’s communication and reporting structure and each individual’s role, responsibilities and tasks throughout the project. See Templates #17 and 18. The Chesapeake Bay Monitoring Collaborative developed a project plan that clearly defines roles and responsibilities for multiple organizations involved in water quality monitoring. Collecting repre- sentative samples using the correct techniques Get expert help to develop a sampling plan. Template #9 helps establish an appropriate sampling design before collection takes place. The design should specify the number, locations and times the samples should be collected to produce data that will answer your research question(s). Template #16 outlines a data review process to ensure data were collected using the right methods and from the correct locations to meet the project goals. The US EPA Air Sensors Toolbox Air Sensor Guide Book provides technical advice on developing site-specific monitoring plans for air pollution using low-cost sensors. The Cyanobacteria Monitoring Collaborative allows citizen scientists to identify types of cyanobacteria via a reference manual that can later be verified by experts. Training citizen scientists Develop a standardized training program. Template #6 helps standardize training for all citizen scientists throughout the project, ensuring measuring techniques are uniform. Template #17 helps maintain consistency despite possible citizen scientist turnover by establishing clear lines for communication. The SC Adopt-a-Stream program protects South Carolina’s waterways through stream water quality and habitat monitoring. Citizen scientists can take trainings and certifications to monitor freshwater systems via physical, chemical, bacterial and macro- invertebrate population parameters. The Lakes of Missouri Volunteer Program (LMVP) trains citizen scientists to use equipment to determine lake ecology and water quality issues. Citizen scientists learn how to store samples until LMVP staff can collect and send them to Missouri University for analysis. Bias against citizen science data Develop and follow a project plan to increase data transparency. Collaboration helps citizen science groups develop a compre- hensive project plan that will provide data users proof that the data collection process followed appropriate quality assurance procedures. The state of Virginia maintains a close relationship with the citizen scientist organizations that collect any data the state uses. https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/sites/production/files/2019-03/documents/508_csqapptemplates3_5_19_mmedits.pdf https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://yukon.next.fieldscope.org/ https://yukon.next.fieldscope.org/ https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.michigan.gov/documents/deq/wrd-swas-LWQA-0110qapp_454495_7.pdf https://www.michigan.gov/documents/deq/wrd-swas-LWQA-0110qapp_454495_7.pdf https://www.michigan.gov/documents/deq/wrd-swas-LWQA-0110qapp_454495_7.pdf https://www.dickinson.edu/info/20173/alliance_for_aquatic_resource_monitoring_allarm/2911/volunteer_monitoring https://www.dickinson.edu/info/20173/alliance_for_aquatic_resource_monitoring_allarm/2911/volunteer_monitoring https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.chesapeakemonitoringcoop.org/wp-content/uploads/2018/10/CMC-Non-Tidal-QAPP_Oct-2017.pdf https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/air-sensor-toolbox/how-use-air-sensors-air-sensor-guidebook https://www.epa.gov/air-sensor-toolbox/how-use-air-sensors-air-sensor-guidebook https://www.epa.gov/air-sensor-toolbox/how-use-air-sensors-air-sensor-guidebook https://cyanos.org/cyanoscope/ https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/sites/production/files/2019-03/documents/508_csqapptemplates3_5_19_mmedits.pdf https://www.clemson.edu/public/water/watershed/scaas/workshops.html http://www.lmvp.org/ https://www.deq.virginia.gov/water/water-quality/monitoring/citizen-monitoring 7 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit A PATHWAY TO IMPROVED DATA QUALITY: POTENTIAL ROLES FOR AGENCIES IN CITIZEN SCIENCE US Fish and Wildlife Carlsbad Office biologist, Clark Winchell, instructs University of California San Diego student intern, Stella Yuan, as they survey the hill side for San Bernardino flying squirrels. (Photo: Joshua Allen Ray/USFWS) From using citizen science data for education to scientific studies and research to decision making, there is a wide spectrum of roles your agency or laboratory could assume to integrate citizen science into your organization’s mission. You could play an advisor role, lend out equipment, be hands-on throughout the entire project, and ultimately use the data to supplement your agency’s sampling or monitoring strategy. You could even develop statewide programs with quality assurance training and standard protocols. This section outlines a pathway of a high-involvement role that an agency might take on to help or implement a citizen science project (Figure 1). It references the EPA QA Handbook throughout, which agencies can leverage to help citizen science groups design and execute projects with increased data quality for mutual benefit. In practice, agencies should pick and choose the tasks that make the most sense to meet their agency’s current mission. Visit www.aphl.org/ cit-sci for interactive, downloadable versions of the individual steps and an editable, plain text version to customize the steps to match your agency’s desired level of engagement. You should communicate that your involvement does not necessarily mean endorsement of the project or its results. Data could be used to sue your state or locality or pursue an agenda that is counter to your mission or goals. If issues arise, confer with your legal department to en- sure your agency is protected. ! Figure 1. Pathway to agency involvement. http://www.aphl.org/cit-sci http://www.aphl.org/cit-sci 8 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit 1 Step 1: Pre-planning for a Proactive Approach Determine the ways citizen science may be integrated into your agency’s mission or to help further your own projects. Develop a running list of potential citizen science project ideas from which your agency might benefit. Engage and recruit citizen science groups to participate in these projects, or keep this running list as a resource if you are approached by citizen science groups with their own ideas. Speak with your supervisor to get permission to incorporate citizen science projects into your work. Point them to the resources in this document for examples on how agencies and laboratories can benefit. Work with leadership and scientific staff to develop agency guidelines for becoming engaged in citizen science. Establish an agency citizen science point-of-contact. Determine if your agency has funding that can be allocated for citizen science projects. Consider how you might compensate citizen scientists for their time, particularly for projects that may have environmental justice implications. This could be accomplished through memoranda of understanding and joint grants with community partners. If funds are provided, a project and its project plan may have additional requirements that need to be met. Help citizen science groups develop a timeline for the different stages outlined in this pathway. Connect citizen science groups to other partners or provide them with the resources found later in this document to help move their project forward if it does not align with your goals. Perhaps you may be able to partner with these other organizations to take on a citizen science project together. Direct the citizen scientists to groups leading duplicate efforts to determine if they could amplify that effort. There are a variety of ways citizen science projects originate. Projects typically fall into two broad categories: Science-oriented (top-down) A more traditional approach, where lab scientists and subject matter experts define research questions and lead project development. Community-oriented (bottom up) Community members raise research questions and may want to collaborate with agencies and laboratories to further their project’s impact. Members of Uintas Pika Watch use a transect to conduct species monitoring. (Photo: Erin Moulding) High school student Lauren Magdaleno providing US Fish and Wildlife Service biologist Michael Glenn weather measurements with the Kestrel weather meter. (Photo: Hazel Rodriguez/USFWS) 9 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit 2 Step 2: Project Planning At this stage, a citizen science group will develop a basic research question, determine project objectives, establish sample collection locations, protocols and scope, think about how to train citizen scientists and minimize variables which may affect results, establish data verification processes, and consider how the data will be used. Take time to plan and participate in the planning and design of the project as a whole. Planning is essential to: • Solidify the research goals • Clarify project participants’ roles • Establish strong communication • Identify processes for citizen scientist training and sampling To help the group solidify the citizen science group’s plans: Clarify to the group what your role will be and what that means to the project. Determine the “sensitivity” of the project and how it relates to public sentiment. Are confidentiality agreements needed? Should results not be discussed with the public or the media before an agreed-upon time? Legal/administrative approval of involvement should precede any agency or laboratory engagement. Consider how your agency’s data quality needs can be met by integrating the EPA QA Handbook and other data quality resources into the project’s scope. Help citizen scientists refine their research question so it tests a defined environmental area in an unbiased way, using the right equipment to measure the correct parameters. Help them consider the EPA QA Handbook’s “Just Getting Started” planning questions ( page 9) in more depth. Determine the citizen science group’s knowledge of and experience in organizing citizen science projects and developing project plans. Clearly express any concerns about using citizen-collected data so the group understands the necessity of taking extra steps to ensure their data is of the appropriate quality for its intended use. Discuss common biases of the collection process and ways to avoid them to maintain integrity. Highlight success stories from Tables 1 and 2. Assist with project plan development by pointing the group to the Citizen Science Quality Assurance Toolkit’s video series, “Make Your Data Count,” on the EPA or APHL Citizen Science webpages for a quick start on how to approach their project planning process. If the project is not already funded, discuss or determine ways to secure funding and resources. Assign distinct roles early on in the process. Create organization charts to outline roles like project lead, quality assurance manager, trainers and field collectors. As the project evolves, this will help preserve the lines of communication and reporting. Templates #17, 18, and 19 in the EPA QA Handbook templates document help outline this process. Determine the citizen science group’s ability to train citizen scientists. (See page 17). Template #6 outlines how to develop a plan to train citizen scientists to the quality assurance level required for their project. One of the most important aspects of working with citizen science groups is communication and establishing clear expectations and responsibilities. ! A certified SC Adopt-a-Stream volunteer prepares to collect a conductivity sample. (Photo: SC Adopt-a-Stream) https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects http://www.EPA.gov/citizenscience http://www.aphl.org/citsci https://www.epa.gov/sites/production/files/2019-03/documents/508_csqapptemplates3_5_19_mmedits.pdf https://www.epa.gov/sites/production/files/2019-03/documents/508_csqapptemplates3_5_19_mmedits.pdf https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects 10 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit 3 Step 3: Project Plan Development Once the citizen science team is identified and project goals are outlined, a project plan is necessary to ensure data quality, standardize sampling processes and help guarantee that citizen-collected data can be used for its intended purpose. Be available to either execute training on citizen science project development or to answer questions during this stage on appropriate sampling methods, data management or documentation. Determine the project’s data use category (education, science or legal). See Figure 2 and page 7. Based on this data use category, discuss with the group the level of quality assurance and documentation needed to meet their project goals. It is important that this level be appropriate to ensure that data can be used for its intended purpose without being overly prescriptive. Page 12 highlights which templates need to be included based on this data use category. Be transparent during the project plan development process to help citizen science groups understand the “why?” behind quality assurance requirements so they understand how the data they produce can be used by your agency or laboratory. Template 4 provides detailed information on the data quality objectives and data quality indicators. Ensure the citizen science project plan includes sufficient details so the study can be recreated. Communicate any agency or laboratory project plan development specifications to ensure the project plan is detailed enough to support data use. Consider making a document similar to the Florida DEP’s Quick Guide to QA for Volunteer Programs to clearly state data quality expectations for various agency programs. Communicate your agency’s or laboratory’s sampling and data documentation requirements ( page 22). Work with the citizen science group to determine how the data they produce will be reviewed, verified and validated to ensure it is usable. Template 16 in the EPA QA Handbook templates document can help map out this process. Figure 2. Determine the project’s purpose and data collection method to identify the level of detail needed for the project plan. Taking the time to develop a project plan helps ensure citizen science data can be used for its intended purpose by: • Outlining data quality measures • Standardizing sampling procedures https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects http://publicfiles.dep.state.fl.us/dear/DEARweb/QA/QA%20resources/FDEP%20Quick%20Guide%20to%20QA%20for%20Volunteer%20Programs_AEQA_JUNE_22_2017.pdf http://publicfiles.dep.state.fl.us/dear/DEARweb/QA/QA%20resources/FDEP%20Quick%20Guide%20to%20QA%20for%20Volunteer%20Programs_AEQA_JUNE_22_2017.pdf https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/sites/production/files/2019-03/documents/508_csqapptemplates3_5_19_mmedits.pdf https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects 11 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit 4 Step 4: Sampling/Data Collection Now that the project is planned, help citizen science groups with sampling methodologies through trainings and collaborating on development of or providing standard operating procedures (SOPs) to establish and streamline sampling and analytical methods. Help citizen science groups understand the opportunities and limits of testing. Consider developing certification or training programs and/or SOPs for citizen scientists. These programs can be used to test citizen scientists’ ability to collect and analyze samples while meeting certain data quality standards. Help train citizen scientists in sample collection methodologies for air, soil, and water monitoring. Page 19 has an additional list of resources on monitoring methods. Discuss temporary sample storage and agency transfer logistics and requirements. Your agency or laboratory likely has access to air, water, and soil monitoring equipment while many citizen scientists do not. Consider lending or renting out this equipment to assist citizen scientists and encourage participation by eliminating cost. This process should involve additional citizen scientist training on how to calibrate, use and maintain your equipment properly. Mesa Public Library’s lending library for citizen science is a great model. Depending on availability, agencies and laboratories can work alongside the citizen science group to provide oversight. Citizen science groups can benefit greatly from your agency’s expertise in sampling and monitoring techniques and access to tools and resources. Left: School maintenance staff helps the IVAN Air Monitoring Network install an air quality monitor on the roof of Brawley Union High School in Imperial County, CA. (Photo: Comite Civico del Valle) Right: Citizen scientists conduct watershed monitoring. (Photo: Srishti Gupta/Penn State) Step 5: Data Documentation and Review By following the written project plan at this point, you can work with the citizen science groups to ensure the data have been collected according to plan, documented correctly and can be used for their intended purpose. Work closely with the citizen science group to ensure proper data documentation, both through the project plan and data user requirements. Help answer the following questions when reviewing the data with the citizen science group to verify and validate it: ○ Were the right types of data collected? (time, location, collection method, etc.) ○ Are the data usable? Do datasets meet quality requirements? Are the data traceable for quality? ○ What quality control issues were encountered? What will be done with the outliers? ○ Do additional measurements need to be taken to inform decision making? Partner with citizen scientists to interpret their data and determine next steps. Local knowledge combined with scientific expertise can be powerful in data interpretation. Determine if, based upon the conclusions, the data identify further actions or a follow-up project to collect more specific or confirmatory data. Help determine the most efficient ways to present data based on the anticipated audience. How can this data be accessible and usable to all stakeholders? 5 Agencies can assist citizen science groups with: • Checking data • Interpreting data • Presenting data https://www.epa.gov/air-sensor-toolbox https://pubs.usgs.gov/tm/2006/tm1D3/pdf/TM1D3.pdf https://www.mesalibrary.org/find/stuffbrary/citizen-science https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects 12 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit RESOURCES FOR FACILITATING CITIZEN SCIENCE Citizen Science Quality Assurance Toolkit To make project plan development more straightforward, APHL has developed several resources to facilitate the use of the EPA QA Handbook by both citizen science groups and state and local environmental and public health agencies and laboratories, known collectively as the Citizen Science Quality Assurance Toolkit. These resources include: • Make Your Data Count Training Videos (citizen science groups) This series of six short videos outlines the importance of data quality in citizen projects, highlights the main steps citizen science groups can take to make their data count. • Fact Sheets (citizen science groups) ○ Essential Elements of a Citizen Science Project This fact sheet is a companion document to the “Make Your Data Count” training videos. ○ 5 Steps to Improve Citizen Science Data Quality This fact sheet outlines the quality assurance steps in field- and laboratory-based citizen science projects. • Webinars (one each for citizen science groups and agencies/laboratories) ○ Make Your Citizen Science Project Count: Strategies to Produce Quality Data ○ Make Their Data Count: How Government Agencies Can Work with Citizen Science Groups to Improve Data Quality This orientation guide can be used alongside the training videos, fact sheets and webinars to help provide agencies, laboratories and citizen science groups with information, strategies and resources to collaboratively develop a project plan using the EPA QA Handbook. Access all of the these resources and more at www.aphl.org/cit-sci or www.epa.gov/citizen-science. Other Citizen Science Resources A number of resources are available to promote citizen science success: US EPA Citizen Science • EPA QA Handbook, template and examples (and downloadable editable template document) • EPA Citizen science resources Project Development • Cornell Laboratory of Ornithology Developing a Citizen Science Program: A Synthesis of Citizen Science Frameworks: Outlines the key components needed to develop a citizen science project • FedCCS toolkit: Provides a walk-through of project design and development Quality Assurance • Citizen Science Association Comprehensive Data Quality Resource Library • SciStarter data quality resources for citizen science • Florida Department of Environmental Protection (FDEP) Quick Guide to QA for Volunteer Programs: States various FDEP program data quality objectives and/or expectations so citizen science groups know what is needed for the state to be able to use their data • California State Water Resources Control Board Surface Water Ambient Monitoring Program quality assurance resource archive • University of Massachusetts—Amherst and Massachusetts Department of Environmental Protection Massachusetts Guidebook to Quality Assurance Project Plans: Provides a thorough list of steps to produce a successful project plan http://www.aphl.org/cit-sci http://www.aphl.org/cit-sci https://www.epa.gov/citizen-science/quality-assurance-handbook-and-guidance-documents-citizen-science-projects https://www.epa.gov/citizen-science https://www.epa.gov/citizen-science/epa-regional-citizen-science-coordinators https://safmc.net/wp-content/uploads/2016/06/CS1_Developing20a20Citizen20Science20Program20GUIDE.pdf https://safmc.net/wp-content/uploads/2016/06/CS1_Developing20a20Citizen20Science20Program20GUIDE.pdf https://www.citizenscience.gov/toolkit/ https://www.citizenscience.gov/toolkit/ https://www.citizenscience.gov/toolkit/ https://www.citizenscience.org/2020/03/13/new-data-quality-compendium/ https://docs.google.com/spreadsheets/d/1C7k_IIHDjxSc56l6ajPydyrdWFzvO8ZOauJe1UDoIzQ/edit#gid=0 https://docs.google.com/spreadsheets/d/1C7k_IIHDjxSc56l6ajPydyrdWFzvO8ZOauJe1UDoIzQ/edit#gid=0 http://publicfiles.dep.state.fl.us/dear/DEARweb/QA/QA resources/FDEP Quick Guide to QA for Volunteer Programs_AEQA_JUNE_22_2017.pdf https://www.waterboards.ca.gov/water_issues/programs/swamp/quality_assurance.html https://www.waterboards.ca.gov/water_issues/programs/swamp/quality_assurance.html http://www.umass.edu/mwwp/pdf/QAPPguidebook.pdf http://www.umass.edu/mwwp/pdf/QAPPguidebook.pdf www.epa.gov/citizen-science https://vimeo.com/channels/1732923 https://www.aphl.org/aboutAPHL/publications/Documents/APHL-EPA-CitizenScience-QualityAssurance-EssentialElements-FactSheet.pdf https://www.aphl.org/aboutAPHL/publications/Documents/APHL-EPA-CitizenScience-QualityAssurance-5StepsImproveDataQuality-FactSheet.pdf 13 A Guide for Government Agencies to Support Citizen Science | Citizen Science Quality Assurance Toolkit Monitoring Methods • SciStarter Tools Database: Provides a comprehensive database of observation and data recording tools, categorized by project type • Alabama Mobile Bay National Estuary Program How-to Guide for Water Quality Monitoring: Provides information and documents to plan and implement a citizen science water quality monitoring program • Arizona Department of Environmental Quality Water Watch Program Handbook: Includes information on collecting field data, sampling, and calibrations for citizen science water quality monitoring • Maine Department of Environmental Protection Stream Survey Manual: Provides stream monitoring information, project initiation steps and water quality protection methods • Virginia Citizen Water Quality Monitoring Program and Virginia Department of Environmental Quality Methods Manual: Provides resources to start a project, develop a project plan, and use correct monitoring methods • USGS guidelines and standard procedures for continuous water monitoring • US EPA nonpoint source pollution citizen science monitoring methods • US EPA air sensor toolbox Existing Projects • SciStarter is an online community dedicated to improving the citizen science experience for project managers and participants. It helps researchers manage projects through resources such as best practices for engaging participant partners. • Zooniverse has a compendium of ongoing citizen science projects looking for citizen scientists. • FedCCS has a catalog of federally-supported citizen science projects and resources. Other Resources • Citizen Science Association: A member-driven organization that connects people from a wide range of experiences around one shared purpose: advancing knowledge through research and monitoring done by, for, and with members of the public. • FedCCS: CitizenScience.gov is an official government website designed to accelerate the use of crowd-sourcing and citizen science across the US government. Resources include a webinar series on federal crowd-sourcing. Students from State College High School present results of their participation in the TeenShale Network, a collaboration with Pennsylvania State University, the National Science Foundation and other partners. (Photo: Francisco Tutella/Penn State) https://scistarter.org/tools http://www.mobilebaynep.com/images/uploads/library/Volunteer_WQM_Guide_narrative_final2017_08.pdf https://static.azdeq.gov/wqd/azww/handbook.pdf https://www1.maine.gov/dep/water/monitoring/rivers_and_streams/vrmp/stream-survey-manual/index.html https://www.deq.virginia.gov/home/showdocument?id=4346 https://pubs.usgs.gov/tm/2006/tm1D3/pdf/TM1D3.pdf https://www.epa.gov/nps/nonpoint-source-volunteer-monitoring https://www.epa.gov/air-sensor-toolbox https://scistarter.org/ https://www.zooniverse.org/projects https://www.citizenscience.gov/ https://www.citizenscience.gov/catalog/ https://www.citizenscience.gov/toolkit/resource-library/ https://www.citizenscience.org/ https://www.citizenscience.gov/ https://www.citizenscience.gov/# https://digital.gov/event/2020/02/11/federal-crowdsourcing-webinar-series-episode-7/ Notification Check Box 3: Off Notification Check Box 4: Off Notification Check Box 5: Off Notification Check Box 6: Off Notification Check Box 7: Off Notification Check Box 8: Off Notification Check Box 34: Off Notification Check Box 35: Off Notification Check Box 9: Off Notification Check Box 10: Off Notification Check Box 11: Off Notification Check Box 12: Off Notification Check Box 13: Off Notification Check Box 14: Off Notification Check Box 15: Off Notification Check Box 16: Off Notification Check Box 17: Off Notification Check Box 18: Off Notification Check Box 19: Off Notification Check Box 20: Off Notification Check Box 21: Off Notification Check Box 22: Off Notification Check Box 23: Off Notification Check Box 24: Off Notification Check Box 25: Off Notification Check Box 26: Off Notification Check Box 27: Off Notification Check Box 28: Off Notification Check Box 29: Off Notification Check Box 30: Off Notification Check Box 31: Off Notification Check Box 32: Off Notification Check Box 33: Off Notification Check Box 36: Off\",\n",
              " 'Guide to Citizen Science You should first ask whether citizen science is the best approach to answer your research question. Consider up- front how involving volunteer participants will benefit your project and the participants themselves. Is it critical, desirable, or will it detract from the overall aims of the project? Citizen science can be a great tool for scientific discovery and engagement, but it is important to recognise that it is not always the best approach to take. Whilst it may cost less than many other methods, it is not necessarily a cheap fix, and citizen science lends itself better to certain types of data gathering and analysis than to others. It also requires ongoing support and engagement with the participants if it is to be successful. This is time- consuming but essential; the more you put in, the more you and your participants will get out of citizen science. In the right situations, citizen science can be extremely effective, not only for carrying out environmental surveys, wildlife recording or monitoring, but also for engaging people with how science works and for increasing their awareness of environmental issues and their local environment. One of the core strengths of the approach is that it can be used to present global issues - such as the impacts of climate change or biodiversity loss – in a way that is locally relevant and meaningful. For many people the opportunity to make a difference at the local level provides the motivation to get involved. It is important to bear in mind at all times that participants take part of their own free will, and are motivated by many factors. They may enjoy taking part, wish to acquire new skills or knowledge, want to contribute to a greater good, or volunteer for other reasons that you may not have even thought about. They are giving their time and efforts voluntarily. It is critical to respect their contributions, to use them wisely and to best effect, and to give something back to the volunteers, wherever possible, in the form of training, feedback of results, or simply by saying ‘thank you’. Key considerations • What geographic or temporal scale are you aiming to cover? • How much data do you want to gather and analyse? • Can volunteers help to gather and analyse these data? • Are there other ways of gathering or analysing the data? • To whom will your project appeal? • What might be their motivation for taking part? • Can you support participants’ involvement by providing training and co-ordination? • Do you have the resources to develop and publicise the project and share findings with participants? • Are similar projects already in existence? It may be more efficient to add to existing schemes or work with other organisations than to set up a new project. Before you start Is citizen science the best approach? 2 Before you start Is citizen science the best approach?1 Ph o to : Su si e Po co ck Citizen science works best when: • it works for the benefit of you (or other end user of the data) and for the benefit of the participant; • the project aims are clearly defined and communicated from the outset; • the members of the project team have the appropriate expertise, not just in data collection and analysis, but also in communication and publicity; • evaluation is built into the project design and there is a willingness to listen and adapt as necessary; • small scale trials are undertaken to test the approach with potential participants; • the participants are carefully targeted and supported; • the motivations and skill- sets of all parties (project team and participants) are understood, because they may vary considerably; • participants feel part of the team, understand the value and relevance of their role(s) and (especially for long-term projects) gain new skills; • the project is an efficient and enjoyable way to gather and analyse the required dataset; • the quality of the scientific data generated is measurable. 3 Is citizen science the best approach? - continued In most citizen science to date, volunteers have been asked to act as data collectors: making and reporting observations. Volunteers have also been successfully involved in labour-intensive analytical tasks that require human expertise in pattern recognition - so-called crowd sourcing of data interpretation. Increasingly, scientists are making use of data that are generated automatically (or with minimal management) from sensors used by volunteers. For the purposes of this guide, an emerging classification of citizen science is useful (Bonney et al. 2009). It focuses on the different methods through which the project is developed. Three main approaches are recognised: 1. Contributory projects are designed entirely by scientists. Participants primarily collect, or in the case of crowd sourcing, analyse data. 2. Collaborative projects are also designed by scientists, but participants are involved in more than one stage of the scientific process (perhaps contributing or analysing data, helping to inform the way in which the questions are addressed or communicating findings). 3. Co-created projects are designed collaboratively. Scientists and participants or communities work together in partnership. At least some of the volunteer participants are involved in most or all steps of the scientific process. If you are considering the option of citizen science, we recommend that you weigh up the relative costs and benefits of the various approaches and decide which is most appropriate. This will have a strong influence on the overall feel of your project and how you should go about setting it up. In reality, some projects use a combination of approaches, perhaps including a core group of highly involved participants who help to develop new research questions and methods, alongside a wider group of participants who contribute their observations. Within the UK, voluntary biological recording schemes and societies are extremely active within the field of citizen science and have a long history of developing projects using each of the above approaches. The contrasting approaches to development of contributory and co-created citizen science are explored further here. Contributory citizen science Contributory citizen science features a top-down approach. Scientists and/or policy makers set the questions, design the survey protocols, process and analyse the data, then communicate the results. Participants are generally invited to collect and submit data according to clearly defined guidelines, but that’s the limit of their involvement. Crowd sourcing projects in which participants help interpret existing datasets can also fall into this category. Most citizen science projects to date have followed this approach, but we anticipate that the proportion of collaborative and co-created projects will increase. Contributory citizen science is well suited to engaging diverse participants, raising awareness of an issue and gathering lots of data over a wide geographic area. However, participants are less involved, so scientists risk developing a project that is not socially relevant to the intended audience; the focus is primarily on the scientists’ needs rather than on those of the participants. Before you start Is citizen science the best approach? 4 Choose a citizen science approach Contributory citizen science works well for projects that: • capture the imagination of a broad audience, e.g. projects on charismatic wildlife, biodiversity and environmental health issues, public health or human interest stories, interesting and topical science questions, and projects that link to the school curriculum; • require large volumes of data that could not be collected efficiently through other routes, for example over large geographic scales or fine resolutions; • involve recording regularly encountered species or phenomena. Conversely, involve species or phenomena that are not often encountered, and for which people are simply asked to ‘keep an eye out’, e.g. recording the arrival or spread of non-native species; • require large-scale analyses that are better done by humans than by computers (e.g. identification of photos of wildlife species or museum specimens). For this, a crowd sourcing citizen science project may be appropriate, provided that the subject matter is sufficiently interesting to participants (e.g. oldWeather and Herbaria@Home). Co-created citizen science At the opposite end of the spectrum from contributory citizen science is the completely open, collaborative approach called co-created citizen science. The project team may be established by a community approaching a group of scientists with a question or issue they would like to resolve, or vice versa (e.g. it could be several members of a natural history group approaching their committee with an idea). The project team includes individuals from the voluntary community working alongside scientists (and/or policy makers) in partnership. The project team members work together to define goals, set the experimental approach, and analyse, interpret and communicate the findings. This approach requires willingness from all parties to listen and adapt, and an ongoing commitment to the project. Co-created citizen science works well for projects that: • benefit from establishing a community-led or volunteer-led monitoring scheme. All parties have a stake in the project and the longevity of involvement provides opportunities for training and sharing of expertise. It does, though, require time and ongoing commitment; • involve small numbers of participants and in situations where all parties are willing to listen and adapt, so that a consensus can be reached; • require repeat measurements over time (and which therefore need a greater commitment from participants); • are targeted at a specific, locally relevant environmental problem or question. 5 Choose a citizen science approach - continued Figure 1: (Opposite) Proposed method for developing, delivering and evaluating a citizen science project within the UK Before you start Is citizen science the best approach? 6 Development phase Analysis and reporting phase Before you start First steps Establish project team Identify funding and resources Design the survey or scheme Consider data requirements, storage & analysis Consider technological requirements Report results Share data and take action in response to data Evaluate to maximise lessons learned Live phase Identify question This could be driven by scientific, community or policy needs Choose a citizen science approach Identify and understand target participants Define project aims Plan and complete data analysis and interpretation Develop supporting materials Test and modify protocols Promote and publicise the project Accept data and provide rapid feedback First establish your project team and engage with all relevant stakeholder groups. These could include researchers or naturalists who are interested in the data that your project will generate, community groups, members of your club or society, landowners and even local and national businesses. • Investigate the benefits of a partnership approach. Teaming up with others is an excellent way to share ideas and expertise, spread resourcing and maximise publicity. It can also help to avoid duplication of effort. • Build a team with the required experience, ideally including communication and promotional skills. You may want to involve participants in shaping the project. • Remember that for co- created citizen science, the project team will be a true partnership between scientists and the community, so ensure that the community is well represented within the project team. Each will bring their own expertise and ideas to the table. Amongst other benefits, community members will be best placed to advise on what will and won’t appeal to their community or peer group. When establishing a team or partnership remember that: • From the beginning, your attitude and commitment set the scene. • Think about the style of language that you use. Don’t overcomplicate, and remember that scientific terminology and acronymns are not always understood outside the academic community. Use accessible language and make every effort to include all interested parties in conversations. • Be open to change. Just because you find an idea exciting doesn’t mean that others will. • When beginning a relationship with a new group, try to meet face to face at their location – it shows that you are both approachable and committed. 7 First steps Establish project team2 Ph o to : N at u ra l H is to ry M u se u m • As with any project, it is important to agree the overall aims at the outset and to establish processes that allow progress to be tracked. • Citizen science projects often have multiple aims, from meeting policy needs to gathering biological data and engaging participants with local environmental issues. Managing these aims can be demanding, in particular the balance between engagement and data gathering. Be clear about what balance you are trying to achieve. • Team members may have their own goals, from delivering policy objectives to tackling a local environmental question. Ensure that communication is effective within your team. Get to know one another. Find common ground, openly explore topics of interest and discuss potential questions that the project might address. Aim to establish consensus - strike a balance that everyone is happy with. • Can you maximise the usefulness of the results to additional end users (e.g. by ensuring data quality is good)? When defining your aims, keep asking: • What are we trying to achieve? • Can we do this with existing resources? • Is someone else already doing this? • Can we work in partnership to adapt an existing project, or utilise another group’s volunteers and tools? • What type and volume of data are needed to meet the scientific aims? • What is the spatial and temporal scope of the project? • What defines the target participants for the project? • What’s in it for the participant and are we asking too much? • What is our selling point? • What will success look like and how will it be assessed? • What is the end point of the project? Before you start Is citizen science the best approach? 8 Define project aims Ph o to : D av e K ilb ey It is important to remember that citizen science is not free! It can represent a highly cost-effective approach, but always requires resourcing in one way or another (e.g. staff time to develop materials and support participants, costs of promotional and training materials etc.). The funding and resources required vary depending on the aims of the project and the aspirations of the people involved, but here are some general considerations: • Ensure that you consider funding and resource requirements within your project plan (see James [2011a] for further information). What staff or volunteer resources are available within your team? How much accessible funding is there? • Seek external funding where necessary (e.g. grants, commercial sponsorship), but plan well in advance and allow enough time for the application process. • The most costly phase of the project is often the start when resources are required, e.g. to establish a website, a smartphone app, produce a participants’ pack, or create publicity materials. • For projects with an on- line interface, consider using free and open-source software such as Drupal (content management system), Indicia (on-line recording toolkit) and iRecord (general purpose implementation of Indicia for recording projects). This can help to minimise the costs of building a website. • Working in partnership is highly recommended (see Example A, overleaf). Included amongst the many benefits is the opportunity to pool resources. 9 Identify funding and resources Ph o to : C o u rt es y o f th e M B A Before you start Is citizen science the best approach? Hints and tips • It is easy to say that a project is aimed at the ‘general public’ and some mass-participation surveys can be, but in most circumstances one size doesn’t fit all. Supporting materials generally need to be tailored to specific audiences and ‘hooks’ to attract publicity may also differ according to the potential participants. • Try to share ideas with potential participants at an early stage, gauge their response and identify local or social relevance. What are their interests and motivations? Do they have specific technology, access or training requirements? • We often make assumptions concerning what will work with a given group of participants. These are usually based on our own personal experiences, which are unlikely to be representative, so take the time to listen to your participants. • If you have the time and existing materials, one of the best ways to learn about participants is to run a small-scale trial. Visit a local community (e.g. a school, residents’ association or local natural history society), present your ideas, and give the audience a chance to try out your initial protocols. A good response does not inevitably mean that your idea would work as a national mass participation project, but a poor one suggests that you need to change your approach, or perhaps even focus your resources elsewhere. It sounds obvious, but we’re all different. Our individual motivations, interests and concerns differ widely. In project terms, what works for one group of potential participants (e.g. naturalists) may be less effective with another (e.g. school children). Research has shown that many people participate in citizen science projects for social reasons (e.g. to meet new people) and to gain practical skills (e.g. botanical identification skills, team working), rather than to gain knowledge. It is important to respect the diverse motivations of volunteers. Identify your target participants early on as the choice will affect the protocols, data capture systems and training approaches that you develop, and the style of language used within these. It will also affect the type, geographical spread, sampling frequency and volume of data that can be gathered. So get the participants involved as early as possible – you may think you understand them, but it is easy to miss something obvious or unexpected about their needs or ways of doing things. Remember that when developing a co-created project, the target participants should be represented on the project team. 10 Identify and understand target participants Ph o to : M ic h ae l P o co ck 11 Example A: Working in partnership - UK Ladybird Survey The UK Ladybird Survey is not eligible for direct funding, because most funding bodies require projects to be run by a legally recognised entity, such as a properly constituted society or charity, or to be associated with one. However, the list of partners that the UK Ladybird Survey has worked with is extensive. This is exemplified by the support received following the selection of the UK Ladybird Survey for the Royal Society Summer Science Exhibition in 2009. The Biological Records Centre (BRC) supported the overall design and implementation of the exhibit. A number of items were provided on loan for the exhibition, including a high specification microscope (Leica), plasma touch-screen (Microsoft Research), and display cabinets containing live plants and insects (Rothamsted Research). Some organizations provided resources, including ladybird mini-identification guides (Field Studies Council), UK Ladybird Survey comics (Rothamsted Research), harlequin ladybird information sheets, podcasts and on-line publicity (Royal Society). Others contributed staff time, including the National Biodiversity Network, University of Cambridge, Anglia Ruskin University and University of Hull. The UK Ladybird Survey is hosted by the Biological Records Centre within the NERC Centre for Ecology & Hydrology. Ph o to : H ea th er L o w th er , C EH http://www.ladybird-survey.org/ x Before you start Is citizen science the best approach? Once you’ve established your team and agreed your aims and target participants, you’re ready to design the practical aspects of your project. Remember to keep the participant audience in mind as this will strongly influence what participants are willing and able to do and the support mechanisms that you’ll need to provide. Whilst designing the survey you should bear in mind your data requirements (p14) and available technology (p16). Develop the survey protocol • What are you asking participants to do and how will they do it? What type of data (p14) do you need, and at what spatial and temporal coverage and resolution? • Don’t overcomplicate – keep protocols as simple as possible, whilst still enabling capture of the data you require. Complex protocols can be off- putting and are likely to result in errors. There is often a trade-off between complexity or length of task and the number of participants. • Can you build in progression for participants, so they begin with a simple task, then progress to more complex tasks as their skills and confidence increase? • Consider standardising the method because this could increase the statistical analyses that are possible. To what extent do you need participants to collect the data in the same way and will this be feasible? Will survey sites need to be pre-allocated, or can participants choose them? These are particularly important questions for projects that aim to monitor wildlife. • Bear in mind the health and safety of the participants and give advice or guidance where necessary (e.g. for the safe handling of equipment or wildlife). • Consider whether any equipment is required, and how participants may obtain this. Will you provide it as part of a project pack? Search for other projects like yours. Look for opportunities that will benefit both you and your potential collaborators. Try not to reinvent the wheel. Will your protocol produce data that can be shared or added to existing datasets, to provide greater value? • Don’t be afraid to contact people developing existing projects. Most scientists are keen to share ideas and experiences (see Resources and Links). 12 Development phase Design the survey or scheme3 Ph o to : H ea th er L o w th er , C EH Develop supporting materials and mechanisms • What supporting materials (e.g. instructions sheets, identification guides etc) will participants need and what form will these take (p17)? • How will training and support be funded and coordinated? Can it be handled centrally, or do you need to establish (and help maintain) a network of local hubs and expertise? • Remember what your participants hope to get out of taking part (e.g. knowledge, skills, entertainment, an understanding of their local area) and ensure that support mechanisms allow them to achieve their aims. • Once they’ve gathered the data, how will participants give them to you, how will you provide feedback and in what format (p22)? • Remember to test supporting materials with potential participants (p19). Plan evaluation Plan evaluation methods at this stage – don’t leave it until the end. Evaluation is often left too late to be useful (p26). 13 Design the survey or scheme - continued Ph o to : Ji m A sh er If your project aim is to generate new data (rather than simply analyse an existing dataset) then data requirements will be highly variable. As a general rule, try to maximise the value of your data to others by using accepted data and metadata standards. These are agreed formats for storing and describing your data which make it easier for you to share your information with others. For biodiversity data, James (2011b) provides an excellent overview. Plan how you will analyse your data before you begin collecting them. Quantity of data There is usually a trade-off between the number of active participants you attract and the complexity of the protocol. Are you hoping to engage large numbers of people in collecting a small number of records each, using a simple protocol, or a smaller pool of people who may each collect large volumes of data perhaps following a complex protocol? Be sure to consider how you are going to store the data securely (be aware of data protection legislation if storing personal data) and make them available in the long-term. Quality of data The quality of data collected by volunteers is heavily influenced by the survey design, training materials and support that you provide. Data accuracy can be excellent, but, as with any project, it’s important both to minimise the opportunities for errors to occur and to understand how data quality varies between samples or even participants. Data of known quality are scientifically useful and are also more likely to be used as evidence by policy makers; data of unknown quality are open to scientific criticism. Validation and verification are two ways in which you can reduce error rates within your data: Validation is an automated process of checking if something satisfies a certain criterion and can therefore be interpreted successfully. It is possible to add validation checks to web-based data entry forms (e.g. ensuring that dates, grid references or postcodes are given in the correct format and within valid ranges). Verification (or ground- truthing) is an additional, usually manual, process through which data can be checked. For example, a photograph of a plant, animal or physical feature (e.g. the amount of water in a stream) can be checked visually to confirm that the information that has been provided is correct. Asking participants to provide photos can add huge value to the data by allowing verification (see Ball 2011). Another approach is to verify a subset of the data, e.g. by requesting samples to be sent in or by accompanying a few participants and observing the measurements that they take. Verification can also be crowd sourced, for instance by asking people to assess each other’s photographs or asking different people to take measurements at the same site. Understand the quality of your data by observing participants and identifying the types of errors they make. Ensure that your protocol allows you to check the quality of a subset or all of the data and maybe even investigate ways to rate and reward participants’ abilities. iSpot assigns badges in recognition of expertise. The Weather Observations Website uses a star rating to indicate data quality (see Example B, overleaf). Although poor quality data can be omitted, this represents a waste of participants’ time! Instead, variation in data quality (including between participants) can be modeled statistically and taken into account when undertaking analysis. Plan data analysis Plan how you will analyse the data, how you will store them and who will have access to them (p23-25). Involving a statistician at an early stage of project development is highly recommended. Before you start Is citizen science the best approach? 14 Consider data requirements 15 Before you start Is citizen science the best approach? Example B: Understanding the quality of your data - the Weather Observations Website The Weather Observations Website (WOW) was launched by the Met Office in June 2011, and offers a new way for weather enthusiasts to submit and share their own manual and automatic weather observations and photographs online. In the near future the website will be updated to enable users to report the impacts of weather (e.g. flooded roads, or damage to trees or property caused by strong winds). In the first 12 months more than 38 million observations were submitted to WOW; over 2000 separate observation sites created, and there were over 165,000 different visitors to the site from 152 different countries. These observations form a valuable extra source of meteorological information for forecasters, particularly in forecasting severe weather events and their onset, and research is planned to evaluate the benefit of using the data within weather forecast models. WOW uses a star rating to reflect the user supplied information about the quality of the observing equipment being employed and the exposure of the weather station location. There are also various quality control rules for identifying gross errors, and it is possible for registered users to flag data that they suspect are erroneous. Finally, specialist software is used to scan photos and text for inappropriate content – thankfully, despite concerns at the outset of the project, this has not been an issue at all for the website. The Weather Observations Website is hosted by the Met Office. Ph o to : Sh u tt er st o ck http://wow.metoffice.gov.uk/ http://wow.metoffice.gov.uk/ http://wow.metoffice.gov.uk/ http://wow.metoffice.gov.uk/ Technological developments are revolutionising citizen science: web-based data capture, analysis and presentation tools and smartphone apps are in common use, and a wide range of next generation environmental sensors are under development. From online recording and real- time mapping to digital photography, there are tools for most tasks. See Understanding Citizen Science and Environmental Monitoring (Roy et al., 2012) for more detail. A good website can enable online data entry, data validation and real-time mapping of participants’ results. It can also provide background materials, training resources and forums and blogs through which participants and project developers can interact as part of a project community. Websites can also serve as foci for promotional activity. Insisting on the entry of data to be via the web can, however, deter some participants from sending in data, so consider accepting paper entries too. Case studies within Understanding Citizen Science and Environmental Monitoring (Roy et al., 2012) include numerous excellent examples of: • Web-based data entry forms (e.g. Indicia, iRecord, BirdTrack, UK Ladybird Survey). • Real-time web-based data visualisation using GoogleMaps and other free software (e.g. The Open Air Laboratories Network - OPAL, Plant Tracker, WOW). • Web-based crowd sourced identification of wildlife photos (e.g. iSpot; NHM forums) and museum herbarium specimens (e.g. Herbaria@Home). • Social media as tools through which to establish and maintain citizen science communities. • Smartphone apps that serve as training guides and data upload tools (e.g. mySoil, OPAL Bugs Count, Plant Tracker). 16 Contents Consider technological requirements Hints and tips • Be flexible. Can you include different approaches within your project? • Citizen science should be innovative and imaginative, combining the collation of high quality and useful data while appealing to the volunteer community. • Select technology appropriate for your target participants. By opting for a particular technology you will be implicitly engaging particular communities, so choose an approach that is both interesting and accessible to your potential participants. If physical items are required (e.g. sampling kits), remember that you’ll need to distribute them. • Mobile phone signal strengths are variable throughout the UK. This can affect projects that rely on data upload via mobile phone or tablet. If you plan to use this technology, consider allowing mobile devices to store data for upload when a signal is available. • Consider the risks of the technology failing (e.g. launch-day website crash) and what you can do to address these. • Only be as high-tech as necessary. Cutting-edge projects can capture people’s imaginations, but there is a cost to all development, and some of the most successful projects rely on simple face-to-face interactions. Well-supported citizen science participants will not only produce higher quality data, but will gain more from participation - from increased skills to improved confidence and new social networks. The OPAL project has found that a common reason for participants not uploading their data is a lack of confidence in the accuracy of their results. Providing supporting materials can help to minimise the risk of this. 17 Before you start Is citizen science the best approach?Develop supporting materials Hints and tips • Training provides an excellent way to build skills and retain involvement in a project, and therefore provides rewards for participants and scientists alike. • Develop supporting materials, such as identification guides and survey instructions, with input from participants – testing by users is vital. • Will downloadable or printed guides be sufficient, or will face- to-face training be necessary? Identification workshops are extremely popular. Consider running ‘train the trainer’ courses, so that the training you give can be shared with others. • When developing supporting materials, pick the format that works best for your participants (see Example C, opposite). • Use as little text as you can get away with. • Video clips can be an excellent way to demonstrate techniques and introduce your team. • Consider developing a project Frequently Asked Questions page or a web forum that allows participants to share skills and experiences. This could also be done via socal media such as Facebook or Twitter. Anything interactive will need to be moderated and requires someone with a passion for new communication technology, otherwise it will quickly become out-of-date. • Consider mentoring (either locally in person, or virtually via the internet) as an approach to developing skills over time. Can you build a network of regional mentors, coordinators or champions? • Make use of existing materials. Contact groups that specialise in the species or environmental topics or training that you are investigating and ask their advice. They may be happy to share their resources or contribute to your project. Ph o to : H ea th er L o w th er , C EH The OPAL Bugs Count Survey investigated the effects of urbanisation on terrestrial invertebrates. A wide range of training materials and supporting resources were developed to accompany the survey. Each element was designed and tested with input from the target audience, or colleagues who had extensive experience of working with the relevant group. The resources were aimed at improving participants’ confidence, increasing both survey uptake and the quality of the resultant data. A teachers’ pack linking the project to relevant areas of the national curriculum and outlining suggestions for extension activities was particularly well received. Resources included: Identification guides tailored to the audience. Identification quiz and Powerpoint training presentation. Teaching supplement. Group leaders support pack. Poster showing where to look for invertebrates in urban settings. Bugs Count was developed by the Natural History Museum with the University of York, University of Birmingham and Imperial College. 18 ContentsExample C: Developing supporting materials - The OPAL Bugs Count Survey Ph o to : Sh u tt er st o ck http://www.opalexplorenature.org/bugscount A critical step now is to test the survey protocols, data entry forms and training materials to ensure that they are fit for purpose. It is easy to make assumptions about what appeals to different people, what kind of language is appropriate and whether instructions are clear or not. To ensure your project has the best chance of success, test all elements with potential participants under realistic conditions. It can be tempting to skip testing when deadlines get tight, so allow plenty of time! 19 Test and modify protocols Hints and tips • Trial the survey protocol and supporting materials with potential participants. Observe how people use materials and ask for constructive feedback. Where do participants go wrong, and what questions do they ask? • Take all constructive feedback on board and adapt your protocol and supporting materials accordingly. How can you amend your approach to maximise both the clarity of materials and the quality of data generated? • Testing by users is often an iterative process. Be prepared to re-test the refined materials and adapt again if necessary. • Testing should be viewed as part of a broader, continuous evaluation process (p26). • Be prepared to make radical changes to your project or even discontinue it if feedback from participants suggests that it will not be successful in its current form. Once initial testing and modification is complete, you are ready to produce and distribute the materials. However modification may continue over months or years as projects evolve. (See Example D, opposite.) Ph o to : B ar n ab y Sm it h , C EH Aque entempore vel molut quatend andebis dellupta verroreratem eatatecae. Untotatem hit ipient, sedio. Sequat is mi, que culpa erro qui inci cupicae senimet velic tem aut alitas quam autem idellique eiciis eate posa sitatusa esectiores esequunt, con et ea volupta tecatquia conse verum quam culpa volorest, suscipsa idesti de re mo excerovid molorum que nihitis res denitat empore re, unt. Onem que aceaqui coratem. Nequatu recture henimagnis apit ad quae sitibus. Exeribusam volupient oditatiam facerion nonsequis nam, cusam, aspero que nos accuptatur, optiis il im conse nobit qui omnihiti blam, ullab inimagnis explam, omnimuscit facidel modipsa enimus dolupta sitio. Endi rest, unt endi suscia nonet occus sunt optatur arum aut occusaes magnatio conseque verro bearum eum dest, sandis et velisit ma vercien ditibusdae. Nimus, coneces aut officium nectior emposantis doluptatur? Latquae modi cumquam venimust, cone iusant, te providem sim consend ucipien dellestiatem quiderum il is earum que non con nia quiam quia enis ab idello tem eum harum quat occuptas dolupicil eumquaspere sunt res eicia verum fugia con conem everiant as doluptatus rem dolor aperum incienimo dem eos ea dit et quossim quibeatur ra estis doluption eni alic tem quuntem iur andam ellendis venis eates ab im hiliquunt. Odigenda volore net ut at quid et est laborup tatemolupiet invende corem des ipide net, quidi id maiore consecto id moluptat quo idem ipsunt litiis ea inihil et ligent officilis dolestrum sinciumenda inusame exerit, sin escilig nimeni cullate ndante dest, suntiis archiciet re, quias reiuntur, is sitatur rem dit quam aut vit, utatiae ne nem renit prae magnatem seque non culparum aut voluptur, officie nienime ndandent essi ut moloritatur, quo dellabo. Nemporem aut asped molo essimusae estium fugit experum nonsequis aut fugia pedipsam, aliquoditio vollabo rerumet ut faceatur adiciendus, occusamet asim la doluptatqui omnim unt laboreperia nonseque officiis mo que il mi, sus, tem remporp orporerentin pra doluptae eos nectae. Liciame persperoviti vellupide voluptassum eos idesti dia nat perum inctaer itaquiatus sitate et labor sim rem iliquassi dolo eosto commoluptat. Fugit acipien isquiaere eostibus adiciuntis etum ad etus, que peliqui aute earum laut latusam estinustrum, solorem quiantibus dere est, que volorum, offic temquis adipsapel eaquis doluptis et ut re sim simin cuscipsae nus eum iur, sunt. Eperiorenes eatium inienis alis ut aditaque ma ped qui alitat aut asperci andeleste que esequi as sandae volest fuga. Nam con re, ipsante cuptatur maio conescium ipsunt officium dolore odiciam, quia dolesequis mi, serioris iur, odipid quat ex ea derupie nisciis es pro temos sum adionempor as sunt quia verro es untur, ad magnimp erferum consendus et mint quiataquiate vitia volupta tionsequis simporum qui simint offici arum ut est eliatum imus pero maxim ipsuntis et atibusc ipsapiet optur sit ma nam sim fuga. Poremqui berrum, que eaque volute dolor mos auda dolecum ut occum ad expla nisit quia venimus, sincian disquod ipiende dolupta ssitatio volendi qui ut etureiur sendi quatemposte raepudam, ipsae. Nem sequisi tiores il explabo restis excepere perciur ma quidenditas dolor alibus evelenis num harum restiur aut alique eosanda et maximollanis eumquam audis modit et idio vel enimus re volorporitia et, et ommolup taspernam qui net ex eum fugit eos dolum labor asperumet, quo magnis modiam, voluptate voloren iminci am nam, opta quaerum enimaxi x Before you start Is citizen science the best approach? 36 Conker Tree Science is a national citizen science project in which scientists seek to answer specific questions about the biology of an invasive non-native insect, the horse-chestnut leaf miner. Conker Tree Science has progressed through a series of steps including the launch of three hypothesis-driven ‘missions’. Two scientists decided they wanted to undertake hands-on communication about the way that people depend on nature. So they: 1. ran a public engagement event in a shopping centre in which they gave out 1,100 pots for people to observe and report insects emerging from horse- chestnut leaves. 2. worked on the horse-chestnut leaf miner with 1,000 local school children. Children reported their results via a website and could view graphs of the overall results. 3. received funding from the Natural Environment Research Council to implement the project at a national scale. They switched to map-based (rather than graph-based) feedback for participants and included log-in procedures for entering data on the website. They worked with a subset of participants to validate the accuracy of the records they received. For part of the project, the participants’ ‘mission’ was to collect horse- chestnut leaves during one week of the year and rear insects from them. 4. widened participation through development of a second ‘mission’ that anyone in the UK could take part in at any time. This second ‘mission’ was developed into a smartphone app. 5. developed a third ‘mission’ in response to enquiries from participants and a project blog was added to the website. Conker Tree Science is run by scientists from the NERC Centre for Ecology & Hydrology, the University of Hull and the University of Bristol. 20 ContentsExample D: Testing and modifying - Conker Tree Science Ph o to : R ic h ar d B ro u g h to n , C EH http://www.ourweboflife.org.uk/ Spend time planning how you will publicise and promote your project – this is critical to the successful recruitment of participants, and should be tailored to your participants. The amount of promotion required depends on the approach you have taken and the size of audience you are aiming to reach. If you’ve been interacting with potential participants through the project development process, then you have already started to promote the project. If you are targeting an existing membership, then promotion via email newsletters, Twitter, Facebook etc. can be very effective. Capturing a new audience of potential participants requires more effort. Start with your contact network. Local subject-related websites and email newsgroups can be particularly useful for reaching enthusiasts, who can then spread the word in their local area. For smaller, local projects, think about promoting the project by means of posters in local parks, post offices and businesses. Local or regional newspapers may also be receptive. 21 Before you start Is citizen science the best approach? Live phase Promote and publicise the project4 Ph o to : H ea th er L o w th er , C EH Hints and tips • Focus on your target participants. • Identify a news ‘hook’ – something catchy that the press and potential participants will pick up on. • Write a press release and prepare what you’d like to say before you contact anyone. • Approach different sectors of the press depending on your aims (national press, local press, specialist publications), and consider different media (TV, radio, print, online). • Don’t assume that press coverage will ensue – it is never guaranteed. • Social media, such as Facebook and Twitter, provide exciting opportunities for citizen science and enable you to reach a broad range of potential participants. • Maximise face-to-face promotion through events and talks. • A launch event can be a great way to kick-start your project, as it gives a focus for participants and press alike. It is often easier and more cost effective to piggy- back on existing events and to work through others than to organise your own events. It is usually inexpensive to hold a stand or run an activity at an existing event, and you can often reach a large audience this way. Some examples include: county fairs, green fairs, BioBlitzes, science festivals. Accepting data Once your project is live, participants will be actively sending in data (hopefully lots!) through the mechanisms that you’ve set up – be that online, via smartphones or as paper records. Keep checking that these mechanisms are working – you may not have spotted all glitches during testing, and websites occasionally crash when traffic is particularly heavy. Providing rapid feedback Thanking participants for taking part not only shows that they are valued, but encourages their continued involvement and gives them a sense of achievement. Rapid feedback is a powerful way of motivating participants. The more instantaneous the better! How you provide feedback will depend on your budget and the routes through which data are received. • Email – think about sending personal or automated replies to thank participants. • Phone – automated text messages can be used to show that data have been received and to thank participants, but can be costly depending upon volume. • Web – real-time results maps are a good way to show that data have been received and have already been incorporated into the dataset. The OPAL project and WOW both show how GoogleMaps can be used to good effect. If you are able to provide participants with a brief summary of results on a regular basis via a newsletter, blog or email then this will also be well received. Don’t swamp your participants though – a monthly update is usually sufficient. If sending an email update, remember to check that any embedded links work. Ideas for rewarding your citizen scientists If your project sounds fun, is interesting and produces useful data, hopefully you will attract a good number of participants. In return for their input and to encourage further participation you could: • hold a feedback or closing event, perhaps incorporating a social activity, to thank them for their input and present results to them; • give free access to all non-sensitive records; • run a competition to encourage repeat participation; • simply say thank you! 22 Contents Accept data and provide rapid feedback Ph o to : Sh u tt er st o ck Planning data analysis before you begin to collect data is vital to ensure that you gather the data required. Can analysis be done continually throughout the project, providing you and participants with a regularly updated picture of the results, or will it be conducted only at the end? Here are some general considerations. • Data cleaning It is inevitable that the data will need to be cleaned to remove (or investigate) spurious results. In part this may have been done through validation and verification (see p14) at the point of data entry or afterwards by experts. You may also need to arrange the data into a format from which you can begin the analysis, although giving thought to subsequent analysis at the start of the project is essential. • Visualising the data Before you begin the statistical analysis, it is useful to visualise the data and produce a range of summary statistics to gain an overall view of the dataset. Which summary statistics you use will depend on the data – perhaps a mean, median, running average or maximum and minimum values. You may want to look at the variation within the data, and produce graphs, charts or maps. Do wildlife distributions or physical environmental measurements appear as expected? • Statistical analysis Which statistical analyses you choose for your data will depend entirely upon the type of data collected, and you may need an expert to help you with this. Most scientists seek either to answer a question or to estimate parameters. Consider what level of precision around the estimate is acceptable. Is power analysis needed to determine the sample size required? As a dataset builds up, new questions may arise and the power to answer them may increase. It may be preferable to undertake simple analysis for quick feedback to participants, followed by thorough analysis later. • Data quality assessment Confidence limits and verification levels should be documented alongside all data, whether gathered through citizen science or not. Clearly stating the quality of your data is likely to increase its use by scientists and policy makers vastly, and will enable you to act on your conclusions with confidence. • Interpretation Once the statistical analyses are complete, you will be able to compare the results to your hypotheses or project aims. • Qualitative analysis Whether you intended to or not, it is likely that you have gathered some qualitative data from participants in the form of comments and feedback given during training, on feedback forms, or at the point of data entry. It is labour- intensive, but worthwhile, to look through these data carefully (perhaps categorising them), as they may provide insights that you had never considered before, or useful evaluation and feedback for improving the project. 23 Before you start Is citizen science the best approach? Analysis and reporting phase Plan & complete data analysis & interpretation5 Ph o to : H el en R o y You will almost certainly need to report and present your results to different people: your participants, data users, funders or the press. Different levels of detail and different types of visual representation of the data will be required. Participants will be interested in seeing how their efforts have helped the project. Audiences vary, but most will be interested in the general trends (a summary of the overall findings), so your summary statistics and visualisations of the data may be most appropriate. Think about what would be the most informative and rewarding way for participants to view the data. You may wish to provide some interpretation of the data, but could also invite interpretation from participants. Try to present your results in a way that shows their local relevance. Present your data through the most appropriate route for your participants – there’s no point making the effort to create a report if no-one reads it! This could mean through your website, group mailing list, or social media. For a community- based project, nothing beats a face-to-face presentation and question and answer session. Scientists and policy makers will be interested in the broad results – what your data show and how the results fit with the wider picture - but also in the details. For example, the methods and analyses employed, data quality, comparison with previous understanding, and types of graphical or statistical outputs. The written communication style for this audience will usually be very different to that used for a public or media audience. The value of citizen science in helping to meet the need for environmental monitoring and to address the challenges outlined within the emerging governmental biodiversity and environment strategies is widely recognised. Effective communication of your project will help to ensure the maximum use of the data you collect - which could also inform future strategies. Media and press want to hear sound bites. Short, snappy phrases that explain what you did, for what purpose and just why your results are so fascinating! Make your communication relevant to the audience (viewers or readers) by phrasing it in a way that will capture their interest and imagination. Regional press will be particularly interested in local stories – what your results mean for local wildlife, which schools or community groups took part, etc. Seeking advice from a press officer or communications expert is recommended. Remember to acknowledge and thank participants – they’ve played a critical role in making your project a success! 24 Contents Report results “The value of citizen science in helping to meet the need for environmental monitoring and to address the challenges outlined within the emerging governmental biodiversity and environmental strategies is widely recognised” Sharing data Sharing non-sensitive elements of the data that you collect will allow them to be used by others, maximising the value of the data. Remember to consider potential intellectual property rights and data protection requirements at an early stage. Try to make data available in electronic format wherever possible, as this will increase their accessibility to others. The UK-EOF aims to promote access to and awareness of current and future information from observation activities across government departments and agencies, the voluntary sector, industry and academia. Their on-line catalogue can be searched to identify environmental observations being undertaken within the UK, including citizen science data. UK-specific suggestions for sharing your data are given below. Biodiversity data is best stored and shared through the National Biodiversity Network Gateway. You may or may not have already planned where you will store the data collected, both in the short and long term. It is important to talk about this during the planning stages to ensure all members of your project team are happy. Data may be passed to a local Environmental Records Centre or National Recording Scheme or Society for verification but you will also need to consider where they will be stored in the long term and how you will share them with others throughout the country, or even globally. Marine data can be shared and formatted through the Data Archive for Seabed Species and Habitats, a service managed by the Marine Biological Association. Environmental data should be shared with relevant organisations, for example the British Geological Survey will be interested in soil data and the Met Office in weather related observations. The Marine Environmental Data and Information Network promotes sharing of, and improved access to, marine data. The Environmental Information Data Centre can host and share environmental datasets of long-term value to future science. Get in touch with relevant organisations early in the project development process and they will be happy to advise. The data can also be hosted on your own website or in repositories such as Dryad, and then listed on international initiatives such as DataONE. DataOne is a repository that lists where different environmental datasets are stored and how they can be accessed by others. Archiving your dataset, in a way that complies with modern data storage guidelines, ensures the data are stored in a secure and accessible place and will reassure participants of the long-term value of their data. Take action in response to data Data collated through citizen science can be used in many ways by many people. In some cases there may be appropriate action arising from the information gathered. For example, both Plant Tracker and the Recording Invasive Species Counts initiative encourage people to gather data on a number of non-native species in the UK. The information can be used for appropriate action such as containment or removal of the species. In other cases citizen science can inform management of specific sites for example through the implementation of measures to improve water or habitat quality. 25 Before you start Is citizen science the best approach?Share data and take action in response to data Ph o to : M et O ffi ce http://www.ukeof.org.uk/ http://data.nbn.org.uk/ http://data.nbn.org.uk/ http://www.dassh.ac.uk/ http://www.dassh.ac.uk/ http://www.bgs.ac.uk/ http://wow.metoffice.gov.uk/home http://www.oceannet.org/ http://www.oceannet.org/ http://www.oceannet.org/ http://www.ceh.ac.uk/data/index.html http://www.ceh.ac.uk/data/index.html http://www.ceh.ac.uk/data/index.html http://www.dataone.org/ Evaluation is an ongoing process through which you can improve your project. For convenience we have placed this section at the end of this guide, but to be effective, evaluation should occur throughout your project. Evaluation will help you to improve your citizen science, both in terms of the data collected and analysed, and in terms of the experiences of your participants. It can help you determine strengths and weaknesses, gather evidence of success, understand your participants’ needs, improve your project, and apply for funding. It also helps you celebrate your achievements! Types of evaluation: Baseline evaluation occurs prior to project development. It helps you to set a baseline (e.g. of scientific understanding, people’s attitudes etc.) from which to measure change. Formative evaluation is carried out as you develop the project and whilst it is running. It will help strengthen and improve your product (in this case your website, survey protocol, supporting materials etc.). It involves assessing the effectiveness of the delivery of the project, and should inform changes that improve your project’s effectiveness. Summative evaluation happens at the end of the project or during a natural break, e.g. over the winter for summer surveys. It focuses on the effects and outcomes of the project. Return to the aims you set at the beginning and assess the extent to which each was met. What were the results in terms of e.g. habitat improvement, peer-reviewed papers, or solving an environmental problem? How did the project affect participants’ learning, attitudes and behaviour? 26 Contents Evaluate to maximise lessons learned Hints and tips • You can evaluate in many ways. Build an evaluation strategy appropriate to your project. • Evaluation is best viewed as an ongoing process, not just a final activity. • Invite all stakeholders to assess the project critically, at suitable stages. • Invite and incorporate feedback from participants and your project team. • Share your findings with other project organisers. What worked well, what didn’t, how would you adapt your approach in the future? “What worked well, what didn’t, how would you adapt your approach in the future?”',\n",
              " 'Initiating a citizen science project – choosing partners, methods and participants 1110 Citizen science for all 3 Initiating a citizen science project – choosing partners, methods and participants Who can initiate a citizen science project? Anybody! All that is required is (at least) one person with an idea, a certain interest in research and enough motivation to promote the idea. This indi- vidual or group of individuals may be working as a scientist or be involved in civil society, working alone or as part of a larger team. The only prerequisite is that the idea has some scientific value. The following points must be considered when planning a large project: • Roles and responsibilities: Who should participate in the project and how? What part do participants play and who is respon- sible for what activities? Having clearly defined roles makes the project transparent. • Clearly defined goals: What do you want to achieve with this project? It is important to define clear and concrete goals together with all participants at the beginning of the project. Having clear goals makes evaluation easier. • Forms of participation: How many people should be involved, and how can they contribute to the project? What kind of commitment is required? Is equipment or training needed for initiators or participants? • Clear research question: Careful consideration on the research objective in early stages prevents collection of unnecessary or unusable data. • Legal requirements: What legal requirements for data pro- tection, communication and the involvement of individuals or groups of individuals should be considered? Potential legal issues vary by jurisdiction and need to be clarified with a legal expert. • Choice of methods: How will the data be collected, evaluated and published? • Evaluation: What types of objectives should be reached and how should they be measured? Story 12 Citizen science for all Data: Important issues for citizen science data 13 4 Data: Important issues for citizen science data Citizen science projects often deal with large datasets, whether they are monitoring projects (Butterfly Monitoring Germany, p. 15), crowdsourcing projects (Artigo, p. 36) or observational studies (Landscape Change, p. 32). It is important to decide befo- re the start of a project which data can be collected, who should have which rights relating to the data and how they can be secu- red and made available in the long-term. Data management must be transparent at all times and comply with legal requirements. Data should be stored and managed in permanent infrastructu- res with availability and clarity in mind. This includes metadata (such as time, method or location of collected data). Sometimes, it is impossible to further use data without such information. Legal framework The following legal categories are important in connection with citizen science: copyright (especially for images/photos, text, video and audio) including the so-called ‘freedom of panorama’ related to property lines, sui generis database rights, freedom of information, federal and state legislation on data protection with provisions for personal data (in particular the right of informatio- nal self-determination), legislation relating to inspection of records (in particular passing on citizen science data to institutions with a statutory obligation to publish information), breach of the duty of care (e.g. in forum contributions) as well as the application of telecommunications and media legislation. Depending on the field of research, additional legislation may apply, (e.g. the Environmen- tal Information Act or Nature Conservation Act). In addition, ethical questions such as the collection of participants’ health-related data, must be considered and, where applicable, discussed in an ethics committee. Data quality Data quality includes validity in terms of how data reflect reality and whether they provide an answer to the research question [14]. A solid research design, based on a clearly formulated re- search question, is crucial. In practice, however, not every citizen science project begins with a clearly defined research question or is focused on data quality. Other objectives may be more impor- tant, such as scientific education or empowerment of participants. tip: Methods The choice of methods depends on the discipline and the research topic, as in all scientific research. A monitoring pro- ject, for example (e.g. Butterfly Monitoring Germany, p. 15) requires a standardised design for data collection and data storage, whereas evaluation algorithms and data infrastruc- tures are the main concern in digital crowdsourcing projects (e.g. Artigo, p. 36) . In any case, it is important to clearly describe the methods used. Story www.beachexplorer.org Further resources • Further guidelines for initiating citizen science projects can be found at: www.citizen-science.at/citizen-science/ wegweiser (GER) • The Centre for Ecology & Hydrology offers a best practice guide: www.ceh.ac.uk/citizen-science-best-practice-guide • The Dialogic Change Model is a good resource for ensuring effective collaboration between diverse stakeholders: www.stakeholderdialogues.net www.beachexplorer.org http://www.citizen-science.at/citizen-science/wegweiser http://www.citizen-science.at/citizen-science/wegweiser www.ceh.ac.uk/citizen-science-best-practice-guide www.stakeholderdialogues.net 14 Citizen science for all Data: Important issues for citizen science data 15 tip Data quality and protection are extremely complex issues. More information can be found on the Citizen Science Wiki: wiki.buergerschaffenwissen.de/w/Kategorie: Datensammlung_und_-verarbeitung (GER) In order to ensure scientific benefits, data quality and data pro- tection issues must be addressed right from the start. There are several ways to guarantee that good data are pro- duced, included volunteer training, distribution of guidelines or manuals and development of other teaching materials. It may be critical to engage a sufficiently large number of data collectors in order to ensure that the data have sufficient resolution, both temporally and spatially. It may be beneficial to design data input protocols or data collection software that restricts data inputs (e.g. date, yes/no, numbers and pre-designed drop-down menus) in order to reduce excessive free text and resulting errors. After data are entered, implementation of a consistency check can alert collectors of implausible or possibly faulty data and further improve data quality. Communication and feedback are important in the data revision process. Such downstream quality assurance is often carried out by experts, but mutual checks can also be carried out by all participants or automated programmes (e.g. to check for statistical outliers) [12]. Data availability and accessibility Digital storage of data in databases is usually required in order to further use the data. Long-term data protection and storage, however, is a challenge, as storage media are constantly chan- ging. It can be beneficial to connect larger databases to the IT infrastructure of organisations that have repositories to store and manage data for long periods of time. An overview of such reposi- tories can be found at www.re3data.org. However, availability does not necessarily guarantee that the data are usable. They must also be accessible, which means well documented and easy to interpret. Here, the use of recognised metadata standards can ensure that data with diverse structures and formats can be described in a way that ensures their long- term accessibility – and usability. Story www.tagfalter-monitoring.de http://wiki.buergerschaffenwissen.de/w/Kategorie:Datensammlung_und_-verarbeitung http://wiki.buergerschaffenwissen.de/w/Kategorie:Datensammlung_und_-verarbeitung http://www.re3data.org http://www.tagfalter-monitoring.de Communication and feedback 1716 Citizen science for all 5 Communication and feedback In coordinating collaborative work between different actors, it is critical to communicate well, both within the project and with the outside world. Communication can therefore take more time in citizen science than in other research projects. It is worth it to spend some thought on internal and external communication. Here, we summarise internal and external communication to- gether, as they require similar considerations. A communication strategy can be helpful and address the following points: • Who are the main participants in the project? Who else is invol- ved? Who communicates with whom? Are there specific coor- dinators for different working groups or tasks? Do researchers (both professional and volunteer) work independently or closely together? • Who has the communication skills and resources to repre- sent the project to the outside world? Can the team mobilise the support of professional science communicators (e.g. an organisation’s press office or a journalist)? • How is communication to take place? What channels are to be used, such as e-mail, workshops or regular meetings? Are there certain forms of communication or a special type of language, e.g. where teenagers or people with less formal education are involved? Are there best practice guides in the research area that could help to explain the topic to a wider audience? • What information needs to be communicated and how often? Weekly newsletters for participants – or better monthly? Is there a helpdesk for participants to contact with questions? Does the project need a dedicated website and what functions and infor- mation should it have? Can social media reach selected target groups and how can people without Internet access be reached (e.g. through print media, radio and TV)? • When and why should communication take place with which target groups? Is communication necessary to gain supporters or to raise funds? What exactly should be communicated and what should be left out? How much needs to be invested in terms of time and personnel? This is often underestimated in citizen science projects. Further resources • A German report on data issues in biodiversity citizen science: Wahl, J., Wiebe, A., Grescho, Krämer, R., Schwarz, J. & Wede- kind, S. (2016). Lebendiger Atlas – Natur Deutschland: Workshop Dateninfrastruktur, Datenmanagement und Datenrecht am 10./11. März 2016 in Göttingen. Helmholtz Centre for Environmental Research (UFZ) and German Centre for integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Leipzig. Available online at: www.ufz.de/lebendiger-atlas/ (GER) • Results of a survey in Europe on the use of data by volunteers: bookshop.europa.eu/en/survey-report-pbLBNA27920/ • Information on the open licensing model Creative Commons: www.creativecommons.org • More information on data issues in citizen science: Richter, A., Mahla, A., Tochtermann, K., Scholz, W., Zedlitz, J., Wurbs, A., Vohland, K. & Bonn, A. (2015). GEWISS Dialogforum: Datenqualität, Datenmanagement und rechtliche Aspekte in Citizen Science. Bericht Nr. 6. Helmholtz Centre for Environmental Re- search (UFZ), Leipzig; German Centre for integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Berlin-Brandenburg Institute of Advanced Biodiversity Research (BBIB), Museum für Natur- kunde (MfN) – Leibniz Institute for Evolution and Biodiversity Science, Berlin. Available online at www.buergerschaffen wissen.de (GER) www.ufz.de/lebendiger-atlas/ http://bookshop.europa.eu/en/survey-report-pbLBNA27920/ www.creativecommons.org http://www.buergerschaffenwissen.de http://www.buergerschaffenwissen.de 18 Citizen science for all Communication and feedback 19 tip Generally, a project website is a first port of call for newco- mers to a project. This means it should be clearly structured and easy to navigate. Answering the following questions can give you a head start when building your website: What is the project about? What people and institutions are involved? How can citizens participate? What will happen to the data collected? What are the benefits for participating? Who is the contact person? Further resources • Fundamentals of science communication: www.aaas.org/page/communication-fundamentals-0 • Guidelines on good public relations (PR) in science: www.wissenschaft-im-dialog.de/trends-themen/blog artikel/beitrag/finale-version-der-leitlinien-zur-guten- wissenschafts-pr-veroeffentlicht/ • How to carry out a usability test: www2.bui.haw-hamburg.de/pers/ursula.schulz/ webusability/quicktest.html • Information on storytelling methods: Pettibone, L., Grimm, M., und Ziegler, D. (2016): Storytelling für Citizen Science: Tipps zur erfolgreichen Konzeption und Durchführung eines Storytelling- Workshops. GEWISS-Trainingsbericht Nr. 1. Helmholtz Centre for Environmental Research (UFZ), Leipzig; German Centre for integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Berlin-Brandenburg Institute of Advanced Biodiversity Research (BBIB), Museum für Naturkunde (MfN) – Leibniz Institute for Evolution and Biodiversity Science, Berlin. Available online at www.buergerschaffenwissen.de (GER) tip Online tools have great potential for communication. A blog can be useful in keeping participants informed on the current stage of research. Wikis, etherpads and similar tools as well as instant messenger services (with appropriate data protec- tion) can aid collaborative work. Offline tools can supplement digital approaches and reach people who do not have access to online media. Citizens who freely commit their time and talents, whether to civil society or citizen science, rightly expect their efforts to be recog- nised. This includes giving feedback, which can take many forms. Existing projects can inspire or advise newer projects. Successful citizen science project communication means treating citizens, the media and other multipliers as equals. Here are some examp- les of how to promote good feedback: • Collective publication of results (open access and in scientific journals) • Links to national and international citizen science networks • Naming participants in acknowledgements or as co-authors • Sending newsletters with scientific results • Organising events (talks, educational programmes, parties) • Communication training for participants to share results It is crucial in all communication efforts in a citizen science project to make it clear how the content of the project relates to citizens. What are they interested in, what makes them tick? Narratives (storytelling) may be helpful in communicating scientific content in a simple, but not simplistic, manner. Sometimes such tools have become all but indispensable in citizen science. Press interviews and media partnerships can also be helpful in spreading news about the project and related activities. A good media partnership should find suitable interview candidates, pre- pare information about the project and provide suitable images. It is also important to identified a contact person for the press. Who can be press officer for what issues and who has a good media presence? www.aaas.org/page/communication-fundamentals-0 http://www.wissenschaft-im-dialog.de/trends-themen/blogartikel/beitrag/finale-version-der-leitlinien-zur-guten-wissenschafts-pr-veroeffentlicht/ http://www.wissenschaft-im-dialog.de/trends-themen/blogartikel/beitrag/finale-version-der-leitlinien-zur-guten-wissenschafts-pr-veroeffentlicht/ http://www.wissenschaft-im-dialog.de/trends-themen/blogartikel/beitrag/finale-version-der-leitlinien-zur-guten-wissenschafts-pr-veroeffentlicht/ http://www2.bui.haw-hamburg.de/pers/ursula.schulz/webusability/quicktest.html http://www2.bui.haw-hamburg.de/pers/ursula.schulz/webusability/quicktest.html www.buergerschaffenwissen.de 20 Citizen science for all Evaluating citizen science projects 21 6 Evaluating citizen science projects What makes a citizen science project a success? This sort of ques- tion is often asked as part of an evaluation process. It is important for both funding institutions [21] and organisers of citizen science projects [19] that projects meet certain quality criteria. Still, evaluating citizen science projects poses its own chal- lenges. Participants may have different objectives from project organisers or funders, which must be considered in the evaluati- on process. The types of goals pursued in existing citizen science projects often include: • the project’s scientific output • how citizens are engaged in the research process • educational outcomes, e.g. scientific literacy or environmental education • increased awareness of socially relevant issues In addition, citizen science projects must often fulfill additional requirements, such as: • project transparency, including communicating various tasks, functions and roles in the project and the use of results • data quality, security and privacy measures • long-term outlook, sustainability of project outcomes and data management Story www.rbb-online.de/fuechse http://www.rbb-online.de/fuechse 22 Citizen science for all Funding instruments 23 7 Funding instruments There are various options for funding a citizen science project. These include tailor-made citizen science funding programmes, such as the recently released call for bids Richtlinie zur Förderung von bürgerwissenschaftlichen Vorhaben (Citizen Science) by the German Federal Ministry of Education and Research (BMBF), and hybrid funding combining complementary elements from a wide range of programmes. In addition to project funding, individuals can be supported through measures such as training, taking part in workshops and being given access to infrastructure. Citizen science may also be funded in the context of other science policy initiatives, such as Responsible Research and Innovation (RRI). BMBF’s budget for citizen involvement and the Preservation Nation initiative by the National Trust are other possibilities here. Here, funding decisions are made based on the project’s or initiative’s creativity and ability to foster innovation and creativity, regardless of issue area. However, before searching databases for funding calls and relevant foundations, it is important to determine which parts of the project need funding. Is more staff needed to develop and manage the project, to collect data or for communication purpo- ses? Is training required? Is more space needed? Does project inf- rastructure need additional resources, such as computers and lab equipment? A particular funding source may only covers specific aspects of a project’s needs. Funding options for citizen science projects Existing funding for citizen science projects in Germany and internationally is very divers – there is a range of funding bodies. At the European level, numerous Horizon 2020 calls for project proposals that involve citizens directly or, more indirectly, help to develop methodology or social transformation. The German Federal Environmental Foundation (DBU) recently published new funding guidelines relating to citizen science projects. Other foundations support citizen science activities in specialised areas of research. Crowdfunding is also an option (e.g. through Science Starter). Citizen science can also be funded through membership fees (e.g. Casualty Lists in World War I, p. 43). Other projected were kick-started by university funding (e.g. KLEKs). In many cases, federal and state governments, BMBF, the German Research In the end, participants must decide how they want to attain which objective levels. It is particularly important to communica- te these objectives within and outside the project organisation. Typical steps in an evaluation process involve: • Defining project objectives: This is best done at the beginning of the project and should important stakeholders and partici- pants. • Planning the evaluation: This include means clarifying the eva- luation team, duration, depth, available resources, methodolo- gy, schedule and procedure (e.g. ex post or iterative evaluati- on). It is often helpful to evaluate a project according to criteria defined at the start of the project. • Conducting the evaluation and analysing results • Implementing results, potentially by adapting the project • Further resources • Evaluation criteria for Austrian citizen science projects: Kieslinger, B., Schäfer, T., & Fabian, C. (2015). Kriterien katalog zur Bewertung von Citizen Science Projekten und Projekt anträgen. Im Auftrag des BMWFW. Available online at: www.zsi.at/object/publication/3864/attach/Kieslinger_ Schaefer_Fabian_CS_Kriterien_2015.pdf (GER) • General thoughts on citizen science evaluation: Ziegler, D., Brandt, M., & Vohland, K. (2015). Workshop: (Weiter)Entwicklung von Kriterien und Indikatoren für Citizen Science in der For- schung. In: Pettibone, L., Ziegler, D., Richter, A., Hecker, S., Bonn, A. & Vohland, K., Hrsg. GEWISS Dialogforum: Forschungsförde- rung für Citizen Science. GEWISS Bericht Nr. 7. Helmholtz Centre for Environmental Research (UFZ), Leipzig; German Centre for integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Berlin-Brandenburg Institute of Advanced Biodiversity Research (BBIB), Museum für Naturkunde (MfN) – Leibniz Institute for Evolution and Biodiversity Science, Berlin. pp. 7–10. Available online at www.buergerschaffenwissen.de (GER) http://www.zsi.at/object/publication/3864/attach/Kieslinger_Schaefer_Fabian_CS_Kriterien_2015.pdf http://www.zsi.at/object/publication/3864/attach/Kieslinger_Schaefer_Fabian_CS_Kriterien_2015.pdf http://www.buergerschaffenwissen.de 24 Citizen science for all Funding instruments 25 Story http://go.wwu.de/3you6 Foundation (DFG) or the Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) can provide funding. Funding through lottery money, businesses and charities have not yet played a major role in the German-speaking world, with some exceptions: the Swiss project Flora at the Can- ton Zürich, FLoZ, is funded by lottery money (www.floz.zbg.ch). Cities and townships are additional potential funding sources. How existing initiatives support citizen science Citizen Science is increasingly seen as an important approach to knowledge transfer. By involving citizens in ways beyond traditi- onal forms of science communication, it encourages innovation. Thus, citizen science is now mentioned within the framework of BMBF funding for sustainability research (FONA) and by the Austria’s research ministry’s (BMWFW) TOP Citizen Science pro- gramme. • Further resources • Current areas receiving funding from the Deutsche Bundesstiftung Umwelt (DBU): www.dbu.de/index. php?menuecms=2505 • FONA, BMBF’s programme for sustainability research: www.fona.de • Horizon 2020, the EU Framework Programme for Research and Innovation: ec.europa.eu/programmes/horizon2020/ • Science Starter, a crowdfunding platform for scientific projects: www.sciencestarter.de • TOP Citizen Science, run by the Austrian BMWFW, which also funds citizen science through Sparkling Science and FWF (Fonds zur Förderung der wissenschaftlichen Forschung) programmes www.fwf.ac.at/de/forschungsfoerderung/fwf-programme/ foerderinitiative-top-citizen-science/ (GER) http://go.wwu.de/3you6 http://www.floz.zbg.ch http://www.dbu.de/index.php?menuecms=2505 http://www.dbu.de/index.php?menuecms=2505 http://www.fona.de ec.europa.eu/programmes/horizon2020/ http://www.sciencestarter.de http://www.fwf.ac.at/de/forschungsfoerderung/fwf-programme/foerderinitiative-top-citizen-science/ http://www.fwf.ac.at/de/forschungsfoerderung/fwf-programme/foerderinitiative-top-citizen-science/ 26 Citizen science for all How to plan a citizen science project – from start to finish! 27 Checklist A citizen science project requires resources and especially time. It makes sense to think about the whole project, from concept development, to identification of partners right through to publishing the results, early in the process. Diagram modified from [15]. How can participants be motivated? What are the benefits for participants? What resources are required? How will the project be evaluated? Do participants need training and if so, how? Is there a clear research question? What is the advantage of using a citizen science approach? Are the necessary skills and interests represented? Who is responsible for communicating with participants? What criteria must be fulfilled so that the project can be considered a success? How long should the project take? What infrastructures are required? Where and how will data be (permanently) stored? What licences will be used for data/ photos/reports? How will the results be published and what is the target audience? How can participants’ role be made visible? It er at iv e ad ap ta ti o n o f p la n Define the research question based on a hypothesis or social problem Before beginning Define the benefits of using citizen science First steps Develop concrete project goals Establish the project team Planning Phase Determine the research design Determine methods Who should participate? Calculate resource requirements Develop a commu- nication strategy Develop data protocols Develop training material Data collection Collect, visualise and analyse of data Give feedback to participants Communication and discussion Publish and present results Evaluate the quality of the scientific results Evaluation Evaluate the process and benefits for all Test and adapt methods 8 How to plan a citizen science project – from start to finish! Project task',\n",
              " '1 INTRODUCTION What is Citizen Science? Citizen science can be defined as a grassroots initiative in which ordinary citizens, sometimes in collaboration with professional scientists, organizations and government agencies, collect, generate, and distribute information either for educational purposes or to address community-centered environmental issues. More simply, it is community-driven science: science engaged in, by, and for the non-scientist populace. There are multiple ways that individuals can get involved in citizen science projects, and these projects can take on a variety of configurations. For example, individuals may choose to find and collaborate on pre-existing projects rather than start their own. Existing projects are often offered by professional citizen science organizations, neighborhood organizations, environmental agencies, and local park and wildlife services. Most existing projects have a specific, and often unique, focus that is set by the organization or agency conducting the project. For instance, a project may be designed to assist with the collection or generation of information needed to support the work of a decision-maker or advocate or to motivate individuals to engage with nature and science. Purpose of this Manual: This manual aims to empower individuals in their roles as citizen scientists and to promote the practice of community-based citizen science as a vehicle for environmental justice. It is our hope that this manual will increase your awareness of how to identify and contribute to existing projects or to initiate and effectively prove your own project. To that end, this manual outlines practical suggestions for how to design and carry out a citizen science project. It also contains an overview of relevant laws and regulations, as well as technical suggestions regarding data collection, analysis, and compliance with relevant scientific and quality standards. Citizen science is community-driven science: science engaged in, by, and for the non- scientist populace. The EPA has defined environmental justice as “the fair treatment and meaningful involvement of all people . . . with respect to the development, implementation, and enforcement of environmental laws, regulations, and policies.” 2 Alternatively, individuals may design and initiate their own project, either for similar goals or with an eye toward regulatory or private enforcement of environmental laws. Individuals may start by identifying an issue in their communities (e.g., groundwater pollution, lead contamination, high asthma rates), and then develop a plan to collect and analyze samples near potential sources of the problem. They might then use these results to educate community members and decision- makers, including by submitting the results of their work to a regulatory agency (e.g., the local board of health or the state or federal Environmental Protection Agency (“EPA”)) to petition the agency to take action necessary to protect the community (e.g., enforcement against a polluter). In short, citizen science projects are and can be organized for many different purposes and with many opportunities for varying levels of involvement. Recognizing the many forms citizen science projects may take, this manual generally focuses on those projects designed to remediate environmental problems that threaten community health and wellbeing. Example of Citizen Scientists in Action: In 2004, residents of Tonawanda, New York, home to some of the state’s largest industrial manufacturing facilities, noticed a marked decrease in local air quality and an increase in chronic health problems and banded together to form the Clean Air Coalition of Western New York. They collected local air samples using simple air sensors readily available online, and their analysis of these samples revealed the presence of high levels of benzene, a known carcinogen, in the town’s air. The residents then presented this information to New York’s Department of Environmental Conservation, which worked with the federal Environmental Protection Agency to perform further air quality tests. Once the state and federal agencies became involved, the local manufacturing facilities tightened operating procedures, ultimately decreasing benzene levels in the air by 86 percent. Many successful citizen science projects tend to follow the process demonstrated by this example. A community of citizens comes together through grassroots organizing to identify and solve a problem through the collection or generation of information. They then leverage this information to gain traction with the relevant enforcement agencies and put pressure on the polluting parties to reform. 3 Technical and Legal Limitations of this Manual This manual describes the legal and technical framework governing citizen science and offers practical suggestions. These suggestions are general and not specific to your locale. Nor are these suggestions comprehensive. It is important that you check the current rules in the specific jurisdiction in which you will carry out or are currently carrying out your project. This manual provides references to resources for those seeking more information. However, these resources are non-exhaustive and are subject to change. Concerning legal suggestions: Many of the laws referred to in this manual are administered and regulated at the state and local levels, with potentially significant differences across jurisdictions. This manual does not attempt to compile and detail every state statute, local ordinance, or agency regulation that may be relevant to a citizen scientist’s efforts. Instead, the manual is intended to give a broad overview of the relevant laws by distilling governing principles and common statutory elements across jurisdictions. Having canvassed these laws generally, the manual identifies types of laws that restrict citizen science – meaning laws that could result in a citizen scientist facing either criminal or civil liability for actions (such as trespass) not conducted in compliance with such law. It is important that you seek to educate yourself about statutes, regulations, and ordinances specific to your own jurisdiction before setting off into the field to engage in sample collection. The tools available in this guide will assist you in doing so. Concerning technical suggestions: The problems addressed by citizen science projects are diverse.1 This manual is primarily focused on citizen science projects that are directed at environmental pollution concerns, and in particular, pollution of air, water, and soil. However, many of the suggestions in this manual are highly generalizable. If your 1 See, e.g., ANNE BOWSER & LEA SHANLEY, NEW VISIONS IN CITIZEN SCIENCE (Woodrow Wilson International Center for Scholars, 2013), https://www.wilsoncenter.org/sites/default/files/NewVisionsInCitizenScience.pdf. It is important that you check the rules in the specific jurisdiction in which you carry out or are currently carrying out your project. It is important that you seek to educate yourself about statutes, regulations, and ordinances specific to your own jurisdiction before setting off into the field to engage in sample collection. This manual is primarily focused on citizen science projects that are directed at pollution concerns, and in particular, the environmental pollution of air, water, and soil. https://www.wilsoncenter.org/sites/default/files/NewVisionsInCitizenScience.pdf 4 project lies outside the focus of the manual, we recommend that you use the chapter headings and introductions to rapidly assess whether the content of the chapter will be relevant to your particular project. Manual Overview This manual is divided into seven major chapters. The needs of individual citizen scientists can differ greatly, and therefore, there are various ways in which the content of this manual might be presented. We have chosen to structure the manual to reflect the sequence of steps that one might follow when initiating a new citizen science project. But, we emphasize that no two projects will follow the exact same path from beginning to end. The following graphic provides a visual representation of how the different chapters relate. This graphic highlights: (i) that there are many paths that can be taken from the beginning of a project (“Identify Project Focus”) to completion of that project (“Goal: Information Use”); (ii) that the chapters of this manual are highly interrelated and need not be thought of as separate steps; and (iii) that many times citizen science projects are iterative: they may involve some cycling back to previous steps as new information is uncovered or if circumstances change. Chapter 1, “Identifying Your Project’s Focus and Designing Its Approach,” describes the initial steps of a citizen science project. This includes guidance on how the focus of your project, or the central environmental issue to which it is directed, should influence your project’s approach. Graphic Legend: Each chapter of this manual relates to one or more of the major categories outlined in this graphic. Areas of the graphic will be expanded in each chapter to highlight information that may be of use to you as you carry out your project. The manual is organized to reflect the sequence of steps one might follow when initiating a new citizen science project. No two projects will follow the exact same path from beginning to end. 5 Chapter 2, “Identifying Your Project’s Goals – Evaluating Potential Information Uses,” assists you in brainstorming the potential goals of your efforts before engaging in information collection or field research. For example: Do you intend to give your data to a regulatory agency for use in an enforcement action? Does that agency have the resources and political will to pursue such an enforcement action? Are there other uses for your data that do not involve an agency enforcement action (e.g., community organizing, media attention)? Your answers to these questions can shape the scope and direction for your project. Chapter 3, “Information Collection: Gathering Publicly Available Information,” assists you in identifying what is already known about the problem with which you are concerned. Specifically, it provides guidance on how to acquire publicly available information with respect to pollutants and pollutant sources. After reading this chapter, you should know how to efficiently gather publicly available information and to determine whether or not it is sufficient to resolve the problem you have identified. Chapter 4, “Information Generation: Potential Liability,” reviews potential legal limitations on information generation by citizen scientists as well as positive rights and privileges you can take advantage of to design the most effective project possible. Think of this as a primer on which laws might be most relevant to citizen science. While we anticipate that most readers will not encounter legal complications in conducting their projects, we nonetheless want to arm you with the knowledge and resources to carry out your project without fear of adverse consequences. To that end, this chapter summarizes a wide range of legal issues like trespass, drone use, and privacy rights. The analysis surveys the laws of all 50 states as well as Puerto Rico, highlighting similarities and differences across jurisdictions. This chapter should be read in conjunction with the material in Appendices 1 and 2 of this manual, which compile specific state statutes and resources. Ultimately, this chapter will help you begin to develop a sense of which actions you can take and which you should avoid, allowing you to plan your project more effectively. Appendices include: 1. High-level comparisons of state laws 2. Individual State Law Summaries 3. Pollutants Monitored by the EPA 4. Publicly available Data and Permits 5. EPA Reference Methods, Standards and Protocols 6 Chapter 5, “Information Generation: Design of Sample Collection, Sample Analysis, and Data Interpretation Methodologies,” highlights ways of increasing the quality of new information that you generate from any field work that your project may involve. Importantly, increasing the quality of the information you generate promotes its utility or usefulness. This chapter also stresses the value of making this process a community endeavor. For example, look for experts in your community who can help you overcome any technical hurdles you may encounter. Finally, Chapter 6, “Information Use: Making the Most Out of Your Information,” provides a few examples of ways in which you can increase the value of the work that you have performed. Use of This Manual Citizen scientists have diverse needs that depend on the nature and status of the projects in which they are involved. As such, we anticipate that readers will differ in how they will use this manual. Some may read the manual from cover to cover; others will seek out specific topics. While most of the examples and discussion provided in each chapter of this manual are geared toward helping citizen scientists begin and complete their own projects, the suggestions are applicable to all citizen science projects that are directed at air, water, and soil pollution concerns. Thus, whether you are interested in finding and getting involved in an existing project or are already involved in an ongoing project, this manual can still be a valuable resource to you. Below are examples of how readers may use this manual: • Individuals interested in initiating a citizen science project: because the manual is structured to reflect the sequence of steps that one might follow when initiating a new citizen science project, these readers may benefit from reading the manual from cover to cover. • Volunteers who are seeking to join an ongoing citizen science project: because Chapter 1, “Identifying Your Project’s Focus and Designing Its Approach,” includes a section with resources for those interested in joining an ongoing project, people looking for a project to join may benefit from starting with this chapter. After joining a project, these readers can explore the chapters of the manual that are most relevant to their specific project roles. There is not a “correct” way to use this manual. Depending on your project’s needs and status, and your type of involvement, you may choose to read the text in full, focus on the chapters that you anticipate will be most relevant, or dig deeper into the references cited in the text or into the appendices. 7 • Organizers, Project Managers and Volunteers who are currently engaged in a citizen science project: for these readers, the manual’s most useful content will likely relate to the project roles in which they are involved (e.g., project design, collecting samples, analyzing available data, interpreting results, preparing forms, disseminating a project’s results, etc.). These readers may refer to the table of contents and to the chapter headings and introductions to identify sections of the manual containing content that addresses their current project needs. This manual is designed to be useful for readers with a broad range of technical and legal backgrounds. Those who are just starting to learn about these topics may find it most useful to focus on the complete text of the chapters. Readers who are more familiar with the issues, and those who possess a technical or legal background may prefer to spend more time investigating the references cited in the text and appendices. 8 Problem Solving as You Read: Some readers may not have a specific problem in mind as they review the contents of this manual. Because reading the manual with a specific problem in mind may help highlight the relevance and application of the topics discussed, the following are hypothetical scenarios that you could consider when reading the manual: First scenario: Imagine that you have just retired and moved to Wyoming for the clean air and fresh water. You bought a home on a hill overlooking and within a short distance of a river. You are hankering for something to do in retirement and decide to become an observer of nature and the environment. You soon learn that there are a couple of ranches near the area in which you have settled. How would you initiate a project to monitor any potential pollution of the river associated with ranching activities? Second scenario: Imagine that you live in a small Pennsylvania community. Many individuals in your community are suffering from headaches and skin rashes, and they are complaining that their tap and well water is discolored with a bad odor. With a little investigation, you discover that some members of the community have recently leased their land to a gas company but cannot discuss the situation because of confidentiality provisions in their leases; others have not leased their land or given the gas company any rights to access or use their property. How would you design a project to determine whether there are pollutants in the water that are causing health impacts? Suppose that the successful completion of your project will require the comparison of water pollution levels that existed prior to the arrival of the gas company (i.e., baseline pollution levels) with levels after its arrival? Third scenario: Imagine that you live in North Dakota and that you are worried that a recently constructed pipeline will leak oil into a lake that is the source of many important resources for the residents in the area, not the least of which is drinking water. How would you initiate a project that will allow you to detect a leak in the pipeline? 9 CHAPTER 1: IDENTIFYING YOUR PROJECT’S FOCUS AND DESIGNING ITS APPROACH Identifying the Focus of a Project of Interest Before beginning a new citizen science project, you should identify the project focus, which is the environmental question, theme, and/or problem at issue. Some who are reading this manual may already have a project focus in mind; others may not. Recognizing the vast breadth of environmental problems that may be of interest to citizen scientists, we do not attempt to list them all here. Instead, we mention a few types of projects and examples of each. Monitoring the condition of an environmental interest – Your project’s focus might relate to protecting an environmental resource or habitat that is currently unthreatened or thought to be Why You Should Read this Chapter: Starting your project in the right way will help assure your overall satisfaction with your project. This chapter provides guidance for those taking these beginning steps. By the end of it, you will know how to identify your project’s focus and how to use that focus to design your project’s approach, which includes (i) the identification of a site (i.e., location) of interest to you (e.g., a river, forest, industrial activity) and (ii) the determination of which pollutant or combination of pollutants will be examined during your project. In addition, this chapter provides resources for those seeking to join an ongoing citizen science project. Graphic Legend: After identifying your project’s focus, or the environmental problem to which you project will be directed, your first step will be designing your project’s approach. This approach should be driven by the project focus that you have identified. 10 unthreatened. By monitoring this resource, your efforts may facilitate the rapid detection of changes in pollution levels. Examples include: • Monitoring water pollution levels in a river or in a national forest. • Monitoring air quality in your community following the construction of a new local pollutant source (e.g., an industrial facility, agricultural facility, land fill, sewage treatment plant, coal mine, etc.) or an announcement that an existing pollutant source in your community will be expanding or increasing its activity levels. • Monitoring water quality in your community because you suspect an increase in pollution resulting from accumulated wear and tear of a known pollutant source near your home. Verifying reported emissions of pollution from a known pollutant source – Your project’s focus might relate to verifying that a known pollutant source is accurately reporting its environmental footprint. For example: • Verifying that a known pollutant source is accurately reporting how much or what it pollutes. • Verifying that a known pollutant source is complying with its current permit obligations. Redressing a known environmental pollution problem – Your project’s focus might relate to correcting a known pollution problem. Examples include: • Identifying the source of an environmental pollutant. • Redressing poor air or water quality. • Decreasing the environmental impact of an oil spill in a national or state forest or in a body of water. Diagnosing a problem that you suspect is caused by pollution – Your project’s focus might relate to solving a problem that has arisen in your community when the cause of the problem is uncertain. You might desire to determine whether the problem’s cause relates to a pollutant present in your community. For example: • Diagnosing unexplainable health problems that individuals, animals, or plants in your community are suffering. 11 Determine Whether Existing Projects Are Already Directed at the Project Focus that You Have Identified The project focus that you are interested in may already be the focus of an ongoing citizen science project. If so, you might consider supporting that project instead of initiating one of your own. Indeed, supporting an existing project can alleviate the burden that some individual citizen scientists may feel in planning and mobilizing their own projects. If your interests align with those of an ongoing project, supporting that project can be ideal for you. There are a variety of resources to help citizens identify ongoing citizen science efforts: • Media Outlets: Local news agencies often cover major ongoing citizen science projects. Moreover, many community-driven citizen science projects increase public awareness through social media. For example, details concerning the citizen science project in Tonawanda, New York were reported in local news. In addition, the project’s task force, the Clean Air Coalition of Western New York, used a Facebook page to advertise public meetings and other ways of getting involved in the project. • Organizational Websites: Various citizen science organizations host websites that consolidate ongoing citizen science projects. Examples include the Citizen Science Alliance (https://www.citizensciencealliance.org/), the government-sponsored https://www.citizenscience.gov, iNaturalist (https://www.inaturalist.org/), SciStarter (https://scistarter.com/finder), and Zooniverse (https://www.zooniverse.org/). • Agency Websites: State and federal environmental agencies also maintain citizen science databases on their websites. The EPA, for example, hosts a robust page dedicated to promoting citizen science involvement at https://www.epa.gov/citizen-science. In addition, many state and local park and wildlife departments host links to ongoing citizen science projects. • Appendices: Appendices 1 and 2 of this manual provide references to various projects that are open to public involvement. Initiating Your Own Project: Designing Your Project’s Approach Many important environmental problems are not addressed by existing citizen science projects. Projects sponsored by government agencies may be limited and constrained by budget https://www.citizensciencealliance.org/ https://www.citizenscience.gov/ https://www.inaturalist.org/ https://scistarter.com/finder https://www.zooniverse.org/ https://www.epa.gov/citizen-science 12 cuts, changes in priorities, and changes in political administrations. Ultimately, you may seek to initiate your own project. The first step in initiating your own citizen science project is designing your project’s approach. A “project approach” has two components: i) the identification of a site of interest to you and ii) the determination of which pollutant or combination of pollutants you will examine. Importantly, the design of your project’s approach should be driven by the project focus that you identified previously (see the first section of this chapter). For example, suppose that your project focus is: • Verifying that a known pollutant source is accurately reporting how much or what it emits to the environment. This project’s site of interest might be the known pollutant source. • Improving the quality of air or water in your community. Here, the project’s site of focus might be your community itself or a known pollutant source located near your community. • Monitoring a natural habitat that you consider valuable (e.g., a river, forest, ocean, etc.). In this instance, the site of interest might be the natural habitat or a known pollutant source located near that habitat. After you have identified your project’s site of interest, you should determine which pollutant or combination of pollutants will be examined during your project. This aspect of your project’s approach is critical because if you spend all of your time examining the wrong pollutant, your project’s goal will not be met. For some projects, determining which pollutant or combination of pollutants to examine will be a straightforward process. In others, this process may be the most difficult aspect of your project’s design. Use what you know about your project’s site of interest to guide you in determining which pollutant or combination of pollutants you will examine during your project (see Chapter 3). For example: • Source Indicators: Pollutant sources are often associated with strong source indicators— meaning that some pollutants are commonly produced by a certain kind of pollutant source. Suppose for instance that your project’s goal is to measure the impact of a newly constructed facility that produces plastics. These facilities are known to emit volatile organic compounds (“VOCs”). Therefore, your project may seek to examine VOC emissions. If you are interested in monitoring water quality in a stream, you could research 13 sources of water pollution flanking the stream to determine which pollutants they discharge and, therefore, which you should examine. • Use Your Senses: Your eyes, ears, and nose can help you figure out which pollutants you should examine (e.g., a distinct smell in the air, the sight of an oil slick on the surface of water, a distinct taste in your drinking water, etc.). Likewise, the health symptoms associated with exposure to a pollutant may prove insightful. For example, the pollutant benzene, which is associated with petroleum products, has a sweet smell and exposure to abnormal levels of benzene in ambient air is associated with a heightened risk of asthma. If you notice a correlation between these two things in your community—for example, a gasoline-like smell and an increase in asthma diagnoses—you might then consider initiating a citizen science project focused on local sources of benzene pollution. • Media Outlets: Local news reports may also provide valuable information. For example, if a local news agency reports that residents of your community have been suffering from exposure to lead, the approach of your project may be determining the lead content of your drinking water. • Smartphone Apps: Some regions may have smartphone applications set up to report pollutants or evidence thereof. For instance, Pittsburghers can use Smell PGH to report air quality on their smartphone; the app can then alert the Allegheny County Health Department to the data.2 Apps such as this may provide useful information as you begin to decide which pollutants require attention in your area. • Government Records & Databases: Government records and databases, especially those created and maintained by the federal and state agencies responsible for regulating the pollutant source in question, may provide valuable information for determining which pollutants are present in your community and a cause for concern. For example, EPA provides access to a number of environmental databases through its Envirofacts website. The agency also makes available online information about specific pollutant sources through its Enforcement and Compliance History Online (“ECHO”) and Toxic Release Inventory (“TRI”) Program websites. 2 Ashley Murray, Carnegie Mellon Scientists Use App to Track Foul Odors in Pittsburgh, PITTSBURGH POST- GAZETTE (July 3, 2017), https://www.post-gazette.com/business/tech-news/2017/07/03/smell-pgh-app-carnegie- mellon-university-cmu-create-lab-foul-smell-pittsburgh/stories/201706300430. https://www3.epa.gov/enviro/ https://echo.epa.gov/ https://www.epa.gov/toxics-release-inventory-tri-program/tri-data-and-tools https://www.epa.gov/toxics-release-inventory-tri-program/tri-data-and-tools https://www.post-gazette.com/business/tech-news/2017/07/03/smell-pgh-app-carnegie-mellon-university-cmu-create-lab-foul-smell-pittsburgh/stories/201706300430 https://www.post-gazette.com/business/tech-news/2017/07/03/smell-pgh-app-carnegie-mellon-university-cmu-create-lab-foul-smell-pittsburgh/stories/201706300430 14 ***** We conclude this chapter by emphasizing that your project’s approach need not be static; it is possible that it will require modification as your project progresses. For example, suppose that the focus of your project is diagnosing the sudden and unexplainable health problems recently afflicting members of your community. Your original project approach may have involved determining the levels of pollutant X in the community’s water supply, but the results of your examination could indicate that the pollutant is absent or within safe levels. In response, you should revisit and modify the design of your project’s approach (e.g., modify it so that you will determine the levels of pollutant Y in the water supply, the levels of pollutant X in the air, or otherwise). 15 CHAPTER 2: IDENTIFYING YOUR PROJECT’S GOAL Why You Should Read this Chapter: If you don’t know where you want to end up, you will never get there. Thus, it is important to identify your project’s goals early. This process involves the evaluation of potential uses of the information that you collect or generate as you carry out your project (i.e., information use). Here, we outline examples of information use and, at the same time, explain the quality standards that can limit the use of information that is collected or generated by citizen scientists. Understanding this information will help assure that your project’s goals are achieved. Graphic Legend: The information that you might collect or generate during your project can be used in a variety of ways, either by you or the government. However, various issues, in particular legal standards, can limit the potential uses of your information. 16 Introduction The use of citizen science-generated information is subject to various legal standards regarding its credibility and reliability, which we refer to generally as “quality standards”.3 These standards serve to establish a level of quality that the information must meet before it can be used in a certain way (for example, in a court proceeding or agency decision). The terms “credible information” or “reliable information” may be used in some places to refer to information of a sufficiently high quality to be used for the desired purpose. Two simple inquiries can help you identify the quality standards that are relevant to your project’s ultimate goals. First, who will use the information? Potentially, you seek to use the information yourself. Alternatively, you may want the government to use the information (e.g., use by a federal, state, or local governmental agency, etc.). Second, how will the identified user ultimately use the information? 3 We emphasize that this chapter is only introductory in nature. Additional background information can be found in Appendices 1 and 2 of this manual and in a report published by the Commons Lab of the Science and Technology Innovation Program. See JAMES MCELFISH, JOHN PANDERGRASS & TALIA FOX, CLEARING THE PATH: CITIZEN SCIENCE AND PUBLIC DECISION MAKING IN THE UNITED STATES (Apr. 2016), https://www.eli.org/research- report/clearing-path-citizen-science-and-public-decision-making-united-states. Making Connections Between Chapters: Chapter 1 was directed at helping you take the first steps of your project. Now that you have established your project’s beginnings, you should take time to consider its possible endings. This involves an examination of the potential uses of the information that might be collected or generated during your project’s progression (i.e., “information use”). This chapter highlights examples of information use. Along the way, we identify legal standards that can limit the use of information that is collected or generated by citizen scientists. Doing so will help reveal the path that you should take to achieve your project’s goals. It may also be useful for you at this point to note that information collection is the topic of Chapter 3, and that information generation is the topic of Chapter 4 and Chapter 5. https://www.eli.org/research-report/clearing-path-citizen-science-and-public-decision-making-united-states https://www.eli.org/research-report/clearing-path-citizen-science-and-public-decision-making-united-states 17 You can use the information that you collect or generate during your project in many ways. Depending on how you want to use the information, it will be subject to different quality standards, which can range from lenient to strict. While the laws and regulations that establish quality standards are too varied to allow a strict differentiation into “lenient” and “strict” categories, we attempt below to indicate where different standards fall along this continuum. It should be stressed at the onset of this discussion that even when use of information is not formally limited by quality standards or when it is limited only by lenient quality standards, the information’s quality still impacts how effective it will be in advancing your goals. Some potential uses of information that you have collected or generated are not subject to legally imposed quality standards. For example, you may use the information to increase knowledge in educational campaigns, to stimulate public awareness, or to foster community engagement. Or you might want to contact your elected representatives to influence the development of new laws. You can provide them with the information that you have collected or generated by phone, email, letter, or otherwise. Although there are no legal rules governing the quality of the data used for these uses, you obviously still want to ensure that it is of as high a quality as possible so that you can make a compelling argument. You might instead want to provide the information to a regulatory agency or use it as evidence in a court case such as a citizen suit against a polluter. In these situations, the use of the information, either by yourself or by a government agency, will be subject to legally-imposed quality standards. You can provide information to regulators in a variety of contexts. First, you can provide an agency with the information that you have collected or generated to influence the development of new regulations. For example, when an agency uses notice and comment rulemaking to propose the adoption of a new regulation, members of the public can submit comments in response to the proposed regulation during an allotted window of time. After closure of this time window, comments are no longer accepted. At the federal level, opportunities for public comment during notice and comment rulemaking are generally published in the Federal Register or can be found Uses by Citizen Scientist No Legally Imposed Quality Standards 1) Education 2) Stimulate Public Awareness 3) Inform Legislators 18 on the agency’s website.4 Each year, the EPA receives millions of comments on its proposed rules, notices, and other actions which are posted on its dockets at regulations.gov.5 If an agency is going to rely on the information you have submitted as a basis for its eventual decision, then the information must satisfy certain quality standards. Federal and state agency decisions are subject to judicial review. For example, the Administrative Procedure Act (“APA”) directs courts that review federal agency actions to “hold unlawful and set aside agency action, findings, and conclusions found to be arbitrary, capricious, an abuse of discretion, or otherwise not in accordance with law” or “unsupported by substantial evidence.”6 Standards in state courts are similar. Although these standards are not particularly burdensome, because courts grant considerable deference to agencies’ scientific expertise, they nevertheless provide a check on the quality of the information that forms the basis for agency decisions. If an agency does not have an ongoing rulemaking proceeding to which your information is relevant and if you believe an agency should issue new or revised rules to address the situation, then petitions for rulemaking provide an additional opportunity for you to use the information that you have collected or generated. Indeed, the APA requires each federal agency to provide “an interested person the right to petition for the issuance, amendment, or repeal of a rule.”7 Federal agencies have implemented different processes for the submission of petitions. The EPA, for example, provides opportunities for the public to submit and view previously submitted petitions on its website.8 Similar opportunities for public engagement to influence the development of new regulations exist at the state level. You might also submit the information to agency in the hope that the agency will use it to bring an administrative or judicial enforcement action against someone who is violating the law. For example, a government may use the information as evidence in a civil lawsuit or a criminal prosecution in a federal or state court. In these instances, the quality standards discussed below 4 For a comprehensive source compiling pending agency actions available for public input, see REGULATIONS.GOV, https://www.regulations.gov/ (last visited Feb. 7, 2019). 5 Additional information can be found on EPA’s website. See EPA Docket Center, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/dockets (last visited Feb. 7, 2019). 6 5 U.S.C. § 706 (emphasis added). 7 5 U.S.C. § 553(e). 8 See Petitions for Rulemaking, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/aboutepa/petitions- rulemaking (last visited Feb. 7, 2019). https://www.regulations.gov/ https://www.epa.gov/dockets https://www.epa.gov/aboutepa/petitions-rulemaking https://www.epa.gov/aboutepa/petitions-rulemaking 19 concerning use of the information in a citizen suit would apply. Alternatively, a state or federal agency may use the information in an administrative adjudication. The hearing officer in an administrative adjudication will follow quality standards that are similar to those in federal and state courts, though generally somewhat more flexible and lenient. For example, at the federal level, the APA indicates that “any oral or documentary evidence may be received, but the agency as a matter of policy shall provide for the exclusion of irrelevant, immaterial, or unduly repetitious evidence.”9 At the state level, the Revised Model State Administrative Procedure Act (“MSAPA”) provides similar guidance;10 not all states, however, have adopted this model statute. Finally, you may use the information that you have collected or generated to stimulate future independent agency action. In these instances, the information serves to call an agency’s attention to the problem. The agency may then independently act to verify the information through its own information generation procedures and may initiate enforcement proceedings.11 Some federal regulations expressly require states to solicit public participation in the collection of information and require state agencies to comment on citizen-generated information. For example, an EPA regulation requires states that implement the Clean Water Act (“CWA”), the Resource Conservation and Recovery Act (“RCRA”), and the Safe Drinking Water Act (“SDWA”) to “provide for, encourage, and assist the participation of the public.”12 With respect to the CWA, EPA regulations require each state that is developing and updating its list of impaired waters to “assemble and evaluate all existing and readily available water quality-related data and information.”13 Moreover, the CWA regulations specify that state agencies should actively solicit the help of members of the public “for research they may be conducting or reporting.”14 EPA regulations also specify that “[e]ach agency administering a permit program shall develop internal procedures for receiving evidence submitted by citizens about permit violations and ensuring that it is properly considered. Public effort in reporting violations shall be encouraged, and the agency shall make available information on reporting procedures. The agency shall investigate alleged 9 5 U.S.C. § 556(d). 10 M.S.A.P.A. § 404. 11 For example, Tonawanda, NY is a success story on this front. 12 40 C.F.R. § 25.3(a). 13 40 C.F.R. § 130.7(b)(5). 14 Id. § 130.7(b)(5)(iii). 20 violations promptly.”15 Some state statutes also require state agencies to actively investigate complaints made by citizens concerning violations of environmental laws (see Appendix 2).16 State and federal laws also provide standards that may limit agency use of some types of information in all kinds of administrative actions. For example, the Information Quality Act (also known as the Data Quality Act) directs the Office of Management and Budget (“OMB”) to adopt guidelines for federal agencies to address the goals of ensuring and maximizing the “quality, objectivity, utility, and integrity of information.”17 Among other ways of promoting these goals, OMB guidelines direct federal agencies to develop a process for reviewing the quality of information before it is disseminated by the agency.18 In a second example, the Endangered Species Act (“ESA”) requires federal agencies to make species listing determinations (e.g., as threatened or endangered) “solely on the basis of the best scientific and commercial data available.”19 At the federal level, EPA’s “Information Quality Guidelines” limit the agency’s uses of “existing data and information generated by third parties to inform its decisions.”20 These guidelines require “the quality and scientific soundness of this type of data to be reviewed and documented prior to use.”21 These quality standards are expounded upon on EPA’s website.22 15 40 C.F.R. § 25.9. 16 See, e.g., N.J. Admin. Code § 7:7A-22.19; N.Y. Envtl. Conservation Law § 19-0503; Utah Admin. Code § R317- 8(1.9); 10 Vt. Stat. Ann. § 8020. 17 Information Quality Act of 2001, Pub. L. No. 106-554, § 515(a), 114 Stat. 2763 (Dec. 21, 2000), https://www.govinfo.gov/content/pkg/PLAW-106publ554/pdf/PLAW-106publ554.pdf. 18 Guidelines for Ensuring and Maximizing the Quality, Objectivity, Utility, and Integrity of Information Disseminated by Federal Agencies, 67 Fed. Reg. 8,452, 8,460 (2002). 19 16 U.S.C. § 1533(b)(1)(A); 50 C.F.R. § 424.11. 20 U.S. ENVIRONMENTAL PROTECTION AGENCY, SCIENTIFIC INTEGRITY POLICY, at 2 n. 2 (2012), https://www.epa.gov/sites/production/files/2014-02/documents/scientific_integrity_policy_2012.pdf [hereinafter, “Scientific Integrity Policy”]; see also U.S. ENVIRONMENTAL PROTECTION AGENCY, GUIDELINES FOR ENSURING AND MAXIMIZING THE QUALITY, OBJECTIVITY, UTILITY, AND INTEGRITY OF INFORMATION DISSEMINATED BY THE ENVIRONMENTAL PROTECTION AGENCY (2002), https://www.epa.gov/sites/production/files/2018-11/documents/epa- info-quality-guidelines_1.pdf. As of February 2019, the EPA continues to refer to these two policy documents. See, e.g., Policy on EPA Scientific Integrity, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/osa/policy-epa- scientific-integrity (last visited Feb. 7, 2019). 21 Scientific Integrity Policy, supra note 20, at 2 n. 2. 22 See How EPA Manages the Quality of its Environmental Data, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/quality (last visited Feb. 7, 2019); see also Quality Specifications for non-EPA Organizations to do business with EPA, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/quality/quality-specifications- non-epa-organizations-do-business-epa (last visited Feb. 7, 2019). https://www.govinfo.gov/content/pkg/PLAW-106publ554/pdf/PLAW-106publ554.pdf https://www.epa.gov/sites/production/files/2014-02/documents/scientific_integrity_policy_2012.pdf https://www.epa.gov/sites/production/files/2018-11/documents/epa-info-quality-guidelines_1.pdf https://www.epa.gov/sites/production/files/2018-11/documents/epa-info-quality-guidelines_1.pdf https://www.epa.gov/osa/policy-epa-scientific-integrity https://www.epa.gov/osa/policy-epa-scientific-integrity https://www.epa.gov/quality https://www.epa.gov/quality/quality-specifications-non-epa-organizations-do-business-epa https://www.epa.gov/quality/quality-specifications-non-epa-organizations-do-business-epa 21 State agency regulations or guidelines function similarly to the EPA’s Information Quality Guidelines. For example, various state agencies have express authority to consider “credible” information in enforcement actions, administrative actions, or both (see Appendix 2). The definition of “credible” varies between states. In some states, information is credible if its collection conforms (i) to accepted scientific practice; (ii) to federally recognized standards; or (iii) to state-specific protocols. Iowa law provides an example of a relatively stringent quality standard imposed to ensure that the information is credible. To submit water data to the Iowa Department of Natural Resources (“IDNR”), citizen scientists must first submit a “volunteer water quality monitoring plan” for IDNR approval. The plan must include a “statement of intent[,]” the names of all participants, the duration of the monitoring effort, the “[l]ocation and frequency of sample collection[,]” the “[m]ethods of data collection and analysis[,]” and “[r]ecord keeping and data reporting procedures.”23 In addition to this, citizen-submitted data must be approved before being considered credible.24 To be approved, data must be submitted by a “qualified volunteer” who must request that it be deemed credible at the time of submission.25 “[Q]ualified volunteers must have the training and experience to ensure quality assurance and quality control for the data being produced, or be under direct supervision of a person having such qualifications.”26 You may want to use the information to bring a lawsuit against a polluter yourself.27 One mechanism for such a lawsuit is a citizen suit under one of the federal environmental laws. Citizen suits are lawsuits that are brought by a private citizen (i) against an individual, corporation, or government body for engaging in conduct prohibited by a statute or (ii) against a government body for failing to perform a duty required by law. Various federal environmental statutes, including the CWA, ESA, RCRA, SDWA, the Clean Air Act (“CAA”), the Comprehensive Environmental Response, Compensation, and Liability Act (“CERCLA”), and the Emergency Planning and Community Right to Know Act (“EPCRA”), allow private citizens to bring lawsuits against violators. 23 Iowa Admin. Code 567-61.11(455B). 24 Iowa Admin. Code 567-61.12(455B). 25 Id. 26 Iowa Admin. Code 567-60.2(455B). 27 For more in-depth explanation on bringing a lawsuit, see Manual Supplement, “Using Citizen Science State in Litigation.” 22 Various quality standards govern citizen lawsuits.28 First, the quality of the information must be sufficient to bring a claim. Federal courts require that an attorney filing a complaint to initiate a lawsuit must certify “that there is (or likely will be) ‘evidentiary support’ for [each] allegation, not that the party will prevail with respect to its contention regarding the fact.”29 Generally, requirements in state courts are comparable (see Appendices 1 and 2). Second, when submitting evidence at trial or in support of a motion for summary judgment, you must authenticate that evidence, which requires, among other things, maintaining records establishing the “chain of custody” of the evidence. To satisfy the requirement of authentication in federal courts, “the proponent must produce evidence sufficient to support a finding that the item is what the proponent claims it is.”30 Generally, requirements in state courts are comparable (see Appendix 2). You should also note that if you are relying on government-generated information or monitoring reports that the permittee submits to the government, then the information is self-authenticating.31 Finally, quality standards specifically serve to limit the introduction of “scientific” evidence in trial. It should be noted that some information that you may collect or generate will not be considered scientific (e.g., a picture of an industrial facility that is discharging a pollutant into surface water). In these instances, layperson testimony is sufficient to introduce the information. However, if the information is deemed scientific (e.g., information generated via an interpretation of a data output from a technical instrument), it must be introduced through expert testimony and is subject to stricter quality requirements. This is because scientific evidence is believed to carry greater weight in the minds of jurors than evidence deemed non-scientific. In federal courts, judges use an approach known as the Daubert standard to make a preliminary assessment of the quality of the information. In doing so, federal judges consider whether: “(a) the expert’s scientific, technical, or other specialized knowledge will help the trier of fact to understand the evidence or to determine a fact in issue; (b) the testimony is based on sufficient facts or data; (c) the testimony is the product of 28 There are a variety of requirements that you must satisfy to successfully bring a citizen suit (e.g., sending a notice letter in advance, establishing that the plaintiff has standing to sue, etc.). Here, our primary topic of interest relates only to the quality of the evidence you will use to support a citizen suit. 29 Fed. R. Civ. P. 11, 1993 Amendment Advisory Committee Notes. 30 Fed. R. Evid. 901(a). 31 Fed. R. Evid. 902(4). 23 reliable principles and methods; and (d) the expert has reliably applied the principles and methods to the facts of the case.”32 While judges in many state courts also use the Daubert standard when assessing the quality of scientific evidence, others use different standards, although these are generally similar (see Appendix 2). Importantly, under each standard, the method by which data is collected and interpreted impacts whether the information will be allowed in a trial. A final point is applicable to multiple uses of the property, but only in certain states. Several states explicitly forbid the use of certain illegally-collected information in court or in administrative decision-making (see Appendix 2). Of these, Wyoming most directly implicates citizen science: information collected in violation of the state’s data trespass law is not “admissible in any civil, criminal, or administrative proceeding.”33 Moreover, any information fitting this description that is “in the possession of any government entity . . . shall be expunged from all files and databases, and shall not be considered in determining any agency action.”34 Several other states forbid the use of information illegally collected by drones under some circumstances (see Appendix 2).35 32 Fed. R. Evid. 702. 33 Wyo. Stat. Ann. § 6-3-414(f). 34 Id. § 6-3-414(g). 35 At the time of writing, the states with these laws were: Alaska, Alaska Stat. § 18.65.903(a); Florida, Fla. Stat. § 934.50(6); Iowa, Iowa Code § 808.15; Kentucky, Ky. Rev. Stat. § 500.130(8); Montana, Mont. Code Ann. § 46-5- 109(1); Nevada, Nev. Rev. Stat. Ann. § 493.112(4); North Carolina, N.C. Gen. Stat. Ann. § 15A-300.1(f); Tennessee, Tenn. Code Ann. § 39-13-905(a)(1); Utah, Utah Code Ann. §§ 72-14-203 & 72-14-204; and Vermont, Vt. Stat. Ann. tit. 20, § 4622(e); see generally Appendix 2. 24 CHAPTER 3: INFORMATION COLLECTION – GATHERING PUBLICLY AVAILABLE INFORMATION Introduction Information collection serves various purposes. It informs and directs the design of your project in both technical and legal ways. It also helps assure that your efforts are not redundant, as there may already be useful information in the public domain. It may lead you to other Why You Should Read this Chapter: Every citizen science project has limited resources (e.g., limited time, finances, volunteer involvement, etc.). You will increase the efficiency of your project by taking time to examine information that already exists (i.e., “information collection”). This chapter provides suggestions as to what information, if publicly available, might be of use to your project. In particular, this chapter focuses on the collection of information related to pollutants and pollutant sources. Resources are provided to aid in your search for this information. Because all citizen science projects should involve this type of “information collection,” we anticipate that this chapter will be useful to all citizen science projects, whether just beginning or ongoing. Graphic Legend: Various public resources can provide background on technical or legal topics relevant to your project. This information can influence your project’s design and increase its efficiency. 25 individuals who are monitoring the problem that you have identified. Here, we provide examples of information that may be worth collecting. Importantly, if you feel unable to collect this information, we recommend that you seek out expertise in your community. High school teachers, university professors, scientists, engineers, lawyers, and many other individuals in your community are likely willing and able to help. Collecting Available Information Concerning a Pollutant A large amount of information concerning specific pollutants is already available in the public domain. Spending time upfront to research your pollutant(s) of interest will help to assure that you get the most out of your efforts and could also help shield you from potential health risks. We recommend that you begin your research by addressing the following technical and legal questions: • Technical Questions Related to Determining the Identity of a Pollutant: Is the pollutant visible, and if so, what does it look like? Can the pollutant be sensed in other ways, such as smell? What health risks are associated with the pollutant? How are potential health risks manifested (e.g., vomiting, dizziness, skin rash, etc.)? What information is available on the pollutant’s material safety data sheet (“MSDS”) (e.g., health effects, first aid measures, flammability and explosiveness, proper storage and disposal, physical properties, toxicity, and necessary protective equipment)? Making Connections Between Chapters: In Chapter 1, you identified your project’s focus and used that focus to identify a site of interest to you (e.g., a natural resource or a pollutant source) and to determine which pollutant or combination of pollutants will be examined during your project. In Chapter 2, you identified how you hope to use the information that you collect or generate during your project and the type of quality standards that might apply. This chapter’s focus is “information collection,” gathering and analyzing information that is already in the public domain. In some instances, the process of information collection alone will provide you with the tools you need to meet your goals. However, many projects will need to supplement the process of information collection with information generation, which is discussed in Chapter 4 and Chapter 5. 26 • Technical Questions Related to Determining the Source of a Pollutant: What sources are typically associated with the pollutant (e.g., natural sources or human sources such as industrial facilities, landfills, sewage treatment plants, mining operations, etc.)? What is the pollutant’s Chemical Abstracts Service (“CAS”) number (a unique chemical identifier that can help you locate sources of a pollutant and any relevant characteristics)? • Technical Questions Related to Collecting, Handling, or Storing Samples: What is the stability of the pollutant in the air, water, or soil? Is the pollutant soluble in water? What instruments or methodologies can be used to measure the amount of the pollutant in air, water, or soil? What is the lowest amount of pollutant that is instrumentally or methodologically detectable (i.e., its detection limit)? What are the baseline/background levels of the pollutant (e.g., in some contexts pollutants are ubiquitous, and so detecting a pollution problem involves showing that the level of the pollutant is higher than previously recorded)? What are appropriate safety measures for the handling of the pollutant? • Legal Questions: Is the pollutant regulated by a federal or state agency (i.e., does a state or federal agency have jurisdiction over the pollutant)? If so, what regulations are in place that are specific to the pollutant (e.g., permissible or reportable quantities)? Various resources exist that can be of aid in answering these or other related questions. Substantial technical and legal information can be found online; however, care should be taken to assure the quality of the references that you rely upon. Generally, peer-reviewed medical or scientific articles are a very good resource to gain technical knowledge; these articles can be found by searching online with Google Scholar (https://scholar.google.com/) or in various public databases (e.g., Web of Science, PubMed, MedlinePlus, etc.) that might be available through a public library. Federal and state agency websites, such as epa.gov, also contain reliable information. For example, the Substance Registry Services (“SRS”) is the EPA’s “central system for information about substances that are tracked or regulated by EPA or other sources. It is the authoritative resource for basic information about chemicals, biological organisms, and other substances of https://scholar.google.com/ https://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=7BpyVsPnwHuxa3jRgCu&preferencesSaved= https://www.ncbi.nlm.nih.gov/pubmed/ https://medlineplus.gov/ 27 interest to EPA and its state and tribal partners.”36 The EPA website also provides links to state health and environmental agencies that play a role in monitoring pollutants.37 Finally, federal and state regulations contain information on how pollutants are monitored. These regulations may be very relevant to your project. For example, in many instances regulations will specify pollution quantities, which if exceeded, must be reported to a federal or state agency. Various federal regulations that may be relevant to your project are listed in Appendix 3. For many facilities, reporting requirements will also be contained in a permit, a topic discussed in the next section. Collecting Available Information Concerning a Pollutant Source A large amount of information concerning specific pollutant sources is also already available in the public domain. Investing time in researching the pollutant source will help to fine tune your project design and will help you avoid wasting time on the wrong potential pollutant source. For example, since news coverage and public records differ based on the individual pollutant source, it is crucial to start your research with the correct one. Identifying the correct pollutant source will allow you to conduct searches to obtain further information more easily. Here, we recommend that you begin your research by addressing the following questions: Are there any media reports that involve the pollutant source? Are third-party monitoring records available? Is this source monitored by a federal or state agency (i.e., does a state or federal agency have jurisdiction over this source) or is the source responsible for self-monitoring and reporting? A good place to begin researching a pollutant source is by reviewing public media releases that might implicate the pollutant source with an environmental concern. You should also seek out publicly available permits and monitoring records (e.g., generated by the source, a third party, and/or a government agency).38 Additional public records may include prior inspections of the 36 See About Substance Registry Services (SRS), U.S. ENVTL. PROTECTION AGENCY, https://iaspub.epa.gov/sor_internet/registry/substreg/home/overview/home.do (last visited Feb. 7, 2019). 37 See Health and Environmental Agencies of U.S. States and Territories, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/home/health-and-environmental-agencies-us-states-and-territories (last visited Feb. 7, 2019). 38 Resources that will help you locate permitting and compliance information for pollutant sources include EPA’s Envirofacts, TRI Program, and Enforcement and Compliance History Online (“ECHO”). See Envirofacts, U.S. ENVTL. PROTECTION AGENCY, https://www3.epa.gov/enviro/ (last visited Feb. 7, 2019); Toxics Release Inventory (TRI) Program, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/toxics-release-inventory-tri-program (last visited Feb. 7, 2019); Enforcement and Compliance History Online (ECHO), U.S. ENVTL. PROTECTION AGENCY, https://echo.epa.gov/ (last visited Feb. 7, 2019). https://iaspub.epa.gov/sor_internet/registry/substreg/home/overview/home.do https://www.epa.gov/home/health-and-environmental-agencies-us-states-and-territories https://www3.epa.gov/enviro/ https://www.epa.gov/toxics-release-inventory-tri-program https://echo.epa.gov/ 28 site of interest, prior compliance records, or reports submitted to governmental agencies by the site of interest. Appendix 4 lists several resources provided by the EPA. Various state agencies also provide similar resources. Additional information can be obtained through a Freedom of Information Act (“FOIA”) request. FOIA requires federal agencies to disclose any records requested by the public unless they fall into one of nine exemptions.39 These exemptions include information that bears on national security and personal privacy, among other concerns.40 Before making a FOIA request, you can conduct a search of information already made available by federal agencies at FOIAonline.gov and in their FOIA libraries to see if the information you seek has already been released.41 If the information you are searching for has not been released by an agency, you can also search online to see whether third parties (e.g., a nonprofit organization, news organization, etc.) have released relevant materials obtained through FOIA. If that fails, then you may want to consider filing your own FOIA request. Submitting a FOIA request does not involve any special forms and does not require any kind of legal expertise. You can simply write a letter to the agency most likely to possess those records, detailing the records you seek with reasonable particularity.42 Generally, the more specific your request is, the better; broader requests take considerably longer to process and are more likely to yield irrelevant results.43 Additionally, some agencies require individuals to submit 39 5 U.S.C. § 552(a)(3)(A), (b); see also U.S. Department of Justice, What is FOIA?, FOIA.GOV, https://www.foia.gov/about.html (last visited Feb. 7, 2019). 40 See 5 U.S.C. § 552(b). 41 FOIAONLINE.GOV, https://www.foiaonline.gov/foiaonline/action/public/home (last visited Feb. 7, 2019). Agencies, and sometimes even their individual component offices, have FOIA libraries. These libraries result from FOIA’s proactive disclosure requirements, which direct agencies to publicly release commonly requested records. See 5 U.S.C. § 552(a)(2); U.S. DEPARTMENT OF JUSTICE, Proactive Disclosures, in DEPARTMENT OF JUSTICE GUIDE TO THE FREEDOM OF INFORMATION ACT, at 9-22 (2009), https://www.justice.gov/sites/default/files/oip/legacy/2014/ 07/23/proactive-disclosures-2009.pdf. For example, EPA has a consolidated FOIA library online. National Online FOIA Library, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/foia/national-online-foia-library (last visited Feb. 7, 2019). 42 5 U.S.C. § 552(a)(3)(A)(i). 43 For a more detailed idea of what information to include in your FOIA request, you should look at the agency’s FOIA regulations. For example, EPA’s regulations provide as follows: “Your request should reasonably describe the records you are seeking in a way that will permit EPA employees to identify and locate them. Whenever possible, your request should include specific information about each record sought, such as the date, title or name, author, recipient, and subject matter. If known, you should include any file designations or descriptions for the records that you want. The more specific you are about the records or type of records that you want, the more likely EPA will be able to identify and locate records responsive to your request.” 40 C.F.R. § 2.102(c) (emphasis added). https://www.foia.gov/about.html https://www.foiaonline.gov/foiaonline/action/public/home https://www.justice.gov/sites/default/files/oip/legacy/2014/07/23/proactive-disclosures-2009.pdf https://www.justice.gov/sites/default/files/oip/legacy/2014/07/23/proactive-disclosures-2009.pdf https://www.epa.gov/foia/national-online-foia-library 29 a fee to cover the cost of record retrieval.44 Broader requests, which tend to require more work on the agency’s part, are likely to be more expensive. For a sample FOIA request letter you can fill out with your specific details, visit the National Freedom of Information Coalition’s website.45 Once you have written your request, you can locate the relevant agency’s FOIA request contact information.46 If the information you seek is more likely to be held by a state agency, then you will want to acquaint yourself with your state’s public records law and see if you can make a similar document request. Every state has its own public records laws pertaining to public requests for information from state agencies. While some are very similar to FOIA, others are broader or more limited. To learn more about your state’s public records law, you can access the National Freedom of Information Coalition’s database of state public records laws.47 This helpful resource also includes sample FOI request letters by state.48 As with federal FOIA requests, you will want to make sure that your state records request is as detailed and specific as possible. If you encounter any difficulty in securing a response to your state FOI request, the Freedom of Information 44 That being said, there are certain provisions that limit fee collection on FOIA requests. The reasonableness of such fees may vary according to whether the information sought is to be used for commercial or noncommercial purposes, with the latter meriting a lesser fee. 5 U.S.C. § 552(a)(4)(A)(ii). Fees may also be waived if the information sought is in the public interest. Id. § 552(a)(4)(A)(iii). Furthermore, the government agency waives its right to collect fees if it does not respond to the request within the statutorily mandated time limits. Id. § 552(a)(4)(A)(viii). 45Sample FOIA Request Letters, NAT’L FREEDOM OF INFORMATION COALITION, https://www.nfoic.org/sample-foia- request-letters#foireq (last visited Feb. 7, 2019). 46 In order to make the FOIA process more efficient, you should try to determine the addressee of the FOIA request based on the topic and location of interest. For example, if you live in Texas and want to learn about the unauthorized release of a pollutant in your community, you should address your FOIA request to EPA Region 6’s FOIA Office. See The FOIA Request Process, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/foia/foia- request-process (last visited Feb. 7, 2019); see also Contact Us about the Freedom of Information Act and FOIA Requests, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/foia/forms/contact-us-about-freedom- information-act-and-foia-requests#Regional (last visited Feb. 7, 2019). If you are concerned with a mining permit in Alaska, on the other hand, you should submit your FOIA request to the Department of the Interior Office of Surface Mining Reclamation and Enforcement’s Western Region Office. OSMRE Freedom of Information Act Program, OFFICE OF SURFACE MINING RECLAMATION & ENFORCEMENT, https://www.osmre.gov/lrg/FOIA.shtm (last visited Feb. 7, 2019). An index of government agencies and departments is available on USA.gov. See A-Z Index of U.S. Government Departments and Agencies, USA.GOV, https://www.usa.gov/federal-agencies/a (last visited Feb. 7, 2019). 47 State Freedom of Information Laws, NAT’L FREEDOM OF INFORMATION COALITION, http://www.nfoic.org/state- freedom-of-information-laws (last visited Feb. 7, 2019). 48 State Sample FOI Request Letters, NAT’L FREEDOM OF INFORMATION COALITION, http://www.nfoic.org/state- sample-foia-request-letters (last visited Feb. 7, 2019). https://www.nfoic.org/sample-foia-request-letters#foireq https://www.nfoic.org/sample-foia-request-letters#foireq https://www.epa.gov/foia/foia-request-process https://www.epa.gov/foia/foia-request-process https://www.epa.gov/foia/forms/contact-us-about-freedom-information-act-and-foia-requests#Regional https://www.epa.gov/foia/forms/contact-us-about-freedom-information-act-and-foia-requests#Regional https://www.osmre.gov/lrg/FOIA.shtm https://www.usa.gov/federal-agencies/a http://www.nfoic.org/state-freedom-of-information-laws http://www.nfoic.org/state-freedom-of-information-laws http://www.nfoic.org/state-sample-foia-request-letters http://www.nfoic.org/state-sample-foia-request-letters 30 Coalition and its affiliates have offices in every state that you can contact for advice and assistance.49 49 NFOIC State and Regional Affiliates, NAT’L FREEDOM OF INFORMATION COALITION, http://www.nfoic.org/members (last visited Feb. 7, 2019). http://www.nfoic.org/members 31 CHAPTER 4: INFORMATION COLLECTION – BEWARE POTENTIAL LIABILITY Introduction In most instances, we anticipate that you will not encounter legal difficulties in conducting research for your citizen science project. Your project’s site of interest (which you identified as part of your project approach in Chapter 1) may be open to all citizens—meaning there are no legal barriers in collecting samples of air, water, and/or soil quality, or taking photographs. Many Why You Should Read this Chapter: While most citizen science projects will not implicate legal concerns, there are nonetheless various laws that can limit your ability to gather information. This chapter gives an overview of these laws and provides suggestions on how to remain in compliance with them. It also notes areas where you may have a legal privilege to engage in certain activity, so that you can respond proactively. The content of this chapter is supplemented by Appendices 1 and 2, which provide a state-by-state analysis of the laws discussed. Graphic Legend: Before you begin collecting samples from your project’s site of interest, you should arm yourself with knowledge of legal issues that might be relevant to the design of your information collection strategy. A primary concern is property ownership. Verifying property ownership will help you avoid trespass, for example. 32 federal and state agencies have issued guidelines that are favorable to the practice of citizen science. In sum, you should not let the fear of legal troubles deter you from pursuing your project. But, you should be aware of the laws that might apply to your project. This chapter outlines the various legal claims that have been asserted (rightly or wrongly) against citizen scientists. It aims to arm you with some general knowledge, including things you are well within your rights to do as well as things you should avoid doing. Should you encounter a legal threat in the course of your project, our hope is that you will be able to figure out whether that threat is real or mere puffery, allowing you to take full advantage of your legal rights. Because many types of potential liability relate to actions you might take on private property, we begin by discussing property ownership. Property Ownership: Who owns the land where you want to gather information or collect samples? In addition to securing any publicly available records that are relevant your project goals (discussed in Chapter 3), you should take steps to learn about ownership of the land where your project site is located, as well as the land surrounding it. One way to determine the ownership status of your project’s site of interest is to use Geographic Information System (“GIS”) maps. Making Connections Between Chapters: In Chapter 1, you identified your project’s focus and used that focus to design your project’s approach, which included the identification of a site of interest to you (e.g., a natural resource or a pollutant source). In Chapter 3, you collected publicly available information on any pollutant sources relevant to your project. This chapter provides resources for you to extend this previous work, helping you to determine or verify property ownership of land on and surrounding your project site (e.g., where you will collect samples). It then gives an overview of legal issues relevant to your sample collection design. This information can guide the scope of your information generation strategy (discussed in Chapter 5). Legal topics covered in this chapter include: 1) Trespass 2) Loitering 3) Stalking 4) Privacy 5) Drone use 6) Critical infrastructure 7) Agency regulations 33 GIS maps layer data over geography, allowing interactive visualization of geographic information on the map.50 Many GIS maps display property lines and ownership information.51 A related resource is your local assessor’s office, which maintains a public database of local property ownership. You can submit a request to your assessor’s office to determine a given parcel’s ownership information so long as you have the property’s parcel number (oftentimes, this parcel number can be found using GIS maps). Note that many offices provide this information online – meaning you do not have to go in person to find certain information or submit a request for further information.52 Property ownership determines whether you may access a property and whether you may collect samples, photos, or other information. For example, strict trespass and privacy laws apply to private property. Public property is managed by various government agencies that have their own special rules about who can access the land and for what purposes. Public lands can be roughly split into the following categories: • Federal Land: Land owned by the federal government is managed either by the Department of the Interior or by the Department of Agriculture’s Forest Service.53 Within the Department of the Interior, the Bureau of Land Management is tasked with overseeing the majority of the federal government’s on-shore landholdings, 50 See What is Geographic Information Systems (GIS)?, GIS GEOGRAPHY, http://gisgeography.com/what-gis- geographic-information-systems/ (last updated Jan. 5, 2019). 51 See, e.g., Mass. Interactive Property Map, MASS. EXEC. OFFICE OF ADMIN. AND FINANCE, http://www.mass.gov/anf/research-and-tech/it-serv-and-support/application-serv/office-of-geographic-information- massgis/online-mapping/massgis-par-vwr.html (last visited Feb. 7, 2019) (GIS map of property in Massachusetts); Tennessee Property Viewer, STATE OF TENN., http://tnmap.tn.gov/assessment/ (last visited Feb. 7, 2019) (GIS map of property in Tennessee). 52 See, e.g., Automated City Register Information System, NEW YORK CITY DEP’T OF FINANCE, https://a836- acris.nyc.gov/CP/ (last visited Feb. 7, 2019) (New York City’s online property database is commonly referred to as ACRIS). Online property databases, such as ACRIS, may also come with helpful instructions. See, e.g., ACRIS Document Search Online Help, NEW YORK CITY DEP’T OF IT & TELECOMMUNICATIONS, https://a836- acris.nyc.gov/acrisHelp/docsearch/default.htm (last visited Feb. 7, 2019). 53 See Summary, in CONGRESSIONAL RESEARCH SERVICE, FEDERAL LAND OWNERSHIP: OVERVIEW AND DATA (Mar. 3, 2017), https://fas.org/sgp/crs/misc/R42346.pdf [hereinafter, “2017 Federal Land Ownership Report”]. Property ownership determines whether you may access a property and whether you may collect samples, photos or other information. http://gisgeography.com/what-gis-geographic-information-systems/ http://gisgeography.com/what-gis-geographic-information-systems/ http://www.mass.gov/anf/research-and-tech/it-serv-and-support/application-serv/office-of-geographic-information-massgis/online-mapping/massgis-par-vwr.html http://www.mass.gov/anf/research-and-tech/it-serv-and-support/application-serv/office-of-geographic-information-massgis/online-mapping/massgis-par-vwr.html http://tnmap.tn.gov/assessment/ https://a836-acris.nyc.gov/CP/ https://a836-acris.nyc.gov/CP/ https://a836-acris.nyc.gov/acrisHelp/docsearch/default.htm https://a836-acris.nyc.gov/acrisHelp/docsearch/default.htm https://fas.org/sgp/crs/misc/R42346.pdf 34 which add up to about 1/8 of the nation’s land.54 These federal landholdings are especially concentrated in western states; 48.4% of Wyoming, for example, is federally-owned land.55 • State Land: Each state has its own land-holding agencies that oversee the use of state- owned property. These generally include a state-wide Parks Department and a Department of Natural Resources. A great deal of state-held land—about 3/4—is in the form of trust lands—lands held by the state to benefit specific public purposes, most commonly to support public schools.56 While some of these trust lands are commercially leased and unavailable to the public, in many cases they are open to public access. You should check with the state’s Department of Natural Resources or Parks Department to see what activities are permitted in state parks and trust lands. Drone use and certain research activities, for example, may be restricted or prohibited. • Local/Municipal Land: A lot of public property is also managed at the local or municipal level. Municipalities can own and rent land within city limits.57 Many local parks, cemeteries, and waterways are subject to local ownership and control.58 Generally, a municipality’s Parks & Recreation Department or Water Department will have authority to administer such lands—and control access. • Maritime Territory: Management of the oceans is split between the state and federal governments. The first three nautical miles from the coast are considered state property and are managed by the states.59 The next nine nautical miles are U.S. territorial waters 54 See id. (The United States has 2.27 billion acres of land); see also U.S. DEPARTMENT OF THE INTERIOR - BUREAU OF LAND MANAGEMENT, PUBLIC LAND STATISTICS 2017, at 7 (June 2018), https://www.blm.gov/sites/blm.gov/files/PublicLandStatistics2017.pdf (the Bureau of Land Management has jurisdiction over 245.6 million acres of public lands). 55 2017 Federal Land Ownership Report, supra note 53, at 9. 56 See Steven M. Davis, Preservation, Resource Extraction, and Recreation on Public Lands: A View from the States, 48 NAT. RESOURCES J. 303, 306 (2008), https://digitalrepository.unm.edu/cgi/viewcontent.cgi?referer=https:/ /www.google.com/&httpsredir=1&article=1264&context=nrj. 57 See, e.g., MUNICIPAL ASSOCIATION OF SOUTH CAROLINA, FORMS AND POWERS OF MUNICIPAL GOVERNMENT (Dec. 2017), https://www.masc.sc/SiteCollectionDocuments/Administration/Forms%20and%20Powers2.pdf. 58 Id. 59 While most state-managed waters only extend out to three nautical miles beyond the shore, the seaward boundaries of Florida (Gulf of Mexico coast only), Texas, and Puerto Rico extend to nine nautical miles. See 43 U.S.C. § 1312; Maritime Zones and Boundaries, NAT’L OCEANIC & ATMOSPHERIC ADMINISTRATION OFFICE OF GENERAL COUNSEL, http://www.gc.noaa.gov/gcil_maritime.html (last visited Feb. 7, 2019) [hereinafter, “Maritime Zones and Boundaries”]. https://www.blm.gov/sites/blm.gov/files/PublicLandStatistics2017.pdf https://digitalrepository.unm.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1264&context=nrj https://digitalrepository.unm.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1264&context=nrj https://www.masc.sc/SiteCollectionDocuments/Administration/Forms%20and%20Powers2.pdf http://www.gc.noaa.gov/gcil_maritime.html 35 that are managed by the federal government (i.e., the territorial sea).60 Different federal agencies are responsible for regulating particular types of activities in federal waters. The Bureau of Ocean Energy Management (“BOEM”) and the Bureau of Safety and Environmental Enforcement (“BSEE”), both part of the Department of the Interior, manage offshore energy exploration and development.61 The National Marine Fisheries Service regulates fisheries and is responsible for the stewardship of marine protected species.62 The EPA has general authority over pollution discharges not associated with energy development and minerals (which would fall under BOEM/BSEE’s purview). The Coast Guard is the primary law enforcement authority in these waters.63 Knowing the property lines and ownership status of properties you want to access is an important first step when it comes to determining what you are allowed to do on the property. Knowing who owns the property on which you want to conduct research may have another benefit: in many cases, reaching out to the property owner or managing agency ahead of time to see if you can conduct your citizen science project on their land will resolve any disputes at the outset. For example, you can avoid the risk of trespass liability if you have already received permission from the property owner to conduct research on his or her land. Potential Legal Challenges In this subsection, we identify categories of laws that restrict access to land. Appendices 1 and 2 provide a 50-state survey64 of the laws discussed in this chapter. Neither this subsection nor the appendices provide complete and detailed answers about the applicable laws in any given state; instead, they are intended to give you a broad overview of the applicable laws. We encourage you to use the tools at your disposal, such as local libraries and the internet, to conduct further research about the local laws where you live. Laws change and evolve; please remember that the resources in this manual do not constitute legal advice, and that you should seek representation should you encounter any legal issues. 60 See Maritime Zones and Boundaries, supra note 59. 61 History, BUREAU OF SAFETY & ENVTL. ENFORCEMENT, https://www.bsee.gov/who-we-are/history (last visited Feb. 7, 2019). 62 See About Us, NOAA FISHERIES, https://www.fisheries.noaa.gov/about-us (last visited Feb. 7, 2019). 63 See U.S. COAST GUARD, https://www.work.uscg.mil/ (last visited Feb. 7, 2019). 64 This survey also includes the Commonwealth of Puerto Rico, an unincorporated territory of the United States. https://www.bsee.gov/who-we-are/history https://www.fisheries.noaa.gov/about-us https://www.work.uscg.mil/ 36 1. Trespass Broadly defined, a trespasser is someone who physically enters or remains on another person’s property without that person’s consent.65 Liability for trespass generally takes two forms: criminal (prosecution by the government) and civil (private lawsuits). In addition, certain states impose heightened liability for trespass—or even taking photographs—around industrial or agricultural facilities. a. General Criminal and Civil Trespass Every state has its own criminal trespass statute. If you are interested in learning more about your state’s criminal trespass statute, you can begin by locating your state’s criminal code online.66 These statutes generally define trespass as unauthorized entry onto someone else’s land. Beyond that basic definition, many states have varying degrees of criminal trespass, meaning that certain forms of trespass may be punished more severely than others. In Alabama, for example, 65 Cf. Restatement (Second) of Torts § 329. 66 Cornell University’s Legal Information Institute has compiled each state’s criminal code at https://www.law.cornell.edu/wex/table_criminal_code (last visited Feb. 7, 2019). Upon locating your state’s criminal code, you can follow the hyperlink to its criminal trespass laws in the table of contents. Summary: You commit a trespass only when you go on someone else’s land without the owner’s permission. If you stay on public or private lands where you have permission to be, then trespass laws will not be a problem for your research. If you need to take samples on private land or cross private land to get to your sampling location, then you can seek permission from the property owner. Otherwise, you will generally be safe if you avoid areas that are marked off by fences or “no trespassing” signs. In a few states (indicated in this subsection), you need to use GIS maps to identify property boundaries and therefore avoid accidentally crossing onto someone’s property. Additionally, a few states have specialized laws that punish trespass and even photography around industrial and agricultural sites. You will want to be aware of whether your state has such a law. For the most part, you can avoid trouble under these laws by not entering any clearly off-limits sites—the same advice as with trespass generally. Nonetheless, it would be wise to utilize GIS maps and to be aware of property lines when conducting research around such a facility. https://www.law.cornell.edu/wex/table_criminal_code 37 first degree criminal trespass occurs when a person knowingly enters someone else’s home without permission;67 second degree trespass occurs when a person crosses, without authorization, onto private land that is fenced off or otherwise bears markers of private property.68 The role of notice varies among state criminal trespass statutes. Most states require that, to be guilty of criminal trespass, an individual must have had notice that he or she was entering private property without authorization (e.g., a “no trespassing” sign or a fenced off area). Six states, however, do not require notice: Colorado, Louisiana, Tennessee*, Utah, Wisconsin, and Wyoming. In these states, trespass is an absolute liability crime—meaning that being unaware that you were not supposed to be on the property in question is not a valid defense in these instances.69 If you are conducting your project in one of these states, you should carefully scrutinize current GIS maps and property records before entering your project’s site of interest.70 In other states, common sense should suffice: avoid entering fenced or marked-off areas without permission. In addition to criminal statutes, every state also allows landowners to bring civil lawsuits for trespass. These are generally governed by common law – meaning there is no statute to look at when determining what constitutes civil trespass; the law is developed by judges in their decisions. Many judges define it in the same way as criminal trespass: voluntary entry onto someone else’s property without consent or authorization.71 Ultimately, this means that you may be liable for any damage you cause to someone else’s property while conducting research on that property. Moreover, even if no quantifiable damage is done, many courts will allow the property owner to recover nominal damages for the very fact of the trespass.72 However, these nominal damages are typically very small. 67 Ala. Code § 13A-7-2. 68 Ala. Code § 13A-7-3. 69 Tennessee is the only state from this list that provides a defense to prosecution. However, lack of notice alone is not a sufficient defense. The alleged trespasser must prove three elements: (1) The person entered or remained on property that she reasonably believed to be property for which the owner’s consent to enter had been granted (i.e., lack of notice); (2) The person’s conduct did not substantially interfere with the owner’s use of the property; and (3) The person immediately left the property upon request. Tenn. Code Ann. § 39-14-405(b). 70 For further information on what these informational tools are and how to access them, see Chapter 3. 71 Restatement (Second) of Torts § 158. 72 See Foust v. Kinney, 80 So. 474, 475 (Ala. 1918); see also Brown Jug, Inc. v. Int’l Brotherhood of Teamsters, Chauffeurs, Warehousemen & Helpers of Amer., 688 P.2d 932, 939 (Alaska 1984); Hale v. Brown, 323 P.2d 955, 963 (Ariz. 1958) (It is a “well-established and deeply-rooted legal principle that a person has the right to vindicate any trespass upon his legal rights . . . for at least nominal damages.”). 38 b. Specialized Trespass Statutes: Ag-Gag, Data Collection & Critical Infrastructure Trespass Laws In addition to basic trespass liability, many states have specialized statutes that address trespass on and monitoring of agricultural facilities, colloquially known as “Ag-Gag laws.” At the time of writing, 22 states had such laws. Ag-Gag laws are compiled in the spreadsheet in Appendices 1 and 2. These statutes tend to have the same basic elements: an alleged trespasser entered or remained on an agricultural facility (i) without effective consent, (ii) intending to disrupt or damage the enterprise conducted at the animal facility, and (iii) had notice that entry was forbidden or received notice to depart and did not. Some also include a separate legal claim for taking photos or videos of such a facility with the intent of damaging its enterprise (e.g., by publishing damning information about it).73 In summary, when seeking to monitor an agricultural facility, you should check to see if your state has an applicable Ag-Gag law. Wyoming is currently unique in that it has a trespass law that specifically targets citizen scientists.74 This statute creates a new criminal offense called “trespassing to unlawfully collect resource data.”75 The offense is defined as trespassing on private property for the purpose of collecting “data relating to land or land use, including but not limited to data regarding agriculture, minerals, geology, history, cultural artifacts, archeology, air, water, soil, conservation, habitat, vegetation or animal species.”76 The statute is triggered either by collecting resource data on private land or by crossing private land to collect resource data on public land. A violation of this statute triggers enhanced penalties, compared to ordinary trespass. For a first offense, the punishment is up to one year in prison plus a fine of up to $1,000; the maximum fine is increased to $5,000 for repeat offenders.77 A related statute allows property owners to bring a civil damages action against trespassers.78 Someone can be liable under both the criminal and the civil statutes even if the private property boundaries are unmarked. 73 See, e.g., Kan. Stat. Ann. § 47-1827(c)(4). 74 Wyo. Stat. § 6-3-414(a). 75 Id. 76 Id. § 6-3-414(e)(iv). 77 Id. § 6-3-414(d). 78 Wyo. Stat. § 40-27-101(d). 39 A number of environmental groups challenged this law in court, arguing that it violates their free speech rights under First Amendment of the U.S. Constitution. A federal appeals court found that subsection (c) of the statute, which defines the prohibited conduct to include crossing private property in order to collect resource data on nearby public lands, implicates protected speech. The court therefore sent the case back to the trial court to determine whether this impact on protected speech violated the Constitution.79 On remand, the trial court held that subsection (c) violated the First Amendment and enjoined Wyoming from enforcing it.80 This decision leaves in place, however, the portions of the law that provide enhanced penalties for trespassing for the purpose of collecting resource data on private property. Citizen scientists in Wyoming should be especially careful about identifying private property boundaries, particularly because these are often unmarked. One way to accomplish this is by using GIS maps and public records to identify the ownership and property lines in any locations where you want to take samples and along your routes to reach those sites. Finally, some states treat it more seriously when someone trespasses on certain industrial, agricultural, or government-owned facilities. These laws, which impose heightened liability for trespass on “critical infrastructure,” are discussed in-depth in Subsection 6_6._Critical_Infrastructure below. 2. Loitering 79 W. Watersheds Project v. Michael, 869 F.3d 1189 (10th Cir. 2017). 80 W. Watersheds Project v. Michael, 353 F. Supp. 3d 1176 (D. Wyo. 2018). Summary: Loitering laws are generally written by local governments rather than states, so it is not possible for us to compile all of the potentially relevant laws. In general, however, you cannot be liable for loitering just because you are hanging around a public place. Courts have held that such “pure” loitering laws are unconstitutional. Instead, loitering laws are typically constitutional only when they target loitering in connection with some otherwise illegal activity. This means that if you follow the suggestions given with respect to the other laws discussed in this chapter, you will likely avoid the possibility of loitering liability. And, you can inform anyone who threatens you with loitering that your conduct is protected. 40 Loitering is a second offense potentially relevant to your citizen science project. Defined broadly, loitering means hanging around a public place or business without an apparent legal purpose.81 Generally, loitering laws are established at the local or municipal level. Thus, you will want to check your local area’s anti-loitering provisions before spending time around your identified project site. At least one state, California, includes loitering within its criminal trespass laws (see Subsection 6_6._Critical_Infrastructure below). The U.S. Supreme Court has invalidated loitering laws that do not include a separate, objective element of criminal behavior (e.g., criminal and specialized trespass, etc.).82 This undermines the validity of any criminal loitering statute that simply criminalizes loitering in and of itself.83 As a result, your potential liability for loitering is likely low if you are not also breaking a separate criminal law. You should feel comfortable taking advantage of this aspect of loitering laws: if someone accuses you of loitering when you are otherwise participating in perfectly innocent activity, then you can respond by saying that whatever loitering law they are referring to is not likely to include your conduct. 81 See Loitering, THE FREE LEGAL DICTIONARY, http://legal-dictionary.thefreedictionary.com/loiter (last visited Feb. 7, 2019). 82 See generally Papachristou v. City of Jacksonville, 405 U.S. 156 (1972); City of Chicago v. Morales, 527 U.S. 41, 41-45 (1999). 83 See Note, Striking a Balance: The Efforts of One Massachusetts City to Draft an Effective Anti-Loitering Law Within the Bounds of the Constitution, 39 SUFFOLK U. L. REV. 1069, 1081 (2006); Kim Strosnider, Note, Anti-Gang Ordinances After City of Chicago v. Morales: The Intersection of Race, Vagueness Doctrine, and Equal Protection in the Criminal Law, 39 AM. CRIM. L. REV. 101, 126 (2002). http://legal-dictionary.thefreedictionary.com/loiter 41 3. Stalking If your project involves recurring interaction with or surveillance of the same individuals (e.g., photographing or video recording), you will want to familiarize yourself with your state’s stalking laws.84 Generally, you can avoid stalking liability if you space your research out temporally and if you avoid repeated contact with the same individuals (unless they have invited the contact or interaction). Every state has a criminal anti-stalking statute (see Appendices 1 and 2 for specific references to each state).85 States tend to define stalking as repeated and willful following of another person, often paired with some malevolent purpose or action, such as threatening or harassing behavior.86 A person violates California’s anti-stalking law, for example, if he or she “willfully, maliciously, and repeatedly follows or harasses another person and makes a credible threat with the intent to place that person in reasonable fear of death or great bodily harm or to 84 As with criminal trespass laws, stalking laws are often classified into varying degrees. Generally, higher degree stalking crimes include the issuance of credible threats, repeated convictions, contact in violation of a restraining order, stalking of a minor, and harassment on the basis of sex, race, religion, or sexual orientation. Because your behavior as a citizen scientist will not likely encompass any of these aggravating factors, this subsection and Appendices 1 and 2 focuses on lower degree stalking violations. 85 See Kathleen G. McAnaney, Laura A. Curliss & C. Elizabeth Abeyta-Price, Note, From Imprudence to Crime: Anti-Stalking Laws, 68 NOTRE DAME L. REV. 819, 821 (1993). 86 Id. Summary: Generally, it is a good practice to maintain a comfortable distance from and to avoid repeated contact with the same individuals in the course of your research (unless they have invited the contact or interaction!). You should especially avoid photographing or filming the same individuals on a recurring basis, which might be interpreted as harassing behavior. You should also review relevant state stalking laws to determine the point at which conduct is considered stalking and whether “stalking” requires general or specific intent. Typically, stalking laws that require “specific intent” will not apply to your role as a citizen scientist. If the stalking laws relevant to your project’s site of interest require “general intent,” you might consider letting the local community know about your project ahead of time to eliminate any cause for alarm. 42 place that person in reasonable fear of the death of or great bodily injury of his or her immediate family.”87 While state anti-stalking statutes are similar in some respects, they can differ in a few key ways. One difference relates to the point at which conduct is considered sufficiently repetitive and continuous to be considered stalking. For example, Arkansas’s statute requires “a pattern of conduct composed of two (2) or more acts separated by at least thirty-six (36) hours but occurring within one year.”88 Other states require conduct that is more repetitive and continuous. For example, Alabama requires “a series of acts over a period of time which evidences a continuity of purpose.”89 Louisiana also requires a “series of acts” for the conduct to rise to the level of stalking.90 State stalking laws also differ in whether they require general or specific intent. For stalking laws requiring specific intent, you are only guilty of stalking if you intended to harass or threaten the person alleging the violation; for those requiring general intent, you can be guilty of stalking even if you did not intend to harass the person(s) alleging that you stalked them. If the relevant state defines stalking as a specific intent crime, it is unlikely that your work as a citizen scientist will expose you to liability for stalking because the purpose of your activity is to conduct research, not to harass anyone. If the relevant state defines stalking as a general intent crime, however, then you may want to take the extra step of notifying anyone residing on or near the property on which you want to conduct research. You might, for example, post flyers in the neighborhood notifying individuals that you are conducting a citizen science project. If people understand what you are doing in or around their neighborhood, then they should not have reason to be threatened by your presence. It might also mobilize the local community around your citizen science project, in keeping with the spirit of citizen science. While stalking is generally a crime, thirteen states—Arkansas, California, Kentucky, Michigan, Nebraska, Oregon, Rhode Island, South Dakota, Tennessee, Texas, Virginia, 87 Cal. Penal Code § 646.9(a). 88 Ark. Code Ann. § 5-71-229(f)(1)(A). 89 Ala. Code. § 13A-6-92(a). 90 La. Rev. Stat. 14:40.2(C)(2). 43 Washington, and Wyoming—also allow civil lawsuits for stalking,91 so that individuals may recover damages for the emotional distress they experience. As with the criminal stalking laws, these are included in Appendices 1 and 2. 4. Invasion of Privacy Repeated contact with the same individual(s), especially involving photographing or video recording, may constitute an invasion of privacy. Privacy claims are only available to individual persons and not corporate entities.92 There are four basic kinds of legal causes of action for invasion of privacy: (i) unauthorized use of name or likeness; (ii) public disclosure of private matters; (iii) publicity placing one in a highly offensive false light; and (iv) intrusion upon private affairs.93 Intrusion upon private affairs occurs when someone intentionally intrudes, physically or otherwise, upon another’s solitude or private affairs in a manner that would be offensive to a reasonable person.94 This could occur when the person alleging the intrusion was at his or her own home or yard when another is taking photographs of him/her; one who enters public space cannot reasonably expect a great degree of privacy.95 91 Civil Stalking Laws by State, STALKING RESOURCE CENTER, https://victimsofcrime.org/our-programs/stalking- resource-center/stalking-laws/civil-stalking-laws-by-state (last updated Oct. 2017). 92 See Restatement (Second) of Torts § 652I(c); see also United States v. Morton Salt Co., 338 U.S. 632, 652 (1950) (“[C]orporations can claim no equality with individuals in the enjoyment of a right to privacy.”); Fleck & Assocs., Inc. v. Phoenix, City of, an Arizona Mun. Corp., 471 F.3d 1100, 1104-05 (9th Cir. 2006) (holding that a corporation is not entitled to “‘purely personal’ guarantees,” such as a right to privacy, which has “historically been granted to protect individuals”); Elizabeth Pollman, A Corporate Right to Privacy, 99 MINN. L. REV. 27, 37-44 (2014). 93 VINCENT R. JOHNSON, ADVANCED TORT LAW: A PROBLEM APPROACH 312 (1st ed. 2010). 94 Restatement (Second) of Torts § 652B. 95 Phillip Hassman, Taking Unauthorized Photographs as Invasion of Privacy, 86 A.L.R. 3d 374 (Originally published in 1978); see also Truxes v Kenco Enterprises, Inc. 119 N.W.2d 914, 919-20 (S.D. 1963) (post office worker’s invasion of privacy claim for an unauthorized photo taken of him while at work failed due to his place of employment not being a private space). Summary: Privacy laws are relevant when you are working in or around residential areas. If this is true of your project, you should try to notify area residents of your project ahead of time to ease any apprehension they may otherwise feel about your presence. You should also avoid taking and, in particular, publishing photos or videos of people in their homes. https://victimsofcrime.org/our-programs/stalking-resource-center/stalking-laws/civil-stalking-laws-by-state https://victimsofcrime.org/our-programs/stalking-resource-center/stalking-laws/civil-stalking-laws-by-state 44 In general, you can go a long way in avoiding claims of intrusion upon private affairs if you (i) do not enter people’s private space and (ii) exercise caution when taking pictures or videos around people’s homes or publishing those pictures or videos.96 If your work occurs near private residences and entails visual evidence, make sure that any materials you publish do not include images of persons within those residences. Taking photographs of individuals who are standing outside on their own property, so long as the photographer does not enter the private property, is not considered an invasion of privacy because the conduct is clearly visible to passersby and is therefore effectively public conduct.97 It can be worthwhile to notify any community members around whom you are working of your project’s goal and scope. Let people know why you are working near their properties, and they will have less reason to feel that you are intentionally intruding upon their privacy. 5. Drone Laws Drones, or unmanned aircraft systems (“UAS”), are an increasingly popular tool for environmental data collection. UAS have been used for, among other things, identifying the trajectory of an oil spill, tracking toxic algae blooms, measuring water temperature, detecting air contaminants, producing high resolution aerial surveys, and taking water samples. Lawmakers are just beginning to respond to UAS use. As such, the current body of law related to drone use is still 96 See Hassman, supra note 95. 97 See, e.g., United States v. Santana, 427 U.S. 38, 42 (1976) (holding that there is no expectation of privacy when an individual is “exposed to public view, speech, hearing, and touch as if she had been standing completely outside her house”); Swerdlich v. Koch, 721 A.2d 849, 857-58 (R.I. 1998) (“The plaintiffs were not entitled, nor could they reasonably have expected, to maintain privacy with respect to those activities taking place outside of their residence in a location visible to any passersby.”); Sundheim v. Board of County Comm’nrs, 904 P.2d 1337, 1351-52 (Colo. App. 1995) (“Because there is no invasion of privacy involved in observing that which is plainly visible to the public, a person’s real property is not protected from observations lawfully made from outside its perimeter.”). Summary: Drones may be subject to three different kinds of law: state drone statutes, Federal Aviation Administration (“FAA”) regulations, and common law. Because state drone law is still developing, you should routinely check state laws. In addition, you should always comply with FAA regulations by appropriately registering your drone. Moreover, you should be careful about using drone photography, as certain states have passed laws criminalizing drone footage of industrial facilities. Finally, drone footage of people in their private residences is also likely forbidden in your state, either by statute or common law. 45 developing. It is very likely that some of the information contained in this section, especially the status of state drone legislation, may have changed by the time you read this manual. Therefore, you should be careful to double check the status of drone laws in the state(s) where you are conducting citizen science. Currently, drone usage is governed by (i) federal law, (ii) state statutes, and (iii) state common law. The following subsections cover each of these categories in turn. a. Federal Law The FAA has statutory authority to regulate airspace to the extent necessary to maintain its safety.98 Drones are considered to be “aircraft” and as such are subject to federal regulation. The treatment of small drones (those weighing less than 55 pounds) varies, depending upon whether they are being used for commercial or recreational purposes. As long as the person operating the drone for a citizen science project is not being paid to do so, citizen scientist use of drones probably falls on the “recreational” side of this dichotomy.99 Recreational use of small drones is governed by the Exception for Limited Recreational Operations of Unmanned Aircraft, which Congress adopted as part of the FAA Reauthorization Act of 2018.100 The Exception requires that UAS operators only fly for recreational purposes; follow a community-based set of safety guidelines; fly the UAS within visual line-of-sight; give way to manned aircraft; obtain permission from the Administrator or her designee before flying within airspace designated for an airport or other restricted uses; fly the UAS not more than 400 98 The use of small drones, defined as those weighing fewer than 55 pounds, is governed by 14 C.F.R. pt. 107. While small drones do not need to undergo the extensive airworthiness certification requirements imposed on larger aircrafts, they are still subject to many of the same rules. Drones weighing more than 55 pounds will need to undergo the airworthiness exemption process outlined in 49 U.S.C. § 44807. See Special Authority for Certain Unmanned Aircraft Systems (Section 44807), FEDERAL AVIATION ADMINISTRATION, https://www.faa.gov/uas/advan ced_operations/section_333/ (last updated Dec. 14, 2018). 99 Commercial use of drones is governed by the “Part 107” rules. See 14 C.F.R. pt. 107. Under these rules, an operator must obtain a Remote Pilot Certificate or be under the direct supervision of someone who holds such a certificate, register the UAS with the FAA, and adhere to a set of operating guidelines, including: (1) fly the drone at or below 400 feet; (2) keep the drone within the operator’s line of sight; (3) be aware of FAA Airspace Restrictions; (4) respect privacy; (5) do not fly near other aircraft, especially near airports; (6) do not fly over groups of people, public events, or stadiums full of people; (7) do not fly near emergencies such as fires or hurricane recovery efforts; and (8) never fly under the influence of drugs or alcohol. See Getting Started, Unmanned Aircraft Systems, FEDERAL AVIATION ADMINISTRATION, https://www.faa.gov/uas/getting_started/ (last updated Oct. 18, 2018). Some of the activities listed above may be allowed after obtaining a waiver. See Certificated Remote Pilots including Commercial Operators, Unmanned Aircraft Systems, FEDERAL AVIATION ADMINISTRATION, https://www.faa.gov/uas/commercial_operators/ (last updated Dec. 18, 2018). 100 Pub. L. No. 115–254, § 349, 132 Stat. 3,186, 3,297-98 (2018) (codified at 49 U.S.C. § 44809). https://www.faa.gov/uas/advanced_operations/section_333/ https://www.faa.gov/uas/advanced_operations/section_333/ https://www.faa.gov/uas/getting_started/ https://www.faa.gov/uas/commercial_operators/ 46 feet above the surface; pass an aeronautical knowledge and safety test; and register the UAS with the agency.101 Operators who comply with the Exception do not need to get pre-approval from the FAA or a Remote Pilot Certificate.102 Because the FAA is primarily tasked with enforcing the safety of public airways, federal law does not touch upon issues of privacy implicated by drone use.103 The federal government has instead left this area of lawmaking to individual states. Once you have verified that your drone complies with the relevant federal laws and regulations, you should determine whether your state has passed any drone privacy laws. b. State Statutes—Drone Privacy Laws Many states have passed statutes pertaining to drone usage and privacy. To date, 27 states impose criminal liability for unlawful drone usage, including the unauthorized surveillance of individuals and certain types of industrial facilities (see Subsection 6 below). In addition, North Carolina provides a civil cause of action against drone surveillance of persons or private real property without consent.104 For an overview of these states, including their specific language, see Appendices 1 and 2. Importantly, these laws apply to drone photography rather than ordinary handheld photography.105 In most instances, taking pictures and video on your own, without drone assistance, will be less susceptible to legal challenges. Accordingly, you should ascertain whether 101 49 U.S.C. § 44809(a). 102 See id.; see also Recreational Fliers & Modeler Community-Based Organizations, Unmanned Aircraft Systems, FEDERAL AVIATION ADMINISTRATION, https://www.faa.gov/uas/recreational_fliers/ (last updated Jan. 30, 2019). 103 Patrice Hendriksen, Note, Unmanned and Unchecked: Confronting the Unmanned Aircraft System Privacy Threat Through Interagency Coordination, 82 GEO. WASH. L. REV. 207, 228-38 (2013). 104 N.C. Gen. Stat. § 15A-300.1(b) & (e). 105 Ark. Code § 5-16-101 is an exception. https://www.faa.gov/uas/recreational_fliers/ 47 your state has passed a drone privacy law before using a drone for data gathering and consider alternative methods of gathering the information you seek. 6. Common Law Causes of Action Even when states have not adopted drone privacy laws, civil common law causes of action against drone use may apply. These include nuisance, trespass, and privacy. a. Nuisance A private nuisance claim is typically brought when a landowner’s quiet enjoyment of his or her land is disturbed.106 To succeed on a nuisance claim, a plaintiff must prove: (i) substantial harm; and (ii) that the imposition of the harm is unreasonable.107 To date, we are not aware of any cases in which a plaintiff has brought a common law nuisance claim against a drone operator; however, plaintiffs have brought common law nuisance claims and succeeded against airplane operators.108 Most of these claims have depended on factors like dust production, noise, vibration, and flight frequency.109 While each of these factors would likely be considered in the context of a nuisance claim brought against a drone operator, they are arguably less applicable to drones than to airplanes. After all, drones produce significantly less dust, noise, and vibrations than airplanes. There are various steps you can take to avoid claims of nuisance. For example, you can avoid flying your drone over the same space with great frequency. In addition, you can determine whether the noise emitted by your drone exceeds your locality’s noise ordinances, which often outline acceptable levels of noise by property type and time of day. Many localities make this information available online.110 106 Restatement (Second) of Torts § 821D. 107 Id. 108 See Michelle Bolos, A Highway in the Sky: A Look at Land Use Issues that will Arise with the Integration of Drone Technology, 2015 U. ILL. J.L. TECH. & POL’Y 411, 422 (2015). 109 See Jack L. Litwin, Airport Operations or Flight of Aircraft as Nuisance, 79 A.L.R.3d 253 (Originally published in 1977). 110 See, e.g., Noise Control, Cambridge, Massachusetts, Municipal Code ch. 8.16, https://www.municode.com/library/ma/cambridge/codes/code_of_ordinances?nodeId=TIT8HESA_CH8.16NOCO& searchText= (last visited Feb. 7, 2019). https://www.municode.com/library/ma/cambridge/codes/code_of_ordinances?nodeId=TIT8HESA_CH8.16NOCO&searchText https://www.municode.com/library/ma/cambridge/codes/code_of_ordinances?nodeId=TIT8HESA_CH8.16NOCO&searchText 48 b. Trespass Operating a drone over someone’s private airspace may also constitute common law trespass. This is less likely than in the case of physical, ground-level trespass because ownership of airspace above a property is not as clearly established. Landowners own as much space above the ground as can be reasonably used in connection with the land.111 What constitutes reasonable use of this airspace remains uncertain; however, one thing is clear: a person’s ownership of airspace above a property is not infinite. At some point, the airspace is in the public domain. Ultimately, the higher you fly your drone, the less likely you are to commit a trespass.112 Recall, however, that federal law includes explicit height limitations for drone operators—recreational users under the Exception and commercial users under the Part 107 rules must operate the drone below 400 feet. 111 United States v. Causby, 328 U.S. 256, 264 (1945). 112 The Restatement (Second) of Torts provides an indication as to how this rule might be interpreted: “In the ordinary case, flight at 500 feet or more above the surface is not within the ‘immediate reaches,’ while flight within 50 feet, which interferes with actual use, clearly is, and flight within 150 feet, which also so interferes, may present a question of fact.” See Restatement (Second) of Torts § 159, Comment on Subsection (2). You should remain attentive to any developments in this area occurring after the publication of this manual. To that end, many online blogs and journals offer up-to-date posts on major developments in drone law. See, e.g., DRONE LAW JOURNAL, http://dronelawjournal.com/ (last visited Feb. 7, 2019); Drone Law Blog, RUPPRECHT LAW P.A., http://jrupprechtlaw.com/drone-law-blog (last visited Feb. 7, 2019). http://dronelawjournal.com/ http://jrupprechtlaw.com/drone-law-blog 49 c. Privacy A final type of claim worth mentioning in relation to drone use is common law privacy, which has already been covered in this chapter. Many of the suggestions relating to privacy that were previously given are equally applicable in the context of drone use: avoid flying your drone near private residences; try to maintain a healthy distance and keep flight frequency to a minimum; and make sure to notify any local residents of your citizen science project before commencing drone operation. If your drone carries a camera, you should avoid taking and, in particular, publishing pictures of people on their private property. 7. Critical Infrastructure Laws As discussed above, critical infrastructure laws provide heightened penalties for individual trespass and unlawful drone surveillance of certain industrial, agricultural, and government-owned facilities (i.e., critical infrastructure). This term, and synonymous statutory phrases, often encompass various sites that may be of interest to citizen scientists. For example, under Arizona’s critical infrastructure drone use law, “critical facility” includes, but is not limited to: “(a) A petroleum or alumina refinery; (b) A petroleum, chemical or rubber production, transportation, storage or processing facility; (c) A chemical manufacturing facility; (d) A water or wastewater treatment facility and water development, distribution or conveyance system, including a dam; (e) An electric generation facility, . . . and any associated substation or switchyard; an electrical transmission or distribution substation; (f) An electrical transmission line of at least sixty-nine thousand volts; an electronic communication station or tower;… (i) An energy control center; (j) A distribution operating center; (k) A facility that transfers or distributes natural gas, including a compressor station, regulator Summary: Critical infrastructure laws heighten penalties for individual trespass and unlawful drone use (see Subsections 1 and 5 above). Because state critical infrastructure law is still developing, you should routinely check state laws. Such research is especially important when you are initially choosing the site of your citizen science project (as discussed in Chapter 1) and the methods of observation. You should carefully consider whether such restrictions necessitate an attenuated sampling location or different site altogether. 50 station, city gate station or pressure limiting station or a liquefied natural gas facility or supplier tap facility; (l) Any railroad infrastructure or facility; ….”113 It is extremely important to check your state law’s definition of “critical infrastructure” as the covered facilities differ state-by-state, and even between trespass and drone laws within the same state. For example, while Minnesota’s critical infrastructure trespass law applies to belowground pipelines housed in underground structures,114 Nevada’s critical infrastructure drone use law explicitly excludes “any facility or infrastructure of a utility that is located underground.”115 Moreover, as these types of laws are grounded in national security concerns and protecting the continued provision of public services, penalties under these provisions can be significant. For example, violation of Arizona’s critical infrastructure drone use law is a class 6 felony punishable by a maximum fine of $150,000, imprisonment not to exceed 1.5 years, or both.116 To avoid these legal dangers, it is essential that you research the critical infrastructure laws in your state before choosing the site of your citizen science project and the means of observation. a. Trespass To date, fourteen states impose a heightened penalty when someone trespasses on critical infrastructure. These states include Alabama, Arizona, California, Indiana, Iowa, Kentucky, Louisiana, Michigan, Minnesota, Nebraska, New Jersey, North Carolina, Texas, and Wisconsin. California’s critical infrastructure trespass statute is unique as the law makes it “unlawful to loiter in the immediate vicinity of any posted property.”117 “Posted property” includes oil wells, gas plants, reservoirs, dams, and sanitary sewage and waste water treatment facilities, among other covered entities.118 While this statute does not separately define “loitering,” another part of the California criminal code defines the term as “to delay or linger without a lawful purpose for being on the property and for the purpose of committing a crime as 113 Ariz. Rev. Stat. § 13-3729(F)(3). 114 Minn. Stat. § 609.6055. 115 Nev. Rev. Stat. § 493.020(2). 116 Ariz. Rev. Stat. §§ 13-3729(E), 13-702(D); 13-801(A). 117 Cal. Penal Code § 555.2. 118 Id. § 554. 51 opportunity may be discovered.”119 This definition limits liability to instances when the person is lying in wait to commit a separately criminalized offense. Accordingly, it is unlikely that a citizen scientist will be found liable under this section unless they loitered with the intent to commit a separate criminal act (see Subsection 2). If your project has identified a power plant, refinery, distribution utility, or other such facility as its site of interest, you will want to take extra care to identify whether your state has a critical infrastructure trespass statute. For this, you can refer to the comprehensive state spreadsheet and summaries in Appendices 1 and 2. However, please recognize that the laws in this area are changing rapidly, so you will need to double-check the current accuracy of the Appendices before you rely on them. On a positive note, state laws that have heightened criminal sanctions for trespass on critical infrastructure typically include a notice requirement, meaning liability for trespass occurs when someone has (i) crossed a fence or passed a “no trespassing” sign to get to a sample collection site or (ii) received personal notice to leave the premises from the property owner and refused to leave.120 Therefore, in those states, you will not be liable for critical infrastructure trespass as long as those situations do not apply to you. b. Drone Use The most common and potentially problematic drone privacy statutes prohibit the use of a drone to surveil the operations of critical infrastructure. Arizona, Arkansas, Delaware, Florida, Kentucky, Louisiana, Nevada, New Jersey, Oklahoma, Oregon, Tennessee, and Texas each have such a law. Arkansas’s law provides that: “A person commits the offense of unlawful use of an unmanned aircraft system if he or she knowingly uses an unmanned aircraft system to conduct surveillance of, gather evidence or collect information about, or photographically or electronically record critical infrastructure without the prior written consent of the owner of the critical infrastructure.”121 While the definition of critical infrastructure varies by state, as discussed above, it generally includes power plants, refineries, public utilities, etc. If the target site of your project fits this 119 Id. §§ 647(h), 653.20(c). 120 See, e.g., Ala. Code § 13A-7-4.3(b). 121 Ark. Code § 5-60-103(b). 52 description, you will generally want to avoid the use of a drone to take pictures or video of that site. Some drone privacy laws contain exceptions. The Arkansas statute excerpted above, for example, provides an exception for “[a]n unmanned aircraft system used under a certificate of authorization issued by the Federal Aviation Administration.”122 Certificates of authorization, however, are available only to public operators of UAS (e.g., state or local governments).123 A few states, including Louisiana124 and Texas,125 have exceptions for UAS that are flown by universities for research or educational purposes. Some drone privacy statutes only impose liability for drone surveillance in furtherance of a criminal offense. Arizona’s for example, states that “[i]t is unlawful for a person to operate or use an unmanned aircraft or unmanned aircraft system to intentionally photograph or loiter over or near a critical facility in the furtherance of any criminal offense.”126 Thus, this statute is presumably inapplicable to drone usage around critical infrastructure in Arizona so long as the conduct does not further a criminal offense, such as trespass. There is not yet any court interpretation of the law, however. 8. Agency Regulations If your project’s site of interest is on public property (e.g., a National or State Park), you should first identify which agency manages that property. You should then locate that agency’s 122 Ark. Code § 5-60-103(a)(2)(B)(v). 123 Certificates of Waiver or Authorization (COA), FEDERAL AVIATION ADMINISTRATION, https://www.faa.gov/about/office_org/headquarters_offices/ato/service_units/systemops/aaim/organizations/uas/coa/ (last visited Feb. 7, 2019). 124 La. Stat. Ann. § 14:337(D)(2). 125 Tex. Gov’t Code § 423.002(a)(1). 126 Ariz. Rev. Stat. § 13-3729(B). Summary: This subsection pertains only to public property, which is managed by different agencies at several levels of government. If your project’s site of interest is on public property, you should first identify which agency manages that property. You should then locate that agency’s regulations to identify the permitted uses of that property. Oftentimes, personally contacting the agency is good way to learn about permitted uses of its properties. https://www.faa.gov/about/office_org/headquarters_offices/ato/service_units/systemops/aaim/organizations/uas/coa/ 53 regulations and policies to identify the permitted uses of that property. Laws, regulations, and policies related to drone use and scientific collection in State Parks are noted in Appendix 2. Some agencies are very permissive with respect to the public’s use of their lands. However, as agency regulations and policies evolve, especially with regard to drone use, you should make sure to contact an agency representative for comprehensive and up-to-date guidance. Many agencies allow collection on public land as long as the land’s natural resources are not significantly disturbed or damaged. The Bureau of Land Management, for example, does not require a permit for “casual uses” of the lands it manages.127 “Casual use” is defined as “any short term non-commercial activity which does not cause appreciable damage or disturbance to the public lands, their resources or improvements, and which is not prohibited by closure of the lands to such activities.”128 Thus, if your research does not noticeably damage the Bureau of Land Management’s lands, then, you should be able to conduct research on this land without fear of repercussion. Likewise, the United States Forest Service permits data collection that does not cause appreciable damage. For example, it allows: “[t]he collection of minor forest products, such as flowers, plants, berries, acorns, nuts, or small amounts of medicinal roots, from areas other than designated recreation, research, natural, or other areas closed to such activities. However, such collections are limited to reasonable quantities for personal use; there can be no disturbance of surface resources; and the products must not be protected by Federal or State laws or regulations.”129 Of course, not all agencies will make guidance materials available to the public, nor will those materials always be clear. In the above excerpted regulation from the Forest Service Manual, for example, you may have questions as to what constitutes “reasonable quantities for personal use” or “disturbance of surface resources.” The answers to these questions might affect the extent of sample collection you feel comfortable conducting in national forests. If you encounter any ambiguity like this in your background research, a logical first step is to contact the agency directly for clarification. Generally, an agency’s contact information is available on its website. Your 127 43 C.F.R. § 2920.1–2(a). 128 43 C.F.R. § 2920.0–5(k). 129 FOREST SERVICE MANUAL § 2719(9) (2014), https://www.fs.usda.gov/Internet/FSE_DOCUMENTS/fseprd52645 5.pdf. https://www.fs.usda.gov/Internet/FSE_DOCUMENTS/fseprd526455.pdf https://www.fs.usda.gov/Internet/FSE_DOCUMENTS/fseprd526455.pdf 54 inquiry should be as specific as possible. While the response will not constitute binding legal advice, it will often be the most authoritative feedback you can get on the particular rules governing publicly-held property. CHAPTER 5: INFORMATION GENERATION – DESIGN OF SAMPLE COLLECTION, SAMPLE ANALYSIS, AND DATA INTERPRETATION METHODOLOGIES Why You Should Read this Chapter: Most citizen science projects that you join or initiate will require generating information that was previously uncollected, unknown, unreported, or unestablished in the realm of public knowledge. Because most projects will involve this type of “information generation,” it is important, and often critical, to your long-term success to think about how you will perform: (i) sample collection (i.e., how will you gather samples of air, water, soil, etc.?); (ii) sample analysis (i.e., how will you examine the samples you collect?); and (iii) data interpretation (i.e., how will you interpret the results of your sample analyses?). Graphic Legend: Your purpose for generating information might vary over time. For example, you might be interested in performing a preliminary site evaluation before beginning a detailed evaluation. Regardless of your purpose for generating information, it can be helpful to consider various technical concerns that can impact the quality of the information that you generate before you begin your field work. 55 Introduction The focus of this chapter is to help you generate high quality information. For some, this may seem like a daunting process. We emphasize that even if it is currently too difficult or expensive for you to comply with the most stringent state or federal quality assurance requirements, any information that you generate can have some use (discussed in Chapter 2). Indeed, in some instances this information could – and perhaps should – still suffice to trigger agency action. In this way, you can play the critical role of alerting the agency to potential environmental problems and enabling the agency to follow-up by utilizing appropriate information collection protocols. Nonetheless, understanding how the design and performance of your project impacts information quality will help assure that your project ultimately meets your goals. As discussed previously, the use of citizen scientist-generated information can be limited by the information’s quality (discussed in Chapter 2). At one extreme, state and federal agency regulations require that only high quality information be used to form the underpinnings of their actions (see Appendices 1 and 2). For example, the Minnesota Pollution Control Agency requires that citizen monitoring data meet the credibility requirements established in its “Volunteer Surface Monitoring Guide” when implementing the state clean water act.130 Likewise, many federal regulations include specific requirements to assure information quality. Although these requirements vary in different contexts, EPA-funded programs generally require the preparation of an EPA-approved Quality Assurance Protection Plan (“QAPP”) before people begin collecting samples.131 Ultimately, high quality information has the highest utility or usefulness. Therefore, this discussion explains several technical suggestions that can increase the quality of the information you generate. In particular, we distill general suggestions that the EPA has established to promote 130 Minn. Stat. § 114D.20, subd. 3(2). 131 See U.S. ENVTL. PROTECTION AGENCY, CIO 2105.0 (May 5, 2000), https://www.epa.gov/sites/production/files/2 013-10/documents/21050.pdf; see also Quality Assurance Project Plan for Citizen Science Projects, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/citizen-science/quality-assurance-project-plan-citizen-science-projects (last visited Feb. 7, 2019). https://www.epa.gov/sites/production/files/2013-10/documents/21050.pdf https://www.epa.gov/sites/production/files/2013-10/documents/21050.pdf https://www.epa.gov/citizen-science/quality-assurance-project-plan-citizen-science-projects 56 information credibility and provide you with supplemental resources for additional information. We draw upon public EPA documents including “The Volunteer Monitor’s Guide to Quality Assurance Project Plans,” “The Quality Assurance Template for Citizen Science Projects,” and “Guidance on Choosing a Sampling Design for Environmental Data Collection.”132 Other resources, such as the Federal Crowdsourcing and Citizen Science Toolkit,133 are available to aid citizen scientists in the design of sample collection, sample analysis, and data interpretation methodologies. 132 See U.S. ENVTL. PROTECTION AGENCY, QUALITY ASSURANCE TEMPLATE FOR CITIZEN SCIENCE PROJECTS (Apr. 2013), https://www.epa.gov/sites/production/files/2016-06/documents/quality_assurance_template_for_citizen_scie nce.pdf; U.S. ENVTL. PROTECTION AGENCY, GUIDANCE ON CHOOSING A SAMPLING DESIGN FOR ENVIRONMENTAL DATA COLLECTION, EPA/240/R-02/005 (Dec. 2002), https://www.epa.gov/sites/production/files/2015- 06/documents/g5s-final.pdf; U.S. ENVTL. PROTECTION AGENCY, THE VOLUNTEER MONITOR’S GUIDE TO QUALITY ASSURANCE PROJECT PLANS, EPA 841-B-96-003 (Sept. 1996), https://www.epa.gov/sites/production/files/2015- 06/documents/vol_qapp.pdf [hereinafter “EPA Volunteer Monitor’s Guide to Quality Assurance”]. 133 Federal Crowdsourcing and Citizen Science Toolkit, CITIZENSCIENCE.GOV, https://www.citizenscience.gov/toolk it/# (last visited Feb. 7, 2019). https://www.epa.gov/sites/production/files/2016-06/documents/quality_assurance_template_for_citizen_science.pdf https://www.epa.gov/sites/production/files/2016-06/documents/quality_assurance_template_for_citizen_science.pdf https://www.epa.gov/sites/production/files/2015-06/documents/g5s-final.pdf https://www.epa.gov/sites/production/files/2015-06/documents/g5s-final.pdf https://www.epa.gov/sites/production/files/2015-06/documents/vol_qapp.pdf https://www.epa.gov/sites/production/files/2015-06/documents/vol_qapp.pdf https://www.citizenscience.gov/toolkit/ https://www.citizenscience.gov/toolkit/ 57 Assessing Information Quality When you present information that you have collected or generated (e.g., a summary of your tests of the water quality in a stream) to a decision maker, he or she must assess the quality of the information without having a chance to perform his or her own data collection or testing. Instead, decision makers often look for “indicators” of high quality data. Examples include: precision, accuracy, representativeness, completeness, comparability and instrumentation. Therefore, by considering these elements as you design and conduct your project, you will increase both your confidence in the information that the project generates and the ability Making Connections Between Chapters: Before designing your data collection, sample analysis, or data interpretation methodologies, it is helpful to review key points from the preceding chapters: • What is your site of interest and which pollutant or combination of pollutants will you be examining? (discussed in Chapter 1). • Who will use the information you collect and for what purpose? (e.g., what legal standards might limit the use of information you generate) (discussed in Chapter 2). • What is known about the pollutant or combination of pollutants you will be examining? (e.g., stability, detection limits, detection methodologies, environmental baseline levels, reporting thresholds, etc.) (discussed in Chapter 3). • What is already known about the source of the pollutant of which you are concerned? (e.g., the source’s current permit requirements and compliance records) (discussed in Chapter 3). • What are potential sources of liability to which you might be exposed when collecting the information (e.g., trespass, stalking, etc.) (discussed in Chapter 4). Answering these questions will shed light on the type and quality of information that is currently lacking (e.g., information that you may seek to generate) and how to acquire the information. Indicators of Quality Data 1) Precision 2) Accuracy 3) Representativeness 4) Completeness 5) Comparability 6) Instrumentation 58 of a decision maker to consider and rely on your findings. The indicators of quality data are each discussed below. Precision relates to the degree of agreement (i.e., similarity) between (i) multiple measurements taken from a single sample or (ii) measurements taken from multiple samples collected as close together in time and place as possible. Collecting multiple independent samples from a single site at roughly the same time in the same manner (i.e., “replica” samples) and analyzing the samples at the same time and in the same manner, allows for robust statistical calculations of precision (e.g., calculation of standard deviation, standard error, or relative percent difference). A high level of precision suggests that your sampling and testing methods are consistent and can be reproduced; this is an indication of high quality information. Accuracy ensures that your data represents reality. You can facilitate the measurement of accuracy by collecting quality control samples that have known values. Examples of various quality control samples are discussed in greater detail in the next section of this chapter. Quality control samples should be collected along with, and in ways that mimic your collection of field samples, and they should be analyzed using the same instrumentation. When the values reported from the control samples consistently and precisely reflect their known values, it suggests that the accuracy of your field samples is high; this is an indication of high quality information. Representativeness relates to whether a sample collected from a site is actually representative of that site. Here, the central concern is to avoid biases in the generated information. How, when, where, and by whom samples are collected will influence the representativeness of your information. For example, if you are collecting samples to determine the typical concentration of a pollutant in a stream, the following factors could bias your results: • How: the samples were collected with unclean tools. This creates a risk of bias because any pollutant detected in the analysis of the samples may have actually arisen from the unclean tools. • When: the samples were collected just after heavy rainfalls. This may create a risk of bias because various pollutants that are not normally in the river might be washed there from various sources due to the rain. Note: this risk of bias would not be present if rain is typical of the location studied or, alternatively, if you were interested in determining the concentration of a pollutant in a stream following heavy rainfalls. 59 • Where: the samples were collected just below a pipe outfall that is entering the stream. This creates a risk of bias because the concentration of pollutant just below the pipe will be higher than the concentration of pollutant in the stream generally. Note: this risk of bias would not be present if you were interested in determining the concentration of a pollutant just below the pipe or, alternatively, if you were interested in determining the abundance of pollution entering the stream from the pipe. • By Whom: the samples were collected by a person untrained in proper sampling technique. This creates a risk of bias because it will be less certain that the samples were collected properly (i.e., in a way that is representative). As demonstrated in these examples, what constitutes a bias that impacts representativeness may be different in each situation. Completeness involves a comparison of the number of measurements you originally planned to collect (i.e., the number that you anticipated would be necessary for the information to be useful) and the number that you actually collected. Collecting more samples than you think will be necessary can help assure information completeness; this is an indication of high quality information. Comparability refers to the relationship between results of multiple studies or a single study over time. Multiple studies that report similar conclusions suggests that data quality is high. Moreover, information reported from a single study that presents realistic results over time (e.g., consistent, gradual changes, or explainable rapid changes) is of higher quality than information reported from a single study that presents sporadic, unexplained fluctuations in values. 60 Instrumentation used to analyze the samples you collect can also impact the quality of the generated information. Each analytical instrument has a range of values, such as the amount of a pollutant in a sample, which it can detect in a reliable manner. If the presence of a pollutant in a sample (sometimes referred to as an analyte abundance) is below the instrument’s lowest detection limit (i.e., limit of blank, limit of detection, or limit of quantitation) the pollutant’s presence will be reported with a value of zero, or less than zero. If the presence of a pollutant in a sample is greater than the instrument’s highest quantifiable limit, the pollutant’s presence will be reported with a value that is no greater than the instrument’s maximum reportable value. As readings approach these detection limits, they become less reliable. In short, if reported values fall within an instrument’s measurement range, it suggests that the values are reliable, which is an indication of high quality information. Information Quality Needs Can Change Over Time: Your anticipated use of the information can change over the lifetime of your project, causing its information quality requirements to increase or decrease (see Chapter 2). Your purpose for collecting data can change over time. For example, your project might originally be directed at monitoring a currently unthreatened natural resource to facilitate a rapid response to any potential increases in pollution. The information quality that you seek may change if a pollution increase is detected. Likewise, you might perform a general preliminary site survey to verify the identity of a potential pollutant or pollutant source before performing a detailed site evaluation. A preliminary site evaluation can include documentation of evidence of: the scent of air at the site of interest; oil slicks on the surface of water; stained soil or pavement; stressed vegetation on land or in water; solid waste (e.g., mounds or depressions suggesting solid waste disposal); wastewater entering a stream; or unmaintained septic systems. In some instances, you might collect and analyze a few field samples from the site to identify pollutants on the site. Perhaps, in this instance, the information quality that you seek will increase after the pollutant or pollutant site has been verified. Ultimately, information generation is, in many instances, an iterative process, so the type of information that you seek to generate can change over time. 61 General Quality Assurance Protection Plan Guidelines A Quality Assurance Protection Plan (“QAPP”) is a formal document that describes how a project will achieve its information quality requirements. In other words, a QAPP lists the quality assurance mechanisms that will be used to assure that the information generated by the project meets the quality criteria discussed above. Importantly, this document is prepared prior to any sample collection. Ultimately, the QAPP is a project feature that decision makers will use to assess the overall quality of the generated information. Preparing a QAPP is part of a project’s quality assurance (“QA”) activities. (Another term you may see is quality control (“QC”), which refers to the overall system of technical activities that are designed to measure the quality of information.) Although the EPA lists twenty-four distinct issues that can be addressed in a QAPP, we focus here on various themes that we deem especially important and useful in the context of citizen science projects: (i) management description, (ii) sampling design, (iii) sample collection methodology, (iv) sample handling and custody, (v) sample analysis, (vi) quality controls, and (vii) data interpretation.134 We stress that the nature or type of pollutant and the pollutant source heavily dictate the content of the QAPP. The EPA has issued a vast number of very specific and detailed protocols for the measurement of pollutants in various contexts (i.e., “EPA Reference Methods” or “EPA Standard Protocols”). A collection of these methods and protocols can be found on EPA’s website.135 They delineate detailed descriptions of accepted sampling methodologies, quality controls, instrumentation functionalities, etc. Including this level of detail here is impractical. Instead, we offer broad, generalizable suggestions and provide additional resources for those who seek greater detail for their individual project needs.136 134 See EPA Volunteer Monitor’s Guide to Quality Assurance, supra note 132, at 23. 135 Collection of Methods, U.S. ENVTL. PROTECTION AGENCY, https://www.epa.gov/measurements/collection- methods (last visited Feb. 7, 2019). 136 Id. Prepare or review a project’s QAPP before collecting samples or information. Put your QAPP into a written format that can be shared with volunteers and decision makers. Key elements of QAPPs 1. Management description 2. Sampling design 3. Sampling collection 4. Sample handling & custody 5. Sample analysis 6. Quality controls 7. Data interpretation https://www.epa.gov/measurements/collection-methods https://www.epa.gov/measurements/collection-methods 62 9. Project Management Description While some projects are small enough that a single person can successfully complete them, many will require the coordinated efforts of many individuals. Indeed, the most successful projects may involve a “community” of individuals. When projects involve groups of individuals, establishing and describing management roles at the onset of the project is important for ensuring project consistency and cohesiveness. Project managers must (among many other responsibilities): (i) identify funding resources and control expenditures of funds; (ii) establish what, when, how, and by whom samples will be collected, analyzed, and interpreted; (iii) ensure that volunteers understand how to clean and calibrate instrumentation; and (iv) assure, if needed, the proper training of those involved in the project (e.g., in proper sample collection) and otherwise ensure information quality. Project managers should also seek to maximize the use of community expertise. For example, even if you lack the training or expertise to design or complete a project, your community may include individuals with technical or scientific training who are willing and eager to participate (e.g., teachers or professors, scientists and engineers, or even members of environmental agencies). 10. Sampling Design Sampling design includes considering the types of samples that will be collected and when and where they will be collected. Sampling design decisions implicate multiple factors that impact information quality, but it is primarily concerned with the representativeness of the information. A well-developed sampling design plays a central role in ensuring that conclusions are adequately supported by data. Thinking about your sampling design at the beginning of a project can help avoid introducing bias at the onset of information generation. Avoiding bias is important; as the saying goes, “Garbage in, is garbage out.” In some aspects, your sampling design will be dependent on the type of sample you are collecting. For example, the placement of air monitors depends on the sampling objective: ground level monitoring, air mass (i.e., circulating air), or source-oriented (e.g., as the air exist a smoke stack), and it is important for air flow around the monitor to be representative of the general air flow in the area to prevent sampling bias. Likewise, water and soil sampling designs can include details concerning the location and depth at which samples will be collected. When contemplating 63 the types of samples that will be collected, you should consider the chemical/physical properties of the pollutant and the potential source of the pollutant (discussed in Chapter 3). The sampling design should include documentation of when and where samples will be collected, including, for example, the following types of information: • The number of times that a sample will be collected per week, month or year; • The duration of the sampling program (e.g., the period of time during which samples will be collected); • At what time of the day or night the samples will be taken (e.g., during or after an industrial facility’s hours of operation); • How weather will impact sample collection (e.g., will samples be collected during rain, wind, or unusual temperature events); and • Where samples will be collected. The chemical/physical properties of the pollutant and the source of the pollutant, along with potential sources of liability (discussed in Chapter 3), should be central to determinations of where to collect samples. Addressing these issues will help reduce potential bias in the ultimate conclusions and promote the quality of the information generated in a project. Selecting sampling locations typically involves one of two approaches: (i) random or probabilistic sampling and (ii) judgmental sampling. While each approach has advantages and disadvantages that can be discussed at length, this discussion merely serves to introduce the topics. In random sampling, as its name implies, sampling locations are chosen randomly. It is most useful when the pollutant of interest is relatively homogeneous in the sampling medium (i.e., it is uniformly distributed, and thus, there are no expected “hot spots”). Because citizen science projects concerned with environmental problems often focus on a pollutant source, random sampling may be less commonly used relative to judgmental sampling. Judgmental sampling, as its name implies, involves the selection of sampling locations based on judgment. Judgmental sampling is most useful when there is historical or physical knowledge of the feature or condition under investigation: for example, when the impact of the pollutant can be visually discerned or when the location of pollutant release is known. Ultimately, the sampling design should match the needs of the project with the resources available (e.g., recognizing constraints of resources related to finances, time, expertise, and geographic access). 64 11. Sample Collection Methodology A well-designed sample collection methodology helps ensure the precision and accuracy of the information that is ultimately generated. The primary question addressed by a sample collection methodology is: how will samples be collected during each sampling event (e.g., site visit)? The answer to this question may include, among other things, a description of: (i) the number of samples to be collected during each sampling event (i.e., the number of “replica” samples that will be collected); (ii) how samples will be taken; (iii) the equipment and containers used to collect the samples (e.g., their composition and procedures for their decontamination); and (iv) holding time length (i.e., the time between taking samples and analyzing them). Some aspects of sample collection methodologies are highly generalizable across projects. For example:137 • Sample collection should be documented (e.g., time, place, name of collector, equipment used, etc.). • The collector should wear “a clean pair of new, non-powdered, disposable gloves each time a different location is sampled and the gloves should be donned immediately prior to sampling. The gloves should not come in contact with the media being sampled and should be changed any time during sample collection when their cleanliness is compromised.”138 • The collection equipment should be clean and sterilized. • “Sample collection activities shall proceed progressively from the least suspected contaminated area to the most suspected contaminated area.”139 Samples that are expected to contain high levels of contaminated media should be kept separate from samples thought to contain low levels of contaminated media. 137 See, e.g., U.S. ENVTL. PROTECTION AGENCY REGION 4, SESD OPERATING PROCEDURE: SOIL SAMPLING, SESDPROC-300-R3 (Aug. 2014), https://www.epa.gov/sites/production/files/2015-06/documents/Soil- Sampling.pdf [hereinafter, “EPA Soil Sampling Procedure”]; U.S. ENVTL. PROTECTION AGENCY REGION 4, SESD OPERATING PROCEDURE: SURFACE WATER SAMPLING, SESDPROC-201-E3 (Feb. 2013), https://www.epa.gov/sites/production/files/2015-06/documents/Surfacewater-Sampling.pdf [hereinafter, “EPA Water Sampling Procedure”]; U.S. ENVTL. PROTECTION AGENCY REGION 4, SESD OPERATING PROCEDURE: PORE WATER SAMPLING, SESDPROC-513-R2 (Feb. 2013), https://www.epa.gov/sites/production/files/2017- 07/documents/pore_water_sampling513_af.r3.pdf; U.S. ENVTL. PROTECTION AGENCY REGION 4, SESD OPERATING PROCEDURE; GROUNDWATER SAMPLING, SESDPROC-301-R3 (Mar. 2013), https://www.epa.gov/sites/production/files/2015-06/documents/Groundwater-Sampling.pdf. 138 See, e.g., EPA Soil Sampling Procedure, supra note 137, at 8. 139 See, e.g., id. https://www.epa.gov/sites/production/files/2015-06/documents/Soil-Sampling.pdf https://www.epa.gov/sites/production/files/2015-06/documents/Soil-Sampling.pdf https://www.epa.gov/sites/production/files/2015-06/documents/Surfacewater-Sampling.pdf https://www.epa.gov/sites/production/files/2017-07/documents/pore_water_sampling513_af.r3.pdf https://www.epa.gov/sites/production/files/2017-07/documents/pore_water_sampling513_af.r3.pdf https://www.epa.gov/sites/production/files/2015-06/documents/Groundwater-Sampling.pdf 65 • “All . . . control samples shall be collected and placed in separate ice chests or shipping containers.”140 • “During sample collection, if transferring the sample from a collection device, make sure that the device does not come in contact with the sample containers.”141 • “All samples requiring preservation must be preserved as soon as practically possible, ideally immediately at the time of sample collection.”142 Other aspects of a project’s sample collection methodology may be specific to the medium being sampled or type of instrument being used. For example, air sample collection methodologies are generally highly specific to the instrumentation used.143 Water and soil sampling designs, however, have various aspects that are more generalizable. Water samples should be collected with as little agitation to the water as possible. Wading or streamside sampling increases the probability of agitation. In instances when agitation is a concern, samples should be collected while facing upstream. Moreover, water sample containers should be filled to their capacity (i.e., no bubbles or headspace should be present after the container is capped). Unpreserved and preserved samples have holding times of one week and two weeks, respectively. (Holding times indicate the period during which the samples should be tested.) Soil samples must be “thoroughly mixed to ensure that the sample is as representative as possible of the sample media;” this rule does not apply if the soil sample will be analyzed for the presence of volatile organic compounds (“VOCs”).144 Moreover, the collector should “place the sample into an appropriate, labeled container(s) by using the alternate shoveling method and secure the cap(s) tightly. The alternate shoveling method involves placing a spoonful of soil in each container in sequence and repeating until the containers are full or the sample volume has been exhausted.”145 Unpreserved samples have a forty-eight-hour holding time.146 140 See, e.g., EPA Water Sampling Procedure, supra note 137, at 7. 141 See, e.g., id. at 8. 142 See, e.g., id. 143 See, e.g., U.S. ENVTL. PROTECTION AGENCY, LIST OF DESIGNATED REFERENCE AND EQUIVALENT METHODS (Dec. 2018), https://www.epa.gov/sites/production/files/2018-12/documents/amtic_list_dec_2018_update_1.pdf [hereinafter, “EPA List of Designated Reference and Equivalent Methods”]. 144 See EPA Soil Sampling Procedure, supra note 137, at 8. 145 Id. at 9. 146 Id. at 12. https://www.epa.gov/sites/production/files/2018-12/documents/amtic_list_dec_2018_update_1.pdf 66 Sample collection methodologies may also contemplate other ways of documenting sample collection. For example, a methodology could direct volunteers to photograph, videotape, or otherwise record the actual sample collection to demonstrate that the activity complies with the sample collection methodology. Typically, notes of visual and olfactory observations should be recorded in a log book to describe, for example, the depth of each sample, whether its color and texture, any odors, etc. The log can also be used for demonstrating sample handling and custody and any field analyses of the samples. 12. Sample Handling and Custody Precision and accuracy are the main information quality concerns addressed by the establishment of sample handling procedures. These procedures apply to projects that do not perform sample analysis in the field. In these instances, the samples must be transported to an alternative site, such as a laboratory. All samples should be properly labeled including: (i) the sample location; (ii) the date and time of collection; (iii) the sampler’s name; and (iv) whether the sample was preserved, and if so, how. Chain-of-custody procedures should be established to keep track of all samples that will be shipped or transported to a laboratory for analysis (i.e., documentation requirements for any changes in the handler of the sample or the sample’s storage location). This information is important for authentication of any information generated by analysis of the samples (discussed in Chapter 2). 13. Sample Analysis Analysis of samples may occur in the field or in a laboratory. In either case, the analytical methods and equipment used in the analysis should be documented. For example, if an EPA Reference Method or approved protocol is used, the method/protocol number should be listed; if the methodology differs from the Reference Method or approved protocol, list the ways in which it differs. In addition, documentation of instrumental calibration, inspection and maintenance should be provided. These procedures promote precision and accuracy of the data. Generally, analytical tools that are EPA approved are documented in the Federal Register. In some instances, the EPA provides lists of analytical tools that are EPA-approved when used in specific contexts.147 Other EPA approved devices can be found in EPA-approved operating procedures or reference methods (see Appendix 5). 147 See, e.g., EPA List of Designated Reference and Equivalent Methods, supra note 143. 67 14. Quality Control Samples The design of a project should include methods for collecting and testing quality control samples; examples include field controls, equipment controls, split samples, replica samples, and spiked samples. • A field control is a sample “collected” in the field that lacks a detectable quantity of the analyte of interest (i.e., the pollutant). While regular sample containers are filled with air, water, or soil from the field, a field control is filled in the same way but with air, water, or soil with a known composition that is brought to the site. If preservation steps are performed to the field samples, they should likewise be performed on the field control sample. • Equipment controls are samples used to verify the cleanliness of sample collection or analysis equipment. Generally, distilled water is used to test equipment’s cleanliness. • A split sample is one that is divided into two or more sample containers and subsequently analyzed independently. • Replica samples or duplicate samples are samples that are collected and analyzed at the same time and in tandem (i.e., they are representative of the same environmental condition). • Spiked samples are samples to which a known amount of the analyte has been added. Because the abundance of the analyte (i.e. pollutant) is known in each of these control samples, they are useful in assessing the precision and accuracy of the data that is ultimately generated. 15. Data Interpretation The project design should include considerations of how the data generated from sample analysis will be interpreted. It is from this interpretation that conclusions will be drawn. In some instances, you, the citizen scientist, may be able to interpret the data. However, as mentioned in Chapter 2, some uses of information generated from your project will require expert interpretation. When data is interpreted by a qualified expert, the quality of the information is enhanced. There are likely to be qualified experts in your community who are willing to assist you. Think about universities, community colleges, high schools, and locally-based environmental engineering companies. 68 CHAPTER 6: INFORMATION USE – MAKING THE MOST OUT OF YOUR INFORMATION General Suggestions After you, the citizen scientist, have put forth the effort to identify the problem (discussed in Chapter 1), to collect currently available pubic information (discussed in Chapter 3), and to generate new information (discussed in Chapter 5), you should put the results of your efforts to good use. As delineated in Chapter 2, there is a broad spectrum of potential uses of your information (e.g., to stimulate public awareness, to influence lawmaking, for enforcement Why You Should Read this Chapter: After all your efforts in carrying out your project, you should put your results to good use. Here we provide suggestions concerning the presentation and sharing of your information. Graphic Legend: After the information collection and generation phases of your projects are complete, it is time compile all of your results. Now is a good time to remind yourself of the various ways in which this information can be used. This reminder can help you put your results to good use. 69 mechanisms, etc.). There are various ways to make the most out of your information. Here, we provide a few suggestions. First, structure your information to make it presentable. Begin by considering ways in which you can present your work concisely and clearly to a broad audience. In many instances, simplicity empowers an argument. Translate your results into plain language and use graphs, tables, and other visualization techniques to facilitate emphasis and rapid understanding of your arguments. Next, consider your primary target audience. In some instances, this audience will require that the information be submitted in a certain format (e.g., documents submitted for court proceedings). Take time to research whether your information use has a formatting requirement. Importantly, when in doubt, seek outside advice and guidance. Second, use your information in any way you can. Although you may have begun your work as a citizen scientist with a specific use or goal in mind, consider other ways in which your information can be used. Maximize the value of your efforts by thinking creatively about other uses of your information. Finally, build upon the information that you have collected and generated. In some instances, you can consider collecting or generating more information to make your argument more sound and convincing with increased evidence. In other instances, your work may bring to light additional issues that merit exploration. Alternatively, you can provide opportunities for others to build upon your work by making your information as accessible as possible. For example, you can consider making your information publicly available on an internet platform. To some extent, this sharing can serve as a “peer-reviewing” mechanism. When other independent individuals reproduce your results, the credibility (i.e., quality) of your information increases. In this way, quantity can be equated with quality.',\n",
              " '5 WHAT IS A CO-RESEARCHER? In contrast to a professional researcher, it is unlikely that a co-researcher does research as their main job. Rather a co-researcher will bring their varied and valuable life experiences to a particular research project. You may have experience of a specific situation, health condition or disability. In this respect you are an expert by experience. Professional researchers will value your expertise, as you will bring an equal but di�erent perspective into the research project which will help to shape and deliver a piece of research. There is no substitute for getting those with first-hand experience of a situation, health condition or disability to share their knowledge with professional researchers and work together on a study. In practice the distinction between a professional researcher and a co-researcher can be blurred and some co-researchers may have had professional research training or have been in a paid academic role previously. However, many people come to the role of co-researcher having not worked within an academic environment before and academic experience is not expected. DIVERSITY We welcome the involvement of everyone with relevant expertise and experience to join the team as a co-researcher and value diversity in terms of disability, sex, gender reassignment, sexual orientation, pregnancy and maternity, race, religion or belief, marriage and civil partnership. You are also welcome if you are homeless, an asylum seeker, a migrant, a member of the travelling community, a carer, or care leaver. We value people’s experience because of their diverse backgrounds. This diversity is something we consider to be positive, and want to encourage, because it reflects the varied communities we live in and makes for richer, more relevant research. 6 SO WHY GET INVOLVED? BENEFITS TO YOU Personal benefits: * You may learn new skills and enjoy sharing your knowledge and skills with others. * It can feel good contributing to making a di�erence to the communities you care about. * It can be rewarding working with a team of people with a range of perspectives. * Involvement can give you a new purpose and make you feel more empowered. * It can be rewarding to see your contributions acknowledged (for example, in publications). * Involvement can build personal confidence and self-worth. * It can o�er opportunities to engage with lots of di�erent people. * It can stimulate your mind in a di�erent way. I fe\\ue16a\\ue171 \\ue18b\\ue188s\\ue179e\\ue18d\\ue16ad \\ue179\\ue18e an\\ue169 \\ue191\\ue16as\\ue175\\ue184\\ue182te\\ue169 It \\ue16be\\ue16a\\ue18bs \\ue184\\ue17d\\ue182it\\ue16e\\ue173\\ue186 \\ue180n\\ue169 re\\ue17c\\ue166\\ue191d\\ue188\\ue173\\ue186 w\\ue16de\\ue18d m\\ue17e \\ue16e\\ue183\\ue184as ar\\ue16a \\ue177\\ue184\\ue182og\\ue173\\ue16e\\ue192\\ue184d It i\\ue172\\ue18fr\\ue174\\ue17b\\ue184\\ue192 m\\ue17e se\\ue171\\ue185-es\\ue179\\ue16a\\ue184\\ue18c, co\\ue173 \\ue183\\ue16an\\ue168\\ue184 an\\ue169 \\ue183\\ue16eg\\ue173\\ue188\\ue193y It \\ue16ci\\ue195\\ue16as \\ue172\\ue184 \\ue192oc\\ue16e\\ue180\\ue171 op\\ue175\\ue174\\ue191t\\ue194\\ue173i\\ue193\\ue16e\\ue184s a\\ue173\\ue183 t\\ue16d\\ue16a c\\ue16da\\ue18dc\\ue16a \\ue179\\ue18e \\ue18ce\\ue16at \\ue18e\\ue179\\ue187er \\ue175\\ue16a\\ue18e\\ue18fle wi\\ue179\\ue187 l\\ue16e\\ue17b\\ue184\\ue183 ex\\ue175\\ue16a\\ue191\\ue188en\\ue168\\ue16a I fe\\ue16a\\ue171 \\ue196\\ue180n\\ue179e\\ue183 an\\ue169 \\ue18d\\ue16a\\ue184de\\ue169 7 It \\ue16be\\ue16a\\ue18bs \\ue16c\\ue18eo\\ue183 t\\ue174 ma\\ue170\\ue16a \\ue180 \\ue183i \\ue16ar\\ue184\\ue173\\ue182e t\\ue174 pe\\ue174\\ue175\\ue18b\\ue184’s \\ue171i\\ue195\\ue16as I en\\ue16f\\ue174\\ue198 wo\\ue177\\ue18a\\ue16en\\ue16c \\ue193\\ue18ege\\ue179\\ue187\\ue16ar as \\ue166 \\ue179\\ue184a\\ue18c Work related benefits: * We will cover all of your out-of-pocket expenses, and will be clear if the role is paid or voluntary. * You can use your existing skills and learn new ones, such as chairing meetings or preparing reports. * You can put your involvement in the project on your CV. * You can ask the professional researchers for a reference. * Being involved may lead to further opportunities to work on research projects. I ca\\ue173 \\ue18b\\ue16a\\ue180r\\ue173 ne\\ue17c \\ue196\\ue174r\\ue170 \\ue191\\ue184la\\ue179\\ue16a\\ue183 s\\ue170\\ue188\\ue18bl\\ue178 \\ue192uc\\ue16d as \\ue168\\ue187\\ue166\\ue188ri\\ue173\\ue186 \\ue166 m\\ue184e\\ue179\\ue16e\\ue18dg \\ue180\\ue173\\ue183 in\\ue179\\ue16a\\ue191v\\ue188e\\ue17c\\ue16e\\ue18dg \\ue178\\ue18a\\ue188l\\ue171\\ue192 I n\\ue169 i\\ue193 s\\ue179i\\ue18c\\ue17al\\ue180\\ue179i\\ue18dg \\ue166\\ue173\\ue183 in\\ue179\\ue16a\\ue191\\ue184s\\ue179i\\ue18dg It \\ue16da\\ue192 en\\ue168\\ue174\\ue194\\ue191ag\\ue16a\\ue169 \\ue18c\\ue184 to \\ue171\\ue174\\ue18e\\ue18a a \\ue16ar \\ue172\\ue198s\\ue184\\ue171\\ue185 an\\ue169 \\ue192p\\ue16a\\ue180\\ue170 u\\ue18f fo\\ue177 \\ue18cy\\ue178\\ue16a\\ue18bf. It \\ue16ci\\ue195\\ue16as \\ue172\\ue184 a \\ue192\\ue16an\\ue178\\ue184 of \\ue175\\ue17a\\ue191p\\ue18e\\ue178e \\ue166\\ue18dd \\ue172\\ue180\\ue18aes \\ue172\\ue16a fe\\ue16a\\ue171 \\ue184\\ue18cpo\\ue17c\\ue16a\\ue191\\ue184d I n\\ue169 i\\ue193 po\\ue178\\ue16e\\ue193\\ue188ve \\ue166\\ue173\\ue183 re\\ue17c\\ue166\\ue191d\\ue188\\ue173\\ue186 8 BENEFITS TO SOCIETY * You can improve things for others who are in a similar situation to yourself by shaping research to focus on things that matter most in the community. * Your involvement sends a clear message to society that communities and their members are valued for their skills and expertise. * The research you help to co-produce may provide information which can, in time, improve policies and services. * You can be a role model for other people in a similar situation, showing others that they too can get involved in research. * Your involvement can make the research more relevant to people in a similar situation to yourself. * Your involvement can make the research stronger and more likely to be implemented successfully. * The community can share ownership of the research. 9 I ca\\ue173 \\ue18fr\\ue174\\ue17b\\ue188\\ue183e in\\ue16b\\ue174\\ue191m\\ue180\\ue179i\\ue174\\ue18d t\\ue16d\\ue180\\ue193 ca\\ue173, in \\ue179\\ue16e\\ue18c\\ue184, im\\ue175\\ue191\\ue174v\\ue184 \\ue175o\\ue18b\\ue16ec\\ue188e\\ue178 \\ue166\\ue18dd \\ue178\\ue184\\ue191vi\\ue168\\ue16a\\ue192 I ca\\ue173 \\ue192p\\ue16a\\ue180\\ue170 u\\ue18f fo\\ue177 \\ue174\\ue193h\\ue184\\ue177 \\ue18fe\\ue174p\\ue171\\ue184 \\ue196it\\ue16d li\\ue17b\\ue16a\\ue183 \\ue184x\\ue175e\\ue191\\ue16e\\ue184n\\ue168e I ca\\ue173 \\ue16e\\ue18cp\\ue177\\ue18e\\ue195e t\\ue16de \\ue192\\ue16et\\ue194a\\ue179\\ue16e\\ue18e\\ue18d fo\\ue177 \\ue18f\\ue16a\\ue18ep\\ue171e in \\ue178\\ue16e\\ue18c\\ue188la\\ue177 \\ue182\\ue16er\\ue168\\ue194\\ue18cs\\ue179a\\ue18dc\\ue16a\\ue178 to \\ue172\\ue198s\\ue16a\\ue171\\ue185 10 WHAT YOU CAN EXPECT IF YOU GET INVOLVED As a co-researcher, the research team may seek your expertise before a project begins to see what questions or issues are most important to address. We may seek your input when a project is underway to help guide and influence how the research is carried out. We may ask you to help with gathering information such as interviewing other people with experience of a similar health condition, disability, or situation to your own. We may involve you in making sense of the results and sharing them with other researchers and interested people. We may also seek your perspective at the end of a project to find out what went well and what could have been improved. You will be provided with training, mentoring by a team member, and buddying with another co-researcher to support you in your role. * You can expect the project lead, or another designated member of the project team, to welcome you and talk to you about the project and how you can help shape it. * You can expect an induction to the role which will give you an overview of why the research project has come about and an opportunity to discuss further how you would like to be involved. You will also find out what to do if you are unhappy about anything. * You can expect the project lead to give you an idea of what the time commitment is likely to be before you start, so you can judge whether you can fit this role in alongside other commitments. * You can expect specialist training if you are asked to do something that requires a particular skill, such as interviewing people or making sense of the results. You can add these skills to your CV. * You may want to contribute in some ways, but not in others. For example, you may be happy to share your experiences within the project team, so we can get to know you, but not want these ex- periences written about publicly. This is fine. Where possible, let the project lead or your mentor know about your preferences. The team may want to make public some information about the project’s co-researchers, but will do so only with their clear agreement. * Your situation, health condition or impairment may fluctuate. You can expect us to recruit several co-researchers to allow you to dip in and out of the project if necessary, depending on your changing needs. You can leave the project at any time or pause your involvement if you need to do so. However, if you are experiencing any di�culties, you should speak to a member of the team or your mentor, as they may be able to help. 11 MUTUAL RESPECT One of the most important ideas behind co-production is mutual respect between all members of the project group, as well as other people you come into contact with while working on the project. With co-production each person and each contribution is valued equally so it is important to be respectful of di�erent viewpoints even if you disagree with them. The group will agree a set of ground rules which promote respect and dignity as well as safeguard-related issues such as confidentiality. The more the group members can foster respect and trust the better the experience will be. Peer researcher at Peer Research and Understanding Homelessness: Exploring Practice, Learning and Innovation conference by Focus Ireland, Dublin, 2019 I ha\\ue17b\\ue16a \\ue18b\\ue184ar\\ue173\\ue193 s\\ue174 \\ue172\\ue180\\ue18dy \\ue178\\ue18ail\\ue171\\ue192 t\\ue16da\\ue193 I c\\ue166\\ue173 \\ue194\\ue192e n\\ue174\\ue179 \\ue18e\\ue18dl\\ue17e a\\ue192 \\ue166 p\\ue184e\\ue177 \\ue191\\ue16as\\ue184a\\ue177\\ue182h\\ue16a\\ue177 bu\\ue179 \\ue16e\\ue18d m\\ue17e \\ue18bif\\ue16a. I ne\\ue17b\\ue16a\\ue191 t\\ue16d\\ue18eu\\ue186h\\ue179 I \\ue196o\\ue17al\\ue169 \\ue181\\ue184 ab\\ue171\\ue16a \\ue193\\ue18e t\\ue177a\\ue195\\ue16al \\ue18e\\ue173 \\ue18cy o\\ue17c\\ue18d \\ue166n\\ue169 \\ue192t\\ue180\\ue17e a\\ue196\\ue166y \\ue16b\\ue191\\ue18em \\ue16do\\ue18c\\ue16a b\\ue184\\ue16bo\\ue191\\ue16a bu\\ue179 \\ue181\\ue16ac\\ue18e\\ue172i\\ue18dg \\ue166 \\ue175\\ue184e\\ue191 r\\ue16a\\ue178\\ue184a\\ue191c\\ue16d\\ue16a\\ue191 h\\ue180\\ue178 \\ue186iv\\ue16a\\ue173 me \\ue171\\ue174\\ue180\\ue183s o\\ue16b \\ue182\\ue174n \\ue169\\ue184\\ue18dce. 12 CHALLENGES * Being involved in a research project can challenge your ways of thinking, your views of the world, and your perspective on your situation, health condition or disability. This can be daunting at first but after a while you may find it empowering. * You may feel daunted about speaking up in team meetings. We will try to make this easier for you by creating a friendly and informal environment. It can help to know that even professional researchers can feel daunted about speaking up in certain situations. We will try to make it as easy as possible for you to contribute and work within the team. Remember you are an ‘expert by experience’! * Research projects are carried out from within particular academic disciplines, for example, social sciences, health sciences, education or medicine. These disciplines have their own traditions, methods and vocabularies which can seem slightly alien at first. You are, of course, not expected to know everything in advance. In a co-produced project, the research team is there to be guided by you and the fresh perspective you can provide. It’s easy to think professional researchers are scary people in ‘white lab coats’ or ‘professors interviewing people’ who know everything. Like everyone else, however, professional researchers have many things they are uncertain about or have not thought about. * Co-production is still quite a new way of working. To some extent the whole team will be feeling their way as they go along. You may find that there are opportunities to encourage new ways of working. As with many things, the more you are able to get involved with a variety of tasks, the more you will get out of the experience. Peer researcher at Peer Research and Under- standing Homelessness: Exploring Practice, Learning and Innovation conference by Focus Ireland, Dublin, 2019 I wa\\ue178 \\ue195\\ue16ar\\ue17e ne\\ue177\\ue195\\ue174\\ue194s \\ue17c\\ue187en \\ue175\\ue191\\ue16as\\ue184\\ue173\\ue193in\\ue16c \\ue174\\ue194\\ue191 p\\ue177e\\ue192\\ue16an\\ue179\\ue180\\ue193i\\ue174n. Pre\\ue178\\ue16a\\ue18dt\\ue188\\ue173\\ue186 in \\ue16b\\ue191\\ue174n\\ue179 of \\ue16a\\ue17d\\ue18f\\ue184ri\\ue16a\\ue173\\ue182\\ue184d a\\ue173\\ue183 p\\ue177\\ue174\\ue185\\ue184s\\ue178i\\ue174\\ue18d\\ue180l re\\ue178\\ue16a\\ue180\\ue191c\\ue16de\\ue191s \\ue17c\\ue166\\ue192 \\ue188n\\ue179i\\ue18c\\ue16ed\\ue180\\ue179i\\ue18dg co\\ue173\\ue192\\ue16ed\\ue184\\ue177i\\ue18dg I’ve \\ue174\\ue173\\ue18by \\ue167\\ue184e\\ue18d \\ue166 p\\ue184e\\ue177 re\\ue178\\ue16a\\ue180\\ue191c\\ue16de\\ue191 f\\ue174\\ue177 \\ue185\\ue18eur \\ue174\\ue177 \\ue192\\ue18e mo\\ue173\\ue193h\\ue178. I wa\\ue178 \\ue185\\ue17al\\ue171\\ue198 s\\ue194\\ue175\\ue18for\\ue179\\ue16a\\ue183 t\\ue16d\\ue191\\ue18eug\\ue16d\\ue174\\ue194\\ue193 t\\ue16de \\ue183\\ue166y \\ue17c\\ue187\\ue188c\\ue16d \\ue196as \\ue168\\ue174\\ue18cf\\ue18e\\ue177\\ue193in\\ue16c an\\ue169 I \\ue183e n\\ue16e\\ue179\\ue184\\ue18by \\ue16da\\ue195\\ue16a m\\ue18e\\ue177e k\\ue173o\\ue196l\\ue16a\\ue169\\ue186\\ue184 on \\ue175\\ue16a\\ue184\\ue191 re\\ue178\\ue16a\\ue180\\ue191c\\ue16d \\ue18dow. 13 THE RESEARCH TEAM’S COMMITMENT TO YOU * If you choose to get involved, we will create a supportive, safe, welcoming, non-judgemental environment in which everyone has a fair opportunity to contribute, and respectfully listen to each other’s views. * We will provide an induction where you will be introduced to the other members of the project team, and be given the background to the project and the potential timescales. We will also discuss with you how you would like to be involved in the project. * We will give you a mentor from among the professional researchers with whom you can raise any questions, needs or concerns, and a buddy from among your co-researchers with whom you can also talk things over. * We will provide you with training to help you to better understand some of the di�erent approaches that researchers may take to a topic, and what these approaches aim to achieve. * We will provide you with training to supplement and enhance your existing skills, ensuring that you have the confidence and know-how you will need to carry out research activities related to the project. * We will keep you up to date with the project and answer any questions you have. Within the constraints of the project’s budget, we will try to involve you as much as possible. * We will avoid using jargon and acronyms as much as possible, and if they are unavoidable we will make clear what they mean. ACCESS AND OTHER NEEDS We want to make sure our involvement opportunities are accessible to you. We will ask about your access needs, and any other needs and preferences you may have. However, do not be afraid to be proactive about sharing your needs. For example, you may need a hearing loop; a lip speaker or a BSL interpeter; materials in an accessible format; a meeting room and travel suitable for a wheelchair user; a carer to come with you; papers or information in a particular format; or a support worker. If you are a carer, you may need an expense payment to cover the cost of someone to take over your carer responsibilities while you are at a meeting. You may need meetings arranged at particular times of the day. Clearly state your needs to us. Leave plenty of time so we can put your requirements in place. We will let you know if refreshments are provided, and if they are, we will ask you in advance about any specific dietary needs. re\\ue178\\ue16a\\ue180\\ue191c\\ue16de\\ue191 f\\ue174\\ue177 \\ue185\\ue18eur \\ue174\\ue177 \\ue192\\ue18e mo\\ue173\\ue193h\\ue178. 14 EXPENSES AND PAYMENTS * If you get involved all your out-of-pocket costs will be covered for travel, parking, accommodation (if needed), carer and support worker costs, printing, telephone, photocopying and postage costs where appropriate. * Your involvement in a project may be voluntary or paid. This will be made clear at the very beginning of the project so you can make an informed decision about whether or not to take part. If you decide to take on a paid position, you may need advice on how this will a�ect your benefits. The Social Care Institute for Excellence has some helpful information on this at: bit.ly/34drVBF, or you can talk to the project lead. * If you decide to get involved in a research project, you can bring this booklet with you to meetings. You can refer to it to make sure you are being fully supported and draw the team’s attention to any areas where they are not meeting their commitments to you. * If anything is unclear, you should not be afraid to ask the project lead, your mentor or another member of the team. http://bit.ly/34drVBF 15 THE SMALL PRINT KEEPING YOU SAFE We prioritise the safety of everyone working on a research project. We will carry out a full risk assessment of any activities which require one, and will help you and all the team to keep safe. For example, if the research involves interviewing research participants, we will have a clear policy on where interviews will be conducted, how to contact team members, and what information may be recorded. If you have any concerns about safety while participating in a research project, you should contact the project lead, your mentor, or another member of the team. ETHICS It is very important that research is conducted in a way that treats people with respect and consideration. All research projects are assessed by an ethics committee to ensure that the research processes and procedures are ethically correct. We take ethical considerations very seriously and are committed to fair and open treatment of co-researchers, and everyone else involved in research. If you decide to participate in a research project and have ethical concerns about the way the research is being conducted, you should contact the project lead, your mentor, or a member of the team. CONFIDENTIALITY AND DATA PROTECTION As with all sta� who work with us, we will hold all the personal information that you give us on secure, password-protected servers. We are legally obliged to follow the comprehensive guidelines set out by the General Data Protection Regulation. More information can be found at: gov.uk/government/publications/guide-to-the-general-data-protection-regulation. SOME DEFINITIONS Professional researcher This is likely to be someone who has been trained in a particular research area and is employed by a university to conduct research. Co-researcher A co-researcher is someone in a particular situation, or who has a particular health condition or disability, and who uses their experience to deliver research on a particular topic as a member of the research team. Expert by experience If you have been given this booklet because you are in a particular situation, such as having a health concern or disability, it is likely that you are already an ‘expert by experience’. This is someone who has a particular understanding of a situation or health condition by virtue of their lived experience. Lived experience This is experience gained by living with a particular situation, health condition or disability.',\n",
              " \"Co-production booklet for researchers 6 1. What is meant by co-production? In the socially revolutionary climate of 1969 the American policy practitioner and researcher Sherry Arnstein articulated a model for citizen participation in civil society and government which uses the analogy of a ladder to describe moving up from a state of non-participation to that of ‘citizen power’ ( Arnstein, Sherry R. (1969). A Ladder Of Citizen Participation, Journal of the American Planning Association, 35(4), 216 - 224, http://dx.doi.org/10.1080/01944366908977225). Since then this model has been widely adopted and adapted to a range of different scenarios relating to members of the public engaging with the political and professional spheres. In the academic sphere, participation of the public in research is often broken down into just three levels: consultation, involvement and co-production. Although these terms are not always used entirely consistently, and others are sometimes substituted, there is nevertheless a general consensus as to what they designate, even if in practice boundaries sometimes become slightly blurred. The common thread which sets apart and defines the ‘co-production’ level is that of work undertaken by ‘professional’ researchers (often based in a university) in equal partnership with ‘lay’ or ‘co-’researchers (people who have a direct interest in, or experience of, the issue being researched) and which brings together complementary skills, experience and knowledge to the benefit of the project. Co-production therefore goes beyond informing the public about research and how it will affect them to actively involving them in shaping the research. Many professional and co-researchers would argue that true co-production requires researchers and the public/carers/end/service users to have an equal say in the design and implementation of the research project. This is a step up from both consultation and involvement where, although members of the public/end/service users input their ideas and perspectives, the researchers drive the agenda and ultimately make the important decisions. One recent book chapter (Williams O., Robert G., Martin G.P., Hanna E., O’Hara J. (2020). Is Co-production Just Really Good PPI? Making Sense of Patient and Public Involvement and Co-production Networks. In: Bevir M., Waring J. (eds) Decentring Health and Care Networks. Organizational Behaviour in Healthcare. Palgrave Macmillan, Cham. https://doi.org/10.1007/978-3-030-40889-3_10) distinguishes the role that members of the public play in a truly co-produced project as ‘transformational’ rather than merely ‘additive’. The principle of power-sharing and equality between professional and co-researchers and any other contributors is at the heart of co-production. This is not just about creating a team environment where people are treated as equals, important as that is, but ensuring everyone is also given a meaningful share of responsibility and ownership for the project. Although co-production is for these reasons the gold standard to aim towards, consultation and/or the involvement of individuals on, for instance, an advisory group may sometimes be http://dx.doi.org/10.1080/01944366908977225 https://doi.org/10.1007/978-3-030-40889-3_10 7 a more viable approach, depending on the nature of the situation and the research. Also different models may be appropriate for different parts of a project. The public participants themselves may not always wish to be equally involved in all aspects of the research process and, though sometimes encouragement is appropriate, it is important to avoid in any way compelling them. (This is a point made on p. 252-253 of Doyle, M., & Timonen, V. (2010). Lessons From a Community-Based Participatory Research Project: Older People’s and Researchers’ Reflections. Research on Ageing, 32(2), 244–263. https://doi.org/10.1177/0164027509351477). A diversity of models can be a sign of effective engagement. The key thing in choosing whether to use co-production or another model is to consider what particular skills/perspectives/insights the lay person can bring to the table relative to the other participants and so, by extension, the degree of responsibility it is reasonable and fair to place on that person. 2. Why do co-production? At one level there is a moral argument based on accountability and fairness: it is only right that those who are going to be most affected by (and who are more often than not paying for) the research, should have the opportunity to directly shape it. There are also plenty of compelling practical benefits, both to the academic researcher and the co-researchers, as well as to the wider public, from co-producing research. These include: ● Increasing the relevance of research by ensuring that it reflects the needs, values and interests of end users, as well as more generally of different groups across society ● Improving the quality of research through broadening the range of expertise inputting into it ● Providing more opportunity for biases and assumptions to be challenged ● Raising the level of impact from the research, through securing public/end/service user buy-in ● Increasing the democratic accountability of researchers to those who pay for their work ● Empowering the public to contribute to and draw on research in order to improve their lives ● Improving the understanding and appreciation of the value of research throughout civil society https://doi.org/10.1177/0164027509351477 8 The overarching rationale is that increasing the involvement of the public, as the ultimate beneficiaries and end users of research, from the very beginning of the research process has the potential to produce stronger research and research outcomes that better fit the needs, values and interests of end-users and society as a whole. While consultation with the public can help to sense-check new ideas, co-designing with the public facilitates the best lines of enquiry and hence the best ideas from the start. Although this makes sense logically there is also a growing empirical evidence base to make this case, although this requires further development in terms of how impact is determined, reported and assessed (see Brett, J., Staniszewska, S., Mockford, C., Herron-Marx, S., Hughes, J., Tysall, C., & Suleman, R. (2014). Mapping the impact of patient and public involvement on health and social care research: a systematic review. Health expectations : an international journal of public participation in health care and health policy, 17(5), 637–650, https://doi.org/10.1111/j.1369-7625.2012.00795.x; also Forsythe L.P., Carman K.L., Szydlowski V., Fayish L., Davidson L., Hickam D.H., et al. (2019). Patient engagement in research: early findings from the Patient-Centered Outcomes Research Institute. Health Affairs, 38(3), 359–367, https://doi.org/10.1377/hlthaff.2018.05067) By virtue of lived experience (for instance of a particular health condition or disability or social/cultural/ political/financial situation) or a particular enthusiasm or passion (for instance for civil rights, justice and equality, environmental issues) lay researchers can often contribute important critical perspectives/ understandings to the design and resolution of a research question or methodology that might otherwise not be addressed. This is why co-researchers are sometimes referred to as ‘experts by experience’. (See for instance Fenge, L. A., Fannin, A., & Hicks, C. (2012). Co-production in scholarly activity: Valuing the social capital of lay people and volunteers. Journal of Social Work, 12(5), 545-559, https://doi.org/10.1177/1468017310393796.) Where they cannot necessarily contribute to the solving of a technical problem, members of the public can nonetheless help in framing the overall question and pointing out vital ethical, social, cultural and political considerations. To simply ask them the questions without seeking their input into shaping them, risks those questions not being the right ones, or, at least, not the ones that matter most. A co-produced approach can provide the evidence of broader public buy-in to providers and commissioners, particularly in the public sphere, considering whether to implement research findings and recommendations. Additionally direct testimony from the public as experts by experience can have a more powerfully compelling and motivating impact on policy makers and statutory agencies, particularly where it relates to suffering resulting from shortfalls in services, than conventional academic analyses. For an example of this see: Littlechild, R., Tanner, D., & Hall, K. (2015). Co-research with older people: Perspectives on impact. Qualitative Social Work, 14(1), 18–35. https://doi.org/10.1177/1473325014556791 Sometimes co-production will involve simultaneous collaboration with professionals - e.g. policy makers, local government officers or health practitioners - who have the capacity to https://doi.org/10.1111/j.1369-7625.2012.00795.x https://doi.org/10.1377/hlthaff.2018.05067 https://doi.org/10.1177%2F1468017310393796 https://doi.org/10.1177/1473325014556791 9 translate the research into practice. This three-way approach, sometimes referred to as a ‘blended’ approach, can be a very powerful route to delivering impact from research. There are however sometimes significant challenges and barriers to overcome including perceptions by policymakers that under time and budgetary constraints engagement is a ‘nice-to-have’ rather than something essential, fears that the consultation may result in things they do no want to here, beliefs that the public do not know what is best for them and unwillingness to relinquish power. These barriers were identified in a report produced by the School of International Futures: Review of Sciencewise and proposed future approaches in November 2020 (see especially p. 33). The demarcations between ‘academic researcher’, ‘public’ and ‘professional’ are not necessarily absolute. Some researchers may well also have expertise by experience, while the public may have relevant professional skills that they can bring to the project on top of their expertise by experience. The argument from social responsibility Co-production is an integral part of what is known as ‘Responsible Research and Innovation’, that is engaging the public and other responsible external actors in research to produce ethically acceptable, sustainable and socially desirable research and innovation outcomes. Further information can be found at https://www.rri-practice.eu/ It can also link to the concept of the university as an anchor institution rooted in its local community through its mission, invested capital, and local business and community relationships, and by making a significant contribution to the local economy. The University of York is a signatory to the ‘Civic University Agreement’ set up by the Civic University Commission and the UPP Foundation. This agreement affirms the University’s commitment to engaging with and addressing the needs of the local community. Further information about the scheme is at: https://upp-foundation.org/civic-university-agreements-list-of-signatories/ Co-production often has a strong social justice dimension – allowing marginalised voices to be heard which may not be adequately represented within the academic research community (e.g. people from socio-economically deprived backgrounds or members of minority ethnic groups). Co-production can clear a path for positive social change for communities of place, interest, identity and need outside of the influence of powerful majority perspectives. There are also overlaps with some of the principles behind ‘Deliberative Democracy’, including helping to ensure broader representation and inclusivity of views across the social and political spectrum in setting agendas and priorities. The argument is that this increases cognitive diversity (from having people who have different ways of thinking and not just schooled in academic modes), which in turn can lead to better problem-solving (see Landemore, Hélène. (2013). Deliberation, Cognitive Diversity, and Democratic Inclusiveness: An Epistemic Argument for the Random Selection of Representatives. Synthese. 190. DOI: https://doi.org/10.1007/s11229-012-0062-6.) An https://sciencewise.org.uk/wp-content/uploads/2020/11/SOIF-Report.pdf https://sciencewise.org.uk/wp-content/uploads/2020/11/SOIF-Report.pdf https://www.rri-practice.eu/ https://upp-foundation.org/civic-university-agreements-list-of-signatories/ https://doi.org/10.1007/s11229-012-0062-6 10 edited book which explores a number of case studies on these themes is Aksel Ersoy (ed.) The Impact of Co-Production: From Community Engagement To Social Justice (Policy Press, 2017). https://policypress.universitypressscholarship.com/view/10.1332/policypress/9781447330288.001.0001/upso-9781447330288 https://policypress.universitypressscholarship.com/view/10.1332/policypress/9781447330288.001.0001/upso-9781447330288 https://policypress.universitypressscholarship.com/view/10.1332/policypress/9781447330288.001.0001/upso-9781447330288 11 3. Underpinning principles and values Equality, inclusion and democracy: ensuring that the full range of relevant voices are respectfully heard and no one voice or agenda predominates; shared decision making. Mutual interest: addressing shared concerns and exploring potential for reciprocal benefits; striving for common ground with each party being prepared to compromise and/or step up to the challenge where necessary. Integrity and humility: a shared commitment to uncovering the truth whatever this might be and however much this might conflict with preconceived ideas. No one agenda should be allowed to skew or misrepresent findings. Mutual trust: each side is able to be honest and transparent about its motivations and goals. This in turn requires: ● Credibility – participants need to learn each other’s language and be seen as valued and relevant sources of knowledge by each other ● Legitimacy – participants need to be clear about on whose behalf they speak (e.g. people in the same profession, users of a particular service, patients with the same condition, employees in a specific organisation) and be supported to do so Respect: each party must be prepared to engage constructively with different viewpoints even if they are in profound disagreement and to acknowledge the value of each other’s contributions. 4. Areas of research which could benefit from co-production Given that most, if not all, research will impact on the general public in one way or another sooner or later, it is hard to imagine many areas that would not benefit from some element of co-production. The following is an indicative (but far from exhaustive) list of possible areas and scenarios: ● Involving the public (and especially those who have relevant lived experience of a particular situation or challenge) in deciding which research questions or aspects of a research area are the most pressing and which should therefore be prioritised; then co-designing a methodology that draws on the skills, knowledge and experience of all those who could contribute to finding answers to that question ● Involving the public in developing a new piece of technology in order to improve the functionality and accessibility of the device and ensure the best fit to need and purpose ● Exploring with the public the ethical dimensions of a new technology or process to ensure that this harmonises with and/or can be reconciled with as broad a set of cultural and moral values as possible 12 ● Collaborating with the public in developing a new set of standards or guidelines for health or social care to help ensure that they are as equitable as possible and best meet and prioritise the needs of end users ● Creating a new art work or creative practice (in any medium – visual, physical, written or sound) which helps to engage members of the public with a particular research challenge 5. How to run an effective co-production project It is vital that researchers are driven by an enthusiasm for the principles and philosophy underpinning co-production as outlined in the previous section. Co-production must be more than a box-ticking exercise and be approached with integrity and commitment. The public will quickly perceive if their contribution is not truly valued and may potentially withdraw from the process disillusioned and less likely to engage in future research projects. Developing trust with public participants however takes time. This can be a real challenge in research environments where funding is often short term, frustrating attempts to form sustainable relationships with groups and to keep their interest and skills alive. It is equally essential to consider and work through all the practicalities of implementing such an approach in order to maximise the quality of the outcomes. Studies have shown that the single biggest influence on the efficacy of a co-produced approach is not the particular context of the study but the quality of the approach and project design. (Reed, Mark et al. (2017). A theory of participation: What makes stakeholder and public engagement in environmental management work?, Restoration Ecology, 26(S1), S7-S17, https://doi.org/10.1111/rec.12541.) Pursuing co-production introduces a number of significant complications and challenges that are explored more fully in section 7 of this booklet. It is a process that requires additional time to be built into the project timescale and significantly increases the administrative workload in terms of organising meetings, finding appropriate venues and ensuring all ethical, health and safety, and GDPR requirements are met. Described below are some ways to maximise the chances of making a success of co-production as an approach. Ensuring engagement from the very start Quite often service/end users are brought in once the project is funded and about to start but it is usually much better, both with respect to giving participants a greater sense of ownership and empowerment and to optimising the design, to involve them from the offset in planning the project. This is also more likely to impress the funder and indeed some https://doi.org/10.1111/rec.12541 13 funders may request evidence that an applicant has already involved the public/service/end users in developing the proposal. Engaging with participants from the start also provides the opportunity to agree with them how long and how intensive their engagement in the project should be and what demands are reasonable in terms of their time and travel. It may be challenging to source funding to support this preliminary work. Your departmental research support officer may be able to help you identify any relevant possibilities. Recruitment It is typically easier and more effective to recruit co-researchers through a community, advocacy, charitable or health organisation that already has strong links into these communities. It is worth therefore spending time building strong relationships with these ‘gate-keeper’ organisations by exploring areas of synergy and mutual research interest. It is good practice to cost in support for this work where possible as the gatekeepers may well not have spare budgetary capacity for this extra work. Community outreach events where researchers share their work through talks, workshops, pop-up stalls etc. with members of the public can also provide opportunities to recruit individuals to join future research projects. It may be possible to reach out to some groups and organisations via social media and it is worth doing some research as to which might be the most age/demographic appropriate platform(s) to use. One advantage is that these groups/organisations may then repost this information so it spreads even further. It is important to remember that these are public platforms and what you put out will be likely visible to everyone and so it is particularly necessary to frame messages sensitively and carefully. Also to make them as engaging as possible - perhaps by adding an image or short video. It is worth encouraging people to privately/directly respond to you rather than publicly posting that they wish to take part. It is important to choose a location for recruitment events and indeed for subsequent meetings which will be convenient and familiar to the co-researchers and a gatekeeper can often suggest suitable venues and even facilitate access. Taking this approach will ensure you are engaging the public ‘on their own turf’ where they are more likely to feel empowered and relaxed and therefore more likely to get on board. Equally, it is important to carefully choose the right language when seeking to engage members of the public, especially those who may not have taken part in research before. The use of the term ‘co-production’ to describe collaboration in the research process may be totally unfamiliar to most members of the general public and conjure up images of the TV and film industries! Even groups that have some existing experience may not favour the term which is seen by some as tainted by having been applied too loosely. Similarly, it is best 14 to avoid using formal titles like ‘Dr’ and ‘Professor’ which may reinforce anxieties about differences in ability and competence to contribute. It may be advisable to recruit a few more public participants than you think are needed, particularly if they belong to groups who, through disability or a health condition or socio-economic circumstances, may need to take time out over the course of the project. Before welcoming anyone onto the project it is recommended to first meet the person and achieve an understanding of their motivations, expectations, relevant experience and personal circumstances and what support they might need. Please see also section 6 below in this context. If the project is likely to involve working with vulnerable individuals, it is important to ensure that all project members comply with appropriate criminal record checks (https://www.gov.uk/dbs-check-applicant-criminal-record). Induction and subsequent group meetings The purpose of induction is to introduce all members of the research team to each other and to the project. It is about laying the foundations for how the group will work together, by establishing an ethos of mutual support and respect and agreeing processes for dividing up responsibilities, communicating with each other and resolving any disagreements. It is especially important to choose an easily accessible and locatable venue for the first meeting and to provide as full directions (by whatever mode of transport is being taken) as possible. Throughout the project however meetings should ideally be held in places with good public transport routes and parking availability, or alternatively reasonable taxi fares should be covered. People’s mobility, dietary and any other health needs should be taken into account from the beginning - for further information please see section 6 under ‘Supporting people with particular health conditions and disabilities’ and the accompanying questionnaire in Appendix D. It is important to create a friendly environment where everyone feels immediately welcomed and at ease. This is partly about ensuring that the co-researchers are greeted properly when they arrive and are included in whatever is going on straight away, but also about designing some initial activities that will encourage people to talk to each other and break down any shyness through injecting a bit of fun and humour. It is equally important to think about how the meeting space is arranged, ensuring that there are accessible corridors and that trip and other hazards are mitigated. Also to consider carefully how people might be seated in relation to each other, particularly so as to avoid clustering into particular groups and to ensure that (especially new co-researchers) are positioned near to their mentors and/or buddies. Many co-produced project meetings commence with lunch as this gives people time to catch their breath, settle in and renew acquaintanceship with each other. Likewise it gives whoever is chairing the meeting time to ‘check-in’ briefly with each of https://www.gov.uk/dbs-check-applicant-criminal-record 15 the team members to see if there is anything in particular they should be aware of during the meeting (e.g. if any of the members is feeling anxious or run-down). It is a good idea to agree some ground rules from the start in terms of how the project members will behave towards each other and towards those they meet during the course of the project. These rules should be collectively created, documented and signed up to. They should include what is important to each team member in terms of team behavior and norms - what is acceptable, and what is not. The rules should be reviewed throughout the project and added to and revised when needed. Here is a suggested list: ● To treat each other equally and with respect ● To be moderate and sensitive in voicing disagreement and to try to understand the perspective of the other person ● To not make any personal criticisms or use sarcasm ● To listen carefully to each person’s input and avoid interrupting, at least initially, and then only by raising a hand ● To allow everyone a chance to speak, and to make sure no one dominates even if they feel strongly about something ● To stay focused on the agreed topic and avoid digressing ● To maintain confidentiality when appropriate and if personal or sensitive issues arise ● To keep an open mind and a positive attitude Having these ground rules means that both service users and staff will feel safer to express themselves honestly, to take risks, make mistakes and learn from them. Responsibility for ensuring that these rules are respected is in the first place with whoever is chairing/ facilitating the meeting or event and thereafter with the project lead. A key objective of the induction process is to enable the lay members to participate fully as equal members of the research team but without overwhelming them with too many expectations and responsibilities. As noted, many members of the public may come to the project with little understanding of, or some misconceptions about the nature of research, even if they attended university themselves as undergraduates. Although some may have taken part in previous research projects, the extent to which they were actively involved in shaping and conducting the research should not be overestimated. Collaborating remotely What has been written so far assumes face-to-face meetings and indeed it is arguably easier to build trust and understanding between a group that comes together under one roof. There may however be certain circumstances where virtual/on-line meetings are a more feasible option. Doing away with the need to travel may not only have environmental benefits but can make it easier for certain groups, e.g. those who suffer from chronic tiredness or pain, to participate. On the other hand not everyone may have access to the required technology and software or be able to find a quiet and private enough space in 16 their home from where to participate. They may also feel uncomfortable about having their home appearing in the background and so it is important to ensure people are able to join without video or via the telephone. Likewise as an academic participant it is important to ensure that your background on the call is not intimidating or contains any images or objects that might be insensitive or boast intellectual or cultural eliteness e.g. rows of academic textbooks. Various platforms e.g. Google Hangouts, Microsoft Teams, Skype and Zoom exist to support online meetings and many of these incorporate functions, such as screen and file sharing, private messaging between group members, and anonymous polling or voting, that may be easier to carry out than would be the case in a physical meeting. As with a physical meeting, it is important to think of creative ways of breaking the ice and forging bonds between members. Likewise it is worth thinking carefully about meeting etiquette, including how contributions from the various group members are managed (which can be harder when visual clues that a person would like to speak are less obvious) - some platforms e.g. allow a person to virtually raise a hand to help with this. Many people find video-conferencing more draining of concentration than meeting face-to-face and so it is important to remember to schedule in sufficient breaks and give individuals the opportunity to request additional time-out. Above all remember to provide clear and simple instructions to participants on signing up for and using these online platforms and do a couple of test runs. It might be worth also setting up a private group on a social media platform, for instance WhatsApp or Facebook, to enable members to keep in touch and build rapport between meetings. The Methods Lab has an online list of resources and articles exploring best practice in conducting qualitative and ethnographic research digitally: http://www.methodslab.org/resources/ The National Coordinating Centre for Public Engagement (NCCPE) website has some useful practical tips for online engagement events: https://www.publicengagement.ac.uk/meaningful-engagement-online-events As does ‘Fast Track Impact’: https://www.fasttrackimpact.com/post/tips-and-tools-for-making-your-online-meetings-and -workshops-more-interactive The ‘Living Life to the Fullest’ project led by the University of Sheffield (https://livinglifetothefullest.org/the-co-researcher-collective/) conducts most of its interactions with its ‘co-researcher collective’ team online and the following article explores the ins and outs of this approach: Liddiard, K., Runswick-Cole, K, Goodley, D., Whitney, S., Vogelmann, E. and Watts, L. (2018) “I was excited by the idea of a project that focuses on those unasked questions”: Co-Producing Disability Research with Disabled Young People, http://www.methodslab.org/resources/ https://www.publicengagement.ac.uk/meaningful-engagement-online-events https://www.fasttrackimpact.com/post/tips-and-tools-for-making-your-online-meetings-and-workshops-more-interactive https://www.fasttrackimpact.com/post/tips-and-tools-for-making-your-online-meetings-and-workshops-more-interactive https://livinglifetothefullest.org/the-co-researcher-collective/ 17 Children and Society, 33: 2, 154-167, https://doi.org/10.1111/chso.12308. They are also working on developing an online collaborative toolkit. The ‘Parenting Science Gang’ project successfully used Facebook as a platform for promoting and sustaining engagement with the project and its goals, and the project website offers some top tips and observations: http://parentingsciencegang.org.uk/evaluation/top-tips-on-using-facebook/ Information about using virtual meeting platforms is available on the University’s IT webpages: - Google Hangouts: https://www.york.ac.uk/it-services/services/hangouts/ - Zoom Video Conferencing: https://www.york.ac.uk/it-services/services/zoom/ Training It is likely that at least some of the participants would benefit from some training and support ahead of launching into the actual project, both on the general research process and/or the specific topic area and the approaches/methods that might be used. Training in ‘softer’ skills might also be beneficial, for instance in project management and in negotiation and dealing with other (and conflicting) viewpoints. Such training and preparation is often cited by the participants as key to giving them the confidence to contribute. Before embarking on such training however, and in order to inform it, getting to know the participants as individuals and to understand their backgrounds, needs and interests is important. When a programme of training has been decided, it would be best to offer this to all the project group, both as a way of getting everyone on the same page, and since there may be some techniques/approaches with which not all the academics are entirely familiar. As well as offering appropriate training, an effective approach in supporting and helping bring up to speed less experienced lay project members could be to provide them with a mentor from the professional research team and also to buddy them with another, and if possible, more experienced co-researcher on the project. Equally it would be worthwhile to entrust one of the professional project staff with particular responsibility for overseeing the welfare and interests of all the lay team members. Managing the project and the members’ expectations and responsibilities Effective management of a project is critical to its success and this is more important than ever when individuals outside higher education, including co-researchers, are involved. Although the project plan should be co-designed with the co-researchers, it is the responsibility of the project lead to ensure that the plan that emerges is: - Coherent, clear and focused enough - As fairly addresses the priorities of the group members as possible - Is realistic and achievable within the given timeframe https://doi.org/10.1111/chso.12308 http://parentingsciencegang.org.uk/evaluation/top-tips-on-using-facebook/ https://www.york.ac.uk/it-services/services/hangouts/ https://www.york.ac.uk/it-services/services/zoom/ 18 - Is adequately mapped out in terms of timescales, responsibilities and resources required In the co-designing of the plan and objectives, it is vital that assumptions are not made about the preferences, interests and motivations of others and that each person is given the opportunity to articulate their views freely (albeit respectfully) and without fear of judgement. Individuals should be taken at their word and not suspected of ulterior motives. It is important for the group as a whole to agree on each member’s rights and responsibilities from the outset and to review these at each subsequent stage of the project’s progress. At the same time however it is important not to micromanage project members and to grant them appropriate autonomy as well as training and support. It is also important to make as clear as possible the timescales for the involvement of the co-researchers and the amount of time and effort likely to be involved and to match this realistically against the amount of time and energy they can afford. If unreasonable expectations are made of the co-researchers, and/or if goals and objectives are announced to them in too ‘ad-hoc’ a manner, they are likely to become frustrated. Having stated this, there does need to be a degree of flexibility on all sides as circumstances can change and the unexpected can arise which may mean that the mapped out route is no longer optimal or even possible. Also what seems best in theory may present unanticipated challenges in practice - sometimes these challenges may emerge from members of the group in which case they need to be handled sensitively and fairly and with due consideration of alternative courses. Above all it is vital to effectively manage the expectations of the lay team members – neither over-claiming nor over-promising on benefits and results, and likewise to foster patience with long academic timescales and need for evidence-based academic rigour. Equally, it is important that the professional researchers understand that co-producing can significantly alter the pace of doing research and that they are working with individuals who are not paid employees and who are fitting the project around their existing work, family and caring commitments. Demonstrating flexibility and being prepared for irregular, interrupted conversations, e.g. on Facebook Messenger at 10 o’clock at night, rather than regularly scheduled meetings, is key to optimising engagement with lay team members. Managing power dynamics and differences Effective co-production requires carefully considering the variety of power dynamics between the members of the group. Power can take several forms and have several different origins: some individuals may have more financial power, others may have more power in terms of external influence, while some may have more psychological power in terms of being more confident in asserting their views. It is vital that the overall project lead and/or sub-group leads/facilitators are able to negotiate these differences and, while mindful that these realities cannot necessarily be erased, to establish an egalitarian ethos for the project 19 and to level the playing field sufficiently for everyone to engage, feel valued and contribute their views/ideas. With respect to determining the overall direction and ambitions of the project a system of consensus voting may well be the most appropriate with each vote weighed equally. Ideally this should be done anonymously. It may not however be possible or even appropriate to grant equal decision-making power in all areas, although this should remain the ideal to aim for. Whereas it may be possible to level power and status differences within the project group, outside these differences will inevitably re-emerge. The project lead must ensure that individuals are not allocated responsibilities that they do not have the power to enact externally as this will only lead to frustration and disappointment. The public participants may not indeed wish to carry equal power and responsibility for the project as the researchers and/or other stakeholders. Although decision-making powers may, by common agreement or by external restraints, vary, it is of greatest importance that the decision-making structures for the project are fully transparent and open to scrutiny and challenge from any member of the group. It is possible in certain scenarios that the interests and outlooks of individuals within a group do not align and that conflicts will arise. It is vital that the project lead has the skills and experience to manage and tactfully mediate, and if necessary arbitrate, between these differences. This can be very challenging and there is a need to balance idealism with pragmaticism. Again the reality of external power differences between individuals and/or the interests they represent has to be acknowledged and a compromise needs to be found that is not only as fair and as realistic as possible but will be tolerated by all parties. This compromise may be less than ideal or entirely socially just but better this than no progress at all. Where power differences are more or less equal and views strongly different, then the achievement of compromise may be especially challenging. Those who are actually living in or with a particular situation or condition can feel a particular sense of ownership of the associated challenges and it is important that researchers (especially where they do not have such first-hand experience) do not seek to position themselves as ‘the absolute authority’ on these matters and that they recognise the right of those with direct experience to find their own ways of naming and describing those situations and conditions. Finally the lead needs to be constantly mindful of their unique position of power and privilege that comes from operating from within their own home academic domain and ensure that this is exercised fairly and wisely. Internal communication and feedback Good communication is vital, and all project members should be encouraged to share brief updates on their activity. It would also be good practice to hold steering meetings of the 20 entire project group at reasonable intervals and to record the minutes and promptly share these both with attendees and those who were unable to attend. It is important to provide a straightforward avenue for individuals to feedback any concerns privately (whether face-to-face or in writing) and for the project lead (with others as appropriate) to fairly consider any reasonable adjustments that might be made (and indeed which might benefit other individuals and the overall project) in a timely fashion. Continually evaluating with the participants what has been working well, what they are finding most rewarding and what they are finding more challenging, and what recommendations they would have going forward, is a particularly valuable exercise. This could be done anonymously through an online form, through face-to-face interviews or indeed as a whole group, depending on the preferences of the members. Finally, everyone wants to know that their contribution is valuable and valued and it is vital that the contributions and achievements of the co-researchers, who may well be freely offering up their time, are continually acknowledged. Involvement in generating outputs and external communication The conventional output from a research project is an academic paper. Academic papers however are governed by particular sets of conventions and expectations which are usually only acquired through academic training. Therefore it can often be challenging for a member of the public to author or coauthor a paper when used to a more narrative, opinion-focused or conversational style of writing. One potential approach is to frame the observations and analyses of the public/lay participants within the wider academic discourse of the article. Here are a couple of interesting and informative examples of this: Fenge, L.-A., Fannin, A., & Hicks, C. (2012). Co-production in scholarly activity: Valuing the social capital of lay people and volunteers. Journal of Social Work, 12(5), 545–559, https://doi.org/10.1177/1468017310393796 Aabe, N. O., Fox, F., Rai, D., & Redwood EdD, S. (2019). Inside, outside and in-between: The process and impact of co-producing knowledge about autism in a UK Somali community. Health Expectations, 22(4), 752-760, https://doi.org/10.1111/hex.12939 Additionally some journals may be reluctant to have co-researchers included as co-authors if they do not have formal qualifications and/or an institutional affiliation and it may be necessary to explain to the journal editor that they are qualified by experience. Many journals however are becoming increasingly receptive to the participation of experts by experience, recognising this can result in more engaging, timely, relevant and application-focused articles. All submissions to the journal ‘Research Involvement and Engagement’ are reviewed by patients and academics. The ‘British Medical Journal’ (BMJ) through its Patient and Public Partnership strategy https://doi.org/10.1177/1468017310393796 https://doi.org/10.1111/hex.12939 21 (https://www.bmj.com/campaign/patient-partnership) promotes co-production of content. The journal produced a special issue dedicated to co-production ‘Increasing the impact of health research through co-production of knowledge’ at the beginning of 2021: https://www.bmj.com/co-producing-knowledge The journal ‘Frontiers in Sociology’ produced a series of special issues that were co-edited by a reference group of service users: Green, G., Boaz, A. L. and Stuttaford, M. C. (Eds.) (2020). Public Participation in Healthcare: Exploring the Co-production of Knowledge. Frontiers in Sociology: https://www.frontiersin.org/research-topics/7471/public-participation-in-health-care-explor ing-the-co-production-of-knowledge Academic papers however are of course far from being the only type of output or way of disseminating findings. Especially when seeking to engage a non-academic audience a shorter one to four page summary of the main findings and conclusions are often more likely to be read than full length articles. Again to write these summary briefings requires some skill and experience but a lack of formal academic training may not necessarily be a handicap here and including the voice of the expert by experience is likely to impress the readership. Co-researchers are quite often invited to present at academic (and non-academic) conferences and there is equally scope for co-researchers to contribute substantially to more creative platforms for disseminating the research e.g. videos, podcasts. Indeed co-researchers can often play a pivotal role in engaging their peers within a particular interest or service user group, or the public more generally, with the significance and value of a research project. This engagement may be through verbal presentations to interest/service user groups at representative meetings or to a wider public through the media and local and national festivals. 6. Responding to the identity and needs of different groups Professional researchers should be encouraged to think about diversity and inclusivity so their research, where appropriate, takes account of the wide variety of views and interests across different social communities. Co-researchers should be welcomed from all sectors of society regardless of age, disability, sex, gender reassignment, sexual orientation, pregnancy and maternity, race, religion or belief, and marriage and civil partnership status. As well as from these nine groups assigned protected characteristics under the Equality Act 2010, professional researchers should think about how to involve other marginalised or heavily stigmatised communities, such as people from a low socioeconomic class, refugees, carers, the travelling community, the homeless, and people with a positive HIV status. It can be challenging to get a fully diverse and representative group of co-researchers as members of some socio-economic groups may have less time and/or autonomy and/or may feel they lack the necessary education level or skills to participate. Having good relationships with community/patient gatekeeper organisations is key here. A useful article on this https://www.bmj.com/campaign/patient-partnership https://www.bmj.com/co-producing-knowledge https://www.frontiersin.org/research-topics/7471/public-participation-in-health-care-exploring-the-co-production-of-knowledge https://www.frontiersin.org/research-topics/7471/public-participation-in-health-care-exploring-the-co-production-of-knowledge 22 challenge in the health area is: Bonevski, B., Randell, M., Paul, C., Chapman, K., Twyman, L., Bryant, J., Brozek, I. & Hughes, C. (2014). Reaching the hard-to-reach: a systematic review of strategies for improving health and medical research with socially disadvantaged groups. BMC medical research methodology, 14(42), https://doi.org/10.1186/1471-2288-14-42. While also focused on clinical and health research, the NIHR website provides some useful guidance in this area: https://www.nihr.ac.uk/documents/improving-inclusion-of-under-served-groups-in-clinical-r esearch-guidance-from-include-project/25435 The following article looks at the challenge from an environmental/citizen science perspective and contains links to further useful studies: Pateman, R., Dyke, A., & West, S. (2021). The Diversity of Participants in Environmental Citizen Science. Citizen Science: Theory and Practice, 6(1), 9, http://doi.org/10.5334/cstp.369 Supporting those with particular health conditions or disabilities It is worth at the outset devising a short questionnaire that asks people about any particular mobility, dietary or other health needs they might have so as to ensure that these are met. See Appendix D for a sample questionnaire. Important practical considerations when designing engagement activities include: - Thinking about the accessibility of the venue, for instance for wheelchair users or those with sight impairments or learning disabilities. - Thinking about the accessibility of presentations and materials used in workshops. Microsoft Office has advice on making Powerpoint presentations more accessible: https://support.office.com/en-us/article/make-your-powerpoint-presentations-acces sible-to-people-with-disabilities-6f7772b2-2f33-4bd2-8ca7-dae3b2b3ef25 It is worth also considering making the slides accessible in advance for people to download and follow via a Screen Reader. Some individuals may need a support worker to help them attend a meeting or they may be carers and need to cover the cost of arranging care cover. It should be made clear at the start of the project if these expenses will be covered. Alongside these more practical considerations, it is important that everyone is treated with understanding, empathy, dignity and respect. In a report entitled ‘Improving Understanding of Service User Involvement and Identity’, the charity ‘Shaping our Lives’, a national network of service users and disabled people, identified the importance of ‘being prepared to ask questions, listening carefully, thinking before you speak, and respecting others’ views’ in relation to professionals working with service users: https://doi.org/10.1186/1471-2288-14-42 https://www.nihr.ac.uk/documents/improving-inclusion-of-under-served-groups-in-clinical-research-guidance-from-include-project/25435 https://www.nihr.ac.uk/documents/improving-inclusion-of-under-served-groups-in-clinical-research-guidance-from-include-project/25435 http://doi.org/10.5334/cstp.369 https://support.office.com/en-us/article/make-your-powerpoint-presentations-accessible-to-people-with-disabilities-6f7772b2-2f33-4bd2-8ca7-dae3b2b3ef25 https://support.office.com/en-us/article/make-your-powerpoint-presentations-accessible-to-people-with-disabilities-6f7772b2-2f33-4bd2-8ca7-dae3b2b3ef25 23 https://www.shapingourlives.org.uk/wp-content/uploads/2017/11/Service-User-Identity-Re search-Findings2.pdf Working with those in vulnerable situations (e.g. politically/socially repressive regimes, refugees) Particular care is needed when engaging populations whose identity, values or beliefs are at odds with the law and/or public attitudes where they live and mean they are potentially subject to harrassment, discrimination and even persecution (whether inside or outside the law). While it is important to give these populations an opportunity to collaborate on research into improving their situation, it is also important to be careful that in the process their personal safety is not put at unreasonable risk and that there are adequate safeguards. It can be very challenging to understand other countries’ rules and laws and the extent to which these are enforced, and adequate research and consultation with on-the-ground partners is essential. Where activities are within the law, people may still be at risk if these activities are seen by established power groups as disrupting their interests. In these circumstances it can take significant time and investment to build the necessary relations of trust with local communities for successful collaboration. The UK Collaborative on Development Research (UKCDR) has produced ‘Guidance on Safeguarding in International Development Research’ that recommends a framework for anticipating, mitigating and addressing potential and actual harms in the funding, design, delivery and dissemination of research with four cross-cutting principles: the rights of victims/survivors and whistle-blowers; equity and fairness; transparency; and accountability and good governance. The guidance includes specific case studies drawing on community and co-produced projects in the Global South and a series of questions to consider under each of the four guiding principles: https://www.ukcdr.org.uk/wp-content/uploads/2020/04/170420-UKCDR-Guidance-for-Safeg uarding-in-International-Development-Research.pdf The following is a useful thought-piece on pursuing co-production in the area of global health which references a number of useful case studies and stresses the importance of ensuring that community representation is diverse and balanced with marginalised groups/voices being properly included: Tembo, D., Hickey, G., Montenegro, C., Chandler, D., Nelson, Erica., Porter, K. et al. (2021) Effective engagement and involvement with community stakeholders in the co-production of global health research. BMJ, 372 (178). https://www.bmj.com/content/372/bmj.n178 It is worth also noting that a number of research groups at the University of York have expertise in working with these groups, including the Centre for Applied Human Rights (CAHR). https://www.shapingourlives.org.uk/wp-content/uploads/2017/11/Service-User-Identity-Research-Findings2.pdf https://www.shapingourlives.org.uk/wp-content/uploads/2017/11/Service-User-Identity-Research-Findings2.pdf https://www.ukcdr.org.uk/wp-content/uploads/2020/04/170420-UKCDR-Guidance-for-Safeguarding-in-International-Development-Research.pdf https://www.ukcdr.org.uk/wp-content/uploads/2020/04/170420-UKCDR-Guidance-for-Safeguarding-in-International-Development-Research.pdf https://www.bmj.com/content/372/bmj.n178 https://www.york.ac.uk/cahr/ https://www.york.ac.uk/cahr/ 24 Working with young people Working with young people presents its own particular set of challenges in terms of differences in levels of endurance, concentration, knowledge and understanding. While they may be more impetuous, impatient and less subtle in their thinking, younger people may also be more open-minded in their outlook and flexible in their approach and address challenges with greater vitality and passion. They tend to be more effective in reaching out to other young people and may be more successful in engaging with them than older adults as there is greater mutual trust and understanding, plus more of a shared culture and language. This improved engagement then tends to lead to higher quality, more relevant, accessible and translatable research. They however may lack confidence entering a traditionally hierarchical environment like a university, especially if coming from less educated backgrounds and so involving them in areas in which they have expertise by experience and drawing attention to this expertise may be helpful as well as making the format and location of meetings as informal and non-intimidating as possible. There are also important practical considerations including the minor legal status of the participants, the need to obtain appropriate parental/guardian consent from those aged under 16 and safeguarding issues (e.g. DBS checks) from working with what is a more vulnerable population. The NSPCC website has a very helpful section covering all the main areas of safeguarding: https://learning.nspcc.org.uk/safeguarding-child-protection The NIHR Involve website has a page linking to several useful resources on successfully involving children in co-producing research: https://www.invo.org.uk/current-work/involving-children-and-young-people/resources-for-i nvolving-children-and-young-people/ The Wellcome Trust ran several studies on the challenges and benefits of involving young people from around the world in health research, their motivations for getting involved, and which approaches work best. https://wellcome.org/reports/involving-young-people-health-research LSE has produced a guide for running focus groups with children and adolescents: http://www.lse.ac.uk/media-and-communications/assets/documents/research/eu-kids-onli ne/toolkit/frequently-asked-questions/FAQ-34.pdf A useful case study of working with vulnerable young people is included in Helen Thomas-Hughes (2018) Ethical ‘mess’ in co-produced research: reflections from a U.K.-based case study, International Journal of Social Research Methodology, 21(2), 231-242, https://doi.org/10.1080/13645579.2017.1364065 Although intended primarily for use by service providers, commissioners and schools, a useful guide for co-producing with young people has been created by Wolverhampton https://learning.nspcc.org.uk/safeguarding-child-protection https://www.invo.org.uk/current-work/involving-children-and-young-people/resources-for-involving-children-and-young-people/ https://www.invo.org.uk/current-work/involving-children-and-young-people/resources-for-involving-children-and-young-people/ https://wellcome.org/reports/involving-young-people-health-research http://www.lse.ac.uk/media-and-communications/assets/documents/research/eu-kids-online/toolkit/frequently-asked-questions/FAQ-34.pdf http://www.lse.ac.uk/media-and-communications/assets/documents/research/eu-kids-online/toolkit/frequently-asked-questions/FAQ-34.pdf https://doi.org/10.1080/13645579.2017.1364065 25 HeadStart Young People’s Engagement Team and Anne Rathbone, Boingboing Resilience CIC and University of Brighton Centre of Resilience for Social Justice which is available online at: https://www.boingboing.org.uk/wp-content/uploads/2018/11/All-together-now-a-toolkit-fo r-co-production-with-young-people.pdf Social media can be an effective means of engaging this age group but it is also important to bear in mind that not all young people use, or have access to, social media platforms. The NCCPE website has specific information about ensuring safeguarding on-line: https://learning.nspcc.org.uk/safeguarding-child-protection/online-safety-for-organisations- and-groups Once a project is running, it is particularly important to make activities rewarding and fun to hold the interest of the young people, especially if they are children. For instance some projects have used a reward system whereby children earn stickers for each activity they engage with that they then paste into a log book. When they acquire sufficient stickers they win a prize (e.g. a gift token) and special recognition/status (e.g. a bronze/silver/gold medal). For an example of this see: https://www.facebook.com/starworksnetwork/ Reaching economically deprived groups Often the demographic profile of projects is weighted in favour of those with previous or existing experience of higher education and it is perhaps not surprising that these individuals are more confident about and/or disposed towards joining a research project. Those with a higher level of knowledge/intellectual capital may additionally be more likely to hear about such projects through tending to belong to communities and organisations where such opportunities can be easily promoted. The challenge of recruiting individuals from under-represented groups is a considerable one. Many, especially members of more socio-economically deprived groups, might not feel confident in contributing to a research project and indeed might feel daunted and alienated by the very term ‘research’, let alone ‘co-production’. As noted previously, often the best way to reach out to more economically deprived groups is by building up close partnerships with the local ‘gatekeeper’ organisations that represent their interests on the ground. Of vital importance too is the effective handling of remuneration and benefits. For guidance on this important subject please see section 9 of this document: ‘GDPR, ethics, budgeting/remuneration & risk assessment’. Increasing ethnic and racial representation Proper representation from different racial and ethnic groups can also be hard to effect. If seeking to engage people from overseas or from a particular cultural or racial group, it is important to take the time to understand the values and attitudes of this group within the local context (both through reading and speaking to individuals) and then ensure these factors are properly taken into account when determining the approach to be taken and the https://www.boingboing.org.uk/wp-content/uploads/2018/11/All-together-now-a-toolkit-for-co-production-with-young-people.pdf https://www.boingboing.org.uk/wp-content/uploads/2018/11/All-together-now-a-toolkit-for-co-production-with-young-people.pdf https://learning.nspcc.org.uk/safeguarding-child-protection/online-safety-for-organisations-and-groups https://learning.nspcc.org.uk/safeguarding-child-protection/online-safety-for-organisations-and-groups https://www.facebook.com/starworksnetwork/ 26 setting, the format and design of any events (this could range from ensuring that work isn’t scheduled during periods of religious observance to making sure that the venue for an activity is not any place that a particular group might find threatening). Accessible language Careful use of language is very important, especially when working with people whose first language may not be English. Even for those whose native tongue is English, the use of academic and technical terminology and expressions can feel alienating. Indeed some have gone as far as to argue that ‘academics could be seen to use professional cultural capital to develop a linguistic power boundary that co-researchers and volunteers engaged in scholarly writing find difficult to breach’. (See Fenge, L.-A., Fannin, A., & Hicks, C. (2012). Co-production in scholarly activity: Valuing the social capital of lay people and volunteers. Journal of Social Work, 12(5), 545–559, https://doi.org/10.1177/1468017310393796.) This is not to deny that there are circumstances when specialised vocabulary is required or that certain shorthand expressions can be useful but these should be kept to a minimum and clear definitions provided - e.g. in a glossary hand-out. Even non-academic terminology can sometimes be ambiguous and it may be useful for the facilitator to sometimes jump in and ask people what they mean by a particular term or word if this might be unfamiliar or mean different things to different people. DEEP – the UK Network of Dementia voices has some useful guidance on language, which whilst specially designed for communicating with dementia sufferers is useful in designing written materials for any lay group: https://www.dementiavoices.org.uk/deep-guides/for-organisations-and-communities/ Looking after mental wellbeing Participation in a project may make particular emotional demands of the co-researcher/ service user who is not just researching but also experiencing the particular condition or situation. It is therefore very important to consider what needs to be done to properly support that person. For instance where there are individuals with mental health issues or where a particularly emotive or sensitive issue is being discussed, there should be a clearly designated individual whose role is specifically to watch out for and offer support to anyone who may experience emotional distress (which may be triggered by discussions). The following article offers useful perspectives on best practice in co-creating with individuals with mental health problems: See Sarah Carr (2019). ‘I am not your nutter’: a personal reflection on commodification and comradeship in service user and survivor research. Disability & Society, 34 (7-8), 1140-1153, https://doi.org/10.1080/09687599.2019.1608424. https://doi.org/10.1177/1468017310393796 https://www.dementiavoices.org.uk/deep-guides/for-organisations-and-communities/ https://doi.org/10.1080/09687599.2019.1608424 27 It is possible that participants may, close to or at the time, find themselves unable to attend meetings/activities, for whatever reason, and it is important that sufficient flexibility is factored in to allow for this. Where the individual has been unable to attend, it is good practice to offer the possibility of a quick one-to-one catch-up (e.g. over the phone) which would not only prevent that individual from feeling left behind but also reassure them that their involvement continues to be welcomed. 7. Different methodologies and approaches There are a number of different approaches to pursuing co-production; which of these might be most relevant or helpful will depend on the nature of the project and of the demographic participating. It is important to remember that a lot will depend on the skill and experience of the facilitator(s) and project lead(s) and that the more a group works together and the members come to respect and trust each other, the more constructive the interactions will be. It is also worth remembering that some people prefer to express themselves in writing and/or to prepare what they want to say rather than to be ‘put on the spot’, so a mixture of methods and approaches as well as properly briefing people in advance on what to expect can help in ensuring everyone has a chance to comfortably express themselves. Focus groups Perhaps the most conventional tool for collaborating with the public in designing and developing research is the focus group where typically questions will be explored and proposals drafted out through facilitated conversations between groups of usually 8-10 people. Having a strong moderator and an effective note-taker are key considerations here. It is also important to think carefully about what would be an appropriate ratio of facilitators to participants. There is no magic number for this; rather this depends on the needs of the group and whether, if they do have particular support needs, they are being accompanied by carers/support workers etc. There is a wealth of online guidance on best practice in running focus groups. Citizens Advice guide to running focus groups: https://www.citizensadvice.org.uk/Global/CitizensAdvice/Equalities/How%20to%20run%20f ocus%20groups%20guide.pdf University of Kansas Community Toolbox: https://ctb.ku.edu/en/toolkits A variation on the Focus Group is the World Café and this works with a rather bigger number of participants overall, around 40-60 typically. In this scenario the participants are spread across a number of tables of between 8-10 people. Each table explores a different theme or angle on a question and is hosted by a facilitator who leads and mediates the discussion. There is also a scribe who notes down the main comments that arise. After a certain period of time - how long will depend on the nature of the topic but 15-20 minutes is a good https://www.citizensadvice.org.uk/Global/CitizensAdvice/Equalities/How%20to%20run%20focus%20groups%20guide.pdf https://www.citizensadvice.org.uk/Global/CitizensAdvice/Equalities/How%20to%20run%20focus%20groups%20guide.pdf https://ctb.ku.edu/en/toolkits 28 ball-park figure - all the individuals apart from the facilitator and scribe move onto the next table and this rotation continues until everyone has had a chance to contribute to the discussion on each table. It is possible though to have a more limited number of rotations and ask individuals to sign up in advance to whichever conversations they are more interested in pursuing. At the end it is common for the facilitator on each table to summarise the main points of the discussion to the whole group. More information is available on the World Cafe website: http://www.theworldcafe.com/key-concepts-resources/world-cafe-method/ See also: https://workshopbank.com/world-cafe If the intention is to prime the focus group session with any short scene-setting presentations, then it is important to consider whether one or more of these presentations could be given by the co-researchers, particularly anyone who have previously worked on a co-produced research project. Finally, it is important to allow sufficient time at the start to run a couple of ‘ice-breaker’ exercises. The more people have a sense of whom they are talking to, the more relaxed they will feel and able to contribute freely to the discussion. A quick internet search will come up with a variety of ideas - pick a quick fun one that will break the ice and energise people (but make sure this does not exclude anyone with e.g. a physical disability) and another to help people introduce themselves to each other. One technique that works well, because it also introduces a fun element of anticipation and guessing as well as a more sensory dimension, is to ask people to bring along a particular object that means or represents something about them, their experiences and/or their interests and ask them to talk about this to the rest of the group. Arts-based methodologies Use of the arts (including story-telling, performance, art and photography) has often been cited as a powerful way of eliciting engagement, by encouraging individual expression and autonomy on all sides and providing a neutral platform where the co-researcher is no longer potentially at a disadvantage compared with the academic researcher through being less familiar with more standard research protocols. This approach may be particularly useful when the research topic is particularly complex or brings up sensitive issues, or when working with vulnerable groups or groups whose first language may not be English. Some methods and technologies can however take time to grasp and so it is important to have an experienced participatory artist as collaborator. It may be worthwhile to start with some simpler creative exercises which can act as stepping stones and help to elicit people’s preferences in evolving the main approach. Some useful reading and resources: http://www.theworldcafe.com/key-concepts-resources/world-cafe-method/ https://workshopbank.com/world-cafe 29 PASAR project - Participatory Arts and Social Action in Research: https://www.ncrm.ac.uk/research/PASAR/ Purcell, R. (2009). Images for change: community development, community arts and photography. Community Development Journal, 44(1), 111–122, https://doi.org/10.1093/cdj/bsm031. Arts-based methods for Transformative Engagement: https://www.sustainableplaceshaping.net/wp-content/uploads/2018/02/SUSPLACE-Toolkit- Arts-based-Methods-2018.pdf Katherine M Boydell, Michael Hodgins, Brenda M Gladstone, ‘Arts-based health research and academic legitimacy: transcending hegemonic conventions’, Qualitative Research, Vol. 16(6) 681–700, https://doi.org/10.1177/1468794116630040. Teti M., Majee W., Cheak-Zamora N., Maurer-Batjer A. (2019) ‘Understanding Health Through a Different Lens: Photovoice Method’. In: Liamputtong P. (eds) Handbook of Research Methods in Health Social Sciences. Springer, Singapore. https://doi.org/10.1007/978-981-10-5251-4_4 Kimberly Diane Fraser & Fatima al Sayah (2011) Arts-based methods in health research: A systematic review of the literature. Arts & Health, 3(2), 110-145, https://doi.org/10.1080/17533015.2011.561357. Arts for Advocacy: http://artsforadvocacy.org/about/ Some useful case studies: World War One Engagement Centre: Voices of War and Peace: https://www.voicesofwarandpeace.org/ ACCORD: Archaeology Community Co-production of Research Data: https://archaeologydataservice.ac.uk/archives/view/accord_ahrc_2015/ Storying Sheffield: Telling Untold Tales: http://www.storyingsheffield.com/about/ Valerie Dunn, Sally O’Keeffe, Emily Stapley & Nick Midgley (2018) Facing Shadows: working with young people to coproduce a short film about depression, Research Involvement and Engagement, 4(46), https://doi.org/10.1186/s40900-018-0126-y. Jennifer MacAnuff and Lucy Barker (2019) Developing an intervention to support participation in leisure of children and young people with neurodisability, case study no. 3 in INVOLVE (2019) Co-production in Action: Number One. Southampton, INVOLVE, pp. 19-27, https://www.invo.org.uk/wp-content/uploads/2019/07/Copro_In_Action_2019.pdf. Digital techniques and tools https://www.ncrm.ac.uk/research/PASAR/ https://doi.org/10.1093/cdj/bsm031 https://www.sustainableplaceshaping.net/wp-content/uploads/2018/02/SUSPLACE-Toolkit-Arts-based-Methods-2018.pdf https://www.sustainableplaceshaping.net/wp-content/uploads/2018/02/SUSPLACE-Toolkit-Arts-based-Methods-2018.pdf https://doi.org/10.1177/1468794116630040 https://doi.org/10.1007/978-981-10-5251-4_4 https://doi.org/10.1080/17533015.2011.561357 http://artsforadvocacy.org/about/ https://www.voicesofwarandpeace.org/ https://archaeologydataservice.ac.uk/archives/view/accord_ahrc_2015/ http://www.storyingsheffield.com/about/ https://doi.org/10.1186/s40900-018-0126-y https://www.invo.org.uk/wp-content/uploads/2019/07/Copro_In_Action_2019.pdf 30 The rise of digital technologies and social media offers a number of opportunities to creatively engage with and involve different publics: This blog looks at best practice in conducting online focus groups: https://blog.flexmr.net/best-practices-online-focus-groups The NCCPE has produced guidance on ‘Engaging the public through social media’: https://www.publicengagement.ac.uk/sites/default/files/publication/what_works_engaging _the_public_through_social_media_november_2018.pdf NIHR Involve has also produced guidelines on the use of social media to actively involve people in research (this is not so much a how-to guide as a list of best practice tips plus of potential pitfalls/risks and suggestions for mitigating these): https://www.invo.org.uk/wp-content/uploads/2017/07/Social-Media-Guide-web-2017.pdf Social media is probably best used alongside face to face meetings as a way of sustaining engagement with a project, maintaining group morale, and providing a forum for concerns/frustrations. Citizen Science Citizen Science is about getting members of the public actively involved in scientific research. The original principles behind the movement, which originated in the 1990s, were to make science more responsive to citizens’ concerns and needs and to draw on the capacity of citizens to help in producing reliable scientific knowledge. The approach is often associated with involving the public in ecological surveys, for instance identifying and counting invertebrates in a local hedge but can be construed more broadly as any form of public participation and collaboration in any area of research. Mobile digital technology is opening up new possibilities, for instance in terms of gathering different types of data through cameras and inbuilt or added sensors, ‘gamifying’ (i.e. turning into a game) the collecting or processing data, and leveraging social media. The NERC-funded Centre for Hydrology and Ecology has a number of useful guides and apps: https://www.ceh.ac.uk/citizen-science The European Citizen Science Association has come up with ‘Ten principles of citizen science’ that underlie good practice: https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_citizen_science.p df These principles are explored further in Hecker, S., Haklay, M., Bowser, A., Makuch, Z., Vogel, J. (eds.) (2018). Citizen Science: Innovation in Open Science, Society and Policy. London: UCL Press. Available as an open access e-book at: https://www.uclpress.co.uk/products/107618 https://blog.flexmr.net/best-practices-online-focus-groups https://www.publicengagement.ac.uk/sites/default/files/publication/what_works_engaging_the_public_through_social_media_november_2018.pdf https://www.publicengagement.ac.uk/sites/default/files/publication/what_works_engaging_the_public_through_social_media_november_2018.pdf https://www.invo.org.uk/wp-content/uploads/2017/07/Social-Media-Guide-web-2017.pdf https://www.ceh.ac.uk/citizen-science https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_citizen_science.pdf https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_citizen_science.pdf https://www.uclpress.co.uk/products/107618 31 The Citizen Science Association website hosts a number of working groups that have compiled lists of resources in such areas as ethics, evaluation and law and policy: https://www.citizenscience.org/get-involved/working-groups/ The Open Air Laboratories (OPAL), a UK-wide Citizen Science project that involves researchers at the University of York, has a number of useful case studies: https://www.opalexplorenature.org/ Extreme Citizen Science (ExCiteS), based at UCL, is a situated, bottom-up practice that takes into account local needs, practices and culture and works with broad networks of people to design and build new devices and knowledge creation processes that can transform the world: https://www.geog.ucl.ac.uk/research/research-centres/excites The Parenting Science Gang is a ground-breaking, user-led citizen science project that has brought together over 2,500 parents in groups to explore what parenting questions we need evidence-based answers for and how to design experiments to answer those questions: http://parentingsciencegang.org.uk/about/ The RAND Corporation has produced a useful e-book focusing on recent developments in the field: Leach, B., Parkinson, S., Lichten, C. A., Marjanovic, S. (2020). Emerging developments in citizen science: Reflecting on areas of innovation. RAND corporation. https://doi.org/10.7249/RR4401 The following open access book captures the current state of geographic citizen science research and provides critical insights to inform technological innovation and future research: Skarlatidou, A. & Haklay, M. (2021). Geographic Citizen Science Design: No one left behind. London: UCl Press. https://www.uclpress.co.uk/products/125702?utm_source=jiscmail&utm_medium=listserv &utm_campaign=21531J Some useful articles on the potential for using smartphones and gamification: Cartwright, J. (2016). Technology: Smartphone science. Nature, 531, 669–671, http://doi:10.1038/nj7596-669a. Odenwald, S. (2019). Smartphone Sensors for Citizen Science Applications: Radioactivity and Magnetism. Citizen Science: Theory and Practice, 4(1), 18, http://doi.org/10.5334/cstp.158. Spitz, J., Pereira, C., Queiroz, F., Cardarelli Leite, L., Dam, P. and Cantini Rezende, A. (2018). Gamification, citizen science, and civic technologies: In search of the common good. Strategic Design Research Journal, 11(3), 263-273, http://doi.org/10.4013/sdrj.2018.113.11. https://www.citizenscience.org/get-involved/working-groups/ https://www.opalexplorenature.org/ https://www.geog.ucl.ac.uk/research/research-centres/excites http://parentingsciencegang.org.uk/about/ https://doi.org/10.7249/RR4401 https://www.uclpress.co.uk/products/125702?utm_source=jiscmail&utm_medium=listserv&utm_campaign=21531J https://www.uclpress.co.uk/products/125702?utm_source=jiscmail&utm_medium=listserv&utm_campaign=21531J http://doi.org/10.5334/cstp.158 http://doi.org/10.4013/sdrj.2018.113.11 32 Simperl, E., Reeves, N., Phethean, C., Lynes, T. and Tinati, R. (2018). Is Virtual Citizen Science A Game? ACM Trans. Soc. Comput. 1(2,. https://doi.org/10.1145/3209960. Bowser, Anne & Hansen, Derek & He, Yurong & Boston, Carol & Reid, Matthew & Gunnell, Logan & Preece, Jennifer. (2013). Using gamification to inspire new citizen science volunteers. Gamification '13: Proceedings of the First International Conference on Gameful Design, Research, and Applications, 18-25, https://doi.org/10.1145/2583008.2583011. This article has a particular focus on ethical considerations: Kreitmair, K.V. and Magnus, D.C. (2019). Citizen Science and Gamification, Hastings Center Report, 49(2), 40-46, https://doi.org/10.1002/hast.992. This article looks in particular at promoting and sustaining participation through creating communities of practice via social media: Liberatore, A., Bowkett, E., MacLeod, C.J., Spurr, E. and Longnecker, N., 2018. Social Media as a Platform for a Citizen Science Community of Practice. Citizen Science: Theory and Practice, 3(1), http://doi.org/10.5334/cstp.108. Peer/community researchers If a project involves interviewing a large number of individuals from a particular background or ethnic community, or maybe those with a particular health condition then it is worth considering recruiting and training up some members of that community or patient group to do the interviewing. The premise here is that interviewees are prepared to open up more with those whom they can relate to better; likewise that the peer-researchers often bring a greater level of empathy and insight to the process that elicits more effective questions and in turn more sincere and revealing responses. There are a number of case studies where this approach has been found to be successful, including the project ‘People with Learning Disabilities and their access to mainstream cultural activity’ run by Staffordshire University and Assist: https://reachasist.files.wordpress.com/2018/07/people-with-learning-disabilities-and-their- access-to-mainstream-cultural-activity.pdf - see particularly appendix 1, pp. 8-9 Also a project run in Dublin in 2007: Doyle, M., & Timonen, V. (2010). Lessons From a Community-Based Participatory Research Project: Older People’s and Researchers’ Reflections. Research on Aging, 32(2), 244–263, https://doi.org/10.1177/0164027509351477. However there is also a risk that peer researchers’ own experience and views might risk introducing some degree of (unconscious) bias such that they might mis or over-interpret https://doi.org/10.1145/3209960 https://doi.org/10.1145/2583008.2583011 https://doi.org/10.1002/hast.992 http://doi.org/10.5334/cstp.108 https://reachasist.files.wordpress.com/2018/07/people-with-learning-disabilities-and-their-access-to-mainstream-cultural-activity.pdf https://reachasist.files.wordpress.com/2018/07/people-with-learning-disabilities-and-their-access-to-mainstream-cultural-activity.pdf https://doi.org/10.1177/0164027509351477 33 testimonies (especially if were not warned of the possibility) and/or seek out and prioritise experiences/views that match their own. Also the peer researchers may not necessarily pursue an interview in as systematic a way as a trained researcher would. This is not necessarily a disadvantage as while it could mean that certain points are not followed up or pursued as thoroughly as the academic researcher would, it could also result in some insights and perspectives emerging that could not have been foreseen and which a more conventional interviewing technique would have been unlikely to bring to light. These risks and possibilities are discussed in: Littlechild, R., Tanner, D., & Hall, K. (2015). Co-research with older people: Perspectives on impact. Qualitative Social Work, 14(1), 18–35, https://doi.org/10.1177/1473325014556791. Finally it is worth noting that there is the potential for the peer researchers to find this interviewing process upsetting, especially if the individuals interviewed have had or are currently enduring adverse experiences. There is also a danger that the peer researchers may start to feel obliged to help these individuals in response to their confiding in them. Thorough preparation and training are needed to help the peer researchers to maintain a reasonable emotional and personal distance. Mentoring and buddying are important ways of ensuring that peer researchers are supported if they do find certain experiences emotionally challenging. Some studies also describe creating an individual wellness plan to anticipate any potential sources of distress and devise strategies for mitigating these. The following article provides a useful overview of the benefits and potential pitfalls of the ‘peer-interviewing’ approach and explains measures, including in relation to selecting the interviewers and providing training and support, that can be taken to maximise success. Devotta, K. et al (2016). Enriching qualitative research by engaging peer interviewers: a case study. Qualitative Research, 16(6), 661-680, https://doi.org/10.1177%2F1468794115626244. An excellent and comprehensive report on the experiences of community researchers in relation to the benefits and challenges of this way of working was produced as part of the AHRC Connected Communities Programme: Thomas-Hughes, H. (2019). Critical Conversations with Community Researchers – Making Co-Production Happen (University of Bristol and AHRC Connected Communities). https://connected-communities.org/index.php/project_resources/connected-communities-c atalyst-fund-reports-2016-18/ Community researchers may also be involved in the qualitative analysis of the resulting interviews and the following piece explores some of the benefits and challenges of this, including ensuring rigour and achieving a fair balance between the lay and academic perspectives. Haddock G. (2021) Involving an individual with lived experience in a co- analysis of qualitative data. Health Expectations, 00, 1– 10, https://doi.org/10.1111/hex.13188 https://doi.org/10.1177/1473325014556791 https://doi.org/10.1177%2F1468794115626244 https://connected-communities.org/index.php/project_resources/connected-communities-catalyst-fund-reports-2016-18/ https://connected-communities.org/index.php/project_resources/connected-communities-catalyst-fund-reports-2016-18/ https://doi.org/10.1111/hex.13188 34 8. The challenges of taking a co-produced approach Although co-production can be a very rewarding and beneficial approach to doing research, it can also exacerbate existing and/or introduce new challenges. Collaborating with co-researchers means increasing the diversity of views which, though arguably a good thing, inevitably leads to more time and effort being spent in reconciling and accommodating different opinions, priorities and interpretations of the data. There can be more systematic tensions as well between academic researchers who are under pressure to produce highly polished research outputs and co-researchers who care more about seeing findings implemented into practice as quickly as possible. Co-production also introduces particular ethical challenges which are considered more fully in the next section. There are a number of more specific challenges to be aware of. Some general barriers to success ● Limited vision/failure to inspire ● Lack of commitment/willing ● Lack of time ● One partner manipulates or dominates, or partners compete for the lead ● Key interest groups and/or people missing from the partnership ● Lack of support from partner organisations with ultimate decision-making power ● Lack of clear purpose and/or inconsistent understanding of purpose ● Lack of understanding of roles and responsibilities ● Differences of philosophies and manners of working ● Unequal and/or unacceptable balance of power and control ● Hidden agendas ● Failure to communicate ● Lack of evaluation or monitoring systems ● Failure to learn ● Financial and time commitments outweigh potential benefits Although it is impossible to mitigate all these challenges, it is possible to head some of them off by ensuring that at the initial design stage a clear set of overriding values, goals and objectives are collectively generated that the team members can all agree to sign up to. Some particular considerations when embarking on projects with new groups ● Those, including both professional and co-researchers, who have lived experience of a challenge bring particularly valuable insights and real commitment and energy but this emotional investment can be problematic if it stands in the way of ‘cool-headed’ analysis when this is needed. 35 ● Both researchers and participants may have strongly held views and feel uncomfortable when these are challenged. It is important to exercise diplomacy and sensitivity in dealing with this circumstance and if possible to find a way of allowing the individual the opportunity to change their outlook without worrying about losing face. Establishing and maintaining a supportive and egalitarian collaborative environment is key here. ● Some participants may be fixated on a particular aspect of a problem that is important to them and need to be reminded of the bigger picture and of exploring all the issues in a holistic and balanced way in order to maximise the chances of success. ● Research timescales can be frustratingly slow and research methodologies can be difficult to comprehend or even appear counterintuitive to those unfamiliar with them. It is important therefore to take the time to properly explain the rationale behind these methodologies and the (sometimes unavoidable) circumstances behind the protracted timescales. ● Some participants may not be used to workshop-like scenarios and may be reluctant or nervous about contributing or alternatively excessively vocal; therefore having an effective facilitator and nurturing a friendly, trusting and respectful environment and good relations between project members is very important. 9. Practicalities - GDPR, ethics, budgeting/remuneration & risk assessment GDPR If you are going to be capturing any kind of personal data about the project partners and participants then you need to consider the GDPR implications. The University has comprehensive guidance and support on this available: https://www.york.ac.uk/records-management/dp/ Be aware that if you are going to be filming or taking photographs of participants then you will need to seek individual consent from them - or if minors, their parents/guardians - which of course they have every right to refuse. Be wary of using publicly accessible social media accounts e.g. on Twitter, particularly where some of the conversations are likely to be of a sensitive nature and/or individuals may be articulating experiences and views that would not wish to be shared outside the room. Participants could be encouraged instead to compose internal tweets on post-it notes that can be stuck up and shared on a board/window or wall. It goes without saying that it is important to fully acknowledge (e.g. in publications) what contributors have brought to the design and progress of the research and to advancing its https://www.york.ac.uk/records-management/dp/ 36 impact. At the same time however some contributors may wish to remain anonymous so it is vital to check with them first before personally identifying them. Ethics Co-production may introduce ethical complexities that will need to be carefully navigated, and critical reflection and peer support will be essential. Principles of fairness, transparency and consent are paramount. Reasonable measures to safeguard the interests and wellbeing of co-researchers need to be established. Please see section 6 above ‘Recognising and responding to the identity and needs of different groups’ for further information (also risk assessment section below). As noted earlier, the responsibility of all parties to treat each other with fairness and respect should be set out and agreed to right from the beginning. The University has comprehensive guidance on ethics in research: https://www.york.ac.uk/staff/research/governance/research-policies/ethics-code/ Departments also have individual processes around securing ethical approval for new projects. Please consult your department web pages and/or research officer. Please note that the companion booklet to this guide for members of the public clearly sets out these priorities and processes and advises participants that ‘should you decide to participate in a research project and have ethical concerns about the way the research is being conducted, you should contact the project lead, your mentor, or a member of the team’. The following book edited by Sarah Banks and Mary Brydon-Miller is a very valuable resource in exploring the ethical challenges that arise with co-production and provides a host of case studies: https://www.routledge.com/Ethics-in-Participatory-Research-for-Health-and-Social-Well-Bei ng-Cases/Banks-Brydon-Miller/p/book/9781138093430 The National Coordinating Centre for Public Engagement (NCCPE) also hosts useful information about conducting ethical research participatory research on its website: https://www.publicengagement.ac.uk/nccpe-projects-and-services/completed-projects/ethi cs-participatory-research The following article outlines a ten-point approach (which is also summarised in a handy table) to ensuring that from the very start of the design stage full consideration is given to the ethical issues in involving the public in health and social care research and not just in relation to the carrying out of the research for which ethical approval is officially required: Pandya-Wood, R., Barron, D.S. & Elliott, J. (2017). A framework for public involvement at the design stage of NHS health and social care research: time to develop ethically conscious https://www.york.ac.uk/staff/research/governance/research-policies/ethics-code/ https://www.routledge.com/Ethics-in-Participatory-Research-for-Health-and-Social-Well-Being-Cases/Banks-Brydon-Miller/p/book/9781138093430 https://www.routledge.com/Ethics-in-Participatory-Research-for-Health-and-Social-Well-Being-Cases/Banks-Brydon-Miller/p/book/9781138093430 https://www.publicengagement.ac.uk/nccpe-projects-and-services/completed-projects/ethics-participatory-research https://www.publicengagement.ac.uk/nccpe-projects-and-services/completed-projects/ethics-participatory-research 37 standards. Research Involvement and Engagement 3(6), https://doi.org/10.1186/s40900-017-0058-y Budgeting/Remuneration It is important to ensure that co-researchers are fully remunerated for any expenses they may incur in contributing to a project. Some of these expenses, for instance travel, are obvious but there are also less visible costs, including for instance printing, postage, telephone and internet usage. If at all possible arrangements should be made (perhaps through or with a gatekeeper partner organisation) so that co-researchers do not have to make any significant payment in the first instance or are subsidised up front. For instance train tickets could be purchased on their behalf and sent on. This is a particular consideration if hoping to engage less advantaged groups. Nevertheless there may well be some expenses which cannot be paid by the organiser ahead of time. Providing instead a payment in advance to the co-researchers to cover such costs may be challenging given public sector financial regulations. Also there is the possibility that some of these individuals may subsequently drop out. The contributions of all those involved in co-production should be equally valued and in many cases this means paying co-researchers fairly for the time they dedicate. Although certain individuals are happy, or may even prefer, to volunteer their time for free knowing that they are contributing to something socially useful and of potential benefit to them, their families and friends, it may be worth considering whether some form of remuneration is appropriate, depending on the nature of the project and the likely outcomes and the extent of their involvement. This is largely a matter for the discretion of the project leads but it may also be worthwhile consulting with the partner gatekeeper organisations as to what might reasonably be expected. Arguably even more important than financial reward, is taking the time to acknowledge and recognise the input of participants as well as being prepared to offer appropriate practical support, for instance by being a referee on a job application. Whatever course is decided, it is vital to make crystal clear to participants what the remuneration arrangements will be. As well as remunerating individual participants, it is worth making financial provision for supporting any partner community/charitable organisation(s) as few will have the budgetary capacity to cover any extra work. The NIHR Involve website has a range of detailed information and advice on budgeting and remuneration: https://www.nihr.ac.uk/documents/payment-guidance-for-researchers-and-professionals/27 392 https://doi.org/10.1186/s40900-017-0058-y https://www.nihr.ac.uk/documents/payment-guidance-for-researchers-and-professionals/27392 https://www.nihr.ac.uk/documents/payment-guidance-for-researchers-and-professionals/27392 38 Regardless of whether the co-researchers are voluntary or paid, it is crucial that every opportunity is taken to provide training and experiences that will enrich their skills. Remuneration and benefits It is very important when considering whether to remunerate co-researchers to be mindful that this may well affect the entitlement of those who are in receipt of benefits. Payment for participation in a research project may count as employed earnings and there are particular earning thresholds beyond which certain benefits may be reduced or even stopped. The rules governing this are complex and also quite regularly change. Below are some suggested sources of guidance on this but you are strongly advised to also get in touch with Philip Kerrigan Philip.Kerrigan@york.ac.uk to discuss this. - The Social Care Institute for Excellence (SCIE): https://www.scie.org.uk/co-production/supporting/paying-people-who-receive-bene fits - Advice from Judy Scott Consultancy: https://judyscottconsult.com/benefits-information/ Risk Assessment If there is the slightest risk of any harm (mental or physical) or injury befalling participants during workshop or other activity then it is necessary to complete a risk assessment. The University forms for this are here: https://www.york.ac.uk/admin/hsas/safetynet/form_store.htm#r mailto:Philip.Kerrigan@york.ac.uk https://www.scie.org.uk/co-production/supporting/paying-people-who-receive-benefits https://www.scie.org.uk/co-production/supporting/paying-people-who-receive-benefits https://judyscottconsult.com/benefits-information/ https://www.york.ac.uk/admin/hsas/safetynet/form_store.htm#r 39 10. Funding co-production To do co-production well can require substantial resources and it is important to carefully consider the nature of these costs from the start. Encouragingly funders are increasingly attuned to the value of co-production across a broad range of subject areas. As mentioned above, this is highlighted in Section 4.3 of the UKRI in their Delivery Plan 2019 (https://www.ukri.org/files/about/dps/ukri-dp-2019/). Including therefore proportionate and reasonable costs for supporting co-production is usually viewed as entirely appropriate. It is however always worth looking carefully through funding scheme guidance notes and checking with the funder if anything is unclear. Some funders may actually request evidence that an applicant has already involved the public/service/end users in developing the proposal. Some internal funding may be available for holding pilot workshops - you are advised to check with your departmental research officer about this. 11. Advice and support for co-production at the University There is a substantial community at the University of people with considerable expertise and experience in co-production across a range of subject areas (including health, the natural and built environment, food production and consumption, education, the arts and culture, social work, housing) and using a variety of approaches (including arts-based methods, participatory technology development, citizen science) and with a variety of publics and community or third sector organisations (local community, health, schools, charities and the third sector, local government, prisoners and offenders). Most are happy to offer their guidance and support and talk through ideas; also some might well be interested in collaborating. Your departmental research officer should be able to help you identify whom best to speak to. If your query is more specifically around developing a PPI (public and patient involvement in research) plan, for instance as part of an NIHR grant application, then there is additional information, resources and expert support in this particular area available through Involvement @ York email involvement@york.ac.uk. It is worth checking for relevant training offered by the University’s Research Excellence Training Team (RETT). There will also be occasional events to share best practice and explore and resolve common challenges – details of this will be posted on the staff web pages and circulated round departments. 12. Evaluating and evidencing the efficacy of co-production As with all areas of research it is important to be able to evidence the positive impact of research generated through co-production. Impact-linked methodologies, for instance https://www.ukri.org/files/about/dps/ukri-dp-2019/ https://www.york.ac.uk/research/themes/health-and-wellbeing/involvement@york/ mailto:involvement@york.ac.uk https://www.york.ac.uk/staff/research/training-forums/research-excellence-training-team/research-staff/ https://www.york.ac.uk/staff/research/training-forums/research-excellence-training-team/research-staff/ 40 Theory of Change, can be useful and it is worth considering from the project’s inception which metrics and measures, quantitative and/or qualitative, will be most useful in helping to evidence this change/impact. The Generic Learning Outcomes Framework, included on the Arts Council website, is another similarly useful tool. The University of York’s Impact Toolkit (https://www.york.ac.uk/staff/research/research-impact/impact-support/impacttoolkit/) is also a good starting point and contains links to a range of other toolkits as well as useful planning templates. There is also a Toolkit more specifically for evaluating impact from public engagement: (https://www.york.ac.uk/staff/research/research-impact/public-engagement-research/). It can sometimes be challenging for the project lead or indeed a project member to assume the role of evaluator as they may find it hard to achieve the necessary critical distance and/or fellow stakeholders may feel reluctant to voice criticism, especially in face-to-face interviews. It may be worth therefore considering employing a professional evaluator. It is important to remember that co-production should be more than a ‘means to an end’ and pursued for more than simply instrumental or utilitarian reasons; above all that this never descends into some target or accreditation/ranking chasing exercise. The very process of co-producing research, when done properly, has an inherent holistic and democratic value that extends beyond achieving the specific project goals towards more generally fostering accountability, power-sharing and mutual learning and growth. The practice therefore invites formative as well as summative reflection. The following is a very informative and thought-provoking article in this vein: Russell, J., Fudge, N. & Greenhalgh, T. (2020) The impact of public involvement in health research: what are we measuring? Why are we measuring it? Should we stop measuring it?. Research Involvement and Engagement, 6 (63), https://doi.org/10.1186/s40900-020-00239-w Specific quantitative outcomes, e.g. the number of people engaged, may be the easiest to measure but evaluating changes both at a procedural level and in terms of broader community and organisational attitudes and practices is perhaps a truer way of gauging real impact. Such deeper impacts may however sometimes prove challenging to evidence and/or to link to an intervention. The N8 Partnership led by Durham University and with funding from the ESRC produced a valuable report on ‘Mapping Alternative Impact’ that explores the challenge of reconciling the idea of impact as a discrete measurable outcome from research with the reality of co-production as a collective, gradual and diffuse process where impact often comes about indirectly and/or is engendered beyond the official end date of a project: http://eprints.gla.ac.uk/115470/1/115470.pdf This argument is also helpfully deliberated in Tina Cook, Jonathan Boote, Nicola Buckley, Sofia Vougioukalou & Michael Wright (2017) Accessing participatory research impact and legacy: developing the evidence base for participatory approaches in health research, https://www.theoryofchange.org/ https://www.artscouncil.org.uk/measuring-outcomes/generic-learning-outcomes https://www.york.ac.uk/staff/research/research-impact/impact-support/impacttoolkit/ https://www.york.ac.uk/staff/research/research-impact/public-engagement-research/ https://doi.org/10.1186/s40900-020-00239-w http://eprints.gla.ac.uk/115470/1/115470.pdf 41 Educational Action Research, 25(4), 473-488, https://doi.org/10.1080/09650792.2017.1326964. Other useful resources include: Queen Mary University of London Evaluation Toolkit: https://www.qmul.ac.uk/publicengagement/goodpractice/evaluation-toolkit/ O'Mara-Eves, A., Brunton, G., Oliver, S., Kavanagh, J., Jamal, F., & Thomas, J. (2015). The effectiveness of community engagement in public health interventions for disadvantaged groups: a meta-analysis. BMC public health, 15, 129, https://doi.org/10.1186/s12889-015-1352-y. NIHR Involve library of examples of the impact of public involvement in health research: https://www.invo.org.uk/resource-centre/libraries/examples/ Public Involvement Impact Assessment Framework Guidance (PiiAF): https://www.invo.org.uk/wp-content/uploads/2017/07/Social-Media-Guide-web-2017.pdf The Welcome Trust International Public Engagement Workshop Report ‘Engaging with Impact: How do we know if we have made a difference’: https://wellcome.ac.uk/sites/default/files/wtp052364_0.pdf 13. Further resources and reading One of the best sets of resources is the collection of materials generated from the AHRC-led ‘Connected Communities’ project: https://connected-communities.org/ Further guidance on creating successful university - community collaborations are available on the National Coordinating Centre for Public Engagement (NCCPE) website under the UK Community Partner Network page: https://www.publicengagement.ac.uk/connect-with-others/uk-community-partner-network NIHR ‘Involvement for Learning’ within the new NIHR Centre for Engagement and Dissemination, has many valuable resources on its website: https://www.learningforinvolvement.org.uk/ A particularly useful report is ‘Exploring Impact: Public Involvement in NHS, public health and social care research’ by Kristina Staley from 2009 which summarises the findings from a literature review of the evidence of the impact of public involvement on health and social care research, citing both positive impacts and things that can go wrong: https://www.invo.org.uk/wp-content/uploads/2011/11/Involve_Exploring_Impactfinal28.10 .09.pdf ‘Involve’ has also created an initial booklet of case studies of co-production in research (2019): https://www.invo.org.uk/wp-content/uploads/2019/07/Copro_In_Action_2019.pdf https://doi.org/10.1080/09650792.2017.1326964 https://www.qmul.ac.uk/publicengagement/goodpractice/evaluation-toolkit/ https://doi.org/10.1186/s12889-015-1352-y https://www.invo.org.uk/resource-centre/libraries/examples/ https://www.invo.org.uk/wp-content/uploads/2017/07/Social-Media-Guide-web-2017.pdf https://wellcome.ac.uk/sites/default/files/wtp052364_0.pdf https://connected-communities.org/ https://www.publicengagement.ac.uk/connect-with-others/uk-community-partner-network https://www.learningforinvolvement.org.uk/ https://www.invo.org.uk/wp-content/uploads/2011/11/Involve_Exploring_Impactfinal28.10.09.pdf https://www.invo.org.uk/wp-content/uploads/2011/11/Involve_Exploring_Impactfinal28.10.09.pdf https://www.invo.org.uk/wp-content/uploads/2019/07/Copro_In_Action_2019.pdf\",\n",
              " '1 The ERA-NET promoting European research on biodiversity, ecosystem services and Nature-based solutions CITIZEN SCIENCE TOOLKIT for biodiversity scientists BiodivERsA 7 B WHAT IS CITIZEN SCIENCE? 1. Houllier Merilhou-Goudard, 2016. 2. www.citizenscience.org 3. See the guide published by the UK Environmental Observation Framework: Tweddle et al, 2012. 4. Participatory action research “includes citizens in research work. It was developed in the 1940s in social psychology as an alternative to mission-free science considered alienating to theory and practice.” (Pettibone et al, 2016) 5. Civic science is “a science that questions the state of things rather than a science which serves the state” and where “professional scientists as facilitators of tools and information for people” (PublicLab; Peter Levine’s blog) 6. Amateur science “describes the scientific activities of citizens who do not earn their living as scientists” (see Finke, 2014; Mahr, 2014 cited in Pettibone et al, 2016). 7. Community science is generally used as synonym for Citizen Science (see websites of the University of Antwerpen and GenR; PPT by Haklay, 2018). 8. Crowdsourced science: “researchers recruiting members of the public to help them collect data” as opposed to “conduct experiments” (for citizen science) (Alan Turing Institute). 9. Do-it-yourself (DIY) biology is “a rather recent phenomenon and can be described as the pursuit of biology outside of scientific institutions by amateurs, students, “hobbyists””. (Landrain et al, 2013) 10. Community-based monitoring (CBM) is a subset of CS in which local SH use their own resources to monitor natural resources to achieve goals that make sense to them’ (Chandler et al, 2017) 11. See BiodivERsA’s Stakeholder Engagement Handbook. Citizen Science has been contributing to various re- search fields such as astronomy, medicine, art history, and social sciences; but it has proven to be particularly relevant and prevalent in the field of environmental sci- ence. BOX #1 A DEFINITION OF CITIZEN SCIENCE CITIZEN SCIENCE noun. The process of pro- ducing scientific knowledge in which non-scien- tific or non-professional actors — whether indi- viduals or groups — actively and intentionally participate1. Citizen Science can be defined as “the involve- ment of the non-academic public in the process of scientific research – whether community-driven research or global investigations.”2 A specific definition in the context of biodiversity and environmental research is: “Citizen Science, restricted to studies of biodiversity and the en- vironment, is defined as volunteer collection of biodiversity and environmental information which contributes to expanding our knowledge of the natural environment, including biological monito- ring and the collection or interpretation of environ- mental observations.”3 The latter definition highlights the aspect of collection of information but that does not mean that it is limited to that. Indeed, Citizen Science covers a diverse array of approaches and provides both scientific (data col- lection) and engagement benefits. It is also expected that mobilising citizens should lead to other outcomes like citizens’ education, awareness-raising, etc. and has great potential for engagement, education, and action (Haklay et al, 2020; Kelemen-Finan et al, 2018). There is a wide variety of terms (e.g. participatory action research4, civic science5, amateur science6, community science7, crowdsourced science8) which are either syn- onyms for Citizen Science, or have an important over- lap with it in terms of meaning. Different fields have developed their own terminology and some terms even have different meanings in different contexts. In addi- tion, there are many related concepts which are not the same as Citizen Science, but closely associated with it (e.g. ‘Do-it-yourself’ biology9, community-based mon- itoring10, stakeholders’ engagement11, etc.). For more information won Citizen Science terminology, see: Eit- zel et al, 2017. To find or discover Citizen Science projects, there are many existing inventories or databases, like the ones listed in Bibliography & resources: e) Inventories and databases. http://www.sciences-participatives.com/en/Report https://www.citizenscience.org https://www.nhm.ac.uk/content/dam/nhmwww/take-part/Citizenscience/citizen-science-guide.pdf https://publiclab.org/wiki/civic-and-citizen-science-the-cocodrie-barnraising-heats-up https://peterlevine.ws/?p=21019 https://www.buergerschaffenwissen.de/sites/default/files/assets/dokumente/handreichunga5_engl_web.pdf https://www.uantwerpen.be/en/research/science-for-everyone/citizen-science/ https://genr.eu/wp/planning-a-community-science-aka-citizen-science-research-project/ https://fr.slideshare.net/mukih/the-role-of-learning-in-community-science-and-citizen-science https://fr.slideshare.net/mukih/the-role-of-learning-in-community-science-and-citizen-science https://www.turing.ac.uk/research/research-projects/crowdsourced-and-citizen-science https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3740105/ https://www.sciencedirect.com/science/article/pii/S0006320716303639 https://www.biodiversa.org/706/download https://www.tandfonline.com/doi/full/10.1080/09500693.2018.1520405 https://theoryandpractice.citizenscienceassociation.org/article/10.5334/cstp.96/ https://theoryandpractice.citizenscienceassociation.org/article/10.5334/cstp.96/ 8 C THE RISE OF CITIZEN SCIENCE Although citizens have contributed for centuries to the collection of data and specimens of ecological value (Miller-Rushing et al, 2012), the concept of Citizen Sci- ence has received increasing attention and popularity more recently. The numbers of peer-reviewed publica- tions and Citizen Science projects and volunteers have dramatically increased over the last 10 years (see Fig- ures 2a and 2b). Among the underlying reasons are the recent tech- nological advancements and the accessibility of the general public to new technologies (e.g. internet, smartphones, apps…) and the web 2.0 philosophy of user-generated content, information sharing and par- ticipatory culture. These technologies have allowed for better access to and sharing of data. Cheap and reli- able environmental sensors are now also more wide- spread: they facilitate data collection and reduce errors in the geographical and temporal information associat- ed with the records (see Newman et al, 2012; Pimm et al, 2015; Skarlatidou et al, 2019). The development of website interfaces and platforms over the last decade have enabled easy data collection or data hosting (e.g. iNaturalist), data or image anal- yses (e.g. Zooniverse), or even recruiting volunteers (e.g. SciStarter). On GBIF, 50% of occurrence records are Citizen Science observations and 6 of the top 10 datasets are citizen science datasets (Waller, 2019). Historically, biodiversity data collected by volunteers was not well ackowledged in scientific publications (neither the names of the volunteers, nor the use of the term “Citizen Science”). This is now changing (see Fig- ures 2a and 2b) and the improved acceptance of Cit- izen Science (Fritsch-Kosmider, 2018) contributes to making it more visible (Theobald et al, 2015; Chandler et al, 2017; Pocock et al, 2018a). Furthermore, Citizen Science is increasingly recog- nised as a tool to democratise science and promote new forms of collaborations with - and engagement of – citizens. For example, to educate citizens on what research is and, when relevant, to engage citizens in the reseach process; or to take into account their views and expectations as it is the case for other groups of stakeholders (Toomey, 2014; Wittmayer & Janssen, 2019). Figure 2a: Increase in the number of published peer-reviewed articles on citizen science (adapted from: Follet & Strezov, 2015). 0 50 100 150 200 250 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 N um be r o f p ub lic at io ns Date of publication The growth of published peer reviewed articles on Citizen Science Listed in Web of Science Listed in Scopus Listed both in Web of Science AND Scopus https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/110278 https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1890/110294 https://www.cell.com/trends/ecology-evolution/fulltext/S0169-5347%252815%252900212-8 https://www.cell.com/trends/ecology-evolution/fulltext/S0169-5347%252815%252900212-8 https://jcom.sissa.it/archive/18/01/JCOM_1801_2019_E https://www.inaturalist.org https://www.zooniverse.org https://scistarter.org https://data-blog.gbif.org/post/gbif-citizen-science-data/ https://www.newscientist.com/letter/mg23931910-900-citizen-science-is-needed-to-track-biodiversity-1/ https://www.sciencedirect.com/science/article/pii/S0006320714004029 https://www.sciencedirect.com/science/article/pii/S0006320716303639 https://www.sciencedirect.com/science/article/pii/S0006320716303639 https://www.sciencedirect.com/science/article/pii/S0065250418300230?via%3Dihub https://e360.yale.edu/features/interview_caren_cooper_how_rise_of_citizen_science_is_democratizing_research https://drift.eur.nl/publications/citizen-science-towards-democratic-science-and-environmental-democracy/ https://drift.eur.nl/publications/citizen-science-towards-democratic-science-and-environmental-democracy/ https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0143687 10 D DIFFERENT TYPES OF CITIZEN SCIENCE PROJECTS Citizen Science activities or approaches have been classified using different categories or typologies. These existing typologies and the scope of Citizen Sci- ence can be determined through different perspectives or aspects, such as: • Who are the volunteers? They range from mass public to expert naturalists or retired professionnals, from stakeholders (e.g. farmers) to school children. • What are their roles and tasks? They can record observations, collect samples on the field, imple- ment simple experiments, contribute to analysing data, providing ideas and input in setting up the re- search questions, etc. • When do the volunteers contribute? Citizen Sci- ence can include volunteers at different stages of the research process, from study design and devel- opment of research questions, to the validation of research results. • How do scientists and citizens collaborate? Many options of collaborations are possible, from scien- tist-led projects where citizens operate under their coordination, to community-driven projects where citizens are taking the lead and partner with research institutions. Bonney et al, 2009 considers the level of participation or engagement of the volunteers in the research pro- cess to distinguish three main types of projects: • Contributory projects, which are generally de- signed by scientists and for which members of the public primarily contribute data; • Collaborative projects, which are generally de- signed by scientists and for which members of the public contribute data but also help to refine project design, analyse data, or disseminate findings; • Co-created projects, which are co-designed by scientists and members of the public. At least some of the public participants are actively involved in most or all steps of the scientific process. Another classification adds to these types the concept of “Extreme Citizen Science” which requires the scien- tists not only to act as experts, but also as facilitators. (Figure 3). Figure 3: Levels of participation in Citizen Science projects (after Haklay, 2013) An more detailed overview of these models of Citizen Science projects and how both experts and volunteers are involved at several stages of the projects has been illustrated in Pocock et al, 2015a. Van Noordwijk et al. (2020) identifies four Citizen Sci- ence approaches that present particularly strong op- portunities for environmental impact: • place-based community action projects - for local communities; • educational/captive research learning projects - for schools, employees, museums, etc.; • interest-group investigation projects - for citizens with prior knowledge/expertise/interest in the topic; • mass-participation census projects - for the general public. This demonstrates that different types of projects tend to attract different audiences, and are suitable in differ- ent contexts. Participation in citizen science • Collaborative science – problem definition, data collection and analysis Level 4 ‘Extreme citizen science’ • Participation in problem definition and data collection Level 3 ‘Participatory science’ • Citizens as basic interpreters Level 2 ‘Distributed intelligence’ • Citizens as sensors Level 1 ‘Crowdsourcing’ https://files.eric.ed.gov/fulltext/ED519688.pdf https://povesham.files.wordpress.com/2013/09/haklaycrowdsourcinggeographicknowledge.pdf https://academic.oup.com/biolinnean/article/115/3/475/2440508 11 E COMMON CITIZEN SCIENCE APPROACHES OR ACTIVITIES 12. https://reeflifesurvey.com/ 13. https://www.bigbutterflycount.org/about 14. Pettibone et al, 2016 15. https://www.nhm.ac.uk/take-part/citizen-science/bioblitz.html In the field of environmental sciences (biodiversity and ecosystems), examples of common methods include: BIOLOGICAL OBSERVATIONS AND RECORDINGS A popular task citizens can do to contribute to science projects is to record their observations (e.g. of species occurrence) in the wild, with no specific objective or indication, and upload them on platforms such as iN- aturalist. After validation, the data are made accessi- ble through infrastructures like the Global Biodiversity Information Facility (GBIF) in particular for researchers who can benefit from large datasets and coverage. Volunteers can support scientists to record observa- tions within a specific scheme, e.g. related to specific species, locations or timeframes (e.g. Reef Life Sur- vey12 in which trained SCUBA divers undertake un- derwater surveys of reef biodiversity; The Big Butterfly Count13, one of the world’s largest butterfly surveys). Another option is to request already existing sources or data to support the research, e.g. for the Landscape Change14 project, people were asked to retrieve old pictures of landscapes from their region and take new pictures of the current landscapes, in order to observe and monitor changes. Depending on the complexity and the framing of the task, biological observations and recordings can be set up as mass participation censuses or long-term in- terest group monitoring schemes. BIOBLITZ A BioBlitz is an intense period of biological surveying in an attempt to record all the living species within a designated area. The aim is to discover as many spe- cies of plants, animals and fungi as possible, within a set location and over a defined time period. A BioBlitz usually comprises a mixture of wildlife experts and the wider public and is considered Citizen Science if the collected data are shared for use by scientists (Robin- son et al, 2013). Many institutions, museums, botanic gardens, etc. or- ganise BioBlitzes each year15 with the aim of engaging the general public in scientific activities. https://reeflifesurvey.com/ https://www.bigbutterflycount.org/about https://www.buergerschaffenwissen.de/sites/default/files/grid/2017/11/20/handreichunga5_engl_web.pdf https://www.nhm.ac.uk/take-part/citizen-science/bioblitz.html https://www.inaturalist.org https://www.inaturalist.org https://www.gbif.org https://www.gbif.org https://www.nhm.ac.uk/content/dam/nhmwww/take-part/Citizenscience/bioblitz-guide.pdf https://www.nhm.ac.uk/content/dam/nhmwww/take-part/Citizenscience/bioblitz-guide.pdf 12 ONLINE CROWDSOURCING The Oxford Dictionary indicates it “consists in obtain- ing information or input into a task or project by enlist- ing the services of a large number of people, typically via the Internet”16. Online crowdsourcing projects use the time, abilities and energies of a distributed commu- nity of volunteers. An example of this type of Citizen Science is the Snap- shot Serengeti project17 hosted by Zooniverse, which uses citizens’ time and skills to identify wildlife species from pictures taken from automatic camera traps in the Serengeti National Park. COLLECTING SAMPLES Scientists can also request the help of volunteers to directly collect specimens or samples on the field, whether in terrestrial or aquatic ecosystems. Two examples are: a Canadian Citizen Science project monitoring plastic pollution in freshwater ecosystems (Forrest et al, 2019) where citizens would filter water samples possibly containing microplastics, and the Bay Area Ant Survey18, where volunteers would collect ants and send them back to the research centre they were working with. Projects can go a step further by providing participants with testing kits, allowing them to analyse samples in the field and uploading data, rather than sending sampels off to a lab. This is for example the case in the global FreshWater Watch19 programme. CITIZEN EXPERIMENTS Citizen experiments are small, controlled experiments set up by the volunteers and reported centrally. Unlike for observational studies, the volunteer must set up the experiment, look after it and report on the results. Some or all of the materials may be supplied centrally, but often the equipment needed is minimal. Some Cit- izen Science projects specifically ask volunteers to set up experiments, e.g. in their own gardens. Examples include the Garden Organic 2019 Members Experiments20, the Teabag Index project21, or the Big Bumblebee Discovery Project22, the BIOVEINS project (see Box #3). 16. https://www.lexico.com/en/definition/crowdsourcing 17. https://www.zooniverse.org/projects/zooniverse/snapshot-serengeti 18. https://scistarter.org/bay-area-ant-survey 19. https://freshwaterwatch.thewaterhub.org/ 20. https://www.gardenorganic.org.uk/2019experiments 21. http://www.teatime4science.org/ 22. Roy et al, 2016 https://link.springer.com/article/10.1007/s10661-019-7297-3 https://scistarter.org/bay-area-ant-survey https://freshwaterwatch.thewaterhub.org/ https://www.lexico.com/en/definition/crowdsourcing https://www.zooniverse.org/projects/zooniverse/snapshot-serengeti https://scistarter.org/bay-area-ant-survey https://freshwaterwatch.thewaterhub.org/ https://www.gardenorganic.org.uk/2019experiments http://www.teatime4science.org/ https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0150794 14 PART II: BENEFITS AND CHALLENGES » A. The benefits of Citizen Science in research projects » B. Potential challenges associated with Citizen Science 16 A THE BENEFITS OF CITIZEN SCIENCE IN RESEARCH PROJECTS 23. See: http://www.biodiversa.org/1738/download Citizen Science can be beneficial to both scientists and citizens, and to society at large (Hecker et al, 2019). In addition, numerous studies have shown that Citizen Science can contribute positively to biodiversity re- search (e.g. Amano et al, 2016; Chandler et al, 2017; McKinley et al, 2017, etc.). We asked researchers involved in BiodivERsA-funded projects about their appreciation of Citizen Science through a survey in 2018: 51 people from 35 different projects responded in the online questionnaire (on a total of 73 projects). A vast majority of respondents po- sitively valued Citizen Science, whether or not they had already used it in their project (see Figure 4). A presen- tation of the detailed results of the survey is available online23. The added-value and positive aspects of Citizen Science for research and researchers were also dis- cussed during the BiodivERsA Citizen Science workshop on the 3rd of April 2019. The results of the group discussions and the answers to the survey (com- plemented by some readings and experts’ contribu- tions) on the perceived advantages of Citizen Science by scientists have been merged into the list below. Figure 4: Pie charts showing the perceived added value of Citizen Science (CS) by researchers with CS experience (left) and the intentions of using CS in the future (right) by researchers with no CS experience. Result from a BiodivERsA survey in 2018. Intention of using citizen science (CS) in the future by researchers with no CS experience. Perceived added value of citizen science (CS) by researchers with CS experience. 71% 9% 9% 11% Intention of using citizen science (CS) in the future by researchers with no CS experience. yes maybe no n/a 69% 31% Perceived added value of citizen science (CS) by researchers with CS experience. high value medium value low or no value http://www.biodiversa.org/1738/download https://theoryandpractice.citizenscienceassociation.org/article/10.5334/cstp.230/ https://academic.oup.com/bioscience/article/66/5/393/2468614/Spatial-Gaps-in-Global-Biodiversity-Information https://www.sciencedirect.com/science/article/pii/S0006320716303639 https://www.sciencedirect.com/science/article/pii/S0006320716301963?via%3Dihub 17 Citizen Science can… 24. https://www.goodreads.com/quotes/3243300-no-one-will-protect-what-they-don-t-care-about-and • Increase and/or improve research data in terms of amount and spatial/temporal coverage, data rich- ness… It allows for large scale research & large/di- verse datasets otherwise unavailable and data col- lection in locations that are not normally accesible to researchers (e.g. private gardens). • Enable the set-up of regular long-term monitor- ing programmes, which are particularly relevant to study species/ecosystems dynamics and to doc- ument impact of environmental changes (which in turn, are very important for putting in place policy measures). • Save time and money: the work done by volunteers has not to be performed by the researchers themselves, hence saving projects costs (and consequently, public funds). E.g. researchers have calculated that the work done by volunteers on Zooniverse has been worth ~37 years of researcher’s time (Cox et al, 2015). Within the FreshWater Watch programme, the return on investment for the lead scientists was more than 9 hours of sampling time for each hour of training (Thornhill et al, 2016). • Help research to account for citizen’s needs: citizens can help formulate research questions which are based on their knowledge and needs. As such, research can become more societally relevant. • Give access to new methods and distributed knowledge, spread within the local area and shared by the community. It thus expands the scope of the researchers who can learn new techniques and methodologies, and enhances capacity-building. • Bridge the gap between scientists and citizens: with citizens involved, their trust in science is in- creased and a better mutual understanding between citizens and scientists is possible. This breaks down the barrier between researchers and society and in- creases public acceptance of research results. • Be a powerful means for awareness-raising and education on environmental issues. Engaging citi- zens in environmental/biodiversity research projects leads to a greater awareness on the current state of the natural world. It is a way of educating people of all ages on these issues and explaining how they can change habits to reduce their impact on nature. It also helps to reconnect people with nature, which might result in better protection of biodiversity (as “No one will protect what they don’t care about; and no one will care about what they have never experi- enced” by David Attenborough24). • Involve a variety of stakeholders: multiple exter- nal actors can be consulted and involved in the re- search process, giving insight into different perspec- tives and, in some cases, to more balanced points of view. • Support/reinforce the uptake of research by practicioners and policy-makers: research can be more impactful when using Citizen Science. Practitioners and policy-makers can take more care of research results when society is involved in science approaches, because the message is then conveyed not only by scientists but also by citizens. This creates a general increase in legitimacy, credibility and visibility of science by society. • Contribute to personal development: Citizen Science also helps researchers to open up to new types of interactions. Not all Citizen Science projects will generate all of these benefits and projects should be designed with the target audience and desired impacts in mind to maximize their success. Two examples retrieved from BiodivERsA-funded pro- jects demonstrate that outcomes of Citizen Science projects can be multifold (Boxes #2 & 3). https://www.goodreads.com/quotes/3243300-no-one-will-protect-what-they-don-t-care-about-and https://www.zooniverse.org https://ieeexplore.ieee.org/document/7106413 https://academic.oup.com/bioscience/article/66/9/720/1753768 18 BOX #2 CITIZEN SCIENCE TO GENERATE DATA, BUT ALSO PROMOTE AWARENESS, SENSE OF BELONGING, AND CITIZENSHIP Highlights from a BiodivERsA project The UrbanGaia project (2017-2020) capitalised on the untapped knowledge of the many existing Green-Blue Infrastructures (GBIs) in urban context. The project aimed to develop realistic indicators to evaluate, manage, and de- velop performant GBIs in cities (i.e. Vilnius, Leipzig, Ghent, Coimbra) and in intensively managed landscapes. Citizen Science aspect and activities implemented: The MapNat (MAPping NATure’s services) smartphone app was developed for users (both citizens and scien- tific researchers) to map nature’s services provided by green infrastructures, in particular: • mapping of biking, hiking, bird watching, and picnicking spots, in urban areas and the countryside • reporting environmental issues such as bad water quality, pests, and plants causing allergies or hay fever Records are sent from the phones of all users to a server that collects and processes the records which are then made accessible to all users. This aims at developing strategies based on the participatory involvement of all citizens expressing their opinions with advanced IT and communication technology (preferences on the planning and management of the green and blue infrastructure). This contributes to increased knowledge on environmental services, which may influence policy and planning processes aiming at more sustainable cities. It is expected to increase participation, citizenship through self-awareness and commitment, the sense of be- longing, and ultimately reinforce democracy. Best practices: what went well? To ensure commitment/motivation: • Citizens need to have full access to information • Citizens’ contributions need to be considered and used • Feedback needs to be provided • Access to the final results by the citizens who were involved What was challenging and which solutions were found? A major challenge was the involvement of the “non-converted” citizens. It was not easy to reach the entire po- pulation, i.e. not only the ‘activists’ or the ones that regularly use urban GBIs’ environmental services for their individual activities. The aim was also to increase awareness and commitment. To this end, the young students’ population, above 10 years old - namely the basic and secondary students - was focused because there are cities without uni- versities and because researchers aimed at targeting all the urban areas. Links: • Contact: Antonio Dinis Fereira, CERNAS-ESAC-IPC Portugal, aferreira@esac.pt • Project website: http://urbangaia.eu • MapNat app: https://www.ufz.de/index.php?en=40618 • Presentation: http://www.biodiversa.org/1732/download mailto:aferreira%40esac.pt?subject= http://urbangaia.eu https://www.ufz.de/index.php?en=40618 http://www.biodiversa.org/1732/download 19 BOX #3 CITIZEN SCIENCE TO EDUCATE CITIZENS TO RESEARCH Highlights from a BiodivERsA project The main objective of the project BIOVEINS (2017-2020) was to pro- vide, together with local stakeholders, the knowledge to identify the critical features of green and blue infrastructure (GBI), to guide the es- tablishment, management and restoration of GBI, and to mitigate the effects of major urban global challenges, like habitat fragmentation, air pollution, and urban heat islands. Citizen Science aspect and activities implemented: Building on the lessons learned from an existing Belgian project (AIRbezen), BIOVEINS has put in place citizen science projects in 6 cities across Europe (Tartu, Poznan, Zurich, Antwerp, Paris and Lisbon). The campaign was called strawbAIRies Europe - Air quality and pollination success of plants in European cities. The objective was to estimate the spatial distribution of (mainly traffic-related) pollution in the city by means of strawberry plants placed and treated by citizens, to collect simultaneously data on air quality and pollination. Citizens had to: • Place pots outside a windowsill at their house, school… • Care for the plants for 2 months (May-June 2019) • Follow-up berry and pod production • Collect and deliver samples (end June 2019) Bringing citizens in contact with science was the primary objective, data collection was secondary. Best practices: what went well? • Win-win situation for citizens and scientists demonstrated • Proposed method easy to understand, simple to do, not requiring too much work • Open and frequent communication (website, Facebook, e-mail, media, info moments...), but avoiding too much communication • If sensitive data: communication with all stakeholders (beforehand) • Take care of communication with the press • Foresee enough time to answer questions, personal feedback is expected What was challenging and which solutions were found? • Language barriers • Attract potential collaborators • Solution: use students Links: • Contact: Roeland Samson, University of Antwerp, roeland.samson@uantwerpen.be • Project website: http://www.bioveins.eu/ • Blog: http://www.bioveins.eu/blog/strawbairies • Presentation: http://www.biodiversa.org/1733/download mailto:roeland.samson%40uantwerpen.be?subject= http://www.bioveins.eu/ http://www.bioveins.eu/blog/strawbairies http://www.biodiversa.org/1733/download 20 B POTENTIAL CHALLENGES ASSOCIATED WITH CITIZEN SCIENCE Although there are many recognised advantages of using Citizen Science, there are still some barriers that prevent it from becoming a widespread and inclusive practice among the biodiversity research community (Figure 5; Table 1). Figure 5: Citizen Science approaches have great potential but also raise many questions that need to be properly considered (credit: Designed by rawpixel.com / Freepik). Table 1: List of potential challenges (FOR engaging with citizens or WHEN engaging with them) from the scientists’ perspectives, as well as some pathways to overcome them. These have been identified through the discussions during the BiodivERsA Citizen Science workshop’s roundtables and from answers to the survey conducted by BiodivERsA, complemented by a selection of relevant readings (Geoghegan et al, 2016; UKEOF, 2017, Pocock et al, 2018b). POTENTIAL CHALLENGES WAYS TO OVERCOME THEM 1. Citizen Science approaches are not always rele- vant nor useful for the research • not all projects require Citizen Science methods or data (e.g. project based on modelling) • not always fit for specific taxonomic groups, re- search questions or scopes • Citizen Science is not compulsory in biodiversity re- search projects… • …but investigate if Citizen Science could be an added-value to the project (e.g. using data of ob- servations by citizens to complement the data pro- duced by the researchers) 2. Citizen Science can lead to poor data quality/ reliability & scientific bias • distrust regarding data collected or generated by non-professionals • scientific bias and sampling issues related to sample size, sample coverage, lack of randomness, sam- pling intensity, under- or overestimation of species abundance, more observations in human-managed areas,… • for interviews: difficulty to synthetize results, to judge the reliability of the information provided,… • risk of ending up with inconsistent data • problems related to data format and availability • use methods and protocols for data validation • ensure protocols are suitable and easy to use for the target audience, test and adjust protocols before ro- ling out the project • run data quality control/verifications by professio- nals • consider mixed approaches where volunteers col- lect certain data, complemented with other data sources See Kosmala et al (2016) on assessing Citizen Science data quality; Aceves-Bueno et al (2017) on assessing data accuracy; Jacobs (2016) for examples. https://www.freepik.com http://www.ukeof.org.uk/resources/citizen-science-resources/MotivationsforCSREPORTFINALMay2016.pdf http://www.ukeof.org.uk/resources/citizen-science-resources/MotivationsforCSREPORTFINALMay2016.pdf http://www.ukeof.org.uk/resources/citizen-science-resources/barriers https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2664.13279 https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/fee.1436 https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/bes2.1336 https://www.ubiquitypress.com/site/chapters/10.5334/bax.f/download/246/ 21 POTENTIAL CHALLENGES WAYS TO OVERCOME THEM 3. The skills or training of the volunteers might be insufficient • difficulty to find citizens with specific skills (e.g. di- vers able to dive at a certain depth of relevance to the research) • citizens may not be suited for tasks which are highly specialised and require strong scientific background • some volunteers have limited technical knowledge (e.g. correct access/use of webportal or app) • lack of time to recruit and train the volunteers • look for volunteers through specialised associa- tions, institutions, networks (e.g. divers’ club, etc.). • Example: The Riverfly Partnership. • recruit among students • facilitate access/use of technical tools by adapting them to citizens • make time for proper training for citizens 4. It can be difficult to find volunteers • difficulty to attract potential collaborators, to make them interested in the project, and to engage them • some citizens are already occupied with other tasks • too few participants in meetings, too few respon- dents to questionnaires/interviews • search for specific associations or organisations (e.g. Earthwatch) • use help of students • ask some representatives (e.g. mayors) to organise meetings and advertise local populations • repeat contacts (phone, email,…) when reaching out to potential volunteers 5. It is difficult to sustain volunteers in the long-term • continuity of the engagement (and sustainability of the project) is not always guaranteed • need to manage expectations of citizens • not easy to keep long-term motivation of volunteers • engage amateurs or students who have an initial in- terest in the research topic or objective • communicate well in advance and understand vo- lunteer’s expectations • provide opportunities for progression and collabo- ration across the volunteer community • take care of acknowledgement, recognition, and feedback (see point 7) 6. Resources, money, time, and skills (for Citizen Science) are often lacking in research projects • Citizen Science practices require time and money investment by research, e.g. coordination of the work, communicating with citizens, training, enga- gement, reporting,… • need to develop and maintain websites, portals, apps,… • Citizen Science approaches require skills (e.g. me- diation, facilitation, etc.) or methods (e.g. how to design interviews) that many scientists do not have • research projects do not last long enough to build a ‘brand’ or reputation among the volunteers (unlike projects like iNaturalist or eBird who have gained the trust of their users over time) • be realistic about the costs of good quality Citizen Science and prepare an adequate funding plan • foresee time/resources dedicated to Citizen Science upstream, at research proposal stage • show that where money and time are spent, it is re- gained elsewhere in the project • mobilise skilled scientists within the research consortium and/or ensure proper training of scien- tists • find supplemental funding https://www.riverflies.org/Get_Involved https://earthwatch.org https://www.inaturalist.org https://ebird.org/home 22 POTENTIAL CHALLENGES WAYS TO OVERCOME THEM 7. Citizen Science is not always acknowledged as good science • it is still not largely accepted by the scientists (e.g. it is not always considered as ‘proper science’) • it is not properly valued in scientific careers • belief that publications based on Citizen Science data are unreliable, less relevant, or even unpubli- shable • some peer reviewers might be more reserved during the peer review process • institutions and funders might be more sceptical and less inclined to support projects using Citizen Science • support better acceptance by the scientific com- munity, e.g. by communicating on successful pro- jects, on the reliability of data derived from Citizen Science, etc. • funders like BiodivERsA partners are increasingly clear that Citizen Science projects are welcomed in their calls for research proposals (On the positive impact of CS in terms of scientific publications: Mallapaty, 2018; Van Vliet et al, 2014) 8. Problems of tools or language can act as barriers • language barriers (access of scientific langage to ci- tizens; and linguistic barriers when working in diffe- rent countries) • lack of a ready-to-use but reasonably customizable web-based platform to collect data from citizens • assumption that citizens have access to internet/ mobile phone and are comfortable with technology • organise meetings and Q/A sessions between scientists and citizens, including regarding scientific terminology • allow processes to take time and drawing on addi- tional expertise • have multiple alternative strategies for working towards the same goal • be flexible with technical aspects (e.g. accept contributions by email) • listen to volunteers and understand their needs, co-create the project where possible, involve them throughout the process 9. The Citizen Science landscape is too fragmented • the field of Citizen Science is very fragmented • no championing of Citizen Science at high level • this fragmentation can even make some scientists unaware that the datasets they use are Citizen Science data • institutions, funders, projects, scientists, etc., should communicate on exemplary projects (this toolkit is an example!) • each project should identify a champion to commu- nicate on the project • use the structures and networks already available (i.e. ECSA, the EU citizen science community,…) 10. Funders are having different expectations towards Citizen Science projects • because more work can be performed through Ci- tizen Science, more outputs and results are expec- ted from the project • funders might not be ready to provide the same fun- ding amount if volunteers are involved in the project (as they will be expected to do part of the work for free) • funding rules may restrict capacity to fund Citizen Science organisations the researchers would like to work with • because more work can be performed through Ci- tizen Science, more outputs/results are expected from the project • describe clearly - at project proposal stage - how much resources are needed to include Citizen Science and how much outputs are expected to be produced (e.g. Citizen Science is not about produ- cing the same amount of data for cheaper, but also about producing more or different knowledge with the same budget) • funders should consider the capacity to support re- levant organisations • build on the growinmg body of scientific literature on the effectiveness and impact of Citizen Science https://www.natureindex.com/news-blog/data-brief-citizen-science-papers-have-more-impact https://europepmc.org/article/med/24705824 https://ecsa.citizen-science.net https://eu-citizen.science 23 POTENTIAL CHALLENGES WAYS TO OVERCOME THEM 11. Conflicts might arise between groups of citizens • conflicts might arise between different groups of citizens participating to the project but having diffe- rent interests • citizens can be partial in reporting the data to reflect their own views or political objectives (Nature, 2015) • use mediation/conflict resolution skills • question and understand volunteers’ motives and interests • cooperating with representatives of local associa- tions or NGOs to coordinate and mediate among the citizens 12. The outcomes of Citizen Science beyond production of data/information should be given increased recognition • some scientists and some funders may forget that the added value of Citizen Science goes beyond the production of data/information: raising awareness, educating citizens to science, developing the sense of a place, etc. is a very important aspect • discuss in advance in the research consortium about the manifold expected outcomes of the planned Ci- tizen Science approaches; explicit them in the re- search proposal • funders should recognise the multiple outcomes of Citizen Science (this is the case in, e.g., the evalua- tion criteria used in BiodivERsA calls) 13. There might be concerns about data privacy and safety • Issues linked with data privacy and safety may be particularly critical in Citizen Science projects • Consider legal frameworks (e.g. GDPR regulation) and data ownership. (see Bowser et al, 2014; Eleta et al, 2019; Groom et al, 2016) Some practical examples taken from BiodivERsA-funded projects illustrate well some of the potential challenges identified above and ways to tackle them (see Boxes #4-6). https://www.nature.com/news/rise-of-the-citizen-scientist-1.18192 https://dl.acm.org/doi/10.1145/2540032 https://theoryandpractice.citizenscienceassociation.org/articles/10.5334/cstp.171/ https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2664.12767 https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2664.12767 24 BOX #4 THE POTENTIAL AND LIMITS OF DATA COLLECTION THROUGH CITIZEN SCIENCE RATHER THAN BY RESEARCHERS ONLY Highlights from a BiodivERsA project The GreenFutureForest project’s (2016-2019) overall objective was to identify national forestry and conservation strategies that produce wood in a sustainable way. The strategies accounted for the future global demand for wood and the supply of wood in EU countries during the coming 100 years assuming different scena- rios of socio-economic development. Citizen Science aspect and activities implemented: The project considered using Citizen Science data (from the website www.Artportalen.se where citizens upload sightings of species; also from GBIF.org) for statistical modelling and future projection of the distribution and dynamics of species instead using systematically collected data. The activities included: • Citizen Science data download, cleaning, and preparation; systematic collection of field data in parallel • Fit species distribution models using Citizen Science data; fit models using systematically collected data • Formulate scenarios of future forestry and conservation • Compare temporal projections using models based on Citizen Science data vs. models based on systema- tically collected data. Best practices: what went well? • Small differences in projected species occurrence between models based on Citizen Science data and mo- dels based on collected data • Citizen Science data seem a suitable source of data for projecting future species occurrence What was challenging and which solutions were found? Citizen Science generates data with quality not always guaranteed. Reporting frequency varies (non-randomly) through time, across space, between habitats. Reports of species presences, but hard to confirm species absences. Solution: For specific taxa, contact professional reporters and ask for verifying that not reported species can be interpreted as absence of these species. Links: • Contact: Tord Snäll, Swedish University of Agricultural Sciences, tord.snall@slu.se • Project webpage: http://www.popecol.org/research/greenfutureforest/ • Presentation: http://www.biodiversa.org/1731/download https://www.artportalen.se https://www.gbif.org mailto:tord.snall%40slu.se?subject= http://www.popecol.org/research/greenfutureforest/ http://www.biodiversa.org/1731/download 25 BOX #5 CITIZEN ENGAGEMENT IN RESEARCH : THE NEED TO SPEAK A COMMON LANGAGE Highlights from a BiodivERsA project The project ENABLE (2016-2019) aimed to advance knowledge of how to design and implement green and blue infrastructure (GBI) to maximize its potential to deliver numerous social and en- vironmental benefits, such as social inclusion, health and human wellbeing, storm water retention and habitat functions. This was achieved by developing and testing multi-method assessment frameworks, analytical tools and approaches for evaluating GBI performance. Citizen Science aspect and activities implemented: ENABLE heavily relied on participatory research. Two main approaches were used, i.e. interviews with citizens and workshop-based exercises (i.e. in Barcelona, Spain). This allowed public engagement, iterative study design, research question definition, knowledge integration, along with evaluation, validation, and co-develop- ment of policy options. In particular, the objectives were: • determining priority areas for green infrastructure • determining demands for ecosystem services • determining spatial ecosystem services deficiencies Best practices: what went well? Two primary tracks with regards to citizen science were followed: • knowledge co-creation, starting with the problem definition • knowledge translation (to make sure knowledge is actionable and packaged to fit with the intended target process/discourse) What was challenging and which solutions were found? It was particularly challenging to find a common language (and other tools) for constructively taking discus- sions forward. Solution: allow the processes to take time, draw on additional expertise, have multiple alternative strategies for working towards the same goal... Links: • Contact: Erik Andersson, Stockholm Resilience Centre, erik.andersson@su.se & Johannes Langemeyer, UAB, Johannes.Langemeyer@uab.cat • Project website: https://www.researchgate.net/project/ENABLE • Presentation: http://www.biodiversa.org/1734/download mailto:Johannes.Langemeyer%40uab.cat?subject= https://www.researchgate.net/project/ENABLE http://www.biodiversa.org/1734/download 26 BOX #6 THE POTENTIAL AND DIFFICULTIES OF MOBILIZING TARGETED GROUPS OF CITIZENS Highlights from a BiodivERsA project The MARFOR project (2017-2019) aimed to understand past and predict future consequences of global change for biodi- versity of marine forests, by the geographical distribution of functional traits, genetic biodiversity and connectivity, and their consequences for stakeholders linked to blue-green ecosys- tem infrastructures formed by marine forests along European coastlines. Citizen Science aspect and activities implemented: The project had a dedicated task on Citizen Science to contribute to the building of a database of distribution records (over space and time) of seaweed, seagrass, corals, sponges and other forest-forming species, initia- ting continuous monitoring of the distribution of NE Atlantic marine forests. The citizens here were volunteer divers able to go as deep as 50-60m. Citizen Science activities included: • recordings of spatial and temporal data on NE Atlantic marine forests • localisation of deep populations of macroalgae species (Laminaria digitata) These recordings could be provided through different sources: pictures taken during dives, references in lite- rature, specimens from herbariums, etc. The main outcome is a worldwide database and worldwide maps with distribution records. Best practices: what went well? • As of 9 July 2019, there were 6,060 records (with pictures) of 629 species by 287 volunteers • Errors and bias: less were found in Citizen Science data than in scientific data • Records were obtained in areas where nothing was known • Allowed awareness-raising & interest of volunteers for the research project What was challenging and which solutions were found? • Access to the site (some people did not like to have to get a login and password) • Solution: accept contributions by e-mail • Finding people able to dive to 50-60 m depth • Solution: contact diving clubs or - much better - directly known divers Links: • Contact: Ester Serrao, CCMAR, Portugal, eserrao@ualg.pt • Project websites: marfor.eu and www.marineforests.com • Presentation: http://www.biodiversa.org/1730/download https://www.marineforests.com mailto:eserrao%40ualg.pt?subject= http://marfor.eu https://marineforests.com http://www.biodiversa.org/1730/download 27 28 PART III: KEY PRINCIPLES & RECOMMENDATIONS FOR SUCCESSFUL CITIZEN SCIENCE » Principles » Factors determining the benefits of Citizen Science, and recommentations 30 A PRINCIPLES The European Citizen Science Association, ECSA, has established ten key principles of Citizen Science (ECSA, 2015a) that underlie good practice in Citizen Science (see Box #7) which have been widely recognised and taken up by many organisations and professionals. Applying these might help to overcome some of the potential challenges identified above (see Part II Benefits and Challenges). BOX #7 PRINCIPLES FOR SUCCESSFUL CITIZEN SCIENCE ECSA’s Ten key Principles: 1. Citizen science projects actively involve citizens in scientific endeavour that generates new knowledge or understanding. 2. Citizen science projects have a genuine science outcome. 3. Both the professional scientists and the citizen scientists benefit from taking part. 4. Citizen scientists may, if they wish, participate in multiple stages of the scientific process. 5. Citizen scientists receive feedback from the project. 6. Citizen science is considered a research approach like any other, with limitations and biases that should be considered and controlled for. 7. Citizen science project data and meta-data are made publicly available and where possible, results are published in an open access format. 8. Citizen scientists are acknowledged in project results and publications. 9. Citizen science programmes are evaluated for their scientific output, data quality, participant experience and wider societal or policy impact. 10. The leaders of citizen science projects take into consideration legal and ethical issues surrounding co- pyright, intellectual property, data sharing agreements, confidentiality, attribution, and the environmental impact of any activities. Other relevant key principles summarised from existing publications include: 1. Consider the Citizen Science aspect at project inception: consider IF and WHICH citizen science ap- proach could fit (Citizen Science is not always possible or relevant). • For a checklist of questions to consider from beginning to end of project, see Pettibone et al, 2016. 2. To ensure the success of a Citizen Science project, volunteers should be well trained and clear methods and protocols should be developed. • Quality of datasets produced by volunteers can be very high, provided adapted methods and protocols are used (Kosmala et al, 2016). 3. Data management is central to Citizen Science, especially if dealing with open data practices. • See DataONE’s Data Management Guide (Wiggins et al, 2013) • For data analysis examples, see Hill et al, 2011; van Strien et al, 2013 • On the openness of Citizen Science data, see Groom et al, 2016 4. Communications with volunteers and feedback on the project are essential. For a list of questions to ask regarding communications & ideas on how to promote good feedback/acknowledge citizens’ work, see Pettibone et al, 2016. 5. Ethical and legal aspects should be considered when working with citizens and Citizen Science data. • For a detailed exploration of these questions, see Resnik, 2019. mailto:https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_citizen_science.pdf?subject= mailto:https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_citizen_science.pdf?subject= https://www.buergerschaffenwissen.de/sites/default/files/assets/dokumente/handreichunga5_engl_web.pdf https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/fee.1436 https://old.dataone.org/sites/all/documents/DataONE-PPSR-DataManagementGuide.pdf https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2011.00146.x https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/1365-2664.12158 https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2664.12767 https://www.buergerschaffenwissen.de/sites/default/files/assets/dokumente/handreichunga5_engl_web.pdf https://theoryandpractice.citizenscienceassociation.org/articles/10.5334/cstp.150/ 31 B FACTORS DETERMINING THE BENEFITS OF CITIZEN SCIENCE, AND RECOMMENDATIONS Although there are many potential benefits of using Citizen Science, whether they are realised depends on a number of factors related to the study design and its execution. Below we provide an overview of the key factors that should be considered when planning for Citizen Science approaches. DEFINE AND UNDERSTAND THE VOLUNTEERS/CITIZENS • Who are they? • Why would they want to get involved? • How will you reach, recruit, and retain them? • Does the task you want them to undertake fit their motivations? DATA QUALITY • What quality is needed? • What knowledge and training will participants need? Does this fit the volunteer profile you are aiming for? • Building data checks into the system PRACTICAL CONSIDERATIONS • Is the time commitment needed to set-up a Citizen Science system, recruit and train volunteers etc. worth the return you are likely to get? • Do you have the resources for marketing, communi- cation and volunteer support needed to run a suc- cesful project? • Partner with experienced Citizen Science organisa- tions • What are the ethical implications? In addition, the guide of the Natural History Museum of London (Tweddle et al, 2012) proposes a flowchart of different steps to follow when setting up a Citizen Science project (see Figure 6). Finally, Van Noordwijk et al. (2020) analyses the link between different forms of Citizen Science (see typology under I.d What are the different types of Citizen Science projects?), their environmental impact, and how the projects should look like to be successful, and proposes six pathways to achieve maximum environmental impact. Identify question This could be driven by scienti�c, community or policy needs Choose a citizen science approach Establish project team De�ne project aims Identify funding and resources Identify and understand target participants Design the survey or scheme Consider data requirements, storage & analysis Consider technological requirements Test and modify protocols Develop supporting materials Promote and publicise the project Accept data and provide rapid feedback Plan and complete data analysis and interpretation Report results Share data and take action in response to data Evaluate to maximise lessons learned Before you start First steps Development phase Live phase Analysis and reporting phase Figure 6: Proposed method for developing, implementing and evaluating a citizen science project (after: figure by the UK Centre for Ecology & Hydrology, see Tweddle et al, 2012). mailto:https://www.nhm.ac.uk/content/dam/nhmwww/take-part/Citizenscience/citizen-science-guide.pdf?subject= mailto:https://www.nhm.ac.uk/content/dam/nhmwww/take-part/Citizenscience/citizen-science-guide.pdf?subject= 32 PART IV: CONCLUSION 33 Citizen Science has increased and developed rapidly in the last decades and its range of activities and approaches have largely expanded beyond data collection. Citizen Science does not refer to a single method or concept, but covers an array of different possibilities and opportunities. Citizen Science should be considered as a tool, which may be the main tool to use in some projects or one tool amongst others, either for data collection or analysis, but also for the engagement of citizens as stakeholders. To make sure that the Citizen Science activities and the colla- boration with citizens are sucessful, the selection and deve- lopment of the format and type of approaches should match the project’s objectives and design. As shown throughout this toolkit, the number of available re- sources to develop and implement a Citizen Science project is very high, but research projects can build on existing in- sights and, where possible, partner with experienced prac- titioners, to avoid some of the common pitfalls. Now, we wish you good luck for your Citizen Science projects and collaborations ! Credit: Albert De Jong https://www.sovon.nl/en/actueel/nieuws/oratie-ruud-foppen-teken-van-versterken-citizen-science 34 BIBLIOGRAPHY & RESOURCES',\n",
              " 'UZH Publikation A4 Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 6 Citizen Science What is Citizen Science? Citizen Science is defined in the Oxford English Dictionary as “scientific work undertaken by members of the general public, often in collaboration with or under the direction of professional scientists and scientific institutions”. In fact, there are different definitions and interpretations of the term in the literature, which depend often on the context. Citizen Science can differ across research fields and in terms of design processes, participation levels, and engagement practices. It includes top-down, researchers- driven approaches and more bottom up, community-driven practices. The European Citizen Science Association (ECSA) defines Citizen Science as “an ‘umbrella’ term that describes a variety of ways in which the public participates in science, with two main characteristics in common: (1) citizens are actively involved in research, in partnership or collaboration with scientists or professionals; and (2) there is a genuine outcome, such as new scientific knowledge, conservation action or policy change.” It may also be useful to examine the two key terms that describe the methodology, as they are still the subject of (sometimes heated) discussions because of their potential for misinterpretation. The term “citizens” has broad meaning in this context and refers to people with a varied range of knowledge and skills, who may or may not have a formal scientific education. It stands in contrast to “scientists”, who have received a formal academic education in the specific field of research of the Citizen Science project, and work in academia or other research institutions. To avoid any potential misunderstanding with the terms, in this document we will refer to two kinds of actors: “project organizers” and “project participants”. Whereas the project organizers are the ones initiating the project, project participants are the collaborators. How that collaboration looks and what role the project participants take depends largely on the level of engagement built-in by design in the Citizen Science project. Concerning the production of “scientific knowledge”, this stands for the kind of knowledge that can be recognized by the (professional) scientific community as following established scientific methods. “Production of scientific knowledge” implies that participants are involved in the material and cognitive process of scientific research or inquiry. Finally, Citizen Science projects may include a social component and may thus be seen as democratizing science and promoting social and/or environmental justice. Again different cases exist, including projects aimed at a pure investigation for research objectives, and/or projects aimed at collecting evidence to influence policy. Typologies of Citizen Science Citizen Science projects come in different forms, involving different degrees of collaboration between project organizers and project participants, and with different levels of quality for the produced scientific result. Several classifications exist in the literature, ladders that reflect the increasing level of involvement of participants in the different phases of the scientific research process. For the purpose of this document, we limit our considerations to the three types according to the most common typology: • Contributory projects – designed by professional researchers, with members of the public primarily contributing data. • Collaborative projects – designed by professional researchers, with members of the public contributing data as well as helping to refine project design, analyse data and/or disseminate findings. • Co-created projects – designed by professional researchers together with members of the public. At least some members of the public are actively involved in most or all aspects of the research process. Citizen Science Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 7 Co-Creation With co-creation, participants are involved in various aspects of the project, including for example the development of the research question, the choice of data collection methods, the process of data analysis, and more. Co-created projects are seen as an excellent model of Citizen Science, as they foster a more significant public’s participation, which brings the most to both communities – the citizens/public and the academic world. For the academic researchers, they not only increase research capacity but they also produce socially robust knowledge. They also enjoy increased likelihood of serendipitous discoveries. With Citizen Science, researchers contribute to increasing scientific literacy in the general public by fostering understanding of the specific topic, insight into science, and development of new skills and abilities. For the citizens/public, it is the opportunity to discover the entire scientific research cycle and gain a deeper understanding of the importance of science – also as a supporter of (social) decision making. Consequences include community development, empowerment, and change of attitudes, values and norms, action to improve the environment, and engagement in policy making. Furthermore, involving participants at an early stage of the process, as with co-creation, gives them the opportunity to express their unique interest in the project and its outcomes. This includes their potential concerns, and what they personally want to get out of participating. Creating common stakes and a shared sense of “ownership” will help engage an active community of volunteers. Citizen Science «Zurich Style» “Citizen Science Zurich Style” is the choice by the two universities supporting the Citizen Science Center in Zurich (University of Zurich (UZH) and ETH Zurich (ETHZ)) to qualify their joint effort as a unique endeavor to focus on co-created projects that feature academic-quality processes and results. This means that the Citizen Science Center in Zurich is striving to support: • activities and projects that maximize the collaboration between project organizers and project participants in all phases of the research process (“co-created”); • projects that apply established Citizen Science processes and standards to ensure the production of academic- quality data and results. The Center believes that the next phase in the development of Citizen Science as a discipline is a better integration of the participants into the whole process of science: not just data collection and analysis, but also participating in the different phases of the research process, from ideation, planning and implementation all the way to analysis of data and publication of results. Typologies of Citizen Science Projects Citizen Science Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 8 If you want to know more about Citizen Science We recommend you to explore the European Citizen Science Association (ECSA) platform, where you can find resources about the methodology in many different languages, a list of projects, and the (ongoing) research work on defining the ‘Characteristics of Citizen Science’. • ECSA platform https://ecsa.citizen-science.net/ • ECSA resources https://eu-citizen.science/resources • ECSA projects https://eu-citizen.science/projects • Research work on ‘Characteristics of Citizen Science’ https://zenodo.org/communities/citscicharacteristics For terminology, see: • ECSA Principles of Citizen Science https://osf.io/xpr2n/ • Eitzel, M. V. et Al. (2017). Citizen Science Terminology Matters: Exploring Key Terms. Citizen Science: Theory and Practice, 2(1): 1. http://doi.org/10.5334/cstp.96 Literature Where is the term coming from? Here the historical papers, still relevant today, that defined the field: • Bonney, R. (1996). Citizen Science: a lab tradition. Living Bird, 15(4): 7–15. • Bonney, R. et Al. (2009). Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education (Report). Center for Advancement of Informal Science Education. https:// www.informalscience.org/public-participation-scientific- research-defining-field-and-assessing-its-potential- informal-science. • Irwin, A. (1995). Citizen Science: A Study of People, Expertise and Sustainable Development. Routledge. • Shirk, J.L. et Al. (2012). Public Participation in Scientific Research: A Framework for Deliberate Design. Ecology And Society, 17(2): 29–48. https://www.ecologyandsociety. org/vol17/iss2/art29/. Other handbooks resources We are not the first ones producing a Citizen Science handbook! While this booklet is sometimes tailored to the specifics of Citizen Science in Zurich, you may find useful tips (especially for applying Citizen Science in specific fields) in the work done by other colleagues worldwide: • Citizen Science for all. A guide for Citizen Science practitioners https://www.buergerschaffenwissen.de/sites/ default/files/grid/2017/11/20/handreichunga5_engl_web. pdf (Germany, all fields) • Open Science Training Handbook https://open-science- training-handbook.gitbook.io/book/open-science-basics/ citizen-science • The Citizen Science Manual https://citizenscienceguide. com/homepage (US, for Environmental Monitoring) • Citizen Science Toolkit https://www.biodiversa.org/1770 (Europe, Biodiversity) • Community Planning Toolkit https://www.community planningtoolkit.org/sites/default/files/Engagement.pdf (UK, Community Engagement) • Federal Crowdsourcing and Citizen Science Toolkit https://www.citizenscience.gov/toolkit/howto/# (US, for Government) • Communication in Citizen Science, A practical guide to communication and engagement in citizen science https://www.scivil.be/sites/default/files/paragraph/ files/2020-01/Scivil%20Communication%20Guide.pdf (2019, Communication) • Citizen Science jenseits von MINT https://www.hof.uni- halle.de/publikation/citizen-science-jenseits-von-mint/ (2020, Humanities) • Kultur und Gesellschaft gemeinsam erforschen https://www.hof.uni-halle.de/publikation/kultur-und- gesellschaft-gemeinsam-erforschen (2020, Humanities) Citizen Science R ES O U RC ES , L IN K S, A N D H IN TS https://ecsa.citizen-science.net/ https://eu-citizen.science/resources https://eu-citizen.science/projects https://zenodo.org/communities/citscicharacteristics https://osf.io/xpr2n/ http://doi.org/10.5334/cstp.96 https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://www.ecologyandsociety.org/vol17/iss2/art29/ https://www.ecologyandsociety.org/vol17/iss2/art29/ https://www.buergerschaffenwissen.de/sites/default/files/grid/2017/11/20/handreichunga5_engl_web.pdf https://www.buergerschaffenwissen.de/sites/default/files/grid/2017/11/20/handreichunga5_engl_web.pdf https://www.buergerschaffenwissen.de/sites/default/files/grid/2017/11/20/handreichunga5_engl_web.pdf https://www.buergerschaffenwissen.de/sites/default/files/grid/2017/11/20/handreichunga5_engl_web.pdf https://www.buergerschaffenwissen.de/sites/default/files/grid/2017/11/20/handreichunga5_engl_web.pdf https://open-science-training-handbook.gitbook.io/book/open-science-basics/citizen-science https://open-science-training-handbook.gitbook.io/book/open-science-basics/citizen-science https://open-science-training-handbook.gitbook.io/book/open-science-basics/citizen-science https://open-science-training-handbook.gitbook.io/book/open-science-basics/citizen-science https://citizenscienceguide.com/homepage https://citizenscienceguide.com/homepage https://citizenscienceguide.com/homepage https://www.biodiversa.org/1770#DEUXdeux https://www.biodiversa.org/1770 https://www.communityplanningtoolkit.org/sites/default/files/Engagement.pdf https://www.communityplanningtoolkit.org/sites/default/files/Engagement.pdf https://www.communityplanningtoolkit.org/sites/default/files/Engagement.pdf https://www.citizenscience.gov/toolkit/howto/# https://www.citizenscience.gov/toolkit/howto/# https://www.scivil.be/ https://www.scivil.be/ https://www.scivil.be/sites/default/files/paragraph/files/2020-01/Scivil%20Communication%20Guide.pdf https://www.scivil.be/sites/default/files/paragraph/files/2020-01/Scivil%20Communication%20Guide.pdf https://www.hof.uni-halle.de/publikation/citizen-science-jenseits-von-mint/ https://www.hof.uni-halle.de/publikation/citizen-science-jenseits-von-mint/ https://www.hof.uni-halle.de/publikation/citizen-science-jenseits-von-mint/ https://www.hof.uni-halle.de/publikation/kultur-und-gesellschaft-gemeinsam-erforschen/ https://www.hof.uni-halle.de/publikation/kultur-und-gesellschaft-gemeinsam-erforschen/ https://www.hof.uni-halle.de/publikation/kultur-und-gesellschaft-gemeinsam-erforschen/ Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 9 Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 10 Ideation Step by step guide in this chapter. Ideation Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 11 Is Citizen Science the right metho- dology for your project? At the origin of a Citizen Science project stands a question or possibly an issue (social, philosophical, …) for which the solution requires a scientific research approach. Once the research question(s) have been identified and there is an initial idea of what the project could be about, the next step is to explore if Citizen Science is the right methodology to use. When and why should Citizen Science be considered? Project organizers should consider Citizen Science as a methodology from the very beginning of their research process, and regardless of the field of research, as successful Citizen Science projects exist in many domains of science, arts and humanities. However, some of its characteristics make Citizen Science a particularly interesting research methodology when in presence of specific requirements. In particular: • The study would benefit from data collected across a large geographical area and/or for a long period of time Certain studies require data from large areas to be scientifically meaningful and reliable. Similarly, data can be needed either continuously or periodically (for example every month or every year) over long periods of time. The contribution of project participants can make collecting such data cost-effective when compared to conventional methods. • The study requires analysis of large amounts of data Despite the fast rise of Artificial Intelligence, humans still outperform computers in tasks such as sorting and analysing certain kinds of digitally-collected data (e.g. images, documents, audio/video files). For example, this may involve identifying and recording presence, behaviour, frequency, or duration of various phenomena in digital media, or geolocating images. When such tasks can be completed online via a web interface, participants with proper training and guidance can accurately perform most of them. • The study has potential for local and/or social impact Citizen Science is well suited for interdisciplinary collaboration, particularly for projects that include both natural and social aspects, or focus on connections between humans and their environment. By connecting and engaging different stakeholders in local communities – scientists, regulators, decision-makers, volunteers, families, and others – Citizen Science can facilitate a shared understanding of research questions and make them more locally relevant, ultimately helping scientists, communities, and policy makers. • The study requires data about meaningful intellectual or social engagement in real life The value of examining when, how, and with which consequences individuals engage in meaningful activities is becoming increasingly recognized, as important research and practical questions about individuals can be analysed and answered. Citizen Science projects and platform naturally provide such “additional” data. Some examples: observation of physical and biological phenomena; seasonal events; air and water quality monitoring; monitoring of environmental changes and effectiveness of management practices; monitoring of personal/health related factors and behaviours. In the humanities, transcription and/or translation of hand-written documents, historical mapping, interpretation of documents, and more. Level of Involvement As already mentioned, there are different typologies of Citizen Science projects. The diagram below illustrates the key differences of the three most common groups, i.e. contributory, collaborative, and co-created projects. While in contributory projects participants may only contribute to the data collection or data analysis, complexity and engagement increase as we move to collaborative and then to co-created endeavours, where project participants are involved in some or all phases of the scientific research process and may also become project co-organizers. Once decided that Citizen Science is the right approach, the next step is to think how much and at which points participants could and should be involved, as the typology that you choose will have an impact on the planning of your project. However, it is important to acknowledge that not all participants are interested in being involved in every stage of the research process. As early as possible – ideally already during project planning – they should have the possibility to decide how and when they wish to contribute in the different stages. This can generate “inner” and “outer” circles of participants, with those in the innermost circle being project co-owners and/or organizers and people in the outer circles that might want to participate only by collecting data. Ideation Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 12 The important aspects distinguishing co-created Citizen Science projects from other projects are that 1) participants are able to decide for every step of the process if and how they wish to be involved and 2) participants are peers and partners, and they do have equal rights when it comes to decision-making. Community Management As Citizen Science projects depend largely on project participants, one of the major tasks is to build and sustain a community around them. A community is defined by the Cambridge Dictionary as a group of people who have similar interests or who want to achieve something together. However, motivations, backgrounds and skill-sets of volunteers may vary considerably. One of the first milestones to build a community is to develop a community management strategy (see Planning and Design phase), which will be based on questions such as: who are the people most likely to be interested in the topic? What would their motivation be to participate? What type and level of involvement would they want and which would be adequate for the project? You should also evaluate which kind of audience you need to address. Some projects are better suited for very specific groups (e.g. amateur herpetologist, German dialects lovers, etc.), while others are general enough to be potentially interesting for the larger public. This choice influences the ways to reach out and promote the project, including channels, formats, and level of the content. Why do people participate? Understanding their motivations is essential to enhance recruitment, ensure good retention rates and ultimately make the Citizen Science project a success. Researchers have investigated reasons for participating in a variety of projects, and these research studies suggest that volunteers are motivated by a combination of many different factors, including: • Interest in the research topic (first motive overall for joining a project!) • Learning new information • Contributing to original research • Enjoying the research task • Sharing the same goals and values as the project • Helping others and feeling part of a team • Recognition of contribution and feedback on contribution Motivations can change over time, so constant monitoring (i.e. communication and feedback loops) are essential to make participants feel appreciated, to make sure they enjoy participating, and to promote any activities surrounding the project (forums, chats, etc.). Stronger motivation can also be achieved by clearly highlighting the benefits of participating. In general terms, the main benefits include increased scientific literacy (increased knowledge of the topic studied, but also insight into science in general and acquisition of new skills and abilities), understanding of the scientific process and method (especially in co-created The level of involvment has effects on when and how you include project participants. Ideation Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 13 projects, it includes the ability to formulate a problem based on observation, develop hypotheses, design a study, and interpret findings), facilitated access to science information (e.g. one-on-one interaction with scientists), facilitated access to a network of peers with similar interests, and increased ability to interpret scientific information and data. Benefits may also include learning something and generating information about oneself, or creating valuable information/data with the potential for multiple uses. Additional benefits, that come from seeing science as a tool to increase social learning and trust among diverse groups, can motivate the more socially engaged participants. These include increases in community-building, social capital, but also changes in attitudes, norms and values (e.g. about the environment, about science, about institutions), and the possibility for Citizen Scientists to take action to influence policy and/or improve their living environment. When does community building take place? This depends on your decision on the involvement of contributors. If you choose co-creation, then you should involve contributors as early as possible in the process, i.e. already in the formulation of the research questions. In the case of contributory or collaborative projects you can start building your community later – shortly before the implementation phase. Definitions • Community: a group of people who have similar interests or who want to achieve something together (Cambridge Dictionary). • Community Building: describes the development of communities. In order to attract a group of participants (through content, common goals or incentives), each project organizer needs a solid concept. • Community Management: follows the community building and refers to the active moderation and organization of the community. It involves taking care of the members, responding to their needs and ensuring interaction. Blogs • ETH Library https://blogs.ethz.ch/crowdsourcing/ • Citizen Science Center https://citizensciencezurich.blog/ • Österreich forscht https://www.citizen-science.at/blog Literature • Aristeidou, M. et Al. (2017). Profiles of engagement in online communities of citizen science participation. Computers in Human Behavior, 74: 246–256. https://doi. org/10.1016/j.chb.2017.04.044. • Bonney, R. et Al. (2009). Citizen Science: a developing tool for expanding science knowledge and scientific literacy. BioScience, 59(11): 977–984. http://dx.doi. org/10.1525/bio.2009.59.11.9. • Citizenscience.gov. Step – 3 Build a Community. https:// www.citizenscience.gov/toolkit/howto/step3/# • Dickinson, J. et Al. (2010). Citizen Science as an Ecological Research Tool: Challenges and Benefits. Annual Review of Ecology, Evolution, and Systematics, 41(1): 149–172. https://www.annualreviews.org/doi/10.1146/annurev- ecolsys-102209-144636. • Raddick, M. et Al. (2013). Galaxy Zoo: Motivations of citizen scientists. Astronomy Education Review, 12(1):010106. http://dx.doi.org/10.3847/AER2011021. • Rüfenacht, S. et Al. (2021). Communication and Dissemination in Citizen Science. In: Vohland, K. et Al. (eds) the Science of Citizen Science: 475–494. Springer. https://link.springer.com/chapter/10.1007/978-3-030-58 278-4_24. • Stukas, A. et Al. (2016). Motivations to Volunteer and Their Associations With Volunteers Well-Being. Nonprofit and Voluntary Sector Quarterly, 45(1): 112–132. https:// journals.sagepub.com/doi/10.1177/0899764014561122. • Veeckman, C. et Al. (2019). Communication in Citizen Science. A practical guide to communication and engagement in citizen Science. SCIVIL. https://www. scivil.be/sites/default/files/paragraph/files/2020-01/ Scivil%20Communication%20Guide.pdf. Ideation R ES O U RC ES , L IN K S, A N D H IN TS https://blogs.ethz.ch/crowdsourcing/ https://citizensciencezurich.blog/ https://www.citizen-science.at/blog https://doi.org/10.1016/j.chb.2017.04.044 https://doi.org/10.1016/j.chb.2017.04.044 http://dx.doi.org/10.1525/bio.2009.59.11.9 http://dx.doi.org/10.1525/bio.2009.59.11.9 https://www.citizenscience.gov/toolkit/howto/step3/# https://www.citizenscience.gov/toolkit/howto/step3/# https://www.annualreviews.org/doi/10.1146/annurev-ecolsys-102209-144636 https://www.annualreviews.org/doi/10.1146/annurev-ecolsys-102209-144636 http://dx.doi.org/10.3847/AER2011021 https://link.springer.com/chapter/10.1007/978-3-030-58278-4_24 https://link.springer.com/chapter/10.1007/978-3-030-58278-4_24 https://journals.sagepub.com/doi/10.1177/0899764014561122 https://journals.sagepub.com/doi/10.1177/0899764014561122 https://www.scivil.be/sites/default/files/paragraph/files/2020-01/Scivil%20Communication%20Guide.pdf https://www.scivil.be/sites/default/files/paragraph/files/2020-01/Scivil%20Communication%20Guide.pdf https://www.scivil.be/sites/default/files/paragraph/files/2020-01/Scivil%20Communication%20Guide.pdf Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 14 Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 15 Planning and Design Scoping Developing a successful Citizen Science project is an iterative process. It includes finding the research question, defining project objectives, exploring effective methodologies, evaluating data requirements for meaningful results, estimating the potential interest for the project participants, verifying compliance with existing Citizen Science criteria and principles, and iterating this process until full optimization of each component is reached. In addition, this process needs to be approached – from the very beginning – with the perspective of all involved players. For all project organizers it is useful to start by thinking of the reasons why participants’ contribution is needed for the desidered study and outcome, and why Citizen Science may be the best approach, exploring the specific strengths and challenges of the methodology for the given application. For instance, it may be that similar studies based on traditional research methods have revealed data gaps that only a more inclusive and participative approach can fill. The choice of Citizen Science over more traditional methods may need to be understood and accepted by key institutional and organizational stakeholders who may not be fully aware of the methodology. This requires having considered, weighted, and articulated how Citizen Science will best achieve the desired outcomes of the project. Also, it is worth including additional benefits specific to Citizen Science, such as effects on learning and education, increased social awareness, or engagement in environmental problems. Trying to identify additional stakeholders who may need, or care for, the data and results of a given research study can be very useful to assess its potential uptake at acceptance at the contributors’ level. Additional stakeholders may be scientists in related scientific domains or similar research fields, but also organizations and communities that could use and/or be impacted by the data and knowledge that the project generates. Being able to express and communicate the overall scientific scope and methodological steps in a clear way, that is understandable by all people involved (i.e. avoiding domain jargon that can make it hard to understand for people outside the specific field) is essential to capture people’s interest and build a core community of volunteer contributors. Citizen Science Zurich support to scoping and co-creation The Citizen Science Center Zurich and Participatory Science Academy (PWA) teams are available to investigate with you on all aspects of your idea to help assess if Citizen Science is an appropriate solution. To make our initial interactions more efficient, we suggest that before contacting us you get familiar with the Center’s “Criteria for project support” https://citizenscience.ch/en/start/criteria. Ready? Get in touch with us: e-mail: info@citizenscience.ch & pwa@citizenscience.ch phone: +41 44 634 21 97 in person: Kurvenstrasse 17, 8006 Zurich Finding early stage contributors (stakeholders/ participants/ volunteers) The Citizen Science Center Zurich is very happy to support you in the building of your communities as well as to connect you with the communities that we already have established at the center (see also “Partnerships” in chapter “Resources”). Additionaly, here are a few ideas for where to look to find potential stakeholders and project participants. • Social media groups, channels, lists • Local communities • Global organizations • Centers of expertise • Facebook groups, hobby associations • Swiss Guides and Scouts • Established networks of interest (i.e. nature organizations) • Advocacy groups • Senior associations • Specialized associations (dialects, etc.) Resources • Bonney, R. et Al. (2009). Public Participation in Scientific Research: Defining the Field and Assessing Its Potential Planning and Design R ESO U RCES, LIN K S, A N D H IN TS https://citizenscience.ch/en/start/criteria https://citizenscience.ch/en/start/criteria Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 16 for Informal Science Education (Report). Center for Advancement of Informal Science Education. https:// www.informalscience.org/public-participation-scientific- research-defining-field-and-assessing-its-potential- informal-science. • Den Broeder, L. et Al. (2016). Citizen Science for public health. Health Promotion International, 33(3): 1–10. https://doi.org/10.1093/heapro/daw086. • Jørgensen, F. A., Jørgensen, D. (2021). Citizen science for environmental citizenship. Conservation Biology, 35(4): 1344–1347. https://doi.org/10.1111/cobi.13649. • Knack, A. et Al. (2017). Open science. The citizen’s role in and contribution to research. RAND Coperation. https://www.rand.org/pubs/conf_proceedings/CF375.html. • Nowotny, H. (2003). Democratising Expertise and Socially Robust Knowledge. Science and Public Policy, 30(3): 151–156. https://doi.org/10.3152/147154303781780461. • Pohl, C. (2011). What is progress in transdisciplinary research? Futures, 43(6): 618–626. https://doi.org/10. 1016/j.futures.2011.03.001. Project Design / Protocol In Citizen Science, “project protocol” (or workflow) refers to the sequence of tasks that need to be performed by project participants, and their associated rules. Depending on the project’s objectives, the research protocol may cover tasks for data collection, for data analysis, or for a combination of both in different forms. Ultimately, all the steps and tasks should fit in a workflow that makes it easy and engaging for participants to contribute while ensuring the overall quality of data and results. Example: your project aims at classifying biodiversity images. The protocol would detail where the images will be made available (i.e. web interface), if contributors will need to register or not, which questions will be associated with each image, how many times the same image will need to be classified, etc. For each protocol’s step, some of the more general aspects to examine include: • whether the task’s format and requirements would be best performed in a virtual (online) or physical environment; • whether the necessary information/data would be best collected digitally (i.e. website, smartphone app, wearable gadgets, digital sensors, etc.), or in other forms (i.e. paper form, low-cost or “Do it yourself” (DIY) kits); • whether volunteers will need training and/or supervision to be able to perform the tasks; • whether the time and commitment required of a contributor are reasonable. If a task is too time demanding, further splitting it into smaller (shorter) tasks may be considered. Make sure you assess the overall feasibility and tailor the protocol depending on the kind of contributors you are targeting (consider attention span, available free time, etc.); • whether data gathering should be organized as a short event (i.e. challenge) or for a longer period of time. Assess if you need information monthly, seasonally, or annually; • at which points in the process the data collected by contributors needs to be validated. Overall, make it easy and fun to participate in your project while ensuring the highest possible quality standards! Gamification As mentioned above, the use of methods and technologies inspired by online gaming can be effective to boost the fun-based motivation of some participants and achieve sustained engagement by providing instant-gratification. A growing number of projects includes for instance a reward system where individuals can gain status (points) and compete to reach the leader board. Gamification techniques include virtual recognition badges, friending, map challenges, and can use a mixture of traditional and social media interactions. The use of gamification allows to establish personal connections and a gentle competition among participants that has shown to stimulate them to engage repeatedly over time. However, the competition element should be introduced with care. Participants must feel that the competition is fair, less competitive individuals should not be discouraged from participating, and above all it should not reduce in any way data quality or accuracy for the project. Planning and Design https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://www.informalscience.org/public-participation-scientific-research-defining-field-and-assessing-its-potential-informal-science https://doi.org/10.1093/heapro/daw086 https://doi.org/10.1111/cobi.13649 https://www.rand.org/pubs/conf_proceedings/CF375.html https://doi.org/10.3152/147154303781780461 https://doi.org/10.1016/j.futures.2011.03.001 https://doi.org/10.1016/j.futures.2011.03.001 Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 17 Iterations It is quite common for projects and processes to require modifications and adjustments after their initial implementation. It’s therefore a good and more efficient practice to start small, with a pilot, as optimization is an iterative process that requires testing, flexibility, and being prepared to adjust the project design as needed as conditions could change, for instance depending on the interpretation of early results. In Citizen Science projects it is a common practice to start by testing procedures with colleagues and with friends who may not be experts in the topic; their feedback often carries invaluable hints on aspects such as clarity, level of difficulty, level of engagement, which help you improve your project by making the necessary adaptations. Some practical ways to collect feedback and exchange with contributors include interactive tools such as feedback forms, forums, chats, and social media channels, most of which have the benefit to allow you to respond and keep your contributors up to date with changes and improvements. It is essential that participants provide their feedback throughout the life of the project. Pre-existing projects As you may be aware, there are plenty of Citizen Science projects out there and chances are high that somebody, somewhere, is already doing something similar in scope and/or method to your own idea. To find out if best practices or protocols exist that can be applied or easily modified to fit your aims, searching existing project databases can be a practical solution. For scholarly as well as generic web searches though, it may be useful to be aware of the many different ways such projects are called in both official and grey literature, including community science, participatory science, participatory sensing, volunteered geographic information, volunteer monitoring, and more. Of course, all other scientific projects – beyond Citizen Science – may be equally relevant and inspiring. Platforms with existing projects • citizenscience.ch The Zurich Center’s platform hosts a number of data analysis Citizen Science projects for you to join! https://citizenscience.ch/en/ • Citizen Science at Zentralbibliothek featuring the projects offered by the Library https://www.zb.uzh.ch/ de/ueber-uns/citizen-science • Schweiz Forscht is an initiative of the Foundation Science et Cité and of the Swiss Academies of Science and Arts. The platform features an ample choice of national Citizen Science projects. https://www.schweizforscht. ch/projekte • Bürger schaffen Wissen The German Citizen Science Platform https://www.buergerschaffenwissen.de/citizen- science • Österreich forscht The Austrian Citizen Science Platform https://www.citizen-science.at/en/ • The Zooniverse is the world’s largest and most popular platform for data-analysis Citizen Science projects, or “people-powered research”. More than a million people around the world contributed to Zooniverse research projects. https://www.zooniverse.org/projects • EU-Citizen.Science is an online platform developed by the homonymous EC Horizon 2020 project, designed to serve as a Knowledge Hub, in aid of the mainstreaming of Citizen Science. https://eu-citizen.science/projects • SciStarter is a popular US-based Citizen Science portal with more than 3,000 projects, searchable by location, topic, age level, etc. SciStarter hosts an active community of close to 100,000 registered citizen scientists and millions of additional site visitors. https://scistarter.org Resources • Eitzel, M. V. et Al. (2017). Citizen Science Terminology Matters: Exploring Key Terms. Citizen Science: Theory and Practice, 2(1): 1. http://doi.org/10.5334/cstp.96. • Bowser, A. et Al. (2013). Using gamification to inspire new citizen science volunteers. Gamification ‘13: Proceedings of the First International Conference on Gameful Design, Research, and Applications: 18–25. https://dl.acm.org/ doi/pdf/10.1145/2583008.2583011. Planning and Design R ESO U RCES, LIN K S, A N D H IN TS https://citizenscience.ch/en/ https://www.zb.uzh.ch/de/ueber-uns/citizen-science https://www.zb.uzh.ch/de/ueber-uns/citizen-science https://www.schweizforscht.ch/projekte https://www.schweizforscht.ch/projekte https://www.buergerschaffenwissen.de/citizen-science https://www.buergerschaffenwissen.de/citizen-science https://www.buergerschaffenwissen.de/ citizen-science https://www.citizen-science.at/en/ https://www.zooniverse.org/projects https://eu-citizen.science/projects https://scistarter.org http://doi.org/10.5334/cstp.96 https://dl.acm.org/doi/pdf/10.1145/2583008.2583011 https://dl.acm.org/doi/pdf/10.1145/2583008.2583011 Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 18 Tools and Methods Making technological choices (i.e. selecting tools and infrastructure that will enable and support the research process) is a challenging part of designing a research project, as the choice has numerous and long lasting repercussions on resources (personnel, skills, timeline, budget) and on the sustainability of processes and data. In the case of Citizen Science projects, one of the first assessments is whether the project needs new technology or it can use existing solutions. When possible, it’s desirable for new projects to consider leveraging existing infrastructures. For many volunteer activities in fact, open (and free) solutions exist that have been developed with the specific needs of Citizen Science initiatives in mind. They have been used by multiple projects, and have been perfected over time thanks to the feedback of researchers and volunteer contributors. They include platforms where anybody can build a data analysis project quickly and without need for any coding skills, and smartphone app generators that allow the design of apps for data collection tailored to each project’s specific needs. Existing solutions may not be perfectly fitting the final requirements, but they are often useful in different ways and at different stages of the project. In the initial testing of protocol design for example, they can facilitate co-creation by allowing quick prototyping and multiple iterations with teammates and early collaborators. Sometimes, existing solutions are all you need for the full implementation of your research project, with significant savings in terms of time and resources. One extra advantage of existing platforms is that they often come with an established community of active users that may be attracted to joining your project as well. If existing solutions do not fit your needs, and you have to build your own platform (web or mobile) you may want to consider leveraging existing open software or open-source content management systems. This choice, in line with the Open Science (OS) principles (and part of the OS UZH policy) is possible even if the development is contracted to a private company. Similarly, if you need specialized sensors and gadgets, consider using open hardware and low tech, DIY solutions to minimize costs of production, especially if you are not planning to provide the devices to your contributors. Beside using tools, there are large practical advantages in sharing also certain elements of the co-creation research process, instead of having to develop each step from scratch. To help this process, the Citizen Science Center in Zurich is collaborating with Swiss and European projects in developing best practices to mainstream co-created Citizen Science. Citizen Science Project Builder (PB) The Citizen Science Project Builder (PB) is a web-based tool that allows researchers, students, and all members of the public to create and run data-analysis Citizen Science projects. Such projects may take many different forms, from classifying images of snakes to transcribing handwritten Swiss German dialect, from collecting samples of water to taking pictures of insects and plants. Typically, project participants are asked to perform complex data classification tasks (i.e. classify, tag, describe, or geo-localize) that are still best performed by human minds and skills. In particular, the PB supports projects based on existing digital data that can be in the form of images, text, PDF documents, social media posts (tweets), audio recordings and video clips. The PB provides an interface that requires limited technical knowledge, and ideally little or no coding skills. Its aim is to facilitate the co-creation of Citizen Science projects between the two communities of academic researchers and volunteer contributors, by starting with the implementation of simple pilots. Any idea for a data classification project can be implemented with little effort, the basic requirement being that the project responds to some simple criteria available in the platform. By building around it an initial community of contributors (colleagues, friends, family, and more!) the research question, process, and data quality can be tested and iterated. An important feature of the PB concerns data and metadata generated through each project. Users have full ownership and control of all their own contributions, which they can easily manage from their profile dashboard. This data includes information on their contribution history and results, and they are free to use it as they prefer, including donating it for additional studies. The potential of such behavioural data for researchers is huge, as we explain in the “data valorisation” section below. The PB has been developed by the Citizen Science Center in Zurich in collaboration with the Citizen Cyberlab at University of Geneva. Its implementation is based on the open source crowdsourcing framework PyBossa and its code is publicly available under the ‘CitizenScienceCenter’ organisation on Github. Planning and Design Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 19 Step-by-Step Project Creation: Setting up a project involves a few simple steps. All you need are the digital data that you want to analyse, that can be text, images, video and audio files, tweets, and PDF documents. • STEP 0: Login or create your Project Builder account • STEP 1: Create your project’s home page • STEP 2: Select the type of source file • STEP 3: Select the type of task for contributors (i.e. survey style questionnaire or survey style questionnaire with geolocation) • STEP 4: Design your task protocol • STEP 5: Select the location of your source files (Import tasks) • STEP 6: Project Overview page – test and adjust your project • STEP 7: Submit for publication Find out more, and start building your project here: https:// lab.citizenscience.ch. Here is a list of additional open and free tools that you may want to check-out before developing your own technical solution: Project co-creation • td-net toolbox Methods and tools for co-producing knowledge https://naturalsciences.ch/co-producing- knowledge-explained/methods/td-net_toolbox Web platform (data analysis) • Zooniverse Builder The Zooniverse Project Builder provides a powerful interface for creating data analysis projects. https://www.zooniverse.org/lab • iNaturalist is a social network of naturalists, citizen scientists, and biologists where contributors share, classify, and map observations of biodiversity across the globe. A joint initiative by the California Academy of Sciences and the National Geographic Society, it offers the option to start your own biodiversity collection project. iNaturalist also includes an App for data collection. https://www. inaturalist.org • Label Studio is an open source data labeling tool for labeling and exploring multiple types of data. You can perform many different types of labeling for many different data formats, and also use it with machine learning models. https://labelstud.io/ Smartphone App (data collection) • Epicollect5 is a mobile and web application for free and easy generation of forms (questionnaires), and freely hosted project websites for data collection.Projects are created by using the web application and then downloaded to the device, both Android (5+) and iOS (8+), to perform the data collection. https://five.epicollect.net/ • KoBoToolbox is a free and open source suite of tools for field data collection for use in challenging environments. Most users are people working in humanitarian crises, as well as aid professionals and researchers working in developing countries. https://www.kobotoolbox.org/ • ExCiteS Software Researchers at University College London (UCL) ExCiteS have created several bespoke technologies, including an open source mobile data collection platform for non-literate and illiterate users (Sapelli) and web platforms for participatory mapping. https://www.geog.ucl.ac.uk/research/research-centres/ excites/software Other platforms with specific tools • sMapshot – the participative time machine https:// smapshot.heig-vd.ch/ • Pybossa –open framework for Citizen Science projects https://pybossa.com/ Planning and Design R ES O U RC ES , L IN K S, A N D H IN TS https://lab.citizenscience.ch https://lab.citizenscience.ch https://naturalsciences.ch/co-producing-knowledge-explained/methods/td-net_toolbox https://naturalsciences.ch/co-producing-knowledge-explained/methods/td-net_toolbox https://www.zooniverse.org/lab https://www.zooniverse.org/lab https://www.inaturalist.org https://www.inaturalist.org https://labelstud.io/ https://five.epicollect.net/ https://www.kobotoolbox.org/ https://www.geog.ucl.ac.uk/research/research-centres/excites/software https://www.geog.ucl.ac.uk/research/research-centres/excites/software https://smapshot.heig-vd.ch/ https://smapshot.heig-vd.ch/ https://pybossa.com/ Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 20 Resources As any other research effort, Citizen Science projects need a careful estimation of the resources needed to carry out the project with the desired timeline and outcomes, and full awareness of limitations in terms of funding, staffing, equipment, etc. Team Members In addition to the skills and responsibilities common to traditional research projects, successfully launching and running a Citizen Science project requires a variety of roles aimed at facilitating the collaboration among players with different kinds of knowledge, goals, and ultimate motivations. With this specific “facilitation” role in mind, it may be helpful to complement traditional research profiles and skills with competencies in public communication, community engagement, visual design, data management, and project management and evaluation. Luckily, such roles can be taken up by the different players involved, including researchers, volunteers, partner organizations, and participants. Project Timeline All major project activities that will be performed during the course of the project should be inserted in a timeline agreed with all stakeholders. These should include communication and recruitment of volunteers, production of briefing materials, training, etc. – practically all major steps up to the publication of results. Particular attention should be paid to scheduling regular interactions to update the volunteers’ community, which can be in the form of regular events and accessible channels to share progresses, solicit feedback, and network among participants. Very relevant (and often forgotten) is the need to identify when the project is expected to end, or under what circumstances it may be terminated or held off to different players. This is a big part of managing expectations and needs to be communicated from the very beginning to everybody involved. Plans should include the creation of a persistent public-facing presentation of the results to replace the no-longer active project website. Potential partners for ensuring long-term sustainability might be university libraries. Budget The costs entailed in Citizen Science projects should be clearly identified and assigned to the different partners in the project plan. These may be related to the development of tools (tailored web platform, smartphone apps, or data collection kits) or to the engagement of the contributors (i.e. training material, events, prizes in case of challenges or gamification of the project). Partner organizations can be key in providing goods and services through direct investment or in-kind contributions. Especially in co-created projects, some stakeholders (such as teachers, young workers, or similar) may find it economically difficult to invest significant personal time in the project. Also, sometimes salaries can be precarious (i.e. in NGO or NPO) and project budgets should include, whenever possible, not only the salary of the academic scientists but also the one of non-academic partners. There are funding institutions, such as foundations, that will accept those costs as part of the proposed budget. Overall, the topic of payment in Citizen Science remains difficult and it is often discussed without a solution that fits all cases. However, from offering vouchers, to inviting for free-meal gathering, to offering childcare, there are many other ways to make sure that participants’ contributions and needs are valued and accounted for. Partnerships Vital to many Citizen Science endeavours, partner organizations may include research groups with similar interests, libraries, existing projects, Citizen Science centers/ labs in other universities, network of practitioners, civil society organizations with similar interests, and more. There are many benefits to establishing partnerships, such as: • strengthening requests for financial support via joint applications to public and private grants; • providing missing expertise and experience; • widening the reach or the project by joining networks; • increasing the project team’s size, also helping ensure long-term sustainability. Overall, it’s useful to ask yourself: who else might have a stake in your project’s data and results? How could they support you in promoting the project and disseminating the results? And what might get them interested in sharing resources or ideas? It is important to talk with the potential partners you identify and learn what would motivate them, including scientific curiosity, hobbies, health-related concerns and more. The Citizen Science Center Zurich has already established partnerships and collaborations with several institutions and organizations in Zurich and Switzerland, representing both researchers and citizens. Such network constitute an invaluable asset for our community of practitioners, as it offers both a pool of potential project’s participants and a spectrum on expertise to tap in. Also, partners have explicitly expressed their interest in collaborating with local and Swiss initiatives. Do not hesitate to contact us if you identify Planning and Design Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 21 among our partners suitable collaborators for your project, we would be happy to build the necessary connections. PWA Seed Grants (specific to Zurich) The Participatory Science Academy (Partizipative Wissenschaftsakademie, PWA) – together with Mercator Foundation Switzerland – awards Seed Grants for the development and implementation of participatory Citizen Science projects. Teams of academic researchers from UZH and/or ETHZ and citizens are given the opportunity to develop and/or carry out a participatory research project. Priority is given to projects which, in addition to the prospect of excellent research, show a high degree of participation in as many phases of the research process as possible. One Seed Grant per year is reserved for research on participatory research. Key criteria for the grant assignment include: • Excellence and innovation • Degree and structure of participation • Added value of participatory approaches in the field of research • Desired impact of the project • Quality and appropriateness of the methods chosen to address the research question • Feasibility of the project • Prospect of follow-up project(s) Make sure you check the PWA website (www.pwa.uzh.ch) for more information, news, application deadlines, etc. Public and Private Grants Here some useful links for an overview of institutions and organizations that (could) provide resources for projects that include Citizen Science: • SNSF, swissuniversities SNSF funding programmes could be used for supporting projects including Citizen Science. Also, as an integral component of Open Science, Citizen Science could potentially be integrated in “Programme Open Science II” (2025-2028) of Swissuniversities, along with Open Innovation and Open Education. https:// www.swissuniversities.ch/en/topics/digitalisation/open- science; http://www.snf.ch/en/funding/Pages/default.aspx • European Commision Citizen Science is expected to play a more prominent role in European Research and Innovation (R&I) in the years to come: it will be mainstreamed across Horizon Europe, and featured as part of the European Commission Communication: “A new European Research Area for R&I”. https://ec.europa.eu/ info/funding-tenders/opportunities/portal/screen/home • Swiss Private Foundations Some Swiss foundations are known for supporting participatory research efforts in different domains, and also specific Citizen Science projects. The majority of the “big” foundations are part of swissfoundations, whose website can be searched by name or by funding area (e.g. Science & Research) https:// www.swissfoundations.ch/fr/a-propos/nos-membres/ It is still recommended to look at each foundation’s website for updated guidelines (information elsewhere can be outdated). One additional tip: make sure you visit the website of projects similar to yours (same research area/ same target groups) to check out the logos of foundations/ institutions who support them! Crowdfunding platforms • wemakeit (founded in Switzerland in 2012 and featuring the Science Booster) https://wemakeit.com/pages/ science?locale=en • Crowd.science https://crowd.science/ • Experiment.com https://experiment.com/ • Consano https://consano.org/ If you are not convinced by crowd funding science, here a Nature’s interesting article “Crowdfunding research flips science’s traditional reward model” https://www.nature. com/articles/d41586-019-00104-1. Existing Citizen Science networks Citizen Science practitioners exist all over the world. Here are some established national networks that may be contacted for further information on potential collaborations: • European CS Association (ECSA) https://ecsa.citizen- science.net/ • CS Association (US CSA) https://citizenscience.org/ • Australian CS Association (ACSA) https://citizenscience. org.au/ • CitSci Africa https://www.usiu.ac.ke/citsci-africa- association • CitSciAsia https://www.facebook.com/CitSciAsia/ Planning and Design R ES O U RC ES , L IN K S, A N D H IN TS http://www.pwa.uzh.ch https://www.swissuniversities.ch/en/topics/digitalisation/open-science https://www.swissuniversities.ch/en/topics/digitalisation/open-science https://www.swissuniversities.ch/en/topics/digitalisation/open-science http://www.snf.ch/en/funding/Pages/default.aspx https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/home https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/home https://www.swissfoundations.ch/fr/a-propos/nos-membres/ https://www.swissfoundations.ch/fr/a-propos/nos-membres/ https://wemakeit.com/pages/science?locale=en https://wemakeit.com/pages/science?locale=en https://crowd.science/ https://experiment.com/ https://consano.org/ https://www.nature.com/articles/d41586-019-00104-1 https://www.nature.com/articles/d41586-019-00104-1 https://ecsa.citizen-science.net/ https://ecsa.citizen-science.net/ https://citizenscience.org/ https://citizenscience.org.au/ https://citizenscience.org.au/ https://www.usiu.ac.ke/citsci-africa-association https://www.usiu.ac.ke/citsci-africa-association https://www.facebook.com/CitSciAsia/ Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 22 Data The generation of different forms of data by volunteers is the common denominator of most Citizen Science projects. National and international Citizen Science associations have been working for years on the principles and values behind such data, and on the elaboration of data and metadata standards. ECSA, for instance, emphases the principle of data sharing, as summarized in “Citizen Science project data and metadata are made publicly available and where possible, results are published in an open access format.” (ECSA “10 Principles of Citizen Science”). More generally, only data properly managed and of high quality have the potential to fulfil two of the main ambitions of Citizen Science data: generate new scientific knowledge and serve evidence-based decision and policy making. Wide-ranging advice for data and data life-cycle management plans in research is available for UZH and ETHZ researchers at the data units of the two institutions: • Data Services & Open Access at UZH https://www.hbz. uzh.ch/en/open-access-und-open-science.html • Research Data at ETH Zurich https://ethz.ch/services/ en/service/a-to-z/research-data.html Citizen Science data management As for any other research project, Citizen Science data management includes aspects of data storage, infrastructures, access, governance, standards, and documentation. Citizen Science practices generally aim at fulfilling the (rapidly evolving) standards of open and FAIR data, while acknowledging that the re-use of existing datasets by researchers is only possible when provenance, methods, constraints, and treatments of datasets are clear and well documented. While each project is unique in its requirements and it is hard to generalize solutions, several peer reviewed papers present a number of recommendations and practices for Citizen Science practitioners that aim at ensuring inter- operability through data standards (see resources). Together with practical data solutions, project organizers should not overlook legal and ethical aspects of the collection, storage, and use of data (for instance protecting private and location-based information), including compliance to national Data Protection Regulation (for Switzerland, see the Swiss Federal Act on Data Protection and the EU General Data Protection Regulation). An obvious advice is to seek guidance by institutional legal and ethical bodies, as it is impossible to generalize recommendations and best practices across institutions (research institutions have their own regulations) and across projects (each project has its own requirements, for instance specific reasons for balancing openness vs privacy). In all cases, data ownership and use, terms, and conditions needs to be easily available in a publically accessible location (typically the website of the project). Data quality The perceived quality of Citizen Science data is a major concern in the scientific community and it is often raised as an issue for the uptake of the methodology. However, there is an emerging body of research showing that citizens are able to make contributions that meet the standards of professional scientists and that the methodology, when appropriately applied, has the same level of accuracy as traditional methods. To achieve optimal quality though, any potential issue needs to be considered in advance and addressed through the application of adequate quality control measures. More than 10 different strategies have been identified in the literature (see box) that can be applied, usually in combination, during planning, training, data collection, data analysis, or program evaluation. They include requesting task redundancy (i.e. having the same tasks performed by multiple participants, a feature which is usually provided automatically by most Citizen Science platforms), providing training, including standardized samples, cross-checking data and results with expert evaluation or existing literature, simplifying the tasks, and more. Whenever possible, it’s good practice to perform testing and prototyping with contributors to get first-hand feedback on the process while monitoring the quality of resulting test data. Note: as the literature highlights, often the issue with Citizen Science data quality is not in actual practices, but in the lack of documentation on the action taken during data acquisition and/or analysis by contributors. Especially for projects that seek to promote the re-use of their data, standardized ways exist (meta-data) to detail as comprehensively as possible all factors related to data management, including origin, treatment, constraints, and biases. Planning and Design https://www.hbz.uzh.ch/en/open-access-und-open-science.html https://www.hbz.uzh.ch/en/open-access-und-open-science.html https://ethz.ch/services/en/service/a-to-z/research-data.html https://ethz.ch/services/en/service/a-to-z/research-data.html Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 23 “Fitness for use” “Fitness for use” is an interesting concept being increasingly used for Citizen Science practices and data. It refers to a new construct of data quality in research projects, where the actual quality of data has significance only in the context of usage, i.e. it must match the research question. For example, in some studies lower resolution may be balanced by a far wider scope and coverage of data. This concept acknowledges the difficulty to establish a unique standard and universal criteria for quality in scientific data. Example: “In air-quality monitoring, low-cost sensors cannot currently compete with professional instruments for achieving precision and accuracy at the levels necessary for regulation. Therefore, one goal of Citizen Science air- quality projects may be to get regulators to take notice when systematically collected data indicates a potential problem meriting further investigation. Low-cost (including commercial or open-source/do-it-yourself) sensors are of suitable quality to be fit for this, and often other, purposes.” (Browser, A. et Al. 2020) Ethical considerations For general consideration and recommendations concerning compliance to legal requirements and the ethical aspects of Citizen Science projects, including some specific information for practitioners in Zurich and Switzerland, we recommend reading “Ethics guidelines in Citizen Science”, a report which is the result of a collaboration between the Citizen Science Center Zurich and the Health Ethics & Policy Lab at ETHZ. The document provides an overview of existing ethics guidelines in Citizen Science as promoted by the Citizen Science Association (CSA), the European Citizen Science Association (ECSA) and similar organizations. The papers listed here provide an overview of the current status and recommendations for Citizen Science data management. • Bowser, A. et Al. (2020). Still in Need of Norms: The State of the Data in Citizen Science. Citizen Science: Theory and Practice, 5(1): 18. https://doi.org/10.5334/cstp.303 • Freitag, A. et Al. (2016). Strategies Employed by Citizen Science Programs to Increase the Credibility of Their Data. Citizen Science: Theory and Practice, 1(1): 2. http:// dx.doi.org/10.5334/cstp.6. • Kosmala, M. et Al. (2016). Assessing data quality in Citizen Science. Frontiers in Ecology and the Environment, 14(10): 551–560. https://doi.org/10.1002/fee.1436. • Weigelhofer, G., Pölz, E. (2016). Data Quality in Citizen Science Projects: Challenges and Solutions. Frontiers in Environmental Science 4. https://www.frontiersin. org/10.3389/conf.FENVS.2016.01.00011/event_abstract. • Wiggins, A. et Al. (2011). Mechanisms for Data Quality and Validation in Citizen Science. Proceedings of the IEEE 7th International Conference on E-Science. IEEE. https:// ieeexplore.ieee.org/document/6130725. Specific work on metadata • Wagenknecht, K, et Al. (2021). EU-Citizen.Science: A Platform for Mainstreaming Citizen Science and Open Science in Europe. Data Intelligence, 3(1): 136–149. https://www.mitpressjournals.org/doi/full/10.1162/ dint_a_00085. • PPSR-Core information: https://core.citizenscience.org/ On the concept of fitness for use • Castell, N. et Al. (2017). Can commercial low-cost sensor platforms contribute to air quality monitoring and exposure estimates? Environment International, 99: 293–302. https://www.sciencedirect.com/science/ article/pii/S0160412016309989 • Chapman, A. D. (2005). Principles of Data Quality. Global Biodiversity Information Facility. https://doi.org/10.15468/ doc.jrgg-a190. On ethics in Citizen Science • Jobin, A. et Al. (2020). Ethics guidelines in Citizen Science. ETH Zurich / Citizen Science Center Zurich. https://doi. org/10.3929/ethz-b-000428502. • Rasmussen, L.M., Cooper, C. (2019). Citizen Science Ethics. Citizen Science: Theory and Practice, 4(1): 5. http://doi. org/10.5334/cstp.235. • Resnik, D.B. et Al. (2015). A framework for addressing ethical issues in citizen science. Environmental Science & Policy, 54: 475–481. https://doi.org/10.1016/j. envsci.2015.05.008. Planning and Design R ES O U RC ES , L IN K S, A N D H IN TS https://doi.org/10.5334/cstp.303 http://dx.doi.org/10.5334/cstp.6 http://dx.doi.org/10.5334/cstp.6 https://doi.org/10.1002/fee.1436 https://www.frontiersin.org/10.3389/conf.FENVS.2016.01.00011/event_abstract https://www.frontiersin.org/10.3389/conf.FENVS.2016.01.00011/event_abstract https://ieeexplore.ieee.org/document/6130725 https://ieeexplore.ieee.org/document/6130725 https://www.mitpressjournals.org/doi/full/10.1162/dint_a_00085 https://www.mitpressjournals.org/doi/full/10.1162/dint_a_00085 https://core.citizenscience.org/ https://www.sciencedirect.com/science/article/pii/S0160412016309989 https://www.sciencedirect.com/science/article/pii/S0160412016309989 https://doi.org/10.15468/doc.jrgg-a190 https://doi.org/10.15468/doc.jrgg-a190 http://doi.org/10.5334/cstp.235 http://doi.org/10.5334/cstp.235 https://doi.org/10.1016/j.envsci.2015.05.008 https://doi.org/10.1016/j.envsci.2015.05.008 Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 24 Community Management Once you have defined the project’s ambitions and the desired level of engagement for the participants, it’s time to plan your concrete ways to reach out to them. In a co-created approach, this step could already be done collaboratively with (some) of them. One way to do this is to hold a workshop in which everyone expresses their view to come out with a common vision, which can then be written down as a concept and agreed by all involved. As a matter of fact, community management is to a large extent communication, and effective communication is important to increase your project’s visibility, reach potential participants, and later keep them actively engaged. Note: Community engagement is iterative and it is important to start the conversation early, constantly elicit feedback, and build trust by implementing easy feedback channels and loops! Map stakeholders and define your target groups In order to set up a good basis for communication, outreach and interaction, it is important to map out your target group, i.e. determine the individuals, organisations, and groups that have an interest in your project and/or are impacted by its outcomes. This includes much more than identifying them! It means understanding their perspectives and interests, knowing their motivation, visualizing their relationships among each other, and finally prioritizing the stakeholders with the highest relevance. Once stakeholders are identified, you may ask yourself: how can you reach out and make your project visible and interesting to them? Once engaged, how can you best exchange with them? What platforms and communication channels are the most appropriate and effective to this goal? HINT: You do not need to reinvent the wheel. Benefit and build upon already existing communities and networks that you might have identified during your stakeholder analysis. Plan the tools and channels to interact with your community To reach out and interact with your community you can choose different tools and several communication channels and media. In a very general way, one can differentiate between online and offline activities. • Thanks to modern technologies, large numbers of people can be reached and engaged in Citizen Science with online channels and tools. These include communication channels such as websites, blogs, apps, newsletters, social media, online networking platforms, forums, chat apps (e.g. Citizen Science group in WhatsApp, Telegram), etc. While evaluating which one(s) to choose, it is worth considering to take advantage of the ones that your organization is already using and build on existing formats and audiences. In the case of online communication and reach out, potential contributors will be geographically dispersed and contribute at a time and with frequency that is convenient to them. Depending on the project, a way to partially control the geographic distribution of participants may be provided by the choice of the language for communication. German, for instance, would probably increase local participation, while English would provide a more global audience. • In-person, on-site activities such as face-to-face meetings and events have the big advantage of providing chances for social interaction and networking opportunities. Project organizers and project participants in-person exchanges provide more immediate and useful feedback, often obtained simply by observing participants acting on the project (e.g. provide their contributors or answer the required questions). Sometimes more structured interactions are needed, for instance in workshops organized for designing/planning aspects of the project, or for sessions of training. Depending on the project, the best solution can be purely online, offline, or a mix of the two formats. In any case, content should be tailored to the specifics of the media and target public (language, level of details, use of visuals, etc.). Overall, make sure to create a clear message that conveys what the project is about and what volunteers benefit from when participating. HINT: Projects gain much more visibility and attention by the general public when they are mentioned in traditional media such as newspapers, radio or television. If possible, use corporate communication offices (or your network!) to get your project featured there. Planning and Design Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 25 Step by step guide for the Community Management. Planning and Design Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 26 Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 27 Impementation, Dissemination and Evaluation It’s finally time to implement and run your project! Once at this stage, project organizers should know how to deal with the implementation and running of the project, including the different phases of data acquisition, analysis, dissemination of results, and overall project evaluation. As such aspects depend on the specificities of each project, it is hard to provide generalized directives. Except, also in this phase community management stays central to the success of the Citizen Science endeavour. As a last recommandation, do not neglect to perform a final evaluation of your experience and results. This is useful to learn from your own experience and improve your own project and approach, but such knowledge is also sought after by the extended community to advance the knowledge on Citizen Science as a research methodology. Community Management After all the work done to attract participants and build a community around the project, the priority now moves to keeping participants motivated and engaged throughout the project’s lifetime and during the final research phases. Alas, depending on several factors, many participants risk to visit the project the moment they hear about it, contribute once, and then never come back. You want to avoid this by providing motivation and incentives to stay actively involved and connected. One aspect becomes central: regular communication and interactions between project organizers and participants. Everything that you have learned about them, all the tips and tricks that have been used while (co-)designing and developing the project and building its initial community, are all essential now as they provide invaluable knowledge to tap into to interact with participants in the most effective way. If the overall engagement seems to be going down, additional offerings may include fun experiences and opportunities to connect and network. Impromptu in- person and virtual activities can be arranged to satisfy social motivations such as getting to know people, having a good time and feeling part of a community. If the kind of project allows, good examples include social events such as competitions, common activities, family games afternoons, group walks, meet & greets between citizens and scientists, movie nights, debates etc. In the field of physical and mental health, especially when the project has been fully co-created, constant and open communication with all stakeholders invovled in the process is paramount. Be aware that facilitating these interactions takes time; depending on the research topic and/or the vulnerability of the participants, it might be advised to get support from an external facilitator to moderate workshops or sensitive discussions (resources that go into the role of the facilitator should be carefully planned and budgeted for). A good example of such practices, developed in the context of co-created participatory research in aging, can be found at the Gerontology Center of UZH (see resources). Above all, project organizers should keep the community updated on all interesting aspects and decision-making milestones, and whenever possible seek their unique contribution. Involving the community gives them the opportunity to ask questions, express their opinion and communicate their ideas, needs, and wishes. A constant show of respect and appreciation goes a long way in gaining participants’ loyalty and reliability. Disseminate research findings Project participants have always the right to know what happens with their contributions, and ultimately what the results of the project are. This needs to be communicated in a form and with a language that is suitable for all participants, and can be understood by all stakeholders involved. You can also ask them directly what findings are important to them and what ways would be used to disseminate. If the academics among the project organizers plan a scientific paper, they should consider including some (or all ... it has happened!) of the project participants, depending on their level of involvement in the overall process. If the project’s topic tackles a societal issue, results and data can be used, also by civil society organizations, to contact policy or decision makers. In this case, the production of policy briefs or the invitation to public presentation of the result can be effective to support social change. Implementation, Dissemination and Evaluation Practicing Citizen Science in Zurich. Handbook | Citizen Science Center Zurich | November 2021 28 Data Valorization Data valorisation is a practice that consists in identifying the potential of data to create value in addition to the primary purpose for which they were collected and determining how this can be achieved. In the case of Citizen Science, the primary purpose of data collection is most often some specific research and innovation (to advance knowledge in a certain field, for instance astronomy) and broad temporal/spatial decision- making (to inform policy decisions, for instance for conservation). Can this data be used for other purposes? What is their value? What is the value of sharing them with others? One of the great advantages of digital data is that they are infinitely shareable (i.e. anybody can use the same data at the same time) and, from a very general point of view, the more they are shared, the greater their value. Data can be combined with other data, and data collected for one purpose can be re-used and valued for other scopes to produce new information and insights. For instance, who would have anticipated that data on water evaporation – originally collected to help farmers optimize crop irrigation – could be used by lawyers to assess road friction coefficients in automobile accidents? However, all data are not created equal. Higher quality data are more valuable as they have the potential to create better information. Note that higher quality data does not mean “more data”, as redundant, duplicated data have no value, and in general value decreases once the amount exceeds the capacity to handle. Similarly, quality itself needs not to exceed the need for the specific purpose (see “fitness for use” above). Privacy and ownership aspects must be considered before sharing, including ownership of researchers and contributors. In general, data re-use is hindered if people do not know data exist, cannot discover data easily, do not have access, or do not have the information (metadata) or knowledge to put the data to appropriate use (FAIR and Open principles). A huge potential for Citizen Science data When data are collected in a Citizen Science project on any topic, the collection contains two different types of information. One type refers to the project topic itself, the other type is about the use of intellectual abilities to engage in a meaningful activity. One can assume that citizens contributing to a Citizen Science project are performing an activity that they consider to be meaningful and that this activity requires the use of intellectual abilities. So in most Citizen Science projects, additional value comes from using the data about the generation of project data, i.e. data on when, where, how and with which outcome contributors engaged in the project. Such data on meaningful intellectual activity or social engagement in real life are of huge value for researchers interested in examining the longterm relations between intellectual engagement in real life contexts and wellbeing or abilities. The diverse approaches of Citizen Science projects provide natural real world experiments. Extra benefit arises when such data are controlled by individuals, as it is the case for the Citizen Science Project Builder (see page 18). In this case contributors have full control of their own data and may individually decide if, with whom, and for what they want to share, allowing the tackling of important research and practical questions. Citizens’ data ownership makes them equal partners of academic researchers. It also creates added value to researchers and citizens because such individual data accounts allow to apply completely new types of person- and context-specific analyses. Evaluation In most scientific projects evaluation is a core management instrument increasingly required by project initiatives, by policy-makers, and by institutions providing grants – also to improve funding schemes. Evaluation of Citizen Science is a relatively new area of research and it is still ongoing, reflecting the evolution of the field itself and its diversity. There are currently no established indicators for evaluating Citizen Science. The challenge is left to individual projects that need to define the most appropriate way to assess their own impact, and continually learn from this assessment. The reward however is that your evaluation criteria and results may be themselves the subject for a publication and may be used to advocate for such evaluations to be relevant for funding decisions. Implementation, Dissemination and Evaluation',\n",
              " '6 In December 2017 the Government of Flanders ran a call for projects. The Department of Economy, Science and Innovation aims to promote citizen science among universities, colleges and other research institutions. With fifty projects entered and thirteen in receipt of funding, the call was an unqualified success. In May 2019 another call for projects followed. In support of these projects the Flemish Knowledge Centre for Citizen Science came into being in early 2019. It was given the name Scivil. In 2015 came the launch of a digital platform for citizen science, Iedereen Wetenschapper (Every- one’s a Scientist), an initiative by EOS Science and the Flemish Young Academy. In 2016 the Young Academy followed with a position statement on citizen science (1). Since then, a lot has changed. Flanders had its first sweeping encounter with cit- izen science in May 2018, when twenty thousand citizen scientists measured air quality for the Cu- rieuzeNeuzen (Nosy People) project. There are al- so quite a number of European initiatives in Flan- ders, which receive funding from the European Union, such as hackAIR, FloodCitiSense and www. hoemeetiklucht.eu. Smaller scale initiatives, set up by local groups, such as the successful Leuve- nair, also exist, as do research projects undertaken by members of the public without any input from professional scientists. What is citizen science? Citizen science is defined as scientific work under- taken wholly or partially by members of the public, often in collaboration with or under the direction of professional scientists (2, 3, 4). Citizens may act as contributors or collaborators in the project and offer up new knowledge and understanding. Citizen science can be employed in the exact sciences, applied sciences and human sciences. For more information on citizen science see the ten principles developed by the European Citizen Science Association (ECSA). Citizen science versus science communication Citizen science is different to science communica- tion. Citizen science is a ‘new’ form of science, in which members of the public actively participate in the research work. Through it, citizens are no longer the target of science communication, but actively engaged in the scientific process. 95% of citizen science is communication Communication is a vital aspect of citizen science. It is a necessary part of the process of recruiting, engaging and keeping participants motivated. And a necessary means of announcing research results (dissemination) and teaching participants more about the project focus and scientific process (ed- ucation). You cannot overestimate the amount of time you will spend communicating with your target au- dience. It is a continuous process of maintaining openness at every stage of the scientific process: from setting your research question to publishing the results. It takes practice to stay open and accessible through your communication. Ideally, your citizen science project will have a community manager, a science communicator and a science trainer. The community manager focuses on direct and im- mediate contact with your citizen scientists and offers a point of contact for any questions. The science trainer directs your citizen scientists and trains them to collect data, write manuals, deliv- er support on the ground, and so on. The science communicator makes sure that your messages, research results included, are communicated in a way that is accessible for the target audience. This practical guide equips you with a few tricks of the trade. In addition, Scivil arranges regular train- ing courses and workshops. Keep an eye on our website and newsletter. Stay tuned! Scivil & the Working Group on Communication and Participation https://www.scivil.be/ http://www.iedereenwetenschapper.be/ http://www.iedereenwetenschapper.be/ https://curieuzeneuzen.be/ https://curieuzeneuzen.be/ https://www.hackair.eu/ http://floodcitisense.webflow.io/ http://www.hoemeetiklucht.eu/ http://www.hoemeetiklucht.eu/ https://leuvenair.be/ https://leuvenair.be/ https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_cs_nederlands.pdf https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_cs_nederlands.pdf https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_cs_nederlands.pdf https://ecsa.citizen-science.net/sites/default/files/ecsa_ten_principles_of_cs_nederlands.pdf https://www.scivil.be/ http://www.scivil.be/ https://rvo-society.us17.list-manage.com/subscribe/post?u=c008abaa4b387e8adbb1fd210&id=8830983986 http://www.scivil.be/ The building blocks of your communication plan Every good citizen science project comes with a rock solid communication plan that you prepare before the research phase. These five building blocks form the basis for this. Before you even think about recruiting citizen scientists you will need to clarify your project aim(s). What is it that you hope to achieve through citizen science? Which target groups will benefit from your project? Only when you have defined your project’s ambi- tions can you think about the target audiences you intend to engage, the level of engagement you hope to achieve and how you intend to recruit them. Although they can differ vastly, most citizen science What citizen science does Contribution to science The hard work that volunteers put into citizen science allows scientists to gather and analyse vast quantities of data in a short space of time. The accumulated time, the number of observation re- cords and the geographic spread of the data far exceed the capacity of the individual research sci- entist (5). Here, citizen science works more like a research approach by which empirical data can be gathered. The contribution made by citizen scientists then becomes a driver for scientific progress. Education You can also employ citizen science from the perspective of informal learning. You want your par- ticipants to learn something new. By contributing to your research project their understanding of the concept behind the scientific process and research subject improves. This can open a path to loftier goals, such as awareness of social problems and engagement. Citizen scientists are better equipped to make informed decisions and have a greater appreciation of certain (policy) decisions and more confidence in the results of scientific studies (6). Engagement of citizens as stakeholders You can also begin your citizen science from an ideological perspective. In that case, members of the public will help decide the study focus, as a pathway to more socially responsible research and insights that actually enhance general wellbeing. Scientists, policymakers and citizens from all walks of life coming together (in co-creation) to solve the social problems that stand at the very heart of people’s lives (6). When your project has the express aim of opening science to the general public, you engage your citizen scientists to do more than gather data. You become a researcher ‘as facilitator’, and in this role you encourage people to ask questions. And you give them support when it comes to answering those questions, by choosing the right scientific approach. Projects like these are also set up by members of the public and go on to receive support at a later stage from professional research scientists. Projects initiated by members of the public are often a response to a (local) social question for which an answer is needed. 1 projects have several aims in common. We identi- fy three desirable results: contributing to science, raising public awareness of a scientific issue, involv- ing members of the public in setting the research agenda and finding solutions to social issues. The essential aim of any citizen science project is to contribute to scientific research by finding the answer to a research question. If it does not, it is science communication or science education. That said, most citizen science projects claim more than scientific progress in their results. Determine your project objective 9 There are many ways to engage members of the public in a citizen science project. Citizen sci- entists can be of help in (7): • setting the research question • searching for information • formulating a hypothesis • choosing the data collection methods • gathering, analysing and interpreting data • writing and disseminating conclusions • discussing the results In the scientific literature we find lots of models in which citizen science projects are classified ac- cording to engagement with the research activ- ities above. The two most popular classifications are by Rick Bonney (8) and Muki Haklay (9). It is important to realise that these typologies are not normative rankings (10). Not every project need involve the general public, and the opposite is also true: not every study needs volunteers at every stage in the scientific process. The level at which you hope to involve the public has a lot to do with the aims you have in mind. For example, a contributory project – in which cit- izens are primarily used to collect data – will gen- erally deliver scientific results and knowledge for you (the project leader), and, depending on how accessible your subject is, you will probably try to draw from a wide recruitment base. Co-creation, on the other hand, is more suitable for a study with which you hope, with the public’s help, to influence policy decisions, or when you have an educational approach in mind. In these projects the research activities are decided with the participants’ help. They require a higher level of participant engagement, measured in terms of skills, effort and time, and will probably lead you to engage a more clearly defined target audience (10). Some citizen science projects are run without professional scientists. In this case, the citizens do the work themselves: they set the research question, conduct the experiments and interpret the results. Enthusiasts at the DIY lab ReaGent, for example, are working on a vegan alternative to leather, made from bacteria. They, and oth- er DIY volunteers around the world, are trying to find a cheap alternative to insulin, which would allow pharmacists to produce and supply insulin themselves. This initiative took root in the United States, where the cost of the medicine is prohibi- tive for some people with diabetes. The research undertaken by these citizen scientists is entirely independent of professional science. 2 Define the level of engagement When it comes to clarifying your project aims, be realistic. What can you achieve with the resourc- es you have? And what is the main aim of your project? Going for all three aims may be highly commendable, but, in reality, might be difficult to achieve. Placing too much emphasis on the educa- tional side of your project could affect the quality of your dataset (7). But the reverse can also lead to problems. Being too particular about a strict method of data collection could prevent citizens from taking part or make them drop out after only a short while. On page 15 of the ‘Choosing and Using Citizen Science’ guide you will find a decision framework Aims of my project: Contribution to science Education Engagement of citizens as stakeholders which you can use to check whether a citizen sci- ence approach is suitable for your project idea. The guide is intended for use in environment monitoring projects, but the decision framework is equally useful for other subjects. 10 https://reagentlab.org/ https://www.ceh.ac.uk/sites/default/files/sepa_choosingandusingcitizenscience_interactive_4web_final_amended-blue1.pdf https://www.ceh.ac.uk/sites/default/files/sepa_choosingandusingcitizenscience_interactive_4web_final_amended-blue1.pdf Type Description Example Crowdsourcing Members of the public offer their time and devices only. The World Community Grid projects make use of the com- puters of thousands of volunteers. You create an account on the website and download a tool to your computer. The application monitors your computer for spare computing power and that power is used to conduct virtual experi- ments. Research has been done in areas such as childhood cancer. The citizen scientists play a passive role, but are notified about the research being done. Distributed i ntelligence Members of the public sift through gathered research material and pro- vide simple interpretations or help categorise the material. Citizen scientists in the Oog voor Diabetes (Eye On Dia- betes) project examine the retinal images of diabetes pa- tients online. They note any signs of diabetic retinopathy, a disorder which can lead to blindness. The catalogue of images can then be used to teach an algorithm to recog- nise the disorder, paving the way for screening by artificial intelligence. Participatory science Volunteers are engaged at the start of the project. They help define the prob- lem, collect data and then help the sci- entists analyse the material. However, the researcher/expert has a high level of control over the analysis and inter- pretation. The idea for CurieuzeNeuzen 2016, in which members of the public measure the air quality in Antwerp, arose in the Ringland community group. Scientists and the Flemish Environment Agency (VMM) then became involved in the research. Extreme citizen science Researchers and volunteers develop the various steps in the research pro- cess together. But here the role of the scientist is confined to that of facili- tator. The volunteers run the citizen science project and do the work. Using the so-called flitsfiets (flashbike), a DIY bike, the ac- tion group 30Max records speeding offences in the centre of Antwerp. The group aims to use the data to show that the speed limit is rarely observed and hopes to force the introduction of measures. The action group itself came up with the idea for the flashbike. Researchers from the imec City of Things helped find the technology and data to make it work. Source: Models of citizen science projects according to Hakley et al. (9), along with examples from Flanders. 11 https://www.worldcommunitygrid.org/ https://www.oogvoordiabetes.be/ https://www.oogvoordiabetes.be/ http://www.curieuzeneuzen.eu/be/ https://ringland.be/ https://www.commonslabantwerpen.org/blog/2019/6/5/bouw-jouw-eigen-diy-flitsfiets https://www.facebook.com/dertigmax/ https://www.imeccityofthings.be https://www.imeccityofthings.be Type Description Example Contributory project A citizen science project run from the top down by research scientists, in which citizens are generally invited to gather data. The researchers decide the re- search focus. In the citizen science project Vespa-Watch members of the pub- lic are asked to look out for Asian hornets (and their nests). This exotic wasp species is a threat to the native bee. When they spot the insect, citizens upload a photo and the GPS coordinates to the project website. Research scientists working at Ghent Uni- versity use the data to map out the dispersal of the species. The citizen scientists merely supply the data. Collaborative project The research scientist decides the research focus. Citizen sci- entists can take part in different phases of the scientific process (e.g. analysis, interpretation and presentation of the data gath- ered). In 2014 the project AIRbezen in Antwerp involved a large group of citizens who collected data (they submitted the leaves of a strawberry plant, which they had left out on the window ledge, for an analysis of the air quality). Research scientists at the Uni- versity of Antwerp were to collaborate in the first phase of the project with volunteers from the Stadslab 2050 group. They and the volunteers brainstormed how the study would be done and what it should be called. This small group of volunteers also helped with the plant distribution and communication. Co-created project A co-created project begins with a question set by members of the public. All of the steps leading on from this are taken by the participants in consultation with the researchers. In Antwerp a citizens’ observatory was set up under the Europe- an Ground Truth 2.0 project. Scientists, policymakers and citi- zens regularly meet around the table to consider study areas and solutions for environment-related challenges like air pollution, drought, flooding, lack of greenery, and heat. In 2019 they began work on the subject of heat stress. Source: Models of citizen science projects according to Bonney et al. (8), along with examples from Flanders. My project is closest to: Crowd sourcing Distributed intelligence Participatory science Extreme citizen science Contributory project Collaborative project Co-created project 12 https://www.vespawatch.be/ https://www.uantwerpen.be/nl/projecten/airbezen/afgelopen-projecten/airbezen-2014/ https://stadslab2050.be/klimaatadaptatie/hittestress-en-technologie/een-burgerobservatorium-over-hitte-sint-andries To set up a good basis for communica- tion it is important that you understand your target audience(s). Do you want to engage with the general public or with active citizen sci- entists? Or passive sympathisers, opinion givers or niche experts? The better you understand your target audience the more personal, and the more effective, you can make your communication. When defining your target audience, you should consider the following: size, age, gender, level of education, prior knowledge of the research sub- ject, initial interest and engagement with the sub- ject and your organisation. It is best to split your audiences into primary, sec- ondary and intermediary target audiences. Your primary target audience would be that group of people who feel the most engaged with your pro- ject, and who are the most affected by the re- search aim. This target audience will contribute the most when it comes to collecting data or giv- ing feedback on your project. Think, for example, of a group of asthma patients, in a citizen science project set up to measure the air quality and dis- seminate the results. A secondary target audience would be a group who are aware of, but not directly involved in your project. This might change in a lat- er phase of the project, when a secondary target audience becomes the primary target audience. Think, for example, of associations or government authorities with an interest in your research fo- cus. An intermediary target audience is a group of people or a person that you can bring in to com- municate about your project. Think of your organ- isation’s internal communication department, the local or regional press or external partners. If your primary target audience is made up of youngsters, for example, a way to engage them might be by en- listing the help of teachers, parents or youth clubs. When defining your target audience and working out your communication strategy you should con- sider the following guidelines: • Create separate communication plans if you want to engage different target audiences. Your message should address each of your tar- get audiences personally. • What tone will you adopt for your target au- dience(s)? Should your communication come across as formal or informal? • How will you approach your target audience(s)? Will you go for a generic, specific or individual approach? (see chapter 2: tactics and tools). What target audience(s) do I hope to engage? Description formal/ informal Primary target audience Secondary target audience Intermediary target audience A special approach is needed for schools and so- called hard-to-reach groups, which we cover in more depth below. How to persuade schools to take part in research? If education is one of your aims and there are chil- dren or youngsters in your target audience, you will soon need to set your sights on a school or classroom collaboration. Citizen science has the potential to bring science home to this young target audience and to awaken interest in the subject and the underlying scientific process. It gets them actively acquainted with re- search and gets them thinking about socially rele- vant themes, such as health and conservation. The educational benefits seem endless. Yet schools, and schoolteachers, are hardly straining at the leash to get involved in citizen science. The school environment is saturated with annual requests to participate in studies (11), and given the busy cur- ricula, time is a rare commodity. If you want your project to stand a chance, it is best to keep the costs of participation as low as possible. Present your project early, to give teach- ers time to fit your project into their annual plan. Provide a visible space in which your schools and teachers can communicate their participation and results to the outside world. And go looking for ways to engage parents in your research too (12, 13, 14). 3 Will you take a proactive, active or passive ap- proach to your target audience(s)? It is not neces- sary to communicate with all your target audienc- es at the same time or with the same intensity. Specify your target audience 13 How to engage teachers T Link your teaching material and assignments to the curriculum Citizen science usually takes the form of an in-house activity, in which the teaching material and assignments are incorporated in the existing lessons. It is therefore vital that the material you provide for the teachers – and the activ- ities you anticipate from the pu- pils – tie in well with the set lesson objectives or curriculum aims and final attainment levels. Look ahead and make things as easy as possible for the teacher by spelling out the links between the separate research assign- ments and subject-specific and cross-curricular learning aims. Or, go a step further and link every component of your lesson pack- age to a learning aim. Actual attainment targets for the Flemish school system can be found at www.onderwijsdoe- len.be, in school curricula and on school administration department websites. Need an even strong- er match between your project and the school curriculum? Think about inviting input from teachers at your project design stage. T Make the lesson package modular Build in plenty of flexibility when designing your lesson packages. Draw a clear line between tasks that are strictly necessary and other supplementary learning content and activities. By doing this you allow teachers the room to adapt the offering to suit their own pace, their own preferences and the classroom context. Not all teachers like a ready-made package or are willing to accept the offering as is. Some prefer to adapt the content to make it more suited to the existing curriculum. With an adaptable package you will reach the teachers who appreciate ready-made lessons and those who like to prepare their lessons themselves. T Offer teachers adequate and adapted support Teachers require an adequate level of support at every stage of your research project and when doing the activities that go with the lesson. Keep the time they spend on preparation to a minimum and make sure they don’t waste extra time looking for additional learning material. Have a thought here for the teacher’s knowledge of the sub- ject. Not all teachers will be famil- iar with the learning content you offer. Some teachers will have to take on new knowledge and get used to doing research activi- ties. Don’t allow them to doubt their own knowledge, and provide plenty of background information. Guidelines on the purpose of the lesson and the components of the lesson package are certainly a welcome addition. Although it is important that a teacher be able to run through the lessons with- out help, a citizen science project with a more complex set of tasks can also come with additional aids, such as teacher training or how-to videos. Think up a procedure by which teachers can get in touch with you if they are experiencing difficulties or have specific ques- tions they need to ask. T Promote your project through ex- isting channels of communication To attract attention to your pro- ject you might consider using the existing channels of communica- tion for teachers. For example, add your project on KlasCement, an online platform which offers free inspiration and learning material for teachers in the Netherlands and Belgium. Or promote your project in teachers’ magazines like Klasse or online newsletters like schooldirect and lerarendirect. Below you will read how to engage schools with your project by hitch- ing a ride on existing networks. 14 http://www.onderwijsdoelen.be/ http://www.onderwijsdoelen.be/ https://www.klascement.net/ https://www.klasse.be/ https://www.vlaanderen.be/onderwijs-en-vorming/organisatie-van-het-onderwijs/schooldirect-de-elektronische-nieuwsbrief-voor-schooldirecties https://www.klasse.be/nieuwsbrieven/ AIRbezen@School (Strawberries at school) In the AIRbezen@School project research scientists from the University of Antwerp give primary and secondary school pu- pils a strawberry plant to look after for a few weeks. The pupils collect a leaf sample at the end the project, which they submit to the analysis lab to measure for particulates. Participating schools are given a box of promotional material (a poster ex- plaining air quality and particulates, three posters saying ‘AIR- bezen@school measurement station’, a set of stickers and a board of NO2 diffusion tubes to suspend at the window) as well as a box containing everything they need to participate in the study. Recruitment and communication are primarily by email. There is also a website, a Twitter account and a Facebook page. The recruitment campaign is widening its base for the second year (2019-2020). An element of competition is being intro- duced. Classes will be able to earn medals. At the end of the school year, participating classes will be able to present their results at a symposium. What about inclusion? As theory has it, citizen science is inherently par- ticipatory. As well as involving people in research and improving scientific literacy, citizen science has as a lot to offer in the way of social inclusion. Although we don’t have the exact figures to sup- port this, it is widely accepted that citizen scien- tists are not generally representative of society. The average citizen scientist seems to be white, middle-aged, well-educated and male, with a keen interest in science and research (15). It needn’t come as a surprise that this particular profile is so well represented. These people have the right motivation. They also have the time, money and expertise to participate in scientific research (9). If you want to be inclusive, you will need to make explicit choices in your project design phase, such as working with at-risk groups (16). In that case, the citizen science will need to have a pur- poseful design, which considers diversity and ac- counts for the needs and expectations of minor- ity groups. You will need to think upfront about how you intend to offer equal opportunities to participate in your project. Start by clarifying how important it is for your project to be inclusive. Always base this on the project aims and the target audience you have in mind. Phrases like ‘science for the general public’, ‘improve knowledge among the population’, and ‘meet the needs and expectations of society’ im- ply an inclusive study which is likely to engage a broad range of participants. But is it really one of your project’s stated aims? And is it achievable? Or do you merely want access to your project to be as open as possible, not to turn its focus onto the engagement of under-represented groups? When the collection of known-quality data over- rides educational aims and participatory science, it is enough to eliminate the potential obstacles to participation as best you can. In this case, it may suffice to recruit your volunteers through gener- al calls and to focus your recruitment campaigns on target audiences who possess the knowledge, time and resources needed to participate (20). If you decide to attract people who are tradition- ally less well represented in citizen science, it is important that you also search out the right part- ners (see tactics). Arrangements with local or- ganisations and intermediaries can help eliminate some of the initial barriers to participation. 15 https://airbezenatschool.be/ Through careful consultation you may be able to reassess your own perception of the target au- dience and, at the same time, identify the needs, wishes and requirements of these local organisa- tions and their members, and so better alter your research to suit. By listening to representatives from the local community at an early stage, you can identify context-specific barriers to partici- pation (17). Tips to engage minority groups In your search for relevant intermediaries you can turn to any of the sociocultural institutions or welfare organisations. Consider the OCMW (CPAS), poverty organisations, youth organisations or community centres. Don’t forget to speak to more local, in- formal organisations and individuals. They tend (more than the formal institutions) to have the confidence of the local commu- nity. Use every consultation opportunity with these institutions and trusted people to identify the best communication channels to use. Is it okay to make an announcement on Facebook or by email, or would it be better to meet face-to-face? Think long and hard about the wording of your message. Academic lan- guage is out of the question. When thinking about getting peo- ple together to inform, persuade, educate or put them to work, consider arranging these meetings at a time and place when your target audience would normally attend. Once citizen scientists sign up to your project you can use the snowball method to attract new participants. Encourage them to contact other people in their personal network. They will close the gap between you (the project initiator) and your target audience. Your communication with participants does not cease after initial contact. Once you have access to a particular group you will maintain the connection during the lifetime of the project. To do this, introduce regular opportunities for contact in which you highlight once again the project aims and its benefits to the community. Be sure to remember to share the (interim) research results at the appropriate times. Source: Tips for engaging minority groups (18). Improving public health in Amsterdam Through a citizen science project entitled Gezond Slotermeer (Healthy Slotermeer) the RIVM (National Institute for Public Health and the Environment) is closely involved in neighbourhood development. The RIVM wanted to learn which environment factors the residents of the Slotermeer neighbour- hood in Amsterdam thought were vital to their health. The researchers wanted to do more than send out yet another tradition- al survey, because these surveys only get through to the people who read, write and speak Dutch, and already have an interest. Through their neighbourhood development work the researchers trained ‘health ambas- sadors’, who were just ordinary residents of the neighbourhood. They helped devise the questions and went around the neighbour- hood interviewing residents. It gave rise to questions and answers which the research- ers hadn’t expected. And it turned out to be a good way of reaching more people from minority groups. Will your project address minority groups? No Yes If yes: With which organisations would you like to collaborate? 16 What specific motivations do you expect of your citizen scientists? Collective motivations Norm-related motivations Motivation based on reward / extrinsic motivation Collective identification Hedonistic / intrinsic motivation At the heart of citizen science lies collabora- tion, between you – as the project initiator – and the citizen scientists. You are not the only party to come to the project with expectations. Citizen scientists have their own aspirations and their own reasons for joining in. Not understanding your potential target audience, and not knowing how to stimulate it as a group, is one of the biggest pitfalls in citizen science. Once you know more about why members of the public like to get involved in scientific research, and why they don’t, you can adjust your aims ac- cordingly and employ the right strategies and tools to recruit and retain them. Understanding the attitudes and motivations of participants al- lows you to refine the target audience and, what is more, helps you select the right set of tasks, the right message and the right media channels for your audience. Resources and schedule permitting, you may de- cide – preferably in the design phase of your study – to survey your potential target audience as a way of logging their attitudes and motivations. Should this reveal nothing, you can always draw on already established general research. Often this relies on self-determination theory (Ryan and Deci (18)) as a theoretical framework. It postulates two types of motivation: intrinsic and extrinsic. If you are intrinsically motivated to participate in citizen science, you take part be- cause you enjoy it. Extrinsic motivation refers to behaviour which is driven by an external reward. Your participation in citizen science is motivat- ed by money, recognition, status, or, also, deeper knowledge or a new skill set. The theory tells us that when people are intrinsically motivated, they remain engaged with a project for longer. Extrinsic motivation weakens over time. Yet both have their place, because people often have more than one reason for taking part in a citizen science project. Often, what drives a participant is a mix of altru- istic considerations – a desire to help the advance of science or make the world a better place – with more self-serving stimuli (20). Why do people get involved in citizen science? Using self-determination theory we can identify five different types of motivation in citizen science. Collective motivations and intrinsic motivations have the big- gest impact on participation. Collective motivation The citizen scientist sees the project aims as important. Norm-related motivation The participant hopes for positive re- sponses from friends, family or work- mates. Motivation based on reward / extrinsic motivation Participation brings real benefits, such as building a reputation or making new friends. Collective identification The participant identifies with the group, its norms and its values. Hedonistic / intrinsic motivation Participation in the project gives pleas- ure. Source: Classification of types of motivation for participation in citi- zen science, according to Nov, Arazy & Anderson (21) 4 Understand what motivates your citizen scientists 17 “I’m proud of the contribution I made to science” So who are these citizen scientists? Jeanine Goossens took part in Grote Schelpenteldag (Big Shell Count). Gitta Camffer- man interviewed her about her experience. Why did you take part? “I think it’s vital that we study the evolution of biodiversity on beaches. The beach, and the sea, offer me relaxation. I love be- ing there. But today is special. You can go hunting for shells at any time, but today, as a member of the public, I got to be part of some important research. The more of us that take part, the better the results. That’s why I’m here on this terribly cold Sat- urday in March.” “We need scientific research to safeguard our future here on Earth. By doing this I’ve been able to do my bit, and I’m very proud of it, even if I only played a tiny part.” What did you think of it? “I thought it was really very interesting. The collection method they got us to use was pretty good and they communicated it to us well beforehand. And the experts from the Flanders Ma- rine Institute (VLIZ), who helped us with the count, were really friendly and told us all a lot about the shells. I even managed to find a pretty rare one!” “I think it’s vital that our children and the generations to come grow up with science. It is our task to educate and train them. And I hope it works out well.” Reasons for monitoring air quality As part of the development process for its engagement strategy the hackAIR project surveyed 370 potential citizen scientists. An online questionnaire gauged motivations for and barriers to air quality monitoring and measurement in the neighbourhood. The leading motivations were: general curiosity about the measurement results (56%), con- cern about the local air quality caused by the perception of living in an area with poor air quality (43%) and personal health problems (30%). These reasons were used as triggers during opportunities to communicate later in the project. Source: Interview by Gitta Camfferman for Iedereen Wetenschapper. 18 https://www.eoswetenschap.eu/tag/de-grote-schelpenteldag https://www.hackair.eu/ https://www.iedereenwetenschapper.be/article/ik-ben-trots-op-mijn-bijdrage-aan-de-wetenschap https://www.iedereenwetenschapper.be/article/ik-ben-trots-op-mijn-bijdrage-aan-de-wetenschap You can evaluate the success and effective- ness of your project at various points: before, during or after the study. Your choice will depend on the purpose of the evaluation: what do you hope to achieve with it? An evaluation before you start the project can help you identify the participants’ expectations, and this will enable you to make early adjustments to the project design. Also, if you plan to measure the effect of participation afterwards, it is vital to look at aspects of knowledge, attitude, skills and behaviour at the beginning. What do people know about the project’s subject, for example, and what do they think about it? With a baseline like this you can gather data before you start, which will allow you to measure change at the project’s end. An evaluation in the live phase of the project can pro- vide an impulse for change. An evaluation after- wards can shed light on the value generated and may also allow you to say more about how effec- tive or successful your project was. Were its aims achieved? How did it work to the benefit of the participants or the project team? By deciding your evaluation points early in the de- sign process you sharpen your research focus. Not only are your project aims and the desired results decided sooner, but they are set with greater clar- ity (22, 10, 23). How to measure the success of your engagement strategy? Your success as project organiser is a measure of the effectiveness of your engagement strategy and the satisfaction of your citizen scientists. The easiest way to measure engagement is by gathering figures (e.g. number of records submit- ted, number of participants, etc.) on the actual behaviour of your citizen scientists. For partici- pant numbers you can look at the number of con- tact opportunities and the number of participants per type of contact opportunity (e.g. workshops, meetings, lectures, etc.) and the number of vis- itors on websites and social media pages. It may also be relevant to look at these numbers at par- ticular junctures in time. This will tell you how long citizen scientists are active, how often they par- ticipate and how many drop out. If your project has a clear educational aim, you will be keen to discover whether change came about 5 Engage and evaluate Tips for a strong evaluation framework T A strong evaluation framework requires pre- cise indicators to tell you whether the goals you set have been attained. The more clearly you define the project aims the more precise your indicators will be. T Set your indicators at the start. This estab- lishes consensus on what to measure in the project team, so you can adjust the design of your project in good time (24). T Get several parties involved in the process of developing indicators. By engaging not only your project team but citizen scientists, partners and sponsors, you will have a better chance of evaluating the success of your pro- ject from a variety of standpoints. T Not all indicators have to be quantitative or objectively quantifiable (consider the number of citizen scientists taking part, or the re- cruitment cost per participant). If you plan to evaluate the engagement of your citizen sci- entists and other stakeholders, quantitative indicators (which focus more on the type and quality of participation) are a welcome addi- tion (24, 25). T Develop different indicators for different evaluation purposes. Does your evaluation focus on a) the success of the citizen science project, b) the operational process, c) the immediate results or long-term impact, or d) a subject in which all these elements should be covered? The indictors needed to evaluate the project outcomes won’t be the same as those needed to evaluate the activities and the process (26). T Allow for adequate flexibility in your evalu- ation. Indicators are never carved in stone. When your project is up and running you will get a better feel for the context in which your research is taking place. Citizen science pro- jects can throw up some surprising results. To be ready for both situations your evaluation should have flexibility built in. It should be possible to add or modify indicators as you go along (22). 19 as a result of taking part. Greater awareness, knowledge and understanding, heightened inter- est, changes in attitude or behaviour, and, finally, better skills (27, 6, 28). These changes may have occurred in relation to the subject (e.g. the partic- ipants know more about the effects of air quality on human health) or the scientific process (e.g. the participants have a better understanding of the importance of representative data). The success of the actual engagement process can be measured against a set of normative crite- ria. For a vast evaluation framework see the study by Haywood and Besley (6). They look at things such as whether citizen scientists have sufficient access to information, equipment or time and whether they are sufficiently engaged in the de- sign of tasks, analysis of data or communication of results. Other indicators in this scheme relate to things such as transparency, honesty, represent- ativeness and inclusion. Be sure to check whether your citizen scientists are satisfied in the end. Did the project satisfy their original motivation to take part? Did they have fun with their tasks? Were the tasks explained clear- ly enough? Did they get enough information? You might also ask if they were satisfied with the pro- ject communication. Was the information given on time, and was it relevant and clear? Did it come through the right channels of communication? Did they at every stage of the project feel that the op- portunity was there to contact the project organ- isers if they needed to? Not only do satisfied participants engage with a project longer, but they can be more easily encour- aged to take part in new citizen science initiatives. When? Evaluation in planning phase Evaluation in live phase Evaluation afterwards When cities are in danger of flooding In the FloodCitiSense project a ‘pluviometer’ and (web) appli- cation keep residents in Brussels, Rotterdam and Birmingham updated on the city’s rainfall and flood risk. Citizens build their own pluviometer and set it up in the garden. Impact measure- ments are set up using an evaluation framework devised by Kieslinger et al. (26). The evaluation framework measures the project’s impact at three levels: the contribution to science, the contribution made by the citizen scientists and the project’s socio-ecological dimension. Special indicators have been de- vised for each dimension in the project context. For example, the evaluation framework is used to monitor the number of scientific publications produced, the number of contributions made by individual citizen scientists and any increments in their knowledge and skills. The socio-ecological dimension is evalu- ated by looking at the innovative nature of the technology and the influence exerted on the city’s water and flood management policy. Quantitative indicators Qualitative indicators When will you evaluate your project and what indicators will you check? 20 http://floodcitisense.webflow.io/ Tactics and tools Recruiting citizen scientists and permanently engaging them can be done in many different ways. We offer six. Before deciding on specific tactics and tools, you will need to think long and hard about your strate- gy. Will you be taking a generic approach or a spe- cific one? Or will it be a combination of the two? When you take a generic approach, you publicise your project through an open call. You can do this via social media, the press, by handing out flyers, etc. You target a huge number of potential citizen scientists, and you do not target specific profiles or specific audience(s). When you take a specific approach, you send out personal invitations or contact people on member lists. Collaborations with existing networks and communities can work well for a specific approach. Your choice of strategy will affect the diversity of your project participants. Research shows that the generic approach does not always deliver a diverse target audience in terms of gender, age or educa- tion level (29). A generic approach is more likely to reach the ‘archetypical’ citizen science profile (30): the white, well-educated man who knows a bit about computers and has a lot of spare time. Initial or continued participation? Once you know your target audience you can check out specific tactics and tools to help you engage it. By tactics and tools we mean the instru- ments or channels of communication you will use to recruit and retain your target audience. It is vital here to be aware of the motivations for initial and continued participation. Why do people take part in citizen science? Initial participation Personal interest in the topic Contributing to science The fun factor, taking pleasure Desire to learn and general inquisitiveness Continued participation Validation and appreciation for the contribution Deeper understanding of the topic Contact with the scientist and/or other like-minded people in the project Willingness to assume tasks or roles in the project Citizen scientists don’t necessarily stay to the end of a long-running project. The drop-out rate is highest at the time of initial participation or shortly afterwards (31). How come? At the time of initial participation, it is usually the use of jargon or a non-user-friendly application. Beyond initial participation it is lack of recognition from the sci- entist or lack of openness about the scientific pro- cess and the research results. So make sure you communicate with your citizen scientists regular- ly, and that you do so beyond the data collection phase. What happens next? When will the results come in? Is there anything else they should know about the research topic? Below is a list of six tactics to recruit citizen scien- tists and keep them engaged. You can also turn to the more traditional tools of communication such as press releases, newsletters or mailing lists, fly- ers, events and trade shows, a project website or a blog. We offer a few practical tips on this subject in chapter 3. Source: Rotman et al. (31). 22 23 Hitch a ride on existing networks TACTIC 1 Whether you set up citizen science on a small or large scale, the best way to effectively engage your target audience is often with help from existing networks and communities. Large societies and networks are usually on the lookout for a new angle or to inject new life into their annually recurring initiatives. How to find and keep the right partner T Look for meetings with a bearing on your research topic and try to introduce your project there. If you are studying air quality, for example, it might be interesting to attend debates on climate change and mobility, and workshops on preventive healthcare, or follow specific activities organised by city labs or local and neighbourhood committees. Other initiatives you can follow include Volunteer Week, Science Day, Biotech Day, Child Universities, History Day, Teachers’ Day and Classroom Day. Introduce your project, try to swap details and try to stay in touch. T Find member organisations or societies. Speak to the person in charge of communication about the possibility of raising your pro- ject’s profile in a systematic way. For example, you might get cover- age from a monthly newsletter to members. T Look beyond national borders. Check out the ‘community’ sec- tion on the ECSA website (Euro- pean Citizen Science Association). It lists European and international organisations all of which are ac- tive in citizen science. The site http://eu-citizen.science/ is cur- rently under construction and gives an overview of citizen sci- ence tools and projects in Europe. T Choose to partner existing net- works and organisations whose objectives complement your own. Improving the knowledge of mem- bers, for example, or just offering a fun experience. The partnership must support information shar- ing and data collection. If you join forces on a similar citizen science project, discuss the data collection protocol. T Be sure to maintain good relations with your external partners and keep them informed of your project’s pro- gress. Give them a say in the project organisation and communication and every three to six months invite them for a face-to-face meeting be- tween all the partners involved. This creates a sense of community, not just among the project participants, but among its leaders too. A partnership with an existing organisation or in- itiative linked to your research subject can mean that you come into contact with the right target audience sooner. It can put you in touch with citi- zens who have prior knowledge and a shared inter- est. Or it may offer a route to those hard-to-reach target audiences. Through partnerships you can reach citizens in greater numbers and increase the expertise in the topic, and you may even be able to engage a niche target audience. In any case, we ad- vise against building up an entirely new communi- ty of citizen scientists. Once you have found your project partners it may be a good idea to discuss a visual identity and com- munication strategy with them. Always list the names of your partners on your website. In your communication with citizens, state who your pro- ject is ‘in association with’. This will tell them that 24 https://ecsa.citizen-science.net/community/map http://eu-citizen.science/ Inspiring examples In the project Oog voor diabetes (Eye on Diabetes) the participants annotate abnormalities on ret- inal images. They are helping to build up a refer- ence database of annotated images, which can be used to train an AI software to recognise diabet- ic retinopathy in future. To recruit citizen scien- tists the project partners Diabetes Liga (Diabetes League), an association for diabetes sufferers and their families and professional care providers. The league promotes the project on its website, so- cial media and in its magazines. The members are naturally sympathetic towards the subject, which makes them an ideal target audience. The Getuigenissen (Witnesses) project invites volunteers to delve into 18th and 19th century criminal law archives for historical witness state- ments. Citizen scientists visit an online platform to view and transcribe the historical texts. The project is partners with Histories, a society of genealogists and heritage and culture enthusi- asts. Histories offers courses on these subjects and its members have often already transcribed historical documents in the past. The MamaMito project, in which citizen scientists go in search of their maternal ancestors, also partners Histories. Other associations and organisations which have partnered citizen science projects to date are Natuurpunt, Arabel, people’s observatory, the Flanders Meteorology Society, beweging.net and The Flanders Cycling Council. Engaging schools and teachers The VLaanderen IN DE weeR (Flanders in the weather) project monitors local weather in Flan- ders by means of weather stations at Flemish schools. The online learning platform is primari- ly of interest to first and second grade secondary school geography teachers. To engage this target audience the project partners the Flemish Asso- ciation of Geography Teachers. There are many more teacher associations, such as the Associa- tion of Science Teachers and the Association of Bi- ology, Environment and Health Education. Engaging youngsters and their parents If you would rather engage youngsters in their spare time than in the classroom you might con- sider partnering local youth clubs or play areas. If your project is STEM-related you may be able to interest local STEM-academies in your offering. Technopolis is obviously an interesting partner. Beyond that, Youth, Culture and Science offers a wide range of leisure activities for children and youngsters. RVO-Society reaches out to young people in and out of the classroom in STEM-relat- ed matters and technology. You can also engage the parents of children and young people through local parent-teacher asso- ciations and by partnering the Gezinsbond (Family Association) or Kind en Gezin (Child and Family). Don’t forget your own organisation! Are you embedded in a research institution? Be sure to look for what you need in your own organ- isation. Often you will have access to the science communication unit, the marketing and communi- cation department, graduate associations, press officers and so on. they are part of a larger, like-minded community with an interest in the research topic. You can also agree on rules about messages post- ed on social media and about joint events and ac- tivities. 25 https://www.oogvoordiabetes.be/ https://www.diabetes.be/ https://www.diabetes.be/ https://www.getuigenissen.org/ https://historiesvzw.be/ http://mamamito.be/ https://www.natuurpunt.be/ http://belgianspiders.be/ https://www.volkssterrenwachten.be/ http://www.weerkunde.be/ http://www.weerkunde.be/ https://beweging.net/ https://fietsberaad.be/ http://vlinder.ugent.be/ http://vlinder.ugent.be/ https://www.vla-geo.be/ https://www.vla-geo.be/ https://velewe.be/ https://velewe.be/ https://www.vob-ond.be/ https://www.vob-ond.be/ https://ambrassade.be/nl/partnerschappen/erkende-jeugdverenigingen https://www.speelplein.net/Speelpleinen https://www.stem-academie.be/ https://www.technopolis.be/ https://www.technopolis.be/ https://www.jeugdcultuurenwetenschap.be/ https://www.rvo-society.be/ https://www.gezinsbond.be/ https://www.gezinsbond.be/ https://www.kindengezin.be/ To reach out to young people you can partner your institute of higher education’s teacher training college, children’s university, marketing depart- ment or science communication unit. University colleges, which lean towards socially oriented, applied research, are often well connect- ed to local communities. Generic or specific approach Initial or continued participation Outcome This tactic is suitable for a specific approach. Through partnerships you can engage specific target audiences or a niche audience. This is a tactic which has its greatest effect during initial participation. By entering into partnerships with existing networks and communities you build or grow a community of people with a shared interest or concern. Good partnerships can also be good for the contin- ued participation of citizen scientists, for example, through working on a shared visual identity and com- munication strategy. Here, a sense of community comes into play. Engagement: building or growing your community Go local Does your citizen science project target a pre- dominantly local audience? Consider partner- ships with local cultural centres, museums, li- braries, sports clubs or music societies. Need help finding the right contacts and partners? Contact Scivil for advice. 26 http://www.scivil.be/ 27 Offer a fun experience TACTIC 2 Participating in citizen science should be educational, but unless it is fun it will not be sustainable. How do you in- crease the fun factor of your citizen science project? History excursions For the Ja, ik wil (I Do) project citizen scientists logged onto their computers to transcribe and analyse historical marriage an- nouncements in Amsterdam. It was all to do with a historical study of marriage relations. For an enhanced sense of community and to keep citizen scientists motivated and rewarded, the Universi- ty of Utrecht’s researchers arranged regular meet ups, at which they talked about the study’s progress and gave the participants a chance to ask questions and interact with them. They also ar- ranged excursions, such as a visit to the Trippenhuis, a historical building in Amsterdam, which is not normally open to the public. See also the interview with project leader Tine De Moor. Most of the activities arranged for citizen scien- tists have an informative or educational focus. Think of workshops, where you (the project lead- er) describe the topic or explain the results. These appeal most to citizen scientists who are intrin- sically motivated (interested or concerned about the topic). But the format offers little room for in- teractivity or experience. We recommend that you also arrange activities that are more about the fun and the social expe- rience. These activities satisfy social motivations such as getting to know people, having experienc- es, having a good time and having a sense of com- munity. 28 Generic or specific approach Initial or continued participation Outcome This tactic is suitable for a spe- cific approach and mostly works for local or municipal citizen sci- ence projects. This is a tactic which works best to support volunteers’ continued participation and satisfies social motivations, such as getting to know new people, gaining a sense of community or having a good experience. The tactic can be employed at any point in the course of the project. Experience: giving participating citizen scientists a positive and enjoyable experi- ence. Social events tend to motivate people in the long term. Their enthusiasm is at its highest when the topic connects to a specific location, such as a street, neighbourhood or city, and involves a group of people meeting to consider a shared concern. Be sure to get the balance right between your pro- ject’s aims and scale, and the ratio of information to experience. Examples of social activities include competi- tions or family games afternoons, meet & greets between citizens and scientists, group walks, a breakfast or appetiser and info opportunity, etc. These activities provide an opportunity to learn about the project interactively and to thank citizen scientists for their efforts. https://www.uu.nl/nieuws/ja-ik-wil-amsterdamse-ondertrouwregisters-gedigitaliseerd https://www.iedereenwetenschapper.be/article/do-ut-des-het-principe-van-wederkerigheid 29 Use social media TACTIC 3 Facebook, Instagram and other social media can really bring your citizen science project to life. They offer op- portunities for interaction between scientists and citi- zen scientists and between the citizens themselves. Which platform? You can put social media to good use for a variety of objectives. You will want to use different plat- forms depending on your project aim and the tar- get audience. In Flanders, Facebook is still the leading choice (32). Three quarters of the Flemish population maintain an account and log on every month. Usage is grow- ing fastest among the older age groups. Face- book’s most interesting features are posting mes- sages and sharing events, sharing photos and vid- eos, paid advertising and setting up closed groups. Closed groups are a good way to let your citizen scientists keep their communication private and help each other out. Instagram has grown rapidly in recent years and is very popular among 16- to 34-year-olds. This platform is particularly good for sharing high- grade visual content. By using #hashtags and In- sta Stories you can interact with your target au- dience creatively and have them submit their own content. Although Facebook has more users, Ins- tagram can be more effective at creating a sense of community. There are relatively few Twitter users in Flanders and growth is still slow. This platform is more suitable for issuing short messages to engage a professional target audience, such as journalists, media professionals and domain experts. You can use Twitter to alert journalists to your project and ‘feed’ the media with relevant information or an- nounce interim results. Vary the message type Make sure you post a mix of message types on your social networking sites. Start by posting ‘project messages’. This type of message draws attention to your project and shares information about its aims, and about events, news items, results, and so on. You can also post ‘subject-specific messages’. These messages offer information about your re- search focus: scientific background information, current affairs, inspiring examples of similar pro- jects, related events and conferences. Finally, it is best to post regular ‘like’ messages. Funny clips, photos, did-you-knows, GIFs, be- fore-and-afters or ‘behind the scenes’ pictures. If you are aiming to post three messages a week, for example, that would mean one of each mes- sage type. Of course, that all requires content. Our advice is to plan ahead. This way you will already know what City spiders The SPIN-CITY project by the University of Ghent asks citizen scientists to photograph spiders and upload them along with addi- tional details as part of a research study on the impact of heat stress on the city’s animal population. SPIN-CITY has a Facebook page and an Instagram page. Striking are the ex- tremely beautiful and appealing photos. The message content is also of considerable in- terest: you learn how to take the perfect spi- der picture, for example, and how to tell the difference between males and females. The lead researcher made a captivating video clip explaining the project’s purpose. 30 You can build a virtual community around social media, through which scientists are able to inter- act with the citizen scientists on their project. So- cial media can also help engage citizen scientists more powerfully with the research, by giving them quick access to results or inviting their feedback. https://www.facebook.com/spiderspotter/ https://www.instagram.com/spinnenspotter/ content you need, and your messages will be more original and creative. You can also plan your mes- sages around your activities and events. There are plenty of tools you can use to plan your messages upfront, for multiple social media platforms con- currently, so they appear automatically when you want them to. Think beyond the standard link or photo Try to post creative messages, rather than a pho- to with a piece of text and a link to an article. We read lots of messages every day, and what you ob- viously want is for people to notice yours. Do this by pricking the emotions of your target audience, keeping the content as relevant as you can and prompting action or interaction of some kind. In- stagram Stories has all kinds of interactive possi- bilities, which you can try as a means of interacting with your target audience. Regularity is the rule! The regularity of your posts is more important than the daily or weekly number. Famous people Generic or specific approach Initial or continued participation Outcome This is a tactic which can be employed in a generic or a specific approach. You can post messages for the general public and publicise, but you can also operate specifically by ‘spying on the neighbours’. Follow similar projects or ambassadors for your topic, to make sure that they become aware of you too. This tactic can be employed to sup- port initial and continuous volunteer participation. Social media can be used to find new participants, and to keep your community motivated for the duration of the project. Interaction: the main thing you do through social media is interact with your target audi- ence. You can also use differ- ent message types to inform, engage and create awareness. and brands post on average 1.5 messages a day on Facebook or Instagram. The more often you post, the more engagement you will create. The less of- ten you post and the fewer responses you receive, the fewer people will see your post on their Face- book wall. If you don’t have the time, make sure you deliver on your promises, and plan to post fewer mes- sages every week or month. The most important thing is to stick to a given regularity. Look at your statistics With a paid Instagram account or Facebook busi- ness page you will be able to look at your statis- tics, which tell you which messages and times are the most popular. This information can help you plan your messages and publish posts at the time you choose. 31 32 Digital storytelling TACTIC 4 Telling stories can create a sense of belonging be- tween citizen scientists. Especially if you let them testify about their experience of taking part in your project. People love stories and some seem to live for them. Digital storytelling (33) is a tactic which uses visuals, audio or text to tell a story about a subject through digital media. You zoom in on a topic and recount the story, often from a personal perspective. The story is short and powerful (2 to 3 minutes’ viewing time, or a single screen of text) and arouses curiosity and interest in the target audience. Personal, historical or educational There are several kinds of stories that you might consider. In personal stories, you focus on the per- sonal experience of one of your citizen scientists, such as the knowledge and skill set they have ac- quired, the challenges posed by the research work, the measurement and collection of data, and so on. To tell the story, you might ask citizen scien- tists to record a short video or audio clip, in which they talk about their experiences, or you might in- vite them for a short interview. Pick a person who is representative of your target audience. This will help other people empathise with the story or character. Why not tell a personal story from the viewpoint of the research scientist? Historical or educational stories can also be told. Historical stories rely on historical elements to convey the more contextual information about your project, whereas educational stories inform and enlighten people about some aspect of the work. Educational stories are of particular ben- efit when reaching out to schools. They can be a fun way of engaging pupils (2). You can also draw attention to your project through social media, by throwing in historical facts and details, for exam- ple. ‘Did you know that exactly x years ago to the day scientist X laid the basis for Y?’ Once you have your stories in place you can begin to share them through all kinds of communication channels, such as your website blog, a newsletter or social media. With permission from the partic- ipant or interviewee you can even share the story more widely, through the press or a selection of magazines. Dead baby in the toilet From time to time the Getuigenissen project highlights a historical witness statement from one of the court cases transcribed by its volunteers. On one occasion we were giv- en the story of 29-year-old domestic servant Marie, who stood trial for infanticide. One day the plumber found the body of a baby in the toilet pan. It was thought to have been Marie’s, but her statements claimed other- wise. She began by saying that she didn’t know that she was pregnant. Then she ad- mitted killing the baby, but said she wasn’t aware of what she was doing as the child- birth had made her psychotic. What was the truth? And what was she sentenced? The re- searchers behind this project give the story an element of suspense, put it into context and contrast it with the situation today. And at the end, of course, they call for more vol- unteers for their citizen science project. 33 http://www.getuigenissen.org/ https://www.getuigenissen.org/post/ongewenst-zwanger-in-de-19de-eeuw https://www.getuigenissen.org/post/ongewenst-zwanger-in-de-19de-eeuw Elements of a good story Generic or specific approach Initial or continued participation Outcome This is a tactic which can be employed in a generic or specific approach. You can share the stories with the general public via the press or scientific magazines or share them specifically via your project’s channels of communication (blog, web- site, social media). This tactic can be employed to sup- port continuous volunteer participa- tion. When your story is about your participants they are recognised for their contribution and you satisfy the social motivations. Illustration and knowledge shar- ing: Personal stories can create a sense of loyalty among citizen sci- entists. Character Who is the story about? Stories are best told from a personal perspective so the reader, listener or viewer can truly empathise with the character’s situation. Setting Where is the story set? To create empathy for the character the story also includes details on the environment, time and season. Language The language must be highly accessible. Use an airy and simple writing style. Message The best stories are the simplest ones. Go for one clear story line. Structure The story follows a scenario, through which the project is introduced, followed by a specific activity to attract attention and interest, then closes on a climax. Authenticity The reader, viewer or listener must be able to identify with the character’s story, so that a level of familiarity or connectedness is created. Hearing a voice, or choosing the right photos and captions play a role in this. 34 35 Gamification TACTIC 5 Adding gaming elements to your citizen science project can benefit your research. Those who feel they can improve, take on a challenge or win a com- petition are more likely to stay motivated and keep participating for longer. Measurement campaigns A measurement campaign motivates citizen sci- entists to gather as many data records as possible You can motivate citizen scientists by introduc- ing gamification or game-design elements. Lots of projects make use of apps or digital platforms that allow for the incorporation of online game el- ements. But gamification also has its uses offline. By gamification we mean ‘the introduction and use of game-design elements in a non-game environ- ment’ (36). Gamification is not a reference to gam- ing or educational games. The concept is specific to the integration or application of game-design elements in an application or activity with a view to making the task more enjoyable. in a given time. The organiser defines a task in very clear terms and sets the geographical area and time frame (from one day to several weeks) within which data records are to be submitted. There are all kinds of ways to be creative about your measurement campaign. Link your measurement campaign to a specific event, public holiday, occasion or season. An air quality measurement on New Year’s Eve, (when lots of fireworks are set off ), for example or a rain- fall measurement during a summer thunderstorm. Get different communities of citizen scientists to compete against each other. For example: school X and school Y compete to annotate retinal images. Set up a large-scale, small-scale or niche cam- paign. For a large-scale campaign it is best to work in collaboration with local organisations that can Game-design element Description Motivation Points, badges and trophies Citizen scientists collect points by pro- gressing in a specific task and completing it. The points can be exchanged for (virtu- al) badges or trophies. Progression towards attainment of a goal A ranking (involving levels) Citizen scientists collect points and are ranked on a list or can go up in levels. Status and recognition in a com- munity A mission, measurement campaign or race against the clock Citzien scientists are challenged to em- bark on a mission or campaign under special conditions, such as a geographical location or time limit. On accomplishment of the mission a (virtual) prize may be awarded. Challenge and competition Group missions or campaigns Missions or campaigns are run in groups and the tasks are divided among the par- ticipants. Challenge, competition, new contacts and sense of a common cause Game-design elements and motivation We list several game-design elements below and link them to the specific motivations they support (36). Most of these game-design elements are integrated in online data collection applications used by citizen scientists. 36 help you with your logistics and promotion. You could also appeal to the media to publicise your campaign. For a niche campaign, be specific about the citizen scientists you appeal to. At the end of a measurement campaign the results are analysed and communicated to the citizen scientists. You might want to organise an event at which you discuss the results with the citizen scientists. Make sure you send each participant a copy and publish the results on your website and social media page. Rewards? In the examples above, citizen scientists were re- warded with points, badges, trophies or prizes. In some cases, coupons or small sums of money are awarded. Rewards relating to the research top- ic can also be considered. Such as, for example, air-purifying plants in return for air quality meas- urements. Your citizen scientists need not necessarily be re- warded in this way. Many of them are intrinsically motivated and need nothing in return for their re- search contribution. Handing out money and cou- pons could even be counterproductive. An equally valuable reward may be a citation for co-author- ship when the results are published. Conditions for success If you wish to incorporate game-design elements in your citizen science project, it is best to give the following aspects a little thought. • Integrating game-design elements in your citi- zen science will have the greatest effect on that group of people who enjoy gaming in general. They will be driven to participate in your project based on extrinsic motivations, such as the ele- ment of competition or the collection of points or badges. But research shows that most citi- zen scientists are intrinsically motivated. With gamification you will manage to motivate on- ly a small (extra) target audience, and most of those will be youngsters (37). • Game-design elements can also be counter- productive. It can demotivate citizen scientists to find that their work is being evaluated, or that they are too slow to reach the targets set (38). • Gamification only works when sufficient thought goes into the design of the game elements. If you can’t spare the time or money, it is best for- gotten. • In citizen science, gamification is at its most ef- fective when gathering data is the main thing the citizen scientists do. Healthy competition Through the website and app run by Waarnemingen.be animal and plant species observations can be submitted to the re- searchers at Natuurpunt. The data is open to all – including oth- er citizen scientists. This helps maintain the quality of the re- cords and keeps the pranksters away. Participants can be named and shamed, in other words. It is also possible to see how many submissions a person has made – how many observations, how many different species. This can lead to healthy competition between the participants. The proportion of each species is dis- played in a pie chart (if birds account for 60% of your observa- tions you can try balancing things up by looking for other animal species). Natuurpunt also arranges Bioblitzes on a regular basis. On these blitzes, everyone is invited to spot a set number of species over a two-day period. 37 https://waarnemingen.be/ https://www.natuurpunt.be/ Generic or specific approach Initial or continued participation Outcome This is a tactic which can be employed in a generic or specific approach. You can promote measurement campaigns among the gen- eral public via the press, or specifically via societies and associations. Integrating game-design elements in your project works best in support of continued participation. Measurement campaigns can be introduced to the project at any time and can motivate new and existing participants to take part. Experience and engagement: an extra fun-factor for the project. 38 39 Find project ambassadors TACTIC 6 An ambassador is a citizen scientist who has been involved since the very beginning. He or she usually knows a lot about your project’s research topic and will often have taken part in other science-led projects. Ambassadors are also known as lead users. They have a strong intrinsic motivation to participate. Ambassadors can help with your project’s logistics, administration or communication. An ambassador can (more readily) engage other, potential citizen scientists and can help you promote the work. Take the following steps to set up a programme for ambassadors. Define the task and the profile An ambassador can handle a wide variety of tasks. Talk to the other partners in the project to see where a little help might be of use, or where it might be a good idea to build trust with (new) citizen scientists. Consider administrative duties, helping with online or offline promotions (distributing flyers, forward- ing newsletters or messages on social media, etc.), being the point of contact at events, helping out at workshops by assembling sensors, and so on. It is vital for potential ambassadors to know what they can do (and when they can do it) and that they be able to choose a role that suits them. Find and train ambassadors The only effective way to set up an ambassador pro- gramme is to put the time into finding and training the right people. Ambassadors may simply offer you their services, but you can also promote the pro- gramme by adding a registration module to your website. You can also feed the promotion through your project’s other channels of communication, such as events, workshops, social media, and such- like. With your ambassadors identified, you will need to provide the necessary training. This might be a one- off event, such as a tutorial on simple tasks. Or you might arrange regular sessions for your ambassa- dors on complex tasks. Through this, they will learn more about how your project works. After a while you might bring in the train-the-trainer model, in which incoming ambassadors are trained by more experienced ambassadors. Give your ambassadors visibility Your ambassadors are still volunteers. They deserve some kudos within the community by way of thanks. After a measurement campaign, thank your ambas- sadors publicly and do it officially, via the website or newsletter or by turning the spotlight on them. You can achieve visible recognition with a captioned T-shirt, or by adding a token or badge to their online profile. Ambassadors are often satisfied with intrinsic re- wards. A day spent with the team analysing the re- Ambassadors for the North Sea In 2014 the Flanders Marine Institute (VLIZ) started a project by the name of SeaWatch-B. Since then, it has trained and en- gaged twenty volunteers. Each has a fixed area of beach to comb and takes ten standard measurements a season. Among other things they count the numbers of washed-up jellyfish, lugworm casts and shellfish species on the tide line. The collab- oration provides the VLIZ with more than just research data. The SeaWatchers have also become ambassadors for the North Sea. They have a sense of involvement in what they do, learn something new every day and take good care of their own ‘back yards’. If they have concerns, they write in or take some other form of action. As seashell experts they take the lead at the annual Grote Schelpenteldag, at which members of the general public identify shells along the coast. The VLIZ provides training at least twice a year, through which the SeaWatchers improve their knowledge. And the institute engages them regularly with other scientific activities aboard its marine research vessel. 40 http://www.seawatch-b.be https://www.iedereenwetenschapper.be/projects/grote-schelpenteldag http://www.vliz.be/ Generic or specific approach Initial or continued participation Outcome This is a tactic which can be employed in a ge- neric or specific approach. You can appeal to the general public for ambassadors through your website, or specifically via email, a news- letter, or by approaching people in person at events. This tactic can be employed to support continuous volunteer par- ticipation. Enthusiastic volunteers can be highly intrinsically motivat- ed to play a role in your project. Illustrative and engaging: project ambassadors can be role models for other participants and be of help in promotion and communi- ty building. sults or an excursion with professional scientists can be both interesting and special. Adjust where necessary Ambassadors can be brought in for a single activi- ty or stay involved with the project during a longer period. Although adjustment and evaluation may be necessary, it could prove counterproductive. Find a balance between the support you provide and any performance evaluations you make. 41 Tips and tricks These helpful tips will set you on the path to clear and successful communication. Be clear Don’t blind your citizen scientists or the me- dia with science. But at the same time, don’t oversimplify. Science is not a walk in the park. It is the result of years of painstaking teamwork. That message can be conveyed, but it must be understandable. ■ Keep to a uniform style, design and terminology Be consistent and always use the same pro- ject name (same spelling), logo and hash- tags. Don’t hesitate to ask your partners, project workers and the press to do the same. Refer to it on your website or at the bottom of your press releases, for exam- ple. Give plenty of thought to the launch of a project name. Does it capture the essence of the project? Does it arouse curiosity? Do the project partners like it? ■ Communication is a matter for everyone The project team on a citizen science initi- ative usually has one person in charge of communication, and this is often the pro- ject leader or project coordinator. This keeps messages about the project’s status, aims and results straightfor- ward and consistent. But communication is a matter for everyone, and so it concerns the project team as a whole. Every communication issued influences the image and perception of the project. But this is no reason for a com- munication blackout. Make clear arrange- ments up front and be sure that everyone in the team knows the lat- est developments and upcoming campaigns. This encourages team members to communi- cate the project via the medium of their choos- ing, which works well for the authenticity of your communication. Just be certain that the wrong message doesn’t go out and that your communication isn’t ambiguous. D e St an d aa rd /B on ka 43 ■ Follow current affairs Follow current affairs and try wherever possible to be topi- cal. When your area of activity comes into the news, quote the news item and refer to your pro- ject. This allows your own citi- zen scientists and ambassadors to appreciate the relevance of their research work. Citizen sci- ence projects can even reinforce and refer to each other on social media, websites, and so on. Re- member that your project and science in general is competing for attention with hundreds of news items every day. ■ Don’t forget cities and municipalities Go and speak to your city or municipality officials to find out about activities before they happen or to reach out through them to societies and associ- ations (if you aim to be inclu- sive, for example). They may tell you that your project coincides with a relevant city campaign or give you space to showcase your project in a temporary ex- hibition or pop-up shop. If the city uses a marketing agency it might be worth asking them if they would be prepared to help you with your campaign pro bono. But be aware of how time-consuming this would be and ask yourself if the results are worth the effort. ■ Exploit the power of the media to the full Decide in advance the two or three core messages you want to communicate and don’t stray from them. Not even in an interview. Always try to return to them . If your project has a baseline, state it consistently. Draw up a list of journalists before you start, give them reg- ular briefings in person about the current status of your citizen science project and follow the items they publish. Inform them about the project at the very start and ask if they would like to see your website or talk to the scientists. Thank them for their interest in your project and, if possi- ble, give them a little extra information from time to time. Don’t leave your local newspapers or TV stations off the list, as they are often the path by which news travels to the national media. The national press can use images broad- cast by the regional television stations. Try to find an original angle for your press briefings. Pro- vide visual material you have adapted yourself, or a clip that places you online (for example, a citizen scientist explaining what he has been doing). Find an appealing testimony, a sci- entist explaining her fascination for the subject, a twelve- year-old excited by the stars, a senior citizen with a lifetime of finding fossils, an animated teacher, etc. For some citizen science projects it may pay to identify a few local celebrities and to ask them if they would care to support or promote the project. They might be media ce- lebrities, or just as easily teachers or nature tour guides. Offer them the chance to act as a patron for your project. 44 ■ Go and blog (or vlog) In blogs and vlogs, you can employ the story- telling tactic and set yourself up as the cen- tral character in a sto- ry. Blogs are best writ- ten from a personal viewpoint: they reveal the person behind the science and what she is up to. They can work to strengthen the en- gagement of (poten- tial) citizen scientists, because they create a bond with the scientist and the research he is doing. Obviously, you don’t have to stop at blogging, and you can also vlog (create a vid- eo message) or release a podcast (an audio re- cording). If you aim to spread the message beyond your website, you can always enter your blog to Eos Wetenschap for publication. Contact the editors and show them your blog/vlog/ etc. If they like it you can start posting ar- ticles, photos, vide- os and other content on the website. You will have a chance of inclusion in the Eos newsletter, which gets mailed to fifty thou- sand subscribers. ■ Use hashtags For messages posted on social media choose simple, clear, relevant and specific hashtags. Keep your brand hashtag (your project’s hashtag) short, unique(!), sim- ple and easy to type, so that your followers can use your brand hashtag easily and as intended. Before you launch a hashtag check the social media to see what kinds of photos, videos and messages are usually shared under that hashtag. Use about four to six hashtags on average and do not keep hashtags when they are irrelevant. Of course, it is okay to use a popular hashtag now and then, such as #throwback #tgif #picoftheday #regram It may be a good idea to turn your social media accounts into a business account, because that will give you access to extra information and allow you to promote some of your messages. You will then be able to keep an eye on how your followers are trending, when they are online and which messages work well for your target audience. ■ Use tags It’s smart to tag institutions, societies and groups when you post on Facebook. Your partners of course, but consider your university, a nature society, a communi- ty group for clean air, and so on. These users will get a notification about your message, and, with a little luck, like or share it, lending your post a broader reach. 45 https://www.eoswetenschap.eu/eosblog ■ Keep communicating to the end Do not think your work as a communicator is done when you find enough volun- teers to take part in your citizen science project. You will need to keep ‘feeding’ your citizen scientists with information right through to the end of the project. Do this with a regular newsletter, messages on social media, lectures or workshops. People also expect to hear from you regularly after the project has ended, until you communicate the results. What are you researching now? Why are the results tak- ing so long? What will be done with the results? Finally, do not forget to thank your citizen scientists for taking part or to let them know how important they were to the study. ■ Get your project on a citizen science platform There are lots of online platforms for citizen science projects. They will often have a newsletter to which many people with a general interest in science subscribe and who gladly participate in projects on a regular basis. The Dutch-speaking platform for citizen science is Iedereen Wetenschapper (Everyone’s a Scientist). You can sub- mit your project to the website www.iedereenwetenschapper.be. They will reply by email. If your project satisfies the conditions for a citizen science project, they will send out a standard questionnaire and the editor at Everyone’s a Scientist will post your project to the platform. Your project is published on the platform freely, and it is mentioned in the monthly newsletter and on the Everyone’s a Scientist social net- working site. Zooniverse.org is an international platform, based on the annotation and transcrip- tion of datasets and counts more than a million interested citizen scientists world- wide. As well as calling for participants, Zooniverse runs the projects on its own servers. If you would like to run your project on Zooniverse, you will need to make that decision upfront and apply to the platform. SciStarter is an (English speaking) international platform which, like IedereenWetenschapper in Dutch, disseminates your project to a community of citizen scientists. SciStarter also allows citizen sci- entists to track and earn credit for their contributions to science projects. Along with these general platforms there are online platforms for specific topics. DoeDat (DoIt), for example, is the online crowd sourcing platform for Meise Botan- ical Gardens, on which citizens can digitise their herbaria. iNaturalist is an interna- tional app and online community through which people identify plants and animals. MijnTuinlab (MyGardenLab) is a Flemish platform which gathers citizen science pro- jects that can be run in your own garden. Vele Handen (Many Hands) hosts citizen science projects which engage citizen scientists in the transcription of historical, of- ten handwritten documents. 46 http://www.iedereenwetenschapper.be/ https://www.zooniverse.org/ https://scistarter.org/ https://www.doedat.be/ https://www.inaturalist.org/ http://www.mijntuinlab.be/ https://velehanden.nl/ Six steps to a communication plan You are now ready to prepare your communication plan. It contains all the steps you will want to take in your communication. As you run through the steps below, return to the answers you provided earlier in this guide. pag47 A communication plan is a detailed description of the communication steps by which you plan to en- gage your project’s target audiences. You list the steps in chronological order, link them to the rel- evant audiences, list the aims you hope to achieve and make sure you know your target audiences. You can also add an engagement strategy. This tells you more about what motivates or prevents citizen scientists from taking part in your project and suggests tactics and tools to secure their ini- tial and continued participation. Your engagement strategy will also allow an evaluation of your citi- zen scientists’ participation. You write your communication plan in the planning phase of your citizen science project and adjust it while the project is live. An important point here is to set a budget, as it will help you set priorities. Do you plan to spread your resources evenly over the life the project, or will you introduce peaks in your communication? Save the plan in an easy-access document, so that project partners and workers can see the latest changes. A communication plan is a tool for you and your team to use, not a sacred scroll. Use the following diagram to create your own communication plan and engagement strategy. COMMUNICATION PLAN ENGAGEMENT STRATEGY Choose tactics and tools Define your project aim Measure the expecta- tions and motivations to partici- pate for potential target audiences, and measure any knowledge or behaviours of relevance to your topic How will you keep your citizen scientists motivated? Choose your continued participation tactics Highlight the right motivations in your messages and go for an open style of com- munication Monitor activity. When do your citizen scientists drop out? What are the barriers? 48 Decide the level of engagement Evaluate Define your strategy Identify your target audience Your aim(s) Contribution to science Education Engagement of citizens as stakeholders Effective description of your aim(s): 1 The project is closest to a: Contributory project Collaborative project Co-created project Crowd sourcing project Distributed intelligence project Participatory science project Extreme citizen science project Explain why: 2 Define your project aim (see also page 9) Define the level of engagement (see also page 10) 49 What target audience(s) have you identified? Be as accurate as you can in your description, by means of these characteristics: Target audience I Target audience II Target audience III Demography (age, gender, profes- sion, socio-economic profile, etc.) Prior knowledge or skills Current behaviour What is your primary target audience? 3 Identify your target audience (see also page 13) What is your secondary target audience? Who are the intermediaries? (Consider organisations which are in contact with your primary target audience) 50 If so, how will you engage a minority group or groups? (See tips in chapter 1) What motivations might drive your target audience(s) to take part in your project? Target audience I Target audience II Target audience III Motivation I Motivation II Motivation III 4 What approach will you take in your communication plan? Generic approach Specific approach Generic and specific approach What is your total communication budget? Remember that some activities may be ‘free’ (e.g. giving a lecture). Define your strategy (see also page 17) Does your project need a diverse and inclusive target audience? (Consider your project aim) No Yes 51 What tactics and tools will you apply to secure initial or continued participation? 5 Tactics and tools (see also page 21) Generic approach initial participation social media gamification continued participation social media storytelling gamification ambassadors Specific approach initial participation hitching a ride on existing networks social media gamification continued participation social events social media storytelling gamification ambassadors Describe how you will apply them for each strategy you choose: for each strategy you choose set the budget you have to spare. Take potential adjustments of the plan into consideration. 52 What channels are most suitable for your target audiences? What exactly do you intend to organise (potential overlap between target audiences and channels)? Target audience I Target audience II Target audience III Social media Event Campaign Printed material Website Press Newsletter Other Write up a microschedule. With what frequency will you use the channels above? For each communication step, put someone in charge, set a deadline and assign a status. Agree on steps to take and create workflows. Now complete the following schedule: 53 Target audience Message (content) What? How? (channel, tactic) When? (phase, timing, frequency) Goal Who executes? Cost estimate Evaluate (see also page 19)6 Which quantitative and qualitative indicators do you want to check in your evaluation? When will you evaluate your project and what indicators will you use? Evaluation in planning phase Evaluation in live phase Evaluation afterwards 54 Selectievakje 20: Off Selectievakje 35: Off Keuzerondje 1: Off Tekstveld 14: Tekstveld 16: Tekstveld 12: Tekstveld 13: Keuzerondje2: Off Tekstveld 17: Tekstveld 19: Tekstveld 20: Tekstveld 18: Selectievakje 32: Off Selectievakje 34: Off Selectievakje 2: Off Selectievakje 5: Off Selectievakje 8: Off Selectievakje 3: Off Selectievakje 6: Off Selectievakje 9: Off Selectievakje 4: Off Selectievakje 7: Off Selectievakje 10: Off Selectievakje 11: Off Tekstveld 2: Tekstveld 3: Tekstveld 15: Tekstveld 195: Tekstveld 198: Tekstveld 193: Tekstveld 196: Tekstveld 199: Tekstveld 194: Tekstveld 197: Tekstveld 200: Tekstveld 4: Tekstveld 5: Tekstveld 6: Tekstveld 24: Tekstveld 203: Tekstveld 206: Tekstveld 201: Tekstveld 204: Tekstveld 207: Tekstveld 202: Tekstveld 205: Tekstveld 208: Tekstveld 7: Selectievakje 14: Off Selectievakje 15: Off Selectievakje 16: Off Tekstveld 8: Tekstveld 9: Tekstveld 33: Tekstveld 134: Tekstveld 142: Tekstveld 135: Tekstveld 143: Tekstveld 136: Tekstveld 144: Tekstveld 137: Tekstveld 145: Tekstveld 138: Tekstveld 146: Tekstveld 139: Tekstveld 147: Tekstveld 140: Tekstveld 148: Tekstveld 141: Tekstveld 149: Tekstveld 127: Tekstveld 128: Tekstveld 129: Tekstveld 130: Tekstveld 131: Tekstveld 132: Tekstveld 133: Selectievakje 26: Off Selectievakje 27: Off Selectievakje 29: Off Selectievakje 30: Off Selectievakje 31: Off Selectievakje 33: Off Selectievakje 17: Off Selectievakje 21: Off Selectievakje 18: Off Selectievakje 22: Off Selectievakje 24: Off Selectievakje 19: Off Selectievakje 23: Off Selectievakje 25: Off Tekstveld 153: Tekstveld 161: Tekstveld 169: Tekstveld 177: Tekstveld 185: Tekstveld 154: Tekstveld 162: Tekstveld 170: Tekstveld 178: Tekstveld 186: Tekstveld 155: Tekstveld 163: Tekstveld 171: Tekstveld 179: Tekstveld 187: Tekstveld 156: Tekstveld 164: Tekstveld 172: Tekstveld 180: Tekstveld 188: Tekstveld 157: Tekstveld 165: Tekstveld 173: Tekstveld 181: Tekstveld 189: Tekstveld 158: Tekstveld 166: Tekstveld 174: Tekstveld 182: Tekstveld 190: Tekstveld 159: Tekstveld 167: Tekstveld 175: Tekstveld 183: Tekstveld 191: Tekstveld 160: Tekstveld 168: Tekstveld 176: Tekstveld 184: Tekstveld 192: Tekstveld 11: Selectievakje 37: Off Selectievakje 38: Off Selectievakje 39: Off',\n",
              " 'Microsoft Word - Final UKEOF Report.docx Geoghegan et al. 2016. 7 important motivation, digitally mediated citizen science projects finding contributing to science most dominant, and environmental volunteering projects finding enhancement or personal gain values as most important. An earlier study (West et al. 2015) of motivations in data submission to environmental citizen science projects found that the most commonly held motivations of participants were wanting to help nature in general, followed by a desire to contribute to scientific understanding, followed by the purely intrinsic motivation, ‘it’s a valuable thing to do’. A desire to please others by participating and a category of ‘other’ motivations came next. The results of our online survey are broadly in agreement with West et al. Further definition of intrinsic values were elucidated through comments on enjoyment of the activity. Differences occurred in those who identified themselves as environmental volunteers in that learning, spending time outdoors and helping future careers had greater importance and contributing to scientific knowledge lesser importance. Motivations for beginning and continuing in citizen science (Chapter 5) The volunteering literature gives some insight into dispositional (personal motivations) and organisational (logistical) variables that influence initial participation with the addition of awareness that the opportunity exists. The environmental volunteering literature suggests that continued participation is motivated by the fulfilment of initial motivations to participate, while poor organisation frequently contributes to fall off in participation. The majority of participants apart from those involved in science-led projects, said that their motivations had not changed across time. Participants in science-led projects said that they were now more motivated by contributing to science, sharing knowledge and a stronger concern for conservation. Whilst the majority of respondents to the online survey who were encouraged to continue participation had had their initial motivation satisfied, there were respondents to the online survey who were still encouraged to continue participation in projects despite being dissatisfied, which suggests that other variables are also involved. Two other potential incentives for continued participation identified via the online survey were skills development, and feedback and communication. In sum, shared motivations and the importance of communication and feedback to worthwhile participation in environmental volunteering projects suggest that citizen science may appeal to many environmental volunteers. Motivations for stakeholders in citizen science (Chapter 6) This chapter reviews the motivations of stakeholders in citizen science, across science, policy and practice communities. The literature focuses mainly on scientists rather than other stakeholders, and mentions motivations such as to inform science, to inform policy, inform conservation and land management, improve buy in, awareness raising and engagement, building partnerships and improving communication. Not surprisingly, for scientists, the most common motivation is in advancing scientific knowledge, using citizen science approaches to collect data on a temporal and spatial scale that would not otherwise be possible. The stakeholder interviews found a much richer picture, with both institutional and personal motivations such as institutional promotion and publicity, education, a need for open data and managing public engagement in a constructive way. Most revealingly a new category of ‘personal satisfaction’ brought out very individual motivations around enjoyment and fulfilment in work and the satisfaction of personal commitments. Stakeholders were also personally motivated by providing benefit for others through enabling equity and self- determination for participants and generating impact for others’ lives. Geoghegan et al. 2016. 8 Matching participant and stakeholder motivations (Chapter 7) Few studies address whether matching participant and stakeholder expectations contributes to the success of citizen science projects, though one study has found that recognition of the interests of participants by stakeholders can increase participation. In some cases an only partial fulfilment of motivations may be enough to ensure that the contributory project satisfies both participants and stakeholders. Where participants have a more immediate interest in the implications and impact of the results of the project on themselves or their communities, namely a more co-designed approach, stakeholders will need to take greater account of ensuring that the data is useful to participants as well as themselves. Barriers and challenges for stakeholder participants (Chapter 8) Existing literature on stakeholder barriers and challenges is confined to scientific and land manager communities, and identifies barriers such as data quality and biases, peer review and mistrust of citizen generated data, the need for specialist equipment or knowledge, time and resourcing issues and lack of skills for working with the public and the potential for political ramifications. The interviews identified some new barriers in relation to data, including scalability and patchiness and specialist data needs beyond the scope of citizen science. Technology can also be perceived as a barrier as advancements are difficult to keep up with and have generated a crowded marketplace. At the same time, enabling participants to use technology can be difficult. While some stakeholders have found communication with participants to be very rewarding, this is also very time consuming. The stakeholder interviews also revealed that citizen science must be promoted at a high level within institutions to maintain profile and resourcing. The needs of participants were also acknowledged in that stakeholders recognised that more attention needs to be paid to the citizen science audience both to recruit appropriate volunteers and to ensure that surveys are designed in a way that works for the participants. Despite the number of barriers identified, stakeholders frequently commented that solutions had been found to many of the issues, for instance using statistical techniques to overcome issues with data. Showing the value of citizen science within institutions has also been effective, whether by raising the profile of their work by concentrating on key findings or by using citizen science work to contribute to academic metrics on impact. Technology in citizen science (Chapter 9) The stakeholder interviewees felt that the use of technology has transformed the potential of citizen science, making it possible to collect and then analyse large quantities of data, and importantly, to share that data. The expectation is now that technology will be integral to citizen science projects, but also that there will be some participants who will continue to use paper based recording. Considerable effort has gone in to long term projects to encourage a digital switchover. Stakeholders also voiced caution, that technologies need to be appropriate for participants to use and to deliver successful outcomes. Stakeholders working with disadvantaged communities were particularly aware that participants might not have access to the internet or a mobile phone data plan, or have time and the skills for technological solutions. Several of the stakeholder interviewees work on air quality projects, which make a particularly good example of some of the issues with technological solutions. Air quality sensors offer the potential to collect data from more areas than the official sensor network, but ‘low cost’ sensors still have cost and data reliability issues and some interpretation is needed to ensure that the data collected is relevant. Evaluation of citizen science (Chapter 10) Many of the stakeholder interviewees demonstrated awareness that evaluation was necessary and useful to the on-going improvement of the project and understanding of its impact. They also identified barriers around resourcing which make this difficult to achieve, specifically staff with appropriate skills and expertise, time for evaluation (not just among practitioners but also participants) and funding to carry out evaluation. As a result, the majority of monitoring is only of outputs, where the evaluation checks that participants are contributing Geoghegan et al. 2016. 9 the data that the project requires and perhaps that participants are happy with their participation. Deeper levels of evaluation, considering outcomes (such as learning and attitudinal change) and impact (such as behavioural change or difference in management or policy) is rarely undertaken, perhaps because in order to achieve this kind of evaluation, it needs to be integrated in the project design. Conclusions: proposed further work (Chapter 11) In addition to the findings relayed in the previous sections and recommendations, our research has led to a number of proposals for further work: (1) whilst evaluation and monitoring are recognised as important by stakeholders, further training is required to move from outputs- based evaluation to outcomes and impacts; (2) to fully understand what it means to participate in citizen science, a longitudinal study would reveal the significance of participation in citizen science to people’s everyday lives; (3) as citizen science takes an increasingly participatory turn, there needs to be a greater focus on participant motivations for collaborative and co- designed projects; (4) social and environmental challenges do not respect national borders, further work needs to understand how motivations differ in/between developing and developed nations. Useful resources and tables for stakeholders At the beginning of each chapter, there is a detailed summary of the key findings. The table below offers details of useful diagrams and lists to help stakeholders. Use Title Location To understand what encourages and discourages participation in citizen science (quick view) Summary of what encourages and discourages participation Table 32 To understand what motivates people to participate in citizen science Primary and top 5 motivations, other responses Tables 14-16 Current methods for communicating with citizen science participants Stakeholder communication and feedback mechanisms Table 31 To understand the motivations of science, policy and practice communities in citizen science Stakeholder motivations Table 33 Scenarios for identifying shared motivations between participants and stakeholders Recognition of motivations scenarios Figures 4 To understand the barriers and challenges surrounding science, policy and practice community involvement in citizen science Barriers and challenges to stakeholder participation in citizen science Table 37 To integrate evaluation into your project at all stages Stage-by-stage inclusion of evaluation Figure 7 Table 1 Useful resources for stakeholders in this report Geoghegan et al. 2016. 10 Chapter 1: Introduction: understanding motivations for citizen science Chapter highlights \\uf02a Without an understanding of why and how people participate in citizen science, some initiatives could miss their mark and fail to provide the expected benefits to science and society \\uf02a Social scientists agree that people’s motivations are broad, that individuals can hold multiple motivations and that these can change over time \\uf02a This project adopts a robust social scientific approach in order to provide evidence-based knowledge of the human dimensions of participation in citizen science Citizen science is broadly defined as the participation of non-professional volunteers in professional science projects (Dickinson et al. 2010). As a scientific method, it is widely acknowledged to have an important role in delivering valuable environmental data from local to national scales (Roy et al. 2012, Haklay 2015a, see also Table 33 on stakeholder motivations in this report). However, without an understanding of why and how people (non- professional volunteers) participate in citizen science, some initiatives could miss their mark and fail to provide the expected benefits to science and society (Roy et al. 2012). These social drivers of evidence-gathering by citizen scientists are often overlooked by stakeholders in favour of discussions around the need for and quality of the resulting data. This report, supported by the UKEOF, balances this with an approach based on social science in order to understand the personal needs, motivations, benefits and barriers which affect participation in environmental-based citizen science, both in terms of the citizen scientists themselves and other stakeholders, including a range of scientists (university, monitoring, policy, education, not using /data only), policy/evidence specialists and practitioners (science, engagement, education, community). Importantly, this study also incorporates the opinions of those volunteers who self-identify as environmental volunteers, in order to understand the motivations of those already engaged in the environment in some way, and how they might be persuaded to participate in citizen science, if they are not already involved. The report is structured around: 1. an extensive desk-based study of literature surrounding citizen science and environmental volunteering; 2. an online survey of environmental-based citizen scientists and environmental volunteers; 3. interviews with citizen science stakeholders, including scientists (university, monitoring, policy, education, not using/data only), policy/evidence specialists and practitioners (science, engagement, education, community); 4. information regarding project evaluation; and 5. conclusions and key findings regarding participant and stakeholder motivations and suggestions for further investigation. In the remainder of this section, we outline the rationale behind the study, the importance of a social science approach, and the research questions that framed this study. 1.1 Rationale and who is this study for Citizen science can deliver robust data to meet the needs of stakeholders from the local to the national scale (POSTnote 2014). However, in order to maximise the amount and quality of data that is returned, the human aspects of citizen science, which so far have been relatively Geoghegan et al. 2016. 11 understudied, need examining. The research presented here is therefore timely and original, providing a wide range of stakeholders in the UK and beyond, including scientists, informed citizens, data users, practitioners such as Government agencies carrying out environmental monitoring, policymakers, social scientists, educators and the media, with insights into: why people participate in citizen science; what benefits they gain from it; and how motivations can be evaluated as part of citizen science projects. This study consolidates and improves current understandings of motivation and participation in citizen science in the UK, so that new and existing initiatives can be (re-)designed to take these factors into account, making them more likely to succeed, and easier to evaluate. 1.2 Social science approach Social science is the study of the relationships between people and their connections with the social and natural worlds that surround them (ESRC 2016). Social scientists agree that people’s motivations are broad, that individuals can hold multiple motivations and that these can change over time (for an introduction to social science, see: della Porta and Keating 2008). Yet, these motivations need to be acknowledged and met by citizen science projects in order to maximise the return rates of participants (Roy et al. 2012). Citizen science is an ever-expanding field with projects varying in their levels of sophistication and demands on participants. Understanding how and why citizen scientists, environmental volunteers and stakeholders (including scientists (university, monitoring, policy, education, not using/data only), policy/evidence specialists and practitioners (science, engagement, education, community)) are motivated to participate in citizen science projects demands a social science approach. To date, projects on the social dimensions of citizen science have tended to be an addition to natural science projects, or rather narrow in focus. This project adopts a robust social scientific approach in order to provide evidence-based knowledge of the human dimensions of participation in citizen science. 1.3 Objectives and research questions The project objectives were three-fold: 1. To conduct a desk-based review of existing literature on motivations and evaluation for citizen science; 2. To design, implement and analyse (a) an online survey of existing and potential citizen scientists and (b) interviews around the motivations of stakeholders in scientific, policy and practice communities (namely, scientists (university, monitoring, policy, education, not using/data only), policy/evidence specialists and practitioners (science, engagement, education, community)); 3. To deliver a final report and online materials to share the findings of the project, and make recommendations for future studies, especially to explore methods of citizen science project evaluation, as well as reinforce the vital role of the social sciences to citizen science, environmental monitoring and surveillance. The research presented in this report seeks to answer 8 important questions identified by the authors and UKEOF partner organisations. Table 2 overleaf outlines the questions and the places where they have been discussed in this report. Two further chapters emerged as a result of this study. Chapter 3 on definitions of citizen science and Chapter 9 on technology and citizen science. 1.4 Outline of report In the remainder of the report, Chapter 2 gives an account of our methodological approach and our analysis of some basic data on our online survey respondents. Chapter 3 examines the Geoghegan et al. 2016. 12 varying ways in which citizen science has been defined in this study. Chapters 4-10 combine material from the desk-based review, online survey and stakeholder interviews to answer our research questions, as well as a discussion on the role of technology in citizen science. Chapter 11 is our conclusion and recommendations for future work. There is an extensive bibliography at the end of this report. Research question Review Surveys Interviews Chapter What are citizen scientists and environmental volunteers motivations? 4 What would encourage citizen scientists and environmental volunteers to participate for the first time or again? 5 How do citizen scientist and environmental volunteer motivations vary over time? 4 & 5 How do citizen scientists and environmental volunteers benefit from participation? 5 What are the key motivations for stakeholders in citizen science projects? 6 What are the barriers for stakeholders in citizen science projects? 8 Do participant motivations match stakeholder motivations? 7 How are citizen science projects evaluated by stakeholders? 10 Table 2 Research questions by data source and chapter Geoghegan et al. 2016. 13 Chapter 2: Researching motivations Chapter highlights \\uf02a Focussed review on environmental-based citizen science literature and work on environmental volunteering \\uf02a 147 citizen science respondents to our online survey, with few aligning with our priority areas of pollination, air quality, weather & climate change, and tree health. The majority undertook traditional recording activities for over 3 years. 47 environmental volunteering respondents to our online survey \\uf02a In comparison to West et al.’s (2015) study revealing that citizen science participation is biased towards white, middle aged men with high incomes, our survey respondents were more likely to be male, in the age categories of 25-34 and 55-64, and a smaller proportion from BME groups. The majority of respondents had participated in citizen science/environmental volunteering for more than three years, and were regular contributors to projects (i.e. more than once per month) \\uf02a 18 stakeholder interviews were conducted across science, policy and practice communities To realise the potential of social science to understanding the human dimensions of citizen science, we adopted a mixed methods approach combining quantitative and qualitative methods. The research involved three distinct research phases, which are discussed below in turn. 2.1 Phase 1: desk-based study We conducted a desk-based review of existing academic literature on motivations for citizen science in order to fill the gaps in our knowledge. Our approach has been to search for existing literature around each of the research questions. It is important to note that we have concentrated primarily on motivation in the environmental-based citizen science literature with the exception of where papers from other disciplines fill large gaps in understanding. We have reviewed the current academic literature relating to: \\uf0b7 the motivations of citizen science participants and stakeholders in science, policy and practice communities; \\uf0b7 the barriers to using citizen science as an approach; and \\uf0b7 the benefits to participants of citizen science. Papers were found using a Web of Knowledge search for the terms “citizen science” AND scientist OR policy OR motivation/s. We also present some data from a recent study by West et al. (2015) which examined the motivations of a large number of citizen science participants. This review enabled us to identify some key findings and data gaps that could be further addressed by our online survey and stakeholder interviews. Rather than offer our review as a stand-alone chapter requiring regular cross-referral to other sections, it forms part of each of the following chapters relating to our research questions. 2.2 Phase 2: online survey For Phases 2 and 3 of this research, we focussed on 4 priority areas (Table 3) where citizen science is widely used and which are current policy priorities. We adopted this approach due Geoghegan et al. 2016. 14 to the broad nature of the citizen science field and we wanted to concentrate on environmental-based citizen science. Priority area Participation-type Policy priority Pollination Number of current contributory citizen science projects involving unskilled volunteers but also has a history of highly skilled recorders of particular groups of pollinators Outlined in Defra’s (2014) The National Pollinator Strategy: for bees and other pollinators in England: “Develop and test a new systematic and sustainable monitoring framework for pollinators to be implemented by professionals and by using a ‘citizen science’ approach involving volunteers logging observations and gathering other evidence.” Air quality Often at a local scale and sitting across research and activism agendas, involving collaborative and co-designed citizen science projects Since 2013 SEPA have been interested in monitoring local air quality through citizen science: http://www.sepaview.com/2015/04/learning- about-air-quality/ Weather & climate change Some of the longest running citizen science projects, such as those recording phenological events JNCC have a long-term relationship with amateur naturalists, recorders and citizen scientists to gather evidence via monitoring. 3-4 million records are submitted annually: http://jncc.defra.gov.uk/page-5549 Tree health Volunteers across a range of skill levels and geographical scales Outlined in Defra’s (2014) Tree Health Management Plan: “We are also using citizen science to contribute in a number of key ways – increasing public awareness of the risks posed by tree diseases; supporting existing networks of individuals with an interest in plant health, and; enhancing public capability and capacity to identify outbreaks of pests and undertake surveillance activities.” Table 3 Justification of priority areas by participation type and policy prioritisation We conducted an online survey of citizen scientists (regardless of skill level) and environmental volunteers in order to access current non-participants in citizen science. Whilst this quantitative method offers a largely static view, with little evidence of citizen science and environmental volunteering in everyday life, it enabled us to ask specific questions of our target groups. Futhermore, an online survey allowed us to gain access to the greatest number of participants across a wide reach of the country and within the permitted budget (Van Selm & Jankowski 2006). This strategy was deemed optimal because there was no direct connection with or contact details for the target audience in the first instance (this related to data protection issues associated with each individual project). Social desirability bias issues were considered (Kreuter, Presser, & Tourangeau, 2008), but it was decided that given the anonymity of the survey, such potential considerations would not be relevant to data gathered. Noting Galesic and Bosnjak’s (2009) observations regarding survey length and participation dropout and quality, the survey was streamlined as much as was possible to keep the survey to a 10-minute anticipated maximum. Sampling We adopted a non-probability sample, i.e. non-random, which is therefore not representative of citizen scientists and environmental volunteers on the whole. We chose to target Geoghegan et al. 2016. 15 participants in 4 key citizen science areas: pollination; air quality; weather & climate change; and tree health. A preliminary list of contact details for citizen science organisations working across the four areas of interest was drawn up. Each project was emailed and agreement was sought that they would send the survey invite out to their participants. An invitation to forward on the survey link more widely ensured some snowball sampling, further widening coverage. Qualtrics software was used to create and administer the survey. The system is aesthetically appealing and allows for ease-of-use on either a computer or a mobile device. Survey questions A full version of the survey can be founded in Appendix 1. The survey began with two initial screening questions to ask firstly for consent to use respondents’ submitted data, and then whether they had taken part in a citizen science project, an environmental volunteering project or neither. This then directed respondents to the appropriate set of following questions and ensured the data could be easily processed and analysed. For citizen scientists, the next set of questions asked about the type of project they had undertaken, frequency and duration of participation, types of activities (data collection, data submission, organising, etc.) and whether activities had been carried out alone or in a group. The third section asked about initial motivations and their satisfaction, skills development, and whether or not these had motivated further involvement. Respondents were then asked about data submission, the means available, feedback, satisfaction with the process and whether this had affected motivations. For those who had not submitted their data, reasons for this and effects upon motivations were asked about. Overall historic and current enthusiasm (for specific projects and contributing to science more generally) were then raised, before moving on to a broad range of high-level demographics to facilitate breaking down the data (age, gender, region, education, income and ethnicity). The volunteering stream worked similarly, covering motivations, skills development, the importance of feedback and enthusiasm. Both volunteers and non-participants were then asked whether they were aware of citizen science projects, whether they were interested and any barriers that they felt prevented them from becoming involved. Respondents We received 147 valid citizen science entries, and 47 for environmental volunteering. We received over 100 further survey attempts, however these were non-valid entries, or the respondent had submitted only demographics and no further information. We’ve identified several reasons for a low response rate in our four target areas: (1) There was a proliferation of similar research work around volunteer and biological recorder motivations at the same moment, including the NBN Biological Recording Survey1 and Bournemouth University Volunteer Motivations Survey.2 This could have produced ‘survey fatigue’ meaning that potential respondents did not want to complete another survey on the topic; (2) Some of the projects we targeted had recently circulated their own feedback forms and were concerned about bombarding their participants with emails (this is a challenge identified by the stakeholders during the interviews in Chapter 10 on evaluation); and (3) There was considerable overlap between projects and the focus on particular species, rather than issues such as pollination. This meant that some participants may have chosen not to attribute their project to our categories but instead select ‘other’. Analysis: quantitative and qualitative answers 1 http://nbn.org.uk/news/nbn-recorder-motivation-research-summary-findings/ 2 PhD project at Bournemouth University looking into how to enhance human well-being and project outcomes through volunteer engagement. Contact: gitte.kragh@bournemouth.ac.uk Geoghegan et al. 2016. 16 We have conducted both (1) univariate analysis of the responses, namely frequency tables; and (2) bivariate analysis, namely contingency tables. However, our non-probability sampling approach, low response rate, and some instances of ‘outliers’ have meant that we have not been able to conduct any multivariate analysis or tests for statistical significance. Instead, we have been able to offer some interesting descriptive statistics throughout the report. As indicated earlier, we asked our respondents some open-ended questions, for example what makes their participation worthwhile and how do they feel their motivations have changed over time. Their answers have proved invaluable for identifying in their own words how citizen scientists and environmental volunteers value feedback and communication for example (answers are reported in full in Appendix 3). 2.2.1 Basic data on respondents by project and involvement In this section, we offer some basic data on our respondents. As indicated earlier we targeted 4 citizen science priority areas: pollination; air quality; weather & climate change; and tree health. We received the most responses in the category of ‘other’. We re-coded this field as ‘biodiversity’. It was clear from the project names listed and their objectives that these were largely traditional recording projects. We attempted to break these down again by subject area, using species or activity relevant codes (see Table 4) below. Subject area Number of respondents Pollination 5 Air quality 1 Climate Change & Weather 4 Tree Health 5 Biodiversity 114 Soils 6 Other 12 Total 147 Table 4 Number of respondents by subject area However, respondents remained clustered around biodiversity. We decided to re-code our citizen science respondents again (Table 5), this time based upon the purpose of the project they contributed to and we identified the following 4 categories: Citizen science type Number of respondents (% of total respondents) Recording (traditional biological recording/monitoring) 108 (73%) Science-led (hypothesis-driven research by university scientists) 17 (12%) Surveillance (early-warning and detection) 7 (5%) Other (e.g. citizen panels on environmental decision- making) 15 (10%) Total 147 Table 5 Number of respondents by citizen science type Once again citizen science respondents remained clustered, this time around ‘recording’. Initial analysis also revealed that the majority of our participants (citizen science and environmental volunteering) had participated for more 3 years (Table 6), more than once per Geoghegan et al. 2016. 17 month and at least more than once per year (Table 7) and that over half were involved in more than one citizen science or environmental volunteering project (Table 8). We can conclude that our survey attracted the most active participants as is often the case when respondents are required to self-select. Number of respondents (% of those who responded) Length of participation Recording Science-led Surveillance Other Environmental volunteering <6 months 13 (12%) 5 (33%) 1 (14%) 3 (20%) 1 (2%) 1-2 years 0 0 0 0 3 (7%) 3-5 years 28 (26%) 5 (33%) 1 (14%) 6 (40%) 7 (16%) >5 years 62 (59%) 5 (33%) 5 (71%) 6 (40%) 34 (75%) No reply 3 2 0 0 2 Table 6 Number of respondents and length of participation in citizen science or environmental volunteering Number of respondents (% of those who responded) Type Recording Science-led Surveillance Other >1 per month 58 (54%) 8 (50%) 3 (43%) 11 (73%) >1 per year 42 (39%) 6 (38%) 2 (29%) 3 (20%) Less often 0 2 (12%) 0 0 Other (seasonal) 8 (7%) 0 2 (29%) 1 (7%) No reply 0 1 0 0 Table 7 Number of participants and frequency of participation in citizen science Number of respondents (% of those who responded) Recording Science-led Surveillance Other Environmental volunteering Yes 68 (65%) 6 (35%) 4 (57%) 9 (60%) 32 (73%) No 37 (35%) 11 (65%) 3 (43%) 6 (40%) 12 (27%) No reply 3 0 0 0 3 Table 8 Number of participants and participation in more than one citizen science project Whilst it has been useful to report on any possible differences in citizen science involvement by citizen type, we have decided to largely combine all citizen science respondents in the following tables and charts rather than separate them as the sample sizes are small. Where appropriate the numbers for environmental volunteers accompany them. 2.2.2 Basic demographic data on respondents Having adopted a non-purposive sampling strategy our demographic data cannot be viewed as representative of the citizen science population on the whole. However, in their study, West et al. (2015) asked a stratified sample of 8220 people in the UK if they had taken part in a citizen science project, 613 (7.5%) responded that they had. Demographic data collected as part of their questionnaire revealed that citizen science participation is biased towards white, middle aged men with high incomes. In what follows, we compare our online survey respondents to those who participated in West et al.’s survey which did involve a purposive sample. There are some differences between both the citizen science respondents and environmental volunteering respondents. The comparison data is set out in Table 9. While the purposive sample obtained by West et al. serves for comparison with the online survey, there is no data obtained from a purposive sample of environmental volunteers, therefore the Geoghegan et al. 2016. 18 comparison table (Table 9) serves only to illustrate the differences between the samples of citizen scientists and environmental volunteers in the online survey. West et al. study 20153 Our study % of total sample % of citizen science respondents % of citizen science respondents % of environmental volunteering respondents Gender Male 48 53 56 62 Female 52 47 44 38 Age 16-24 15 14 2 3 25-34 17 9 14 5 35-44 15 18 17 5 45-54 15 16 17 8 55-64 12 15 31 28 65+ 26 27 20 51 Ethnicity White 86.2 93 97 95 BME 13.8 7 1 0 Social grade Income AB 17 38 £75K+ 8 5 C1 26 31 £60-75K 3 10 C2 21 15 £45-60K 12 15 DE 35 16 £30-45k 20 26 £15-30k 29 10 <£15k 7 13 Table 9 Demographic comparison with West et al.’s study The primary differences in the citizen science sample are: \\uf0b7 Citizen science and environmental volunteering respondents to our survey were both more likely to be male than in the purposive sample; \\uf0b7 There was a smaller proportion of respondents in the youngest age category, but a greater proportion in the 25-34 category and a significantly greater proportion in the 55-64 category; \\uf0b7 There was a significantly smaller proportion of participants from BME groups; \\uf0b7 While there is no directly comparable data on social grade from the online survey, if social grade is taken as a proxy for income, the income data from the online survey suggests that a greater proportion of respondents were on lower incomes than in the purposive sample. Some of these difference may be explained by the fact that the online survey is less likely to have captured those who were encouraged to carry out citizen science activities by someone 3 Demographics of survey respondents. Table 9 shows the percentage of all survey respondents in each demographic category and the percentage of each group that had taken part in the citizen science project. BME = black and minority ethnic groups. Social Grades are AB: higher & intermediate managerial, administrative, professional occupations; C1: supervisory, clerical & junior managerial, administrative, professional occupations; C2: skilled manual occupations; DE: semi-skilled & unskilled manual occupations, unemployed and lowest grade occupations (Market Research Society 2015). No Social Grade information was available for participants in the Survey Monkey survey. Geoghegan et al. 2016. 19 else, and so may have missed younger respondents. The online survey is also more likely to have captured data from keen and engaged citizen scientists rather than those who have participated to a lesser extent (as identified in section 2.2.1). The primary differences in the sample of environmental volunteers were: \\uf0b7 Significantly more likely to be male than the citizen science sample \\uf0b7 The age distribution of environmental volunteers was very much skewed towards older age groups \\uf0b7 Environmental volunteers were both more likely to be on high incomes (60-75K) and low incomes (under 15K). It is important to note, that our desk-based study revealed that similar patterns have been observed in other studies of citizen science participants. Trumbull et al. (2000), for example, found that participants in a citizen science project run by the Cornell Laboratory of Ornithology were older and more highly qualified that the general population. Crall et al. (2013) examined participants in an invasive species citizen science project and found that participation was biased towards highly educated, middle-aged females from higher income households. Johnson et al. (2014) studied participants of wildlife conservation citizen science projects in India and found 91% of respondents had completed at least 12 years of education (the average in India is 5.7 years); and that participants had higher than average income. Wright et al. (2015) examined contributors to the Southern African Bird Atlas Project and found that there was a bias towards white males who are highly educated, high earners over the age of 60. Within wildlife recording schemes in the UK, unemployed and low-income people are under-represented (Hobbs and White 2012). Raddick et al. (2013) found a particularly strong bias towards men participating in the online citizen science project Galaxy Zoo, with over 80% of participants being male. Furthermore, similar patterns are seen more broadly in environmental volunteering (Ockenden 2007, West & Pateman in press) and other information science education programmes (Nicholson et al. 1994, Overdevest et al. 2004, Brossard et al. 2005, National Science Board 2008). It is important to note that under-representation and exclusion, although related, are not the same thing (OPENspace 2003). Groups may be under-represented but not actually excluded from taking part, i.e. they may not want to be involved. Some reasons why certain groups are underrepresented have been identified, however. For example, some people from black and minority ethnic groups do not feel that they belong in the countryside (OPENspace 2003) and there is a common perception that such habitats are landscapes inhabited by white people (Agyeman and Spooner 1997). Possible barriers amongst black and minority ethnic groups are that free time tends to be devoted to ‘intra-community’ activities, or educational activities. In addition, access to the countryside can be difficult and costly (Agyeman and Spooner 1997). 2.3 Phase 3: Telephone semi-structured interviews with stakeholders The team conducted 18 stakeholder interviews (including scientists (university, monitoring, policy, education, not using/data only), policy/evidence specialists and practitioners (science, engagement, education, community)) in order to facilitate more detailed discussions of motivations for involvement (or lack of) in citizen science and the evaluation methods currently used to investigate participant motivations. Interviewees were selected using a matrix of criteria including each of the four topic areas mentioned above and a range of stakeholder types from science, policy and practice communities – with one interview with each type of stakeholder in each of the four focal areas (see Table 10 outlining the stakeholder types interviewed in our study). Please note that many of our stakeholder respondents also self-identified as biological recorders, environmental volunteers and/or citizen scientists. This is important as stakeholders are also citizen scientists and/or environmental volunteers in their Geoghegan et al. 2016. 20 spare time. We also conducted interviews with scientists who are active in one of our four topic areas but are not involved in running citizen science projects. Instead they may be using citizen science data. In order to maintain the anonymity of our respondents we have not included any identifying materials in this report. This approach allowed us to conduct interviews across a breadth of different types of citizen science project allowing for the identification of commonalities and discrepancies between both project focal areas and stakeholders as part of the analysis. Interviews were preceded by a request for information from interviewees, allowing them time to reflect and also to gather relevant data. Interview plan In order to be consistent during our interviews, we developed an interview sheet (see Appendix 2) that was used in all interviews. Questions focussed on: the interviewee’s career, projects and how citizen science is defined within their organisation; institutional/personal and participant motivations and how they’ve evolved over time and how citizen motivations are accommodated within their projects; the benefits, challenges and barriers to citizen science institutionally, personally and for their participants and how they overcame them; and the role of communication, feedback and technology in their projects, and the place of evaluation. Analysis The telephone interviews were transcribed and analysed using an interpretation framework (Ritchie & Lewis 2013) based on the interview topic guide, drawing out interviewees references to practices and opinions from institutional, personal and participants’ perspectives, together with motivations and barriers and challenges identified in the literature. As the interviews were coded, additional motivations, barriers and challenges were identified and also coded, allowing themes across the interviews to be identified and drawn out. 2.4 Ethics The project was submitted to and approved by the University of the West of England Faculty of Environment and Technology Research Ethics Committee (FET-REC). This approval was then passed to the Universities of Reading and York to gain approval at those institutions, ensuring ethical clearance across all involved parties. Beyond this, the team worked to ensure good micro-ethics of research in practice (Guillemin and Gillam 2004), especially with the interviews, ensuring transparency with participants and sensitivity to their needs or what they felt they could or could not say, going beyond the ‘procedural ethics’ of University committees to ensure respect was shown to those who had kindly volunteered their time (Israel and Hay 2006). All interviewees were requested to complete consent forms and gave their permission for us to record, transcribe and analyse their responses. Geoghegan et al. 2016. 21 Stakeholder group Participant category4 Description Science Scientist/university Scientists funded by university and/or research council funding, often interested in applied research, running citizen science initiatives, using citizen data, sometimes advocating for public engagement and advising on policy Scientist/monitoring Scientists employed by government agencies and working with policy stakeholders, delivering improvements through policy mechanisms, sometimes commissioning citizen science, sitting on advisory boards, sometimes using citizen generated data but no first-hand experience of citizen science Scientist/policy Scientist/education Scientists working in science education (e.g. with young people), only part of job, also involved in science/policy Scientist/not using /data only Scientists working in university or government agency but not currently hands on in citizen science but may be using citizen generated data Policy Policy/evidence Policy/evidence specialists working for government department or agency. Key users of citizen science outputs, thinking about how citizen science projects can support policy development work and support wider initiatives. Sometimes funding Practice Practitioner/science/ engagement Practitioners involved in recruiting citizen scientists for charitable organisations and agencies with science remit. Role ranges from promoting citizen science, public engagement, communicating with participants and institution, using data and initial analysis, project management, sharing best practice, recruiting and managing volunteers Practitioner/community Practitioners working with and in a variety of different communities utilising citizen science approaches to understand what’s going on in different locations, and seeing if this process can enable people to push for change at the local level Practitioner/education Practitioners working in the education sector with remit for citizen science, e.g. university citizen science project manager Table 10 Description of stakeholder groups and participant categories 4 Used to identify verbatim responses throughout report Geoghegan et al. 2016. 22 Chapter 3: Defining citizen science Chapter highlights \\uf02a Citizen is subject to multiple definitions \\uf02a The most widely accepted definition by stakeholders involved the collection of data by citizens for use by scientists with an acknowledgement that citizens must also benefit \\uf02a Stakeholder use of term citizen science appears to be primarily inward facing, with the term rarely used in communications directed towards participants \\uf02a Most projects are contributory, but collaborative and co-designed projects are gaining followers \\uf02a Environmental-based citizen science activities in the UK must be considered in relation to the long-term tradition of biological recording \\uf02a Stakeholders must consider how they are using the term citizen science in relation to their project to avoid confusion and mixed messages As outlined in Chapter 1, we have defined citizen science in this study as the participation of non-professional volunteers in professional science projects. However, as our survey results and interviews with stakeholders revealed, citizen science is subject to multiple definitions. There is no one size fits all (UKEOF 2011; Haklay 2015a). In this chapter, we highlight some of the challenges surrounding the definition of citizen science revealed by this study and some points for consideration by readers who are contemplating citizen science for the first time or who are already using the term. We begin by considering the different ways in which the term citizen science has been used by stakeholders. 3.1 Stakeholder definitions of citizen science “So [citizen science] has to be something that excites and interests members of the public and really gets them thinking about environmental science and learning about it … It also strikes up a conversation between researchers and members of the public so that they’re talking about what they’re doing and why it’s valuable and members of the public are able to input into that process. ... science can benefit from having non-expert input as well.” (Practitioner, science, engagement). This quote from a stakeholder indicates the challenges surrounding the definition of citizen science. For them, it has to interest and excite participants, engage them in science and learning, involve dialogue between professionals and the public, and allow citizens to input into science. This is just one definition. We asked all of our stakeholder interviewees: “How do you define citizen science?” Interviewees suggested this was a difficult question and a subject that they could spend ages talking about. Table 11 (overleaf) indicates the common ways in which citizen science was defined by our stakeholder respondents. The two most widely accepted definitions by stakeholders were ‘people have to get something out of it’ and the collection of data by citizens for use by scientists. Whilst the latter echoes our existing definition, the former indicates the increasing value placed upon supporting participants. Geoghegan et al. 2016. 23 Definition of citizen science S ci en ti st /u n iv er si ty S ci en ti st /m o n it o ri n g S ci en ti st /p o li cy S ci en ti st /e d u ca ti o n S ci en ti st /n o t u si n g /d at a o n ly P o li cy /e v id en ce P ra ct it io n er /s ci en ce /e n g ag em en t P ra ct it io n er /c o m m u n it y P ra ct it io n er /e d u ca ti o n Collection of data by citizens for use by scientists (often in partnership, sometimes working with community scientists) Widening participation beyond the usual suspects (e.g. inclusion of young people) People have to get something out of participation (e.g. learning, interaction with data, empowerment) Collection of data by citizens for analysis and use by citizens to inform individual and collective activities Other attributes People who love their subject People with skills from basic identification to expert taxonomic skill (e.g. amateur naturalists) People undertaking a range of activities from running schemes, quality control checking to BioBlitz activities and school projects People mobilised and engaged around particular issues looking, monitoring and feeding back Involving non-specialist volunteers in science who are giving their time free of charge Citizen science has to have some measurable scientific output Citizen science has to be science, not purely engagement on an issue Citizen science and traditional science are not separate Citizen science is a subset of public engagement Table 11 Citizen science definitions by stakeholder type The inclusion of a wider group of stakeholders from policy and practice communities shows how in the table above a broader definition of citizen science is emerging around the following principles: 1. Widening participation in science; 2. Recognising benefits of participation to citizen; 3. Leading to measurable academic output and/or being used by citizens; Geoghegan et al. 2016. 24 4. Harnessing emotional attachments to particular subjects; 5. Carrying out activities across a range of skills levels; 6. Sharing data between experts (paid and voluntary); 7. Prioritising science over engagement; and 8. Talking about ‘science’ only without separating citizen science and traditional science. It is also important to consider the type of project being discussed here. The majority of projects and stakeholders being considered within our study form part of what has been described by Bonney et al. (2009, 11) as “(1) Contributory projects, which are generally designed by scientists for which members of the public primarily contribute data”. However, the principles above suggest an increased interest in citizen science beyond a simple data collection exercise, with thinking (but not yet action) moving projects towards more collaborative and co-designed frameworks: “(2) Collaborative projects, which are generally designed by scientists and for which members of the public contribute data but also may help to refine project design, analyse data, or disseminate findings” and “(3) Co-created projects, which are designed by scientists and members of the public working together and for which at least some of the public participants are actively involved in most or all of the scientific process” (Bonney et al. 2009, 11). Definitional issues were also evident when we consider the types of projects/organisations that the stakeholders that we interviewed represented: Stakeholder Institutional Experience Main projects Origins Key activities *Practitioner /Science/ Engagement *Scientist/ Monitoring *Scientist/ Policy 50+ years Long-term surveillance Established institutional commitment to delivering evidence Partnership working to deliver long-term monitoring through citizen science; support and advice; facilitation *Practitioner /Science/ Engagement 20+ years Weekly recording Recorders had data; institution wanted to communicate; scientists needed people management Recruiting and maintaining long- term volunteer recording projects *Policy/ Evidence 10 years Monitoring and surveillance Foot and Mouth disease changed evidence need Advisory boards, using citizen data *Practitioner /Community <10 years Community- led projects Inclusion of public in planning decision- making Participation of local people, create dialogues, engage in discourses around issue Table 12 Stakeholder involvement in citizen science Here long-term engagement with people through environmental monitoring programmes, follows a largely contributory model of citizen science. However, more recent additions to the sector, in particular the policy/evidence and practitioner/community respondents are using the language of citizen science to enrol the public in dialogue and feeding back on issues (see Chapter 6 for more information on stakeholder motivations in these areas). Geoghegan et al. 2016. 25 As highlighted by Table 12, it is possible to discern that the term citizen science is not widely used in the environmental-based citizen science sector to label projects. In the UK, the definition of environmental citizen science is compounded yet further by the long and successful history of amateur naturalism and biological recording (Silvertown 2009, Sparks & Carey 1995, Pocock et al. 2015). As Roy et al. (2012, 8) suggested in an earlier UKEOF report: “Volunteer participation with environmental science and natural history has a long history, especially in Britain, long before it was termed ‘citizen science’”. This is significant because whilst citizen science may be familiar terminology to some stakeholders this is by no means universal, nor is it used universally. Words such as volunteers, amateurs, amateur naturalists, natural historians and biological recorders have all been used to indicate similar recording and monitoring activities by our stakeholder interviewees. However, as Haklay (2015a, 6) notes, in 2014, the term citizen science was added to the Oxford English Dictionary as “scientific work undertaken by members of the general public, often in collaboration with or under the direction of professional scientists and scientific institutions”. This marks a significant milestone in the acceptance and current definition of citizen science. Nonetheless challenges remain around the use of the term citizen science, as one respondent explained: “We don’t necessarily make a distinction between different levels of citizen science, I still think that within the organisation there’s a lot of belief that all citizens are equal and I’ve spent a lot of time trying to say, well no, it’s like science, not all scientists are the same, and not all citizens are the same” (Scientist, monitoring, policy). This was echoed by another practitioner stakeholder who said: “I think some people see citizen science as people are robots, they are just blindly collecting data, blindly taking a photo or blindly measuring some air quality or something” (Practitioner, science, engagement). There remain issues around the ‘type’ of citizen scientist involved in projects and their treatment by project teams, for example as mere sensors collecting data or as highly trained specialists who are unpaid. Furthermore, from our interviews, several stakeholders mentioned their involvement in national, European and global citizen science working groups (such as the UKEOF Citizen Science Working Group or the Tree Health Citizen Science Network, European Citizen Science Association and the Citizen Science Association) and/or their attendance at citizen science conferences and workshops (for example events hosted by the British Ecological Society Citizen Science Special Interest Group or the European Citizen Science Association) to share and develop good practice surrounding citizen science. This indicates the involvement of a broad range of stakeholders in the emergence of citizen science as a field, not just scientists, but also policy/evidence specialists and practitioners, as well as the national, regional and global networks connecting stakeholders from science, policy and practice communities. Indeed, one participant (scientists, monitoring, policy) used a definition of citizen science from the academic literature to define what it means to them, showing the engagement stakeholders are having with academic literature. 3.2 Defining citizen science projects As identified in Chapter 2, the majority of our respondents defined themselves as undertaking citizen science relating to biodiversity and more traditional forms of biological recording projects. It is useful to consider further the activities involved in the citizen science projects discussed in our online survey. The majority of citizen science participants were undertaking data collection (90%), recording species (88%) and data submission (85%), with 51% contributing to supporting others’ involvement (see Table 13). Less than half were involved in leading and organising activities, and a much smaller proportion were involved in developing survey tools (18%), designing the overall research questions (12%) and moderating forums (8%). This suggests that the majority of respondents were involved in projects in a contributory way, rather than as part of a collaboration or co-designed form of citizen science. However, of those who responded ‘other’, activities included: software design for data collection; attending local/regional networking events; analysis of data; liaising with Geoghegan et al. 2016. 26 land management bodies; designing and mapping out new transect routes; editing regional atlas of butterflies; annual contribution to annual reports; and beta testing a biological recording app. Citizen science activity (respondents were able to select more than one activity) Number of citizen science respondents (% of those who responded) Data collection 131 (90%) Recording species 130 (88%) Data submission 125 (85%) Supporting others’ involvement 76 (51%) Leading activities 69 (47%) Organising activities 61 (41%) Developing the survey tools 27 (18%) Designing the overall research questions 17 (12%) Moderating forums 12 (8%) Other 4 (3%) Table 13 What citizen science activities do you undertake? The complexity surrounding the definition of citizen science was highlighted again by the ways in which our online survey respondents decided to classify their participation. All respondents were asked whether they had participated in environmental-based citizen science, 147 responded the survey as citizen science participants. Those that did not select this option were asked whether they undertook environmental volunteering activities, 47 responded the survey as environmental volunteers. Whilst these surveys asked different questions to reflect that distinction, upon closer inspection of the environmental volunteering projects listed by survey respondents, there is considerable overlap between those projects identified as citizen science by our citizen science respondents. This warrants further investigation, whilst we do not have the space to interrogate this further here, it is important to note that this suggests there is some confusion surrounding how both projects and their participants identify themselves. 3.3 Considerations for stakeholders when using the term ‘citizen science’ Stakeholders should consider the following questions when using the term ‘citizen science’: (1) What does the term citizen science mean to your project? (2) Is your project contributory, collaborative or co-designed citizen science? (3) What are the implications of each approach? (4) Will citizen science help attract participants or dissuade more traditional recording audiences? (5) Are you being consistent in your use of the term? (6) What role might amateur naturalists/biological recorders/exiting communities play in your project? See also Haklay 2015a for a report on Citizen Science and Policy. Geoghegan et al. 2016. 27 Chapter 4: Citizen scientist motivations Chapter highlights \\uf02a Stakeholders need to understand participant motivations in order to recruit participants, achieve sustained participation and enhance the quality of the outputs \\uf02a Little has been written about environmental-based citizen science motivations, but a lot has been written about motivations for environmental volunteering \\uf02a Motivations can be intrinsic (or inherently valuable or satisfying) or extrinsic (or leading to some other benefit, such as future career prospects) \\uf02a Participants vary individually and will not necessarily conform to type \\uf02a Top motivations for citizen science and environmental volunteering respondents to our survey: “To help wildlife in general” and “To contribute to scientific knowledge” \\uf02a Shared motivations suggest that citizen science projects may appeal to many environmental volunteers It is important to understand participant motivations, (1) for practitioners wishing to recruit people to their projects and (2) as being a key factor in the success of projects (in terms of both achieving sustained participation and enhancing the quality of the outputs of participants’ activities). In this section, we consider the existing academic literature relating to the question of participant’s intrinsic and extrinsic motivations, before detailing the results of our online survey of citizen scientists and environmental volunteers, specifically their motivations and how they vary over time. 4.1 Existing work on motivations for environmental volunteering Very little has been published in the academic literature regarding what motivates people to participate in citizen science projects. This is despite its importance for practitioners wishing to recruit people to their projects and hence it being a key factor in the success of projects (Wright et al. 2015, West & Pateman in press). In their review of the wider volunteering literature, however, West and Pateman (in press) outline the attempts that have been made to identify and categorise what motivates volunteers in general. The many parallels between volunteering and citizen science participation mean that lessons can be drawn from this wider body of literature (West & Pateman in press). Volunteer programmes that acknowledge the importance of volunteer motivations and incorporate them into their projects fare better in attracting and retaining volunteers (Grese et al. 2000). The same would be expected to be true of citizen science projects. Considering volunteering in general, Finkelstein (2009) identifies intrinsic and extrinsic motivations: intrinsic motivations describe the desire to volunteer because it is in some way inherently interesting or satisfying; extrinsic motivations, however, describe the willingness to volunteer because it leads to some other outcome, such as getting a new job. Clary and Snyder (1999) developed a functional approach to volunteering, originally introduced by Katz (1960), to describe motivations which lead to individuals beginning and continuing volunteering. This approach has identified six motivation categories: 1. Understanding: where people want to learn new things. 2. Values: where people have an altruistic concern for others. 3. Social: where people are motivated by the desire to meet new people and because volunteering is a socially desirable thing to do. Geoghegan et al. 2016. 28 4. Enhancement: where people wish to improve themselves personally through their volunteering. 5. Career: where people hope to gain experience that will benefit their future careers. 6. Protective: where people volunteer to reduce negative feelings or to address personal problems. All of these, apart from career motivation, would be classified as intrinsic motivations. Some studies have looked more specifically at motivations in environmental volunteering and have found all of Clary and Snyder’s motivation categories to be present, although in some cases the protective motivation was absent (Bruyere & Rappe 2007, Van Den Berg et al. 2009). Different types of values motivations have also been identified in this context; for example, an altruistic concern for the environment rather than people (Bruyere & Rappe 2007); and users of a site (e.g. for recreation) wanting to work in or improve that site (Bruyere & Rappe 2007, Measham & Barnett 2007, Jacobson et al. 2012). ‘Project organisation’ has also been identified, with people being motivated to be involved in a well- organised project (Bruyere & Rappe 2007, Jacobsen et al. 2012). Bell et al. (2008) found that some volunteers were motivated by wanting to share their knowledge with others. 4.2 Existing work on motivations in citizen science A small number of studies have looked at the motivations of participants in citizen science projects. Studies so far have largely concentrated on individual case studies. However, inconsistent methodological approaches mean that there is a lack of understanding of how motivations vary between different types of project. In the context of biological recording and monitoring, Hobbs and Wight (2012) questioned participants of a national and a local recording scheme and invited open responses to the question “What was the main reason(s) for you to get involved in the scheme”. They categorised responses as follows: interest in wildlife; saw the wildlife; as a response to a request; learning; to provide data for/help conservation; social reason/asked by a friend or a family member; was doing activity anyway/to give purpose to recording. Wright et al. (2015) studied the motivations of volunteers involved in creating a bird atlas of South America using a modified version of Clary and Snyder’s categorisation. They found the following motivations to be present (in order of importance): (1) recreation or nature-based, which they define as people wanting to spend time with nature and/or engage in the atlas work for recreational purposes; (2) personal values; (3) personal growth; (4) social interactions (5) project organisation. Some studies have also looked at motivations in computer-based citizen science projects. In their study of volunteers involved in the astronomy citizen science project, Galaxy Zoo, Raddick et al. (2013; see also Nov et al. 2011 and Jennett et al. 2016) found additional motivations to those that have already been identified, including: contributing to science; the opportunity for discovery and sense of wonder; to use the resources for teaching; and fun and enjoyment. Rotman et al. (2012) looked at the motivations of participants in a variety of ecological citizen science projects and divided these into: \\uf0b7 egoism, where people participate to increase their own welfare; \\uf0b7 altruism, where people participate to increase the welfare of another individual or a group; \\uf0b7 collectivism, where people are motivated to increase the welfare of a group of which the participant is a part; and \\uf0b7 principlism, where people volunteer in order to uphold their personal principles. Geoghegan et al. 2016. 29 With the rise of increasingly sophisticated online platforms for citizen science, ‘gamification’ is being used by some projects to appeal to some participants’ motivation to compete with others. This can include reputation points, leader boards, digital badges, and individual or team challenges (Azavea & SciStarter 2014). A survey of the popular protein-folding citizen science project Foldit found that many players wanted to contribute towards science, but others were specifically attracted by the competitive nature of the game (Cooper et al. 2010, Curtis 2015). Hochachka et al. (2012) report that after adding a competitive element (of leader boards) to the eBird website, where people record bird distributions, participation in the network increased. Massung et al. (2013) is one of the few studies which has empirically tested the efficacy of gamification for motivating participants. This was a small study but they found that although gamification increased data return rates, this difference was not significant, and some participants were actually de-motivated by the competitive element if they felt they were not able to compete with those leading the competition (see also Chapter 9 on Technology in citizen science). Again, this highlights the point that different people have different motivations for participating. Using the motivation categories identified in the literature, West et al. (2015) asked a sample of 613 people who have participated in citizen science projects what their motivations for taking part were (Figure 1). The most common responses were altruistic motivations; either people wanted to help wildlife in general, to contribute to scientific knowledge or because they felt that participating is a valuable thing to do. Another altruistic motivation, to help a specific site, was a far less common response, which could be due to the type of project people were participating in. Personal development motivations were second most common, including the motivation to learn something new, to help future careers and to enhance development. Other personal motivations, not related to development – to get exercise/fresh air and to meet people/for fun – were less common responses. To share personal knowledge was also uncommon, as was being asked by someone else to participate. Open-ended responses to “Other” did not add any additional categories. Geoghegan et al. 2016. 30 Figure 1 Motivations of participants in citizen science projects (West et al. 2015) 4.3 Existing work on variations in motivations The importance of different motivations appears to vary between projects. For example several studies of both environmental volunteers and citizen scientists have found environmental (values) motivations to be the most important (Hobbs & White 2012, Bruyere & Rappe 2007, Jacobsen et al. 2014). Similarly, Raddick et al. (2013) found the altruistic motivation of wanting to help science to be the most dominant in their sample of Galaxy Zoo volunteers. On the other hand, Asah et al. (2014) studied volunteers who were working on landscape restoration and conservation, and found that socio-psychological (enhancement) motivations such as wanting to learn and help future careers were mentioned nearly twenty times more than environmental motivations. In addition, volunteers should not be seen as a homogenous group of people, rather they can have a variety of different motivations (Asah et al. 2014). The same volunteering activity may engage people for very different motivations (Clary & Snyder 1999), and participants may hold multiple motivations at once (Clary & Snyder 1999, see also Bell et al. 2008, Asah et al. 2014). It is also important to note that a person’s motivations for participating can shift over their lifetime (Ryan et al. 2001), and there is some evidence to suggest that age may be important here. Clary and Snyder (1999) and Jacobsen et al. (2012) note that career related motivations may be particularly important for younger people, who may be volunteering to gain skills, whereas older volunteers may be more likely to want to share their skills and pass on their knowledge to others (Unell & Castle 2012). These studies emphasise the importance of longitudinal studies to examine people’s motivations over time. 0 5 10 15 20 25 30 35 40 45 P e r c e n ta g e o f r e s p o n d e n ts Geoghegan et al. 2016. 31 4.4 Participant motivations for citizen science and environmental volunteering We used our online survey to consider our participants’ motivations. Using categories identified by West et al. (2015) in their study of motivations in citizen science, we asked our participants ‘What motivated you to take part?’ Table 13 and Figure 2 indicate the primary motivations for citizen science and environmental volunteering participants. Figure 2 % of primary motivations by citizen science and environmental volunteering Primary Motivation Number of respondents (% of those who responded) Citizen Science Environmental Volunteering To help wildlife in general 66 (52%) 20 (54%) To help a specific site 7 (6%) 2 (5%) To contribute to scientific knowledge 37 (29%) 7 (19%) To meet people/for fun 0 0 To learn something new 4 (3%) 3 (8%) To spend more time outdoors 3 (2%) 1 (3%) To get some exercise 1 (1%) 0 To share my knowledge and experience 2 (2%) 0 Someone wanted me to do it (e.g. family, 4 (3%) 0 0 10 20 30 40 50 60 P e r c e n ta g e o f r e s p o n d e n ts Citizen Scientist Environmental Volunteer Geoghegan et al. 2016. 32 teacher) To develop new skills 0 0 To help my future career 1 (1%) 3 (8%) Other 2 (2%) 1 (3%) Table 14 Primary motivations for citizen science and environmental volunteering ‘To help wildlife in general’ was the most common primary motivation for citizen science participants, followed by ‘To contribute to scientific knowledge’. These findings are in agreement with West et al.’s findings, in particular the acknowledgement that people were not particularly motivated by ‘To help a specific site’. It is useful here to compare these findings with the results from the environmental volunteer survey. Whilst the numbers participating in this survey are small, the highest scoring motivation is ‘To help wildlife in general’, followed by ‘To contribute to scientific knowledge’, echoing the findings for the citizen scientists. However, as indicated in Chapter 3, the distinction between who identified as a citizen science respondent and environmental volunteering respondent is not a simple one. Number of respondents R a n k in g T o h el p w il d li fe in g en er a l T o h el p a sp ec if ic si te T o co n tr ib u te to sc ie n ti fi c k n o w le d g e T o m ee t p eo p le /f o r fu n T o le a rn so m et h in g n ew T o sp en d m o re ti m e o u td o o rs T o g et so m e ex er ci se T o sh a re m y k n o w le d g e a n d ex p er ie n ce S o m eo n e w a n te d m e to d o it (e .g . fa m il y , te a ch er ) T o d ev el o p n ew sk il ls T o h el p m y fu tu re ca re er 1 66 7 37 0 4 3 1 2 4 0 1 2 18 16 46 7 18 3 0 8 0 5 2 3 10 8 18 5 21 12 2 17 2 10 6 4 7 3 6 5 13 15 4 15 2 13 6 5 2 2 2 2 11 9 6 9 1 11 3 Total 103 36 109 19 67 42 13 51 9 39 18 Table 15 Top 5 motivations for citizen scientists We asked our citizen science respondents to rank their top 5 motivations from our list (based upon West et al. 2015). As evidenced in Table 15, following ‘To help wildlife in general’ as the primary motivation for the majority of citizen scientists, this was followed by ‘To contribute to scientific knowledge’, ‘To learn something new’, ‘To spend time outdoors/To share my knowledge and experience’ and ‘To learn something new/To develop new skills’. We also gave respondents the opportunity to identify ‘other’ motivations, as per West et al.’s (2015) study. 30 citizen science survey respondents selected this option, echoing some of the motivations identified above, but stating that sharing their enthusiasm, career objectives, contribution and enjoyment as significant motivations. Motivation In their own words… Sharing enthusiasm “To encourage others to take an interest”; “I did the recording at lunchtimes with a friend and we liked to take a walk in the open air. We were a \"group\" of two”; “To motivate children to take part; It\\'s fun to do, especially if it helps Geoghegan et al. 2016. 33 get the children involved”; “To meet up with like-minded people around the country” Career “I am a professional academic it is part of my research; some work was through grant funded research projects”; “I wish to publish from this work; part of my work”; “To help my future career”; “It is my job”; “To understand more about citizen science projects with a view to helping develop a citizen science project (as an academic)” Help wildlife “To help British birds”; “To help bats and to learn more about them”; “Particular interest in bats”; “To help a specific species”; “Because nature is amazingly awesome?!”; “To help butterfly conservation and to help the management of my local nature reserve” Contribution “To contribute to scientific knowledge”; “To contribute to my community”; “To help to fill in \\'white holes\\' in recording effort”; “Feeling of responsibility to use knowledge”; “I think it is a good thing to do”; “To feel part of something worthwhile”; “I was originally asked by one of the wildlife Trust Rangers to help monitoring at some of their sites” Enjoyment “Because it\\'s what I enjoy doing”; “Because it is interesting and fun”; “Enjoyment” New knowledge “To understand more about a new local neighbourhood”; “To improve species identification skills”; “Learning” Engage others “As a home educator it is a useful project to engage the children with” Table 16 Citizen science responses to ‘other’ The findings from our survey reveal that notwithstanding the definitional issues surrounding the project types, it is possible to conclude that there are similar motivations across the citizen science and environmental volunteering ‘divide’ and as such citizen science projects may appeal to many environmental volunteers. 4.5 Variations in motivation over time In addition to identifying the primary motivations of citizen science participants, it is important to understand whether motivations change over time and whether this might influence their continued participation in a citizen science project. Only 1 in 4 of our citizen science respondents said that they felt their motivations had changed over time (Table 17). When asked to elaborate, a number of people commented upon becoming more interested in the science and contributing to this, as well as moving from gaining knowledge to passing it on. This did not suggest that their motivations would lead them to leave a project, although it did indicate that they might take on additional responsibilities within a project – a point echoed in Section 5.5 on participant careers and what encourages participation. Number of respondents (% of those who responded) Citizen science Environmental volunteering Yes 36 (26%) 8 (20%) No 84 (62%) 31 (76%) Not sure 16 (12%) 2 (4%) No reply 11 6 Table 17 Did you motivations change over time? We also asked our respondents in what ways they thought their motivations had changed. Of the 37 people who responded to this open-ended question, the majority stated they are now motivated by knowing that they are contributing to scientific knowledge, followed by sharing knowledge and having a stronger concern for conservation, for example observing Geoghegan et al. 2016. 34 environmental destruction/species decline: “As I’ve got older, I’ve become more aware of the decline in British Wildlife caused by people and it infuriates me to see our wildlife and countryside taken advantage of and viewed as a non-essential commodity that gets in the way of our ‘progress’”. Some respondents acknowledged they were motivated because of the importance of their involvement due to their career and professional roles. Others stated that they had now moved from contributing to leading projects. The social aspects and meeting people were also strong motivators, and now they felt they had the skills, they were no longer motivated by the aspect of learning. Furthermore several respondents commented on the responsibility they feel to take part: “Initially awe/wonder. Increasing sense of responsibility.” Other individual responses included: “no longer just motivated by their career”; “now motivated by it being practical/they are spending time outdoors”; “finding it difficult to keep up others’ motivation”; “now motivated by the physical exercise they gain from involvement”; “they are now more interested”; and “they are now involved in developing new project”. These responses indicate the complexity surrounding individual motivations for citizen science participation. Beyond an initial desire “To help wildlife in general” and “To contribute to scientific knowledge”, respondents to this open-ended question indicated an enhanced sense of ownership and responsibility for their involvement, participation and contribution to science. Extrinsic motivations shifted from career and learning motivations to become more intrinsic with an enhanced experience as a result of participation. Geoghegan et al. 2016. 35 Chapter 5: Motivations for beginning and continuing in citizen science Chapter highlights \\uf02a Existing literature indicates that dispositional (personal motivations) and organisational (logistical) variables influence initial participation with the addition of awareness that the opportunity exists \\uf02a Poor project organisation frequently contributes to a fall off in participation \\uf02a Two potential incentives for continued participation that were identified in the online survey were skills development, and feedback and communication. The latter was also identified by stakeholder interviewees and the literature \\uf02a Long-term participation often transcends the initial involvement and becomes a deeper enthusiasm for the subject \\uf02a Feedback, knowing their impact and contribution, enjoying the project, being involved in a good project, skills development, and support were identified as factors that encourage continued participation by citizen science respondents \\uf02a Citizen science participants felt their involvement had been worthwhile if they knew their participation had an impact, received feedback and communication, knew their data was useful, and had enjoyed their participation. Environmental volunteering participants concluded that they needed their involvement to have an impact, lead to new skills and receive feedback \\uf02a Shared reasons for feeling fulfilled suggest that citizen science projects may appeal to many environmental volunteers \\uf02a More awareness raising is required to attract environmental volunteers to citizen science projects \\uf02a Barriers to increased volunteering by environmental volunteers include: already doing as much as they can; lack of time; access; shortage of people to do it with; lack of funding; lack of time; age; work/family commitments; poor health; and the weather \\uf02a To be useful, feedback must be immediate, specific to the locale or individual, interpretable, and offer online and offline options For many environmental-based citizen science projects, successful fulfilment of project objectives requires long-term/continued participation. In this chapter, we consider what encourages people to participate in citizen science projects for the first time and then maintain their involvement. We combine material from our desk-based study, online survey and stakeholder interviews to understand what encourages and discourages continued participation. We begin with our literature review findings. 5.1 Existing work on factors for participation in citizen science West and Pateman (2015) have developed a model which summarises the key factors for participation in citizen science. This highlights the three distinct stages to participation: the decision to participate; the initial participation in the project; and sustained participation. This model (Figure 3) was originally based on Penner (2002), taken from the volunteering literature, and was adapted using the citizen science and environmental volunteering literature in order to make it more relevant to citizen science. Geoghegan et al. 2016. 36 Figure 3 Model of influences on participation in citizen science (from West & Pateman 2015, based on Penner) Penner (2002) states that there are two types of variables which affect a person’s involvement in volunteering, namely dispositional variables and organisational variables. Dispositional variables are the attributes of individuals, such as motivation, personal circumstances and demographics (see top of Figure 3). Organisational variables are the attributes of the organisation leading the project, the most important of which are project organisation and communication. In their study of volunteers involved in biological recording projects, Hobbs and White (2012) identified three main factors which influence a person’s decision to participate: they need to be motivated, the volunteering opportunity needs to be appropriate for them, and they need to be aware that the opportunity exists. These are shown in Figure 3 as ‘Motivation’, ‘Personal attributes, circumstances and demographics’ and ‘Awareness of opportunity’. Missing from existing studies is a focus on the emotional dimensions behind participation in citizen science. Drawing on literature from the fields of serious leisure (Stebbins 1992) volunteering (Smith et al. 2010) and enthusiasm (Craggs et al. 2013), it is possible to discern the importance of enthusiasm, specifically an emotional affiliation towards a particular activity or thing, in participation in group projects (Geoghegan 2013), and more recently citizen science (Everett & Geoghegan in press, Fradera et al. 2015). Relatively few studies have looked at motivations of citizen science participants over time, or the experience of volunteers more generally once they are in place (Wilson 2012). One that has is Jackson et al. (2015), although this was in the astronomy rather than the environmental citizen science field. They explored the motivations of just three volunteers who were involved in tasks which were peripheral to the main project, such as moderating forum comments, welcoming newcomers, etc. They found that such tasks helped people move from initial participation to sustained participation, and as they became more engaged, they became more central to the community and understood its culture, language, organisation, etc. This helped the participants form an identity as a volunteer (Jackson et al. 2015), which, as highlighted by Penner (2002), is one of the key factors leading to sustained volunteering. In the context of environmental volunteers, Ryan et al. (2001) looked at the motivations of long- term volunteers and found that new volunteers tended to be motivated by wanting to help the Geoghegan et al. 2016. 37 environment and learn new things, but that social factors were more important for retaining them in the long term. This has also been shown to be the case in citizen science projects (Couvet & Prevot 2014) and research with other enthusiast communities (Geoghegan 2013). Peachey et al. (2014) note that, in general, volunteers are more likely to continue participating if their initial motivations for volunteering are fulfilled, and their satisfaction with their role is a good predictor of their intention to continue (Wu et al. 2015). Therefore, it is important to provide participants with the opportunity to reflect on their work and discuss their motivations (Ryan et al. 2001). Newton et al. (2014) have shown that people’s motivations for initial involvement are an important factor in how long they volunteer for, with McDougle et al. (2011) finding that young people involved in environmental volunteering who were motivated for social reasons spent more time volunteering than those who volunteered in order to learn. This highlights the importance of providing opportunities for social interaction (Locke et al. 2003, Van Den Berg et al. 2009, Jacobsen et al. 2012, Asah et al. 2014). Poor organisation is often cited as a reason for dropping out of volunteering (Ryan et al. 2001, Lock et al. 2003, Jacobsen et al. 2012), and therefore it is important to give participants the opportunity to provide feedback (Garner & Garner 2011, Unell & Castle 2012). Participants need to feel that their time is being used well (Bruyere & Rappe 2007), so monitoring the impacts of their work and regularly communicating it to them can help to retain them (Van Den Berg et al. 2009, Unell & Castle 2012). 5.2 What encourages continued participation in citizen science? In the following sections, we reveal the factors relating to disposition and organization that encourage/discourage participation in environmental-based citizen science. We focus on both the dispositional variables identified above, as well as the organizational variables relating to leadership, feedback and communication as discussed in the stakeholder interviews, identifying how important communication and effective feedback is to stakeholders in the process. Whilst the review above indicates the benefits of evaluation and monitoring, we discuss these separately in Chapter 10. 5.2.1 Dispositional variables We asked our citizen science survey respondents whether their motivations had been satisfied. The following table (Table 18) shows if there motivations were satisfied overall did their involvement motivate them to do more. Number of participants (% of those who answered) Did your involvement in the project motivate you to do more Citizen Science? Dissatisfied Neutral Satisfied Discourage 0 0 1 (1%) Neutral 0 0 17 (16%) Encourage 2 (100%) 3 (60%) 60 (55%) Strongly Encourage 0 2 (40%) 31 (28%) Table 18 Satisfaction and increased involvement in citizen science With exception of two participants who felt their motivations to help wildlife and a specific site were dissatisfied, the majority of participants felt their primary motivations had been satisfied. The majority of participants were encouraged or strongly encouraged to participate further. We continued this line of enquiry by asking respondents whether they felt their skills had developed over time. For both citizen science and environmental volunteering, the majority of respondents felt their skills had developed over time and this had encouraged over half of them to do more citizen science/environmental volunteering (Table 19). Geoghegan et al. 2016. 38 Number of respondents (% of those who responded) Type Citizen science Environmental volunteers No difference 34 (25%) 20 (50%) Encouraged 66 (49%) 17 (43%) Strongly encouraged 36 (26%) 3 (7%) No reply 11 7 Table 19 If you felt your skills had developed over time, did your involvement encourage or discourage you to do more? 5.2.2 Organisational variables In order to examine the importance of organisational variables such as communication and feedback we asked our citizen science respondents whether they received feedback and how important feedback was to their continued involvement (Table 20). With exception of 15 respondents who received ‘no feedback’, emails, online maps and graphics and project reports were identified as the most used forms of feedback. Feedback method Number of citizen science respondents (% of those who responded) Received an email 55 (37%) Received a text 1 (1%) Data was displayed on an online map or graphic 65 (44%) Received a copy of a project report 48 (33%) Other 15 (10%) No feedback 15 (10%) Table 20 Did you get any feedback? We asked our citizen science and environmental volunteering respondents about the importance of communication and feedback to their continued participation (Table 21). With the exception of 5 environmental volunteer respondents, the majority of our citizen science and environmental volunteering respondents found communication and feedback to be important to their continued participation. This tallies with the suggestion in our desk-based study that people are motivated to be involved in a well-organised project (Bruyere & Rappe 2007, Jacobsen et al. 2012). Number of citizen science respondents (% of those who responded) Citizen Science Environmental Volunteers Very unimportant 0 2 (5%) Unimportant 4 (3%) 3 (8%) Neutral 12 (10%) 5 (13%) Important 11 (9%) 5 (13%) Very important 94 (77%) 23 (61%) No reply 26 9 Table 21 How important is communication and feedback to your continued participation in the project? Geoghegan et al. 2016. 39 The importance of other organisational variables such as leadership, project management, evaluation and monitoring is emphasised throughout this report. We also extended our survey here to include the free text option for participants to let us know in their own words: “What encourages or discourages your further involvement in citizen science?” 91 citizen science respondents answered this question. Whilst these opinions were diverse, it is useful here to list the many ways in which people felt encouraged or discouraged to participate as this is fundamental to the success of any citizen science initiative (see Appendix 3 for detailed answers). Feedback was the overwhelming factor, followed closely by contribution and impact. What encourages involvement? In their own words… Feedback “Some evidence of the data being used to extend scientific knowledge and this informing policy change”; “See the end result, the report, hear about the findings and whether this has resulted in changes being made”; “Receive some feedback even if it is the first year of the project. Data analysis can wait but a general outline of coverage and initial observation sis very welcome”; “Response from the organisers”; “To feel that my sightings information and site data have been recognised and used”; “Evidence that the data is being used intelligently”; “FEEDBACK! :-) (but I\\'m now one of the converted and self motivated anyway)”; “Affirmation from professionals”; “A copy of the report where I could see the general results and, perhaps, find out how my contribution fitted in”; “Knowing that my data has been included in reports, charts and maps and that these are available for me to see” Impact and contribution “To see the impact that it has had”; “know and understand how the data is contributing to ecology and conservation and how it will be used for positive change”; “That I\\'m making an important contribution which would not otherwise be made”; “not just feedback, but feeling that the general project is doing some good ie at policy making level”; “See outcomes that reverse loss of biodiversity or could do so in the future”; “To see that the whole is greater than the sum of the parts i.e. that by contributing data from a specific area, you are contributing to a national perspective”; Enjoyment “Enjoy doing it. My area of interest and knowledge. Confidence I know enough to report accurately”; “It has to be an enjoyable process”; “Happy participants and a completed survey!”; “I think it is the intrinsic enjoyment of identifying wildlife and being outside”; Good project “A feeling that the project is well organised, with serious scientific intent, and that the results are of regional/national importance.” Skill development “Learning”; “New skills, helping others.” Involvement “Deep involvement in all aspects, not participating as a sensor.” Support “Clear instructions, support if needed, access to a forum to discuss with other citizens.” Table 22 Open-ended answers to what encourages or discourages your further involvement in citizen science We also asked our citizen science survey respondents: “What do you need to get out of a citizen science project for you to feel your participation has been worthwhile?” We received 103 responses. In Table 23 we list the answers in order of frequency mentioned. Geoghegan et al. 2016. 40 What makes participation worthwhile? Specifically Examples from open-ended responses Knowing their participation had an impact Evidence that their participation had an impact or had led to positive change “Robust scientific analysis and reports leading to conservation action where required.” “The knowledge that the data is being disseminated and used to inform policy” Receiving feedback and communication Feedback, acknowledging receipt of data and seeing results based upon their data in papers or reports “To start with it was an acknowledgment but now I understand how important it is I am more than happy to do it for no thanks at all” Knowing their data has been useful Evidence that their data is useful or has been used, and in some cases how the data would be used. “To know that data is of value to organisations and others and that it will be used to aid conservation / further research / education etc.” Enjoying participation, and achieving a deeper involvement Respondents wanted to feel they enjoyed the project “Deep involvement in all aspects, not participating as a sensor” Other one-off responses included: Clear instructions, support from professionals, well- organised project, understanding relevance of project, developing new skills, helping others. “Seeing the project succeed and more people join in” Table 23 Examples of what made citizen scientists’ participation worthwhile 5.2.3 Continued participation from environmental volunteers We also asked anyone identifying as an environmental volunteer the question: “What do you need to get out of a volunteering project for you to feel your participation has been worthwhile?” 31 respondents answered the question (for full responses see Appendix 3). The responses indicate a common need to know that their participation in the project has had an impact, and evidence that their participation was useful through papers/reports. Less frequent answers related to receiving feedback, thanks and acknowledgement, meeting people and networking, well-organised project, good instructions, tools and training and increased public awareness of the issue. What makes participation worthwhile? In their own words… Impact “I want to know that something useful has happened, from which people or wildlife will benefit”; “See obvious improvements in species numbers and distribution”; “The feeling that I\\'m doing something worthwhile”; “To make some improvement over what went before, or to feel I\\'m adding to the body of knowledge about a particular site”; “That we have done the best we can, that a site is in great condition and the species we have worked for will use it” Publication “An output”; “A report that says clearly what was achieved”; Networking “Get good connections”; “meeting likeminded people” Skill development “Learn new skills”; “Clear understanding of what is required, appropriate tools and training (if necessary),” Getting job done “A satisfactory end result. Something has been completed which may be Geoghegan et al. 2016. 41 anything from planting a tree to recording moths over the whole year”; “seeing benefits of work e.g. Coppicing increasing woodland flora and bird habitats; partly the feeling that I am able to make a worthwhile contribution without suffering from the cold and dampness of winter working parties” Data collection “Collecting useful data” Feedback “Occasional thanks always welcome”; “Feedback from the organisation(s) you volunteer with and from people that use your material”; Table 24 Examples of what made environmental volunteers’ participation worthwhile The shared responses between citizen scientists and environmental volunteers for what makes their participation worthwhile suggests that citizen science may appeal to environmental volunteers. We consider this in more detail in the following section. 5.3 Encouraging environmental volunteers to participate in citizen science As outlined in the introduction and methodology, we were also interested in responses from people who had not participated in a citizen science project before, but already had an interest in the environment. We targeted those people who self-identified as environmental volunteers and asked them what might motivate them to participate in citizen science in the future, or what had discouraged them from participating at this present time. An important question that we posed to environmental volunteers is: “Are you aware of any citizen science projects?” Of those that answered the question, the response was an almost even split between being aware and unaware of citizen science projects (Table 25). Of those who were aware of the citizen science projects, 52% of respondents were interested in participating (Table 26). Furthermore, 56% of volunteers would like to do more volunteering (Table 27). The overwhelming reason preventing respondents for doing more volunteering was ‘lack of time’ (Table 28). Number of respondents (% of those who responded) Yes 17 (44%) No 18 (46%) Not sure 4 (11%) No reply 8 Table 25 Are you aware of any citizen science projects? Type Number of respondents (% of those who responded) Not at all 9 (26%) Neutral 3 (12%) Very much 13 (52%) No reply 22 Table 26 If you are aware of citizen science projects, have you ever been interested in participating? Number of respondents (% of those who responded) Not at all 9 (20%) Neutral 11 (24%) Absolutely 25 (56%) No reply 2 Table 27 Would you like to be able to do more volunteering? Geoghegan et al. 2016. 42 Reason Number of respondents (% of those who responded) Lack of time 29 (54%) Difficulty getting to activities 1 (2%) Shortage of other people to do it with 4 (7%) Lack of money (for travel, materials, food etc.) 5 (9%) Other 15 (28%) Table 28 If you would like to do more volunteering, what prevents you? Those who responded ‘other’ also indicated ‘lack of time’, as well as the following factors: What prevents your participation In their own words… Lack of time I already do all I can; Fully committed already; Spend all my spare time working on wildlife projects; I go on every work party that is organised. I do a huge amount in the background too. Advancing age Age; Lack of physical strength to do sawing and for using brushcutter (largely due to advancing years!); My age; Old Age! Bureaucracy Red tape and unwillingness of local authorities to work together. Family Family not happy if I spend too much time away moth recording. Knowledge Lack of knowledge Inclination Need to make myself do it. Health Poor health Job Inflexibility of employer Weather The weather!! Table 29 Other reasons preventing environmental volunteers from doing more 5.3.1 Benefits of environmental volunteering The desire of volunteers to do more suggests that there are multiple benefits gained by participating in environmental volunteering and citizen science. According to the existing literature, the benefits that people derive from participation have not been rigorously assessed (Crall et al. 2013). Those studies that have examined outcomes for participants from engaging in citizen science projects have done so in the context of the project initiator’s expectations or hopes for their participants. For example, some case studies show that participants have learnt something as a result of participation (Bonney et al. 2009, Sirbu et al. 2015), shown increased scientific literacy (Bonney et al. 2009), have changed their behaviour (Bonney et al. 2015, Couvet & Prevot 2014) and shown advocacy by spreading their new knowledge (Johnson et al. 2014). However, the perspective of the participant is not taken into consideration i.e. studies have not asked what participants hoped to gain from participating and what they felt they actually gained from participating. Furthermore, there is the potential for bias in the literature towards scientists reporting on what they consider to be successful projects (Riesch & Potter 2014). We asked our environmental volunteer respondents how they currently benefit from volunteering, 85% of our respondents stated “I feel I am helping the wildlife”, 72% stated “I develop new skills” with spending time outdoors, getting some exercise and sharing knowledge and experience also rated highly. Respondents were able to select as many options as applied to them. Geoghegan et al. 2016. 43 Benefit Number of respondents (% of total number of environmental volunteers who participated in the survey) I feel I am helping the wildlife 40 (85%) I develop new skills 34 (72%) I spend more time outdoors 30 (64%) I get some exercise 29 (26%) I get to share my knowledge & experience 31 (66%) I feel I am helping a specific site 27 (58%) I meet people & have fun 27 (58%) I feel better about myself 20 (43%) I am helping my future career 3 (6%) Other 9 (19%) Table 30 How do you currently benefit from environmental volunteering? 9 respondents answered ‘other’ and identified contributing to science, fulfilling their personal enthusiasms, institutional affiliation and making the world a better place for future generations as key benefits they derive from volunteering. More work needs to be done to understand two key areas. Firstly, the differences in benefits between projects. There is some evidence to suggest that people need to be involved in more collaborative or co-created than contributory projects, and over a long term rather than short term to benefit (Couvet & Prevot 2014). It has also been noted that it is currently difficult to compare between projects because of the different methods used and there has been a call for common questions and indicators to be used in project evaluations (Crall et al. 2013). Secondly, the lifetime of these benefits. This is unknown, perhaps due to the difficulties in tracking participants in the long-term after they have finished their involvement with a project. 5.4 Importance of communication and feedback for continued participation The literature review and the online survey revealed the importance of communication and feedback to the continued success of citizen science projects, specifically the continued participation of citizen scientists and environmental volunteers (which as we know involve considerable overlap). We asked our stakeholder interviewees about the importance of communication and feedback to their projects and roles. We outline below our findings: 5.4.1 Communication and feedback All stakeholder respondents recognised the importance of communication with participants in citizen science. As one practitioner respondent suggested: “[Communication is] absolutely essential because you have to motivate people to keep going. They have to be reminded that their records are useful, how they’re being used” (Practitioner, science, engagement). Feedback and communication allow participants to understand their contribution: “what people want, is to get that kind of feel for the bigger picture and how they’ve contributed” (Practitioner, science, engagement) Geoghegan et al. 2016. 44 It also enables citizen science projects to build rapport with their participants: “[Communication allows] citizens to engage over a longer period of time to build trust and to understand each other because what scientists think the public needs is quite often not what the public thinks they need and vice versa” (Scientist, university) Whilst it was acknowledged by some as a time-consuming activity requiring considerable resource, as well as an activity that is apportioned to one area of a larger citizen science project team (e.g. some government agencies had devolved responsibility of communication to the frontline organisations dealing with the public), all respondents expressed the need to communicate and feedback to participants who volunteer their time and expertise to their projects. Respondents agreed that feedback must be immediate, specific to the locale or individual, interpretable, and offer online and offline options. The following reasons for, and benefits of, communication were identified by stakeholders: \\uf0a7 Keeping participants in touch with the project; \\uf0a7 Offering participants local, personal and quick feedback; \\uf0a7 Sharing intellectually interesting content; \\uf0a7 Opportunity for face-to-face feedback offering a personal touch/link with the project; \\uf0a7 Able to share what the data is telling participants; \\uf0a7 Able to treat people as social creatures (e.g. it’s not a club, but there can be interaction); \\uf0a7 Target individual requirements (e.g. daily for some, others not bothered); \\uf0a7 Sharing how data is used in scientific papers to communicate what their data is able to do; and \\uf0a7 Educate participants by sharing top tips, what to look for, advice, answers to letters. 5.4.2 Communication methods Stakeholders identified methods they have used within their own projects to communicate with their participants, and methods they had experienced through their own volunteering. Table 31 highlights possible communication methods and their usefulness. A more detailed discussion of the technologies surrounding feedback is offered by Roy et al. (2012). Method Reasons for use and associated challenges Online/Offline Communication (the role of technology is discussed in more detail in Chapter 9) Websites Allows quick and easy updates, using maps and pictures to visualise results Enables all partners to contribute/share content Allows for dedicated research findings websites (e.g. BTO BirdTrends) Host forums for discussion and chat, moderation of the forum is often by volunteers Newsletters Online: PDFs enable projects to save money, printing and posting Offline: importance of materiality acknowledged (e.g. newsletter to flick through), particularly for those who have been involved long-term (i.e. older participants) Social media (e.g. blogs, Facebook and Twitter) Allows easy information sharing (e.g. retweets) Emails Two-way communication with individuals Must be careful not to bombard participants Geoghegan et al. 2016. 45 Can be time-consuming with direct contact Media (e.g. press, radio, TV) Opportunity for mass communication with participants Face-to-face communication Annual conference, workshops, community meetings Meeting in person (e.g. annual get together) Offers talks (e.g. first look at data, researchers talk directly to participants) Involves some training NOTE: Communication and feedback were also linked to monitoring and evaluation processes – for more on these see Chapter 10. Table 31 Stakeholder communication and feedback mechanisms 5.4.3 Face-to-face with the community The majority of our stakeholder respondents are involved in citizen science for academic research, long-term monitoring, surveillance and education. However, one respondent was heavily involved in community-led science, whereby the participants set the research questions and are supported by a team of experts to develop a methodology based upon scientific protocols. Working ‘in’ the community, led to the need for increased face-to-face communication with the community, as this practitioner interviewee explained: “But there was another portion, there was an older demographic who didn’t have email, or didn’t really use email and, so we’d call them, so we’d actually phone them up. So we knew the list of people that we needed to phone and speak to over the phone. And then we’d go to their houses.” (Practitioner, community) “We would have like[d] a community meeting in a location which is in the heart of the community so it’s accessible to them, and say this is what you did and this is the results that we found and this is what it looks like, and what do you want to do now that you’ve got this and how do you want to move forward with this. So we would always have that.” (Practitioner, community) Whilst we are not advocating that every citizen science project should involve face-to-face communication with all participants, for many projects this would be impossible, but these examples alert us to the need to consider alternative ways of communicating with participants. Technology is only part of the solution here, and techniques involving co- presence are often vital for developing long-term rapport with participants. This becomes increasingly important in the context of collaborative and co-designed projects. 5.4.4 Good practice in communication Unsurprisingly, practitioner stakeholders working in science, engagement, education and community had the most knowledge to share on good practice for communication and feedback. Drawing on over 10 years experience of working with citizen science projects, two practitioners identified the following top tips for those looking to communicate effectively with their citizen science audiences: “I think the more you can get under the skin of your recorders5 and understand what inspires them. We have quite good analytics on our email so I can check and see what stories people have responded to and which ones have rather left them cold. So over the years I’ve discovered … the majority are very passionate about birds. So if I can find bird stories, that is useful. The most popular story we ever ran was a silly one I found, I think on the BBC website, and it was called, blue tit nests in ashtray. And it sent the email absolutely wild” (Practitioner, science, engagement). 5 Note the use of the term ‘recorder’ to identify their citizen science participants. This reinforces the challenge of definition. Geoghegan et al. 2016. 46 “It is about recognising the fact that people enjoy learning and recognising the fact that they want to do something that makes a difference. And both of those things really underpin everything that we do in [our project],… if we can give people new information about what they’re seeing, … they want to know more information about it, which can be very practical information about what kind of food to put out, … what to do to help [species] on a basic level. But also thinking that by sending us information about [the target species], they’re actually helping to build up information about garden wildlife, urban wildlife. More generally which is then used for, you know, to increase scientific knowledge, which down the line can hopefully be used for things like urban planning and to find out how we can design maybe towns and cities, how we can design gardens, you know [our organisation] may be able to … work with developers. Or even if we don’t work with developers, the information we can put out there for it to be used by government or developers to actually help [the target species] in general, so I think that is where the motivation comes from for our [participants] and that’s where the, that’s why our feedback is effective because … we’re telling people this information” (Practitioner, science, engagement). It is possible to identify three important messages from this advice. First, project organisers need to understand what inspires and motivates their participants; second, organisers must commit to sharing information consistently on what participants are interested in (practical and science-related); and third, project teams should take advantage of communication analytics to determine which ‘communications’ received the most traffic from participants and tailor their communications accordingly. Finally, during one interview with a policy/evidence stakeholder, it was suggested that a future mode of offering feedback to participants could involve the consolidation of data into a central citizen portal: “We would be quite interested in a universal data solution where all the data is being pooled into a shared database that has the functionality to allow people to say, well, what are the trends in my local area, what can I find on this particular site, what occurs within X kilometres from my house? … that’s the sort of open data solution that potentially brings a lot of benefits. Data can flow directly to it. You can have your quality control partially automated and people can then access, download, visualise large chunks of data from multiple sources” (Policy, evidence). Geoghegan et al. 2016. 47 5.5 Summary of what encourages and discourages participation in citizen science Encourages Discourages Organisational Immediate, specific and interpretable; detailed, local/individual, accessible data; explains usefulness of participants records Feedback Slow or late feedback; inconsistent; not saying thank you or acknowledging receipt of data Online, Updated websites, online forums, newsletters, social media. Offline: meetings in person, printed newsletters Communication Bombarding participants; one-way communication only; inaccessible Dedicated websites with research findings; highlighting bigger picture and contribution; sharing that data made a difference Impact and outputs Incomprehensible Training sessions, skills development, feeling skilled/prepared Training/education Left feeling unprepared; being a sensor; little support Knowing their interests, what people love, feeling valued/useful Knowing your participants Feelings of being used Measure benefits/impacts on participants terms Participant benefits Too focussed on stakeholder benefits Good advertising and marketing, people have heard of project Awareness of opportunity Unaware that citizen science is an activity Well-organised Organisation Disorganised Able to move from passive to more active role Participation career Few opportunities to progress within the project Dispositional Satisfied Motivations Unsatisfied Access to resources Funding Not enough money to travel to sites or access technology Accommodate diverse personal barriers through tailored project Personal circumstances Already over-committed; weather; inflexible employer; health; inclination; family commitments; bureaucracy; age; lack of time Becomes about being in the company of others Enthusiasm Little opportunity to share interest Meeting like-minded people Networking/social factors Doing activity alone; no opportunities to meet people (even if only occasionally or online) More time spent participating more likely to continue Length of time One-off involvement (although not always) Table 32 Summary of what encourages and discourages participation Geoghegan et al. 2016. 48 Chapter 6: Motivations for stakeholders in citizen science Chapter highlights \\uf02a Existing literature on stakeholder motivations focuses only on scientists. This study is the first to include the motivations of a range of scientists (university, monitoring, policy, education, not using/data only), policy/evidence specialists and practitioners (science, engagement, education, community) \\uf02a As would be expected, advancing scientific knowledge is the most common motivation given for scientists to be involved in citizen science. This was found in both the academic literature and through our stakeholder interviews \\uf02a Other scientist motivations include: informing policy; informing conservation and land management; education; improving buy in for decisions; raising awareness and engaging people; building partnerships and improving communication \\uf02a In addition to those mentioned above, gaining personal satisfaction from participation was revealed as a significant motivation across all groups \\uf02a Personal satisfaction included: enjoying their work; commitment/enthusiasm; equity and self-determination for participation; fulfilling career objectives, ambitions, building on previous education; generating impact for people’s lives; working with unpaid experts and harnessing their enthusiasm for science \\uf02a Changes in stakeholder motivations over time related to increases in: institutional commitment to citizen science; technology; bottom up approaches; public engagement; media reports; sophistication of activities In this chapter, we examine the motivations and actual/perceived benefits for stakeholders of participating in citizen science. This is followed by a later chapter on some of the associated barriers and challenges. We begin here with a search of the existing literature on motivations for stakeholders getting involved in citizen science. Interestingly, we were only able to find research relating to scientists, this includes both scientists who have established their own citizen science projects and those who have used data collected by citizen scientists. There was no information on the motivations of other stakeholder types. Using scientist motivations as our starting point, we explore through our stakeholder interviews the motivations of those involved in science, policy/evidence and practice. First, we outline the motivations of scientists identified in the literature. 6.1 Existing work on scientist motivations (1) To contribute to science As would be expected, advancing scientific knowledge is the most common motivation given for scientists to be involved in citizen science. Citizen science projects tend to fall into two categories: hypothesis testing and monitoring. In both cases, the primary motivation for using a citizen science approach is often to collect data on a temporal or spatial scale that would not be feasible using traditional methods and within the funding available (Bonney et al. 2009, Devictor et al. 2010, Dickinson et al. 2010, Gardiner et al. 2012, Crall et al. 2013, Hardisty et al. 2013, Anderson & Alford 2014, Bone et al. 2012, Casanovas et al. 2014, Duputie et al. 2014, Fairclough et al. 2014, Winfield 2014, Kampen et al. 2015, Sirbu et al. 2015, Wright et al. 2015) or to collect data from areas otherwise inaccessible to scientists such as private land (Ferster et al. 2013, McClintock et al. 2015). The assumed cost-saving element is also more likely to make projects sustainable in the long term (Danielsen et al. 2005). More specifically, in some cases, the motivation is to collect data on a scale that is relevant to the question that is being asked (Sullivan et al. 2009, Kaartinen et al. 2013, Bird et al. 2014) or across multiple Geoghegan et al. 2016. 49 scales which can be helpful for answering specific questions (Lottig et al. 2014, Loss et al. 2015). The motivation may also be the ability to respond rapidly in response to global changes and emerging questions (Theobald et al. 2015) and to use findings to prioritise further research (Bone et al. 2012) and develop new hypotheses to be tested (Sullivan et al. 2009). In some cases it is also recognised that by engaging with citizens, researchers can gain valuable local or traditional knowledge (McKinley et al. 2013). Furthermore, some researchers have suggested that engaging in citizen science could help to meet the requirements of funding bodies to demonstrate impact (McKinley et al. 2013). Indeed, a recent review found that for 97% of citizen science projects surveyed, advancing scientific understanding was an explicit primary goal (Theobald et al. 2015). An analysis of 133 scientific papers which use data collected by volunteers found that nearly 75% had “extractive” aims of increasing the amount of useful data available to them (Lawrence 2009). A much smaller proportion had motivations relating to educating volunteers (19%), fostering stewardship (6%) and promoting cooperation between citizens and government (3%). (2) To inform policy Beyond collecting data for purely scientific purposes, another motivation identified by scientists for engaging citizens in data collection is to inform policy making (Bonney et al. 2009, Bone et al. 2012, Isaac et al. 2014, Lottig et al. 2014, Hollow et al. 2015, Hyder 2015). It has been noted that professional scientists on their own are not capable of delivering the volume of data, analysis and interpretation needed to match the speed at which policy decisions are made (Theobald et al. 2015 and references therein, White et al. 2015). More specifically, it has been recognised that citizen science can help to provide the data needed to present the scientific evidence to justify biodiversity conservation initiatives at a political level (Braschler 2009, Anderson & Alford 2014, Johnson et al. 2014). Vast amounts of data are also needed to report against policy targets and citizen science has been recognised as a way of achieving this (e.g. Aichi targets, Braschler et al. 2009, Arvanitidis et al. 2011, Danielsen et al. 2014; EU Habitat and Birds Directives, Isaac et al. 2014). (3) To inform conservation and land management Scientists may also be motivated to use a citizen science approach to inform land or conservation management (Bonney et al. 2009). The purpose may be to generate the data (Gollan et al. 2012, Lottig et al. 2014, McClintock et al. 2015, White et al. 2015) or ecological understanding (McKinley et al. 2013) needed to inform environmental management. Data can be used, for example, by local authorities to inform planning and development activities (Jansujwicz et al. 2013) or to support agencies and organisations to address environmental management issues (Rosevelt et al. 2013). In some cases, the topic lends itself in particular to a citizen science approach. For example, the presence of new nuisance species in an area is usually detected initially by members of the public and so establishing effective communication between communities, scientists and authorities may contribute to early detection and action (e.g. mosquitos, Kampen et al. 2015; invasive plants, Jordan et al. 2011). The advantage of engaging people at a local level has also been recognised as this is often operational level of resource management and so can improve the speed of decision-making (Danielsen et al. 2014). Locally-based monitoring schemes are also likely to focus on issues of greatest concern to stakeholders and so has the potential to influence on-the-ground management activities. Action takes place at a local level (Loss et al. 2015) to advance locally-relevant and practical conservation goals and strategies (Haywood & Belsey 2014). (4) To educate Beyond data collection to inform science, policy and conservation and land management, education appears to be the next most commonly mentioned motivating factor for scientists to use a citizen science approach. Often, this is education about the topic of the project (Trumbull et al. 2000, Bonney et al. 2009, Bone et al. 2012, Groffman et al. 2010, Ferster et Geoghegan et al. 2016. 50 al. 2013, Kaartinen et al. 2013, McKinley et al. 2013, Casanovas et al. 2014, Lucky et al. 2014, Winfield 2014, Sirbu et al. 2015). In many cases, however, there is a general aim to improve scientific literacy in participants, that is to improve their understanding of the scientific process (Trumbull et al. 2000, Bonney et al. 2009, Braschler 2009, Bone et al. 2012, Groffman et al. 2010, Casanovas et al. 2014, Varner 2014). (5) To improve buy in Getting buy in from participants for policy decisions (Bone et al. 2012) and land management decisions (McKinley et al. 2013) is also stated as a reason for involving the public in scientific projects. (6) To raise awareness and engage people Raising awareness, for example of environmental and conservation issues, is often stated as a reason for adopting a citizen science approach (Jansujwicz et al. 2013, Bird et al. 2014, Liu et al. 2014, Kampen et al. 2015, Loos et al. 2015, Sirbu et al. 2015, Wright et al. 2015). Related to this are the motivators to foster a sense of ownership, shared responsibility, concern and stewardship (Bone et al. 2012, Ferster et al. 2013, Anderson & Alford 2014, Danielsen et al. 2014). Wanting to change participants’ behaviours is also given as a reason for engaging citizens in scientific research. The motivation may be to change behaviour related to specific activity, for example, to engage the public in helping to prevent the spread of mosquitoes (Kampen et al. 2015), invasive species (Jordan et al. 2011, Crall et al. 2013) or wildfires (Ferster et al. 2013). Alternatively, the motivation may be not just to inform participants of a particular issue, but also to help citizens to understand problems and concerns so that they can make informed decisions of their own (Liu et al. 2014). It has also been suggested that by engaging people in monitoring against policy targets, this could help to engender on-the-ground action and hence help to achieve targets (Danielsen et al. 2014). Involving people in citizen science could also empower and build capacity in participants to make change at a local level (Danielsen et al. 2005) which could in turn improve their livelihoods (Danielsen et al. 2014). Related to this is the motivation to inspire activism in participants and to identify “opinion leaders” who can spread messages and action through communities (Groffman et al. 2010). (7) To build partnerships and improve communication Building partnerships and improving communication between different stakeholders (including scientists, amateur experts, local interest groups, the public, land managers, agencies and authorities, community organisations, government officials, educators and policy-makers) is also stated as a motivation for scientists taking a citizen science approach (Bone et al. 2012, Liu et al. 2014). The purpose may be: to share data between these different groups (Kampen et al. 2015); to help with decision-making (Rosevelt et al. 2013); to promote cooperation (Lawrence 2009); and to build trust, for example between scientists and the public (Grand et al. 2012) or between the public and decision-makers (Ferster et al. 2013), for example by increasing transparency in the scientific process and having data that is accessible to everyone. Improving communication and building partnerships between different stakeholders can also assist in the process of democratisation of science and policy and land management decision-making (Couvet & Prevot 2014). This is also mentioned as a motivation for a citizen science approach, usually in the context of co-created rather than contributory projects (Haywood & Belsey 2014, Liu et al. 2014). This literature review only revealed the motivations of ‘scientists’ and their involvement in citizen science. Several areas warrant further investigation. First, the need for further work with other stakeholders to accommodate the motivations of ‘others’ involved in citizen science, including: scientists working outside of academia in monitoring, policy and wider education; policy and evidence stakeholders commissioning, facilitating and using citizen Geoghegan et al. 2016. 51 data; and practitioners involved in working with citizen scientists through science, engagement, community work and education. Second, our literature review identified the need to consider the relative importance of these different motivations. Third, the review suggested the need to consider stakeholder motivations in relation to the type of project being led, for example do different types of project (e.g. with different topics, geographies, modes of engagement, skill level required, one-off or long term, contributory / co-designed / co- created projects, sphere of influence of the project) tend to have different motivations? It should also be noted that there are biases in the types of project on which publications are based (i.e. there are more publications related to long running large scale data collection projects, Theobald 2015, Danielsen et al. 2014). As such, this review may be missing motivations from other types of project. Finally, it was identified that there needs to be a consideration of how motivations change as a project evolves. Whilst changes in motivations over time have been noted from some projects, such as eBird (Sullivan et al. 2014), this question is relatively under-explored. In order to access the motivations of a wider group of stakeholders, we conducted interviews with them to ask about both their institutional and personal motivations, how they take into account participant motivations in their citizen science projects, and how the motivations for the projects they have worked with have evolved over time. We present the findings below in two parts: first, new and emerging stakeholder motivations (6.2) and how they vary between stakeholder groups (6.3); and second, how stakeholder motivations change as projects evolve (6.4). 6.2 New and emerging stakeholder motivations Taking the categories of motivations for scientists involved in citizen science from the academic literature outlined above, we expanded on this for other stakeholder groups and were able to identify new and emerging motivations across a range of stakeholders. Table 33 (overleaf) reveals those categories of stakeholder motivations and will be useful for stakeholders setting up projects and those doing it on behalf of others, for example practitioners working with policy/evidence teams. Our stakeholder interviews revealed motivations relating to all of the aforementioned ‘scientist’ motivations in the literature. However, no new or additional motivations were added to the categories of \\uf0b7 To inform conservation and land management; and \\uf0b7 To raise awareness and engage people. It is important to note that policy/evidence and practitioner stakeholders were also motivated by these causes. A new category of ‘To gain personal satisfaction’ was added as a motivation as a result of our interviews with practitioner stakeholders, and new motivations were incorporated into ‘To contribute to science’; ‘To inform policy’; ‘To educate’; ‘To improve buy in’; and ‘To build partnerships and improve communication’. Geoghegan et al. 2016. 52 Motivation (* indicates newly identified) S ci en ti st /u n iv er si ty S ci en ti st /m o n it o ri n g S ci en ti st /p o li cy S ci en ti st /e d u ca ti o n S ci en ti st /n o t u si n g /d at a o n ly P o li cy /e v id en ce P ra ct it io n er /s ci en ce / en g ag em en t P ra ct it io n er /c o m m u n it y P ra ct it io n er /e d u ca ti o n Science Policy Practice To contribute to science Increase temporal and spatial scale of data collection Cost saving Collect data from inaccessible areas Collect data on a scale relevant to the question being asked Collect data across multiple scales Respond quickly to new questions Prioritise future research Develop new hypotheses Meets funders requirements * Need for open data (e.g. access to existing data sets declined) * Work unrestricted by UK academic funding landscape To inform policy Inform policy-making Generate data at spatial and temporal scales required for policy- making Report against targets Help justify conservation initiatives at a political level Geoghegan et al. 2016. 53 Science Policy Practice * Develop a sustainable solution for monitoring * Fulfil specific evidence need To inform conservation and land management Generate data needed to inform management Generate understanding needed to inform management Supply the right people with data Enable rapid decision-making at a local level To educate Educate participants about the topic of the project Improve scientific literacy of participants * Connect people with nature (e.g. cultural ecosystem services, harness people’s love of nature) To raise awareness and engage people Raise awareness of issues Instil a sense of shared ownership, responsibility and stewardship Change specific behaviour of the public e.g. to help control spread of invasive species Help the public make informed decisions Empower and build capacity in local communities to make a change at a local level Inspire activism and spreading of messages through communities To improve buy in Get buy in from the public for policies and management * Facilitate involvement in surveillance in a managed way To build partnerships and improve communication Building partnerships between multiple stakeholders Share data Help with decision-making Foster cooperation Build trust and transparency Democratisation of science and policy and land management decision-making Geoghegan et al. 2016. 54 Science Policy Practice * Engage with external audiences interested in institution * Meet charitable objectives (including education and communication) * To gain personal satisfaction * Enjoy your work * Personal commitment/enthusiasm (e.g. counter to institutional disapproval, religious faith) * Equity and self-determination for participants * Fulfil career objectives, ambitions, building on previous education * Generate impact for people’s lives * Work with unpaid experts and harness their enthusiasm for science Table 33 Stakeholder motivations Geoghegan et al. 2016. 55 We outline below the additional stakeholder motivations identified for each motivation category: To contribute to science As well as the motivations identified from the literature, scientists in university, monitoring, policy and education fields using citizen data (as well as some policy/evidence and practice stakeholders) detailed two new motivations: (1) the need to be able to access data that was currently inaccessible to them – as one scientist explained “Sometimes I\\'ve asked for data from the organisation that I\\'ve contributed towards, and it’s been declined” (Scientist, university). This prompted them to collect their own data via citizen science; and (2) citizen science offered access to a new funding landscape in the UK for academia that was less restrictive: “So when those research projects ended we were still getting people approaching us and saying, we’d like to do such and such, and we just saw that as an opportunity to not be quite so restricted by a funding, a research funding programme where you’ve obviously got deliverables, you need to make sure you tick off all the tick boxes but would enable us to be a bit more flexible in the way in which we worked with people based on their interests and needs. So that’s kind of how it came about” (Practitioner, community). To inform policy Policy/evidence stakeholders, as well as some scientists (particularly those involved in monitoring and policy), held similar motivations for being involved in citizen science. However, two further motivations emerged from our interviews: (1) citizen science offers a sustainable solution for monitoring activity that is a policy priority – as one respondent explained: “Well, it offers a sustainable solution for monitoring. I’m not sure that we’ve ever described it as something that we rely on. It’s part of our programme of monitoring. So I think that it’s quite hard to categorise our strategy on monitoring but it’s probably a combination of utilising best available new technology whilst also using partnerships. So very often we’ve got citizen science doing this kind of stuff that you just can’t do through satellites or remote sensing. So I think that’s, so if I had to characterise it, it’s adding value to our on going professional funded survey” (Scientist, policy); and (2) citizen science allows stakeholders the opportunity to fulfil specific evidence needs, although as indicated in Chapter 8 (Barriers and challenges for professional participation), it is clear that this is also a challenge as some data needs can be too specialist/niche for citizen science approaches. To educate Education has always been an important motivation for citizen science. Interestingly our survey revealed a new motivation orientated less towards a ‘public understanding of science’ model of education (whereby the public simply need to be supplied with the ‘correct’ information), and more towards the connection of people with nature: “we’re connecting people with nature. But we do that through, engag[ing people] with our surveys rather than trying to do it as a standalone thing and also there’s the fact that the more we engage with people and the more they learn, the more, obviously the more benefit they get out of it, but also the more benefit we get out of it. So … it’s a win, win situation for us to invest that time in training and in developing people’s interest and moving them on from maybe quite a simple survey to a more complex one as they grow in confidence and as they learn more about … identification or whatever it might happen to be” (Practice, engagement, science). Scientists who have traditionally been linked to a preoccupation with ‘data’ also acknowledged the importance of citizen science to education, as well as harnessing people’s love of nature To improve buy in In the academic literature, the stakeholder motivation of ‘buy in’ related to the acceptance of policy decisions by the public. Our survey identified an additional motivation to increase public involvement in surveillance and its associated science and policy activities. This motivation links to the mention of civil society as a key stakeholder group for the Geoghegan et al. 2016. 56 implementation of surveillance and monitoring: “it’s about helping support people to provide information and to help undertake surveillance activities in the right way.” (Policy, evidence). Importantly, one practitioner indicated the changing role of citizen scientists as a result of emerging pest and disease threats: “the benefits are definitely to have eyes and ears on the ground in a way that isn’t possible in any other affordable way for us” (Practitioner, science, engagement). To build partnerships and improve communication A new sub-category of motivation emerged around institutional publicity as a result of interviews with practitioner stakeholders who are often heavily involved in the communication of the project and the recruitment of volunteers. Two motivations were identified: (1) citizen science as a means of engaging with external audiences who might be interested in the institution/organisation; and (2) meeting the charitable objectives of the organisation, linked, for example, with education, engagement and communication. As one practitioner recounted: “Obviously below that there are a huge number of other benefits and reasons for engaging with people, … one of our charitable objectives is all about education and improving people’s experiences with wildlife and with nature and, because we’re connecting people with nature” (Practice, engagement, science). Interestingly, one of the scientists who is not currently using citizen science, but is considering how to adopt the approach in their research, indicated that citizen science has the potential to raise the profile of the institution and research group. From the new motivations identified here, it is possible to discern a shift in the motivations being identified. As citizen science has become more mainstream, although by no means universally accepted, the motivational range has expanded to incorporate additional ways in which participants and stakeholders might benefit from citizen science which are not purely attributed to data collection. Indeed, they incorporate intrinsic motivations similar to those identified in Chapter 4. As a result of the growth of the sector, and to a certain extent our decision to make sense of both institutional and personal stakeholder motivations, a new category has emerged, namely ‘To gain personal satisfaction’. In the next section, we cover this category in more detail. 6.2.1 To gain personal satisfaction The literature search revealed only one motivation on the theme of personal gain which related to helping scientists fulfil funder obligations. Our interviews asked stakeholders to consider what motivated both their institutional and personal involvement in citizen science. It was important to consider personal motivations as progress may be impeded within an organisation because of the motivation of a staff member. As a result we were able to discern six new motivations that we outline below in more detail: (1) Enjoy your work A key motivator for scientists and practitioners was the way in which citizen science enabled them to enjoy their work. As one scientist explained when talking about their involvement in citizen science as a career: “There’s always a degree of serendipity as to what roles are available to you … in my young and idealistic days … I was motivated as an ecologist who wanted to be working in nature conservation. I wanted to understand how things worked or why they didn’t work, how we could make it better. And I still have that, but … in terms of choosing a career path, it’s what’s available to you as much as how you create that career path. I’ve been exceedingly lucky in that I have remained in the field that I trained for and been able to have some small influence on the way that it works within my organisation. So it’s very rewarding in that sense, being able to work, as I say, in something that you were motivated to do when you were young and ideal” (Scientist, monitoring, policy). Another said: “… for me, it’s stuff that I do as a hobby and it’s a way of melding my work, which I’m not sure whether is a good or a bad thing, but I think that’s a lot of my motivation for it” (Scientist, monitoring, policy). One of our science respondents who is not yet using citizen Geoghegan et al. 2016. 57 science suggested that “The things I really like about my job are where I have an enabling, personal and career development role with my colleagues. I can imagine that involvement with participants might be like that. Citizen science could be part of general enjoyment and benefit from life” (Scientist, not using). (2) Personal commitment/enthusiasm Scientists can be motivated by a range of personal interests, from an enthusiasm and care for science and data to religious beliefs and strong commitment to justice, often drawing on their own experiences as citizen scientists and volunteers. The following interview extracts reveal these significant and varying commitments: o “So, if you like there is a religious motivation to do it and that’s just partially driven by the fact that I fundamentally believe in universal education, and I don’t believe that it’s just a preserve of the privileged” (Scientist, university). o “I think there is a real disconnect between science and how the world is run really. And I think that for a lot of academics they don’t really feel that they need to explain what they are doing, despite the fact that what they’re doing is essentially paid for by member[s] of the public in the first place” (Scientist, university). o “Well I’ve had a chequered past. In my first life I was [a management job], which looking at my bank account now maybe I can regret it from a financial point of view that I’m not still doing that, but I think for me, having spent years where you, it’s about making rich people richer, I really wanted to have a job and do something which actually had a positive impact on people and the environment, and making our world sustainable in some degree of sense, whether that be from an environmental point of view, a social justice point of view. So that’s kind of been my drive … irrespective of the fact that sometimes, yeah, all of the unsociable hours and the fact that Saturdays are not really the weekend depending on what you’re working on. I think that what enables you to continue to do it is the fact that actually it’s doing some good somewhere along the line, and it’s for the betterment, and I think that that for me is key” (Practitioner, community). o “Oh, I guess on the personal side it was just being involved … I’m particularly interested in [insects], and so the [national society], which I became a member of and contributed to some of their recording. At that stage, it was in the old days with bits of paper and producing atlases and the like. So I’ve been involved at that personal level. And then through my work, over many years I’ve worked alongside some of the smaller schemes and societies in previous roles. [In my first job], I was working alongside other, often citizen scientists. And many of the citizen scientists I was working with at that stage, and still am, to be honest, were, are experts in their field, and the only reason that they’re not called professionals is they’re not paid for it. So it’s been a very long relationship, and a very fruitful one, I find” (Scientist, policy). o “I was aware that as a keen volunteer, I was providing data that was going to end up on a national database. And I think for me there was very much that feeling that I was just contributing to something that was going to better help us understand the [species] population, but there was no specific goal” (Scientist, university). (3) Equity and self-determination for participation Linked to the motivations surrounding ‘To raise awareness and engage people’ identified in the literature, for some scientists and practitioners a key motivation is being able to facilitate the participation of civil society in science and decision-making. As one scientist explained, it is about empowerment and actually giving people the tools to engage with. Another Geoghegan et al. 2016. 58 mentioned: “for me I think the key is to get the kids involved. I quite enjoy that aspect of it because it gets them a chance to think about it and to improve their own environment and that, to me, is the driver for me” (Science, education). Another talked about a personal commitment to scientific literacy: “I feel really strongly about the scientific literacy agenda. I think if members of the public understand science … and the process of research, they’re more likely to be, think it’s something that’s important even if they don’t want to do it themselves and I think make decisions that are based on research if they’re able to understand the whole context. … The more we can talk about it the more we can just interest people and let them know that their opinion is valid. I think that’s always going to be a good thing” (Practitioner, science, engagement) (4) Fulfil career objectives, ambitions, building on previous education Surprisingly the existing literature has not identified ‘career’ as a motivator, however, with the rise of citizen science and the value now placed on involving the public in science, it has become a key motivator. Several respondents indicated how their involvement in citizen science had helped them to secure new appointments, and fulfil their career ambitions of combining educational qualification in natural science disciplines with considerable experience of working with volunteers. (5) Generate impact for people’s lives It is widely acknowledged that citizen science empowers individuals and communities (Davies et al. 2011). This is a key motivator for scientists and practitioners, particularly for continued involvement and examples include: o impact on quality of life: “since we are an … organisation that’s funded by public money our work ought to have implications as well as impact on improving the life of, the quality of life for the general public. So in the end of the day generating impact in a positive sense and improving public health or contributing data and information for people to better understand what’s happening in the environment and how it affects their life and their wellbeing it’s, for me is a central part of the kind of work that we do” (Scientist, university). o changing people’s lives through participation: “I think when you look back and you see that it could be the smallest of things where, I remember one lady who got involved with [our projects] in a really run down estate …, and she had a number of health issues. But after participating in both of those programmes she said that the whole process has made her realise that she can do things and she knows more than she thought she knew, and now she’s going to go and do a course, I think it was a management course … but she hadn’t done anything other than basic tertiary education and had been out of education for like 25 or something years. So for her, just that being involved in what she felt was a kind of scientific process empowered her to feel that she had the ability to go on and further her education or attainment. So that’s kind of one degree” (Practitioner, community). (6) Work with unpaid experts and harness their enthusiasm for science This motivation has been located here within the category of personal satisfaction. However, it could, we suggest, also form a motivation under ‘science’. Citizen science is changing scientific practice for those involved, both professionals and volunteers. Working with and acknowledging the considerable expertise of unpaid experts is a motivation for scientists and policy/evidence stakeholders, who are keen to harness this enthusiasm. As one scientist explained, in their organisation, they are: “going to those who are already interested, by their own, they’re self starters, they are naturalists, you can find a lot out” (Scientist, monitoring, policy). Followed by a stakeholder who explained: “And also there was a need to, we know that people are passionate about [species], they want to, would they want to help too?” (Policy, evidence). Geoghegan et al. 2016. 59 6.3 Relative importance of motivations by stakeholder type Whilst there is significant overlap in a number of areas regarding stakeholder motivations, it is possible to identify the following preoccupations (see Table 34). \\uf0a7 Practitioners are motivated by all aspects/benefits of citizen science, often undertaking roles ranging from recruiting volunteers, survey design and data analysis to advocacy, communication and meeting institutional objectives. \\uf0a7 Policy/evidence stakeholders are interested in robust evidence to inform policy and decision-making. For this group, personal satisfaction relates to harnessing enthusiasm for science, engaging with unpaid experts and fulfilling career interests in environment, conservation and policy. \\uf0a7 Scientists only using citizen collected data are motivated by its potential to inform their science and allow them to fulfil a specific evidence need. For this group, personal satisfaction relates to using citizen data in decision-making. \\uf0a7 Scientists are motivated by all sub-groups. Practice Policy/ Evidence Science /not using/data only Science To contribute to science To inform policy To inform conservation and land management To educate To raise awareness and engage people To improve buy in To building partnerships and improve communication To gain personal satisfaction Table 34 Importance of motivations by stakeholder type 6.4 How stakeholder motivations have changed over time It is also important to consider how stakeholder motivations have altered over time, focussing on institutional motivations in particular. Whilst few were able to articulate this directly, it was possible from the material to draw out how their citizen science projects or institutional engagement with citizen science had changed over time (see Table 35). Driver of change Result Stakeholder Type Institutional commitment: institutions of all kinds are recognising the value of citizen science to not only scientific endeavour but also their public profile: “Initially there was a lot of discussion about whether we should communicate with the general public at all, … there’s been a big sea change in that through our organisation, and I think people are also recognising that there’s a benefit from a reputational point of view” (Scientist, monitoring, policy). Furthermore many organisations have a long term relationship with volunteers (50 years or more in some cases) Increased buy-in from high-level institutional strategy Scientist/university; Scientist/monitoring/ policy; Practitioner/science/e ngagement Technological innovation: Use of computers, websites, apps, regular technological updates; move Improved engagement and data Practitioner; Policy/Evidence; Geoghegan et al. 2016. 60 from data submission on paper to online. Technology has improved data flow, better feedback to volunteers, e.g. “In particular things in the same period the quality and the availability of low cost sensors has become more viable because it’s not that long ago when measuring air quality required something above tens of thousands of pounds at least to get anything reliable out” (Science, university) Scientist Bottom up rather than top down: People are more aware of citizen science initiatives, projects being approached by citizens for assistance Citizen science is becoming collaborative and co- designed Practitioner/communi ty Data issues giving way to awareness and behavioural change: Whilst projects remain committed to collecting more data of better quality, participant engagement has become increasingly important Move towards awareness raising and behavioural change Scientist/education; Scientist/policy Saturation point: concerns are emerging surround recruiting enough participants and the idea that there is a limited pool to recruit from when projects demand a quality assured dataset Suspicion around next generation of participants and skill levels Policy/evidence Media interest and public profile: citizen science has become a popular media item and is being used by organisations to increase the number of participants Use of media to encourage participation in citizen science Practitioner; Science/engagement Increased sophistication: citizen science has evolved to allow for: increased numbers of taxa and species: “[We started with…] where do you find all your interesting dragonflies or where’s the nice place for water beetles, things like that, that very basic level of trying to understand where things are before we could even begin to think about how you might conserve them or the relation to what academic researchers needed” (Science, monitoring, policy); and more challenging research questions: “they have become more specific and in some cases looked at trends that have been noticed by participants” (Science, hypothesis-led) Asking more challenging questions Scientist/university; Scientist/monitoring; Scientist/policy From mass participation to targeted surveillance: citizen science is now being viewed as an approach to surveillance and citizens as early-warning systems. Whilst this approach is still in its infancy, there is increased attention on a move away from simple engagement/mass participation. Training citizen scientists as early- warning systems Policy/evidence; Practitioner, science, engagement Table 35 Changes in stakeholder motivations over time It is possible to suggest that: once institutions started using citizen science they largely remained involved; technological advances have enhanced data collection and the experience for the participant; there is a slow move from largely contributory projects towards those involving co-design, with linked changes relating to a shift away from purely data collection to engaging people and improved communication and media interest; there remains a common misconception that projects will run out of participants, however the growth of Geoghegan et al. 2016. 61 citizen science indicates this is not the case, and that projects are diversifying in order to attract new audiences; and finally, citizen science has evolved in relation to the sophistication of questions asked by projects and the increasing specialisation of participation, for example citizens acting as early-warning systems. Geoghegan et al. 2016. 62 Chapter 7: Matching participant and stakeholder motivations Chapter highlights \\uf02a Recognition of the motivations of participants by stakeholders and incorporating this into the design of projects can increase participation \\uf02a In some instances partial fulfilment of motivations may be enough to ensure a contributory project satisfies both participants and stakeholders \\uf02a In a more co-designed approach to citizen science requires stakeholders to take a particular interest in the impact of the project on the participants themselves We are unaware of any studies that look explicitly at whether volunteer motivations match stakeholder expectations or motivations. Only a few studies have explicitly stated the motivations of the project initiator or gathered information about the motivations of participants, and so any attempt to match motivations is difficult to test from the existing literature. However, recognition of the motivations of participants by stakeholders and incorporating this into the design of projects can increase participation e.g. eBird (Sullivan et al. 2009). In this chapter, we consider the challenge of matching motivations and offer some scenarios. 7.1 Variations in citizen scientist motivation by project type Table 36 (below) displays the primary motivations of citizen science survey respondents by citizen science-type. Whilst the sample size is small, this breakdown reveals that participants in different types of project report different motivations. ‘Recording’ refers to traditional biological recording type activity’; ‘Science-led’ to hypothesis driven research; ‘Surveillance’ to early warning and detection; and other to other types of activity such as citizen panels on local environmental issues. ‘To help wildlife in general’ remains the most important motivation for the first three types, with ‘To contribute to scientific knowledge’ second most important to the first three types. The ‘Other’ category, while having smaller number of respondents, gives contributing to scientific knowledge greater importance. Participants in surveillance projects are more likely to be motivated by helping a specific site than participants in other types of projects. Primary motivation Number of citizen science respondents (% of those who responded) Recording Science-led Surveillance Other To help wildlife in general 51 (55%) 9 (64%) 3 (50%) 3 (21%) To help a specific site 6 (6%) 0 1 (17%) 0 To contribute to scientific knowledge 24 (26%) 3 (21%) 2 (33%) 8 (57%) To meet people/for fun 0 0 0 0 To learn something new 3 (3%) 1 (7%) 0 0 To spend more time outdoors 3 (3%) 0 0 0 To get some exercise 1 (1%) 0 0 0 To share my knowledge and experience 0 1 (7%) 0 1 (7%) Someone wanted me to do it (e.g. family, teacher) 3 (3%) 0 0 1 (7%) To develop new skills 0 0 0 0 To help my future career 0 0 0 1 (7%) Other 2 (2%) 0 0 0 Table 36 Primary motivations for citizen science by respondent type Geoghegan et al. 2016. 63 Equally, motivations for stakeholders involved in different types of activity also varied (see Table 33). Just taking the scientists, the purpose of their involvement reveals that motivations vary in focus, but also in range. 7.2 Recognition of motivations In some types of project the motivations of the stakeholders involved and the motivations of participants are aligned in a way that makes it easier to create a project that is able to meet its objectives, for example co-designed citizen science. Of course, the motivations of stakeholders and participants do not need to be completely aligned, and it is possible that meeting some of the participants’ key motivations may be enough to result in an effective project. It is also possible that the participant’s and stakeholder’s motivations do not ‘match’ but that the motivations that are held result serendipitously in the data that is needed being collected. Drawing on the data from this study, it is worth considering some scenarios of projects with different objectives to understand which motivations might be important and how other motivations might be taken into account. The first scenario (Figure 4 left) shows a typical contributory/recording type project with the primary motivations of scientists involved in monitoring (in blue) and the primary motivations of participants involved in recording (in orange), the size of the bubble represents the importance of the motivation. In this scenario, the most important motivations for both the stakeholders and participants are met. There are some lesser motivations which were not matched, which would need to be sufficiently recognised and satisfied in the design and conduct of the project to make it appealing to the participant and to meet the stakeholder’s needs. This scenario underlines the importance of feedback to participants to show how their contribution is helping wildlife and aiding scientific understanding. Figure 4 (right) shows a scenario of a project where the participant has an immediate interest in and use for the data, involving varying levels of collaboration and co-design. Motivations for scientists involved in education are shown in blue, and participants with an interest either a particular site, or in impact on well-being in orange. Again, the major motivations of both stakeholders and participants are met, but this relies on the stakeholders having a particular interest in the impact of the project on the participants themselves. Figure 4 Recognition of motivations scenarios Involving participants at an early stage in project development will help to develop a shared understanding of both the participants and stakeholders motivations and to recognise where these motivations can and cannot be met within a project. Geoghegan et al. 2016. 64 Chapter 8: Barriers and challenges for stakeholder involvement in citizen science Chapter highlights \\uf02a Existing literature on stakeholder barriers and challenges is confined to scientific and land manager communities, and identifying: data quality and biases; peer review and mistrust of citizen generated data; the need for specialist equipment or knowledge; time and resourcing issues; and lack of skills for working with the public; and the potential for political ramifications \\uf02a Interviews revealed ‘mobilising and maintaining citizen science projects’ (related to funding and time) as a common barrier across stakeholder groups \\uf02a Interviews revealed a distinction between science and policy/evidence stakeholder barriers around data quality and biases and practitioner barriers relating to survey design and over reliance on technology \\uf02a Citizen science must be promoted at a high level within institutions to maintain profile and resourcing In this chapter, we present the findings of our literature search relating to the barriers preventing the participation of organisations, professionals and practitioners in citizen science. Whilst it was possible to identify a number of barriers relating predominantly to scientists, our interview responses have enabled us to carry out a more thorough assessment of the barriers of other stakeholder groupings, including scientists, policy/evidence specialists and practitioners. 8.1 Existing work on barriers for stakeholder participation The existing literature on barriers for stakeholder participation is largely confined to scientific and land management communities, however, it is possible to discern the following barriers: (1) Data quality and biases Data quality and biases in data are the most frequently cited reservation about the citizen science approach (e.g. Riesch & Potter 2014) and a large body of literature has built up around these issues and how to overcome them, particularly in relation to observational data (e.g. Tulloch et al. 2013, Isaac et al. 2014). In some cases this may also relate to the equipment used in citizen science projects; for example, the quality of data collected by low- cost sensors can be low (Sirbu et al. 2015). These issues mean that scientists, policy-makers and land managers can be mistrustful of data collected by non-professionals (Gollan et al. 2012, Kaartinen et al. 2013, Bird et al. 2014, Lucky et al. 2014, Fuccillo et al. 2015, Hyder 2015), and particularly by non-naturalist volunteers (Couvet & Prevot 2014). In a land- management context, for example, inaccurate data on habitat quality could result in further funding being spent unnecessarily, or habitat restoration being declared a success in error (Gollan et al. 2012). It should be noted that there is often a failure to recognise errors, biases and uncertainty in data collected by professional scientists (Bird et al. 2014). Furthermore, concerns have been raised about the motivations of citizens and their potential biases which might influence their activity in citizen science projects (Nature 2015), without recognising that professional scientists also hold their own motivations and biases (Haklay 2015b). (2) Peer review/mistrust Mistrust of citizen science data also extends to peer-reviewers (Bahls 2015) which may in part explain why there are more citizen science projects than publications about citizen science (Gardiner et al. 2012) or why scientists may not advertise the origins of the data Geoghegan et al. 2016. 65 included in publications (Theobald et al. 2015). Some scientists may, therefore, be reluctant to get involved in citizen science because they are unsure if their results will get published (Riesch & Potter 2014). Related to this is the concern that engaging with the public will detract from spending time on writing papers and acquiring grant income, i.e. the metrics by which academics are often judged (McKinley et al. 2013). (3) Requirement of specialist equipment/knowledge In some cases, the reason for not using a citizen science approach is that some scientific questions need to be answered using specialist equipment or analytical or data collection skills which require extensive training (Arvanitidis et al. 2011, McKinley et al. 2013, Danielsen et al. 2014, Hyder 2015), although some authors note that volunteers could still be involved, for example, in collecting samples (Mackechnie et al. 2011). For tasks that require regular and frequent input from participants, difficulties with recruitment and commitment of volunteers may be a barrier to achieving the initial scientific aims (Riesch & Potter 2014). Furthermore, inaccessibility of areas may be a barrier, for example, most people do not live near the coast which is problematic for engaging people in marine citizen science (Hyder 2015). In some cases, the scale of citizen science projects may be a barrier to using its data. For example, in the context of marine conservation, there are a lot of local projects which are not particularly useful for policy-making (Hyder 2015). (4) Time consuming and resourcing issues There may also be issues related to resourcing: efforts to mobilise and maintain large citizen science or community-based initiatives can be costly and time-consuming (Danielsen et al. 2014). In addition, there may also be concern about volunteers doing the jobs of professionals and the consequences of this for job opportunities or security of professionals (Ferster et al. 2013, Riesch & Potter 2014). (5) Politics Land managers have raised concerns that distributing data without professional interpretation of results may lead to unrealistic or poorly-informed demands by the public for particular actions (Ferster et al. 2013). Concerns have also been raised about liability; for example, in the context of wildfire management, could project organisers be held liable if volunteered assessments of wildfire risk led to the decision not to treat an area where a wildfire then occurred (Ferster et al. 2013)? (6) Uncomfortable/unprepared to work with the public Finally, it should be noted that some scientists may feel uncomfortable or unprepared for engaging with the public (McKinley et al. 2013, Varner 2014). 8.2 Barriers and challenges by stakeholder type In what follows, we extend the barriers summarised above to incorporate additional barriers and/or challenges identified by other stakeholder groups (see Table 37 – containing examples and stakeholder quotes). We asked our interviewees to identify the challenges they faced in relation to participation in citizen science. Unsurprisingly new barriers emerged as a result of talking to policy and practice communities. Geoghegan et al. 2016. 66 Table 37: Barriers and challenges to stakeholder participation in citizen science Over-arching theme (* indicates newly identified) Barriers or challenges (* indicates newly identified) Examples from stakeholder interviews S ci en ti st /u n iv er si ty S ci en ti st /m o n it o ri n g S ci en ti st /p o li cy S ci en ti st /e d u ca ti o n S ci en ti st /n o t u si n g /d at a o n ly P o li cy /e v id en ce P ra ct it io n er /s ci en ce /e n g ag em en t P ra ct it io n er /c o m m u n it y P ra ct it io n er /e d u ca ti o n Science Policy Practice Data quality and biases Inadequate equipment (e.g. low quality sensors) Mistrust data from non- professionals Biases influencing decision to participate * Scalability of data Quality of data is not at a level for use on a wide scale * Partnerships with local authorities Local authorities can lack manpower to assist * Patchiness of data Statistical techniques available to even out patchiness * Specific evidence need beyond the scope of citizen science “it wasn’t our direct need, and with only, with low resources that wasn’t our priority” (Scientist, monitoring, policy) Geoghegan et al. 2016. 67 Peer review/mistrust Peer reviewer reservations during publication process * Citizen science frowned upon by colleagues Scientist (university) told: “should be doing proper science”. Another explained: “We as scientists are often a bit snobbish about our ability in comparison to other people’s ability, and you see that in the response to citizen science from a lot of the policymaking community. Citizen science equals poor data, that’s their starting point” (Scientist, monitoring, policy). *Institutional reservations about citizen science Need to get board members on side Requirement of specialist equipment/knowl edge Training required Scientist indicates specialist training is required due to challenges of identification: “Gone are the days where you used to have a huge visible injury on vegetation has gone, because there’s been acute exposure to everything.” (Scientist, policy, monitoring) Difficulties of recruitment and commitment of volunteers “And then continued engagement, the enthusiasm barrier, because you get a drop off, an exponential drop off of participation as time goes on. So how do you keep the exponential drop off … as low as possible?” (Practitioner, science, engagement) Inaccessible sites Linked to patchiness of data, “It’s difficult to tell people to go to a site that they think will be rubbish as well, I think if you want to see lots of dragonflies you go to a good dragonfly site rather than just anywhere” (Scientist, policy, monitoring) * Unable to keep up with technological developments Once technology is in place, it must be maintained and updated Geoghegan et al. 2016. 68 * Getting people to use technology “a survey that started off on paper, it’s actually very hard to move people over” (Practitioner, science, engagement) * Crowded marketplace for citizen science projects in certain areas Technology and online data options are flooding the market with similar citizen science projects Time consuming Resourcing issues * Promoting citizen science Launching apps requires time and resources for promotion and maintenance * Communication No time to explain key scientific ideas (e.g. recording absence is as important as presence) * Slow process Policy want answers yesterday * Individual interactions Individual requests for support are time- consuming Mobilising and maintaining citizen science project Time spent validating, verifying, selecting appropriate technology, calibrating sensors; lack of funding, short-termism of funding, unable to prove concept, no funding for essential technical development Volunteers threaten job opportunities/security of professionals Lack of interpretation may lead to poorly-informed public demands Politics Liability of organisers if don\\'t act on citizen data * Unaware using citizen science data Scientists often use published data sets unaware that they are citizen science data * Nobody championing citizen science on high level Projects should identify someone high level to champion their project institutionally * Differing science and engagement objectives “you have to decide where you sit along the spectrum for the mass engagement versus data quality question” (Practitioner, science, engagement) Geoghegan et al. 2016. 69 * Activities require legislatory approval Challenges when species become scientific instruments. Legislated by Home Office Lack of interest in engaging the public through citizen science Uncomfortable/un prepared to work with the public * Lack of attention to needs and expectations of the citizen science audience Must understand demands on participant’s time and project’s requirements may differ * Unaware of audience * Little acknowledge of different types of volunteer Differing volunteer types, e.g. paper-based volunteer, don’t want to lose them due to quality * Need more volunteers There aren’t enough volunteers participating * Survey design by committee, by professionals only Too much discussion of small issues, but decision has to eventually be taken * Survey design and implementation issues * Lack of clear research question Avoid “reverse engineering to a question”, instead “[be] led by a question” (Practitioner, science, engagement) * Survey is inaccessible Participants must be able to understand questions * Language barrier (scientific and linguistic) Avoid over-complication * Assumption that people have access to the internet and to a mobile phone Not everyone has a mobile phone on a data plan, nor access to the internet * Assumption that people are comfortable with technology Technological literacy should never be assumed * Over-reliance on web-based solutions Web-based solutions can be exclusionary * Designed a ‘boring’, yet scientifically important, survey “Well, it could be that it’s a really … important square, and when you go there and count your … your butterflies, you might only, you might see none, you might see one. And that’s a really boring day out” (Scientist, policy, monitoring) Table 37 Barriers and challenges to stakeholder participation in citizen science Geoghegan et al. 2016. 70 Table 37 highlights some interesting findings. \\uf0a7 Data quality and biases Whilst the majority of our scientist respondents were already using citizen science, those who were not or were only using citizen data remained interested in questions of data quality. However, questions from users have progressed to consider how citizen data might be up- scaled and used in other ways. Scientists indicated some of the challenges of collaborating with local authorities on scientific endeavours and the challenges of lack of resource. In addition, a key barrier for those scientists working in government agencies is the need for data to answer very specific evidence needs, which fall beyond the scope of a citizen science approach. This barrier is also linked to requirement for specialist equipment and knowledge \\uf0a7 Peer review/mistrust As indicated above, the issues identified by this study do not relate to data quality per se, and instead highlight how citizen science is perceived by colleagues and institutions. For example, citizen science has yet to be fully accepted as a scientific approach and it is often difficult to get senior management in institutions to buy into citizen science. \\uf0a7 Requirement of specialist equipment/knowledge Training remains a concern for scientists, particularly as the issues being examined through citizen science increase in complexity. Scientists also appreciate the challenge of asking someone to visit a site only to identify that a species is absent. This category has, as a result of our study also been extended to include challenges relating to technology and keeping up to date, as well as encouraging participants to use online submission forms. Furthermore, as practitioners and scientists indicated, technology has enabled the increase in citizen science projects and there is now increasing amounts of competition in the marketplace for citizen science collected data. \\uf0a7 Time consuming Citizen science is not free science (Pocock et al. 2014). It is time consuming and requires resourcing (discussed further below in point (5)). Scientists and policy respondents indicated the time demands when initiating and maintaining a project, the need for promotion of activities and communication of key scientific ideas, as well as the often slow process of data collection and analysis (a particular concern for policy/evidence respondents). Further calls on time stem from individual participants requesting one-to-one support from a scientist. Resourcing was identified by ALL respondents as a key barrier to mobilising and maintaining a citizen science project. Scientists were concerned about a lack of time and funding, particularly for maintaining the longevity of the project. Policy colleagues indicated the challenges around increased partnership working associated with reduced funding. Practitioners were particularly concerned with issues of funding for specific technical roles (e.g. website development), the need to prove the value of citizen science, and, in common with other respondents, the funding short-termism related to citizen science, and science projects in general. \\uf0a7 Politics Interestingly, none of our stakeholders referred to ‘Lack of interpretation may lead to poorly- informed public demands’ or ‘Liability of organisers if don\\'t act on citizen data’. However, in other sections of this report, it is acknowledged that these are important concerns to consider when a key benefit and motivation for citizen science is to increase public involvement with decision-making (see Chapter 6). Furthermore, this study has broadened the ‘politics’ discussed in relation to citizen science barriers. One scientist respondent working with citizen data, but not involved in citizen science per se, indicated that one challenge is that scientists often don’t realise that they are working with citizen science data, and suggested that more work could be done to raise the profile of such data. Furthermore, one practitioner indicated Geoghegan et al. 2016. 71 that without buy in from senior colleagues, projects may not secure the funding they require, and another noted that projects must be clear on their aims and objectives: to collect data for science and/or to engage the public. Related to politics is the need in some instances for legislator approval from the Home Office to license the use of animals as scientific subjects in a citizen science project. \\uf0a7 Uncomfortable/unprepared to work with the public None of our respondents have mentioned this as an issue, although several did indicate that currently their roles were not public-facing, and recognised there was more work to be done in this area. \\uf0a7 Unaware of audience It was suggested by policy/evidence stakeholders and practitioners that there was often a lack of awareness of the audience for the citizen science project, i.e. stakeholders lacked information about the target audiences, including their motivations. \\uf0a7 Survey design and implementation issues Practitioners we interviewed were very concerned about barriers and challenges surrounding survey design and implementation. Whilst partnership working and survey design by groups of stakeholders are increasingly common in citizen science, particularly as this lends strength to the end product, there can be much debate on what to include that can lead to delays in survey design. A number of respondents indicated the importance of a strong idea of what ‘science’ question the citizen science project was contributing to from the outset. 8.3 Change in barriers over time As indicated in Chapter 3, we interviewed stakeholders who had been involved in citizen science for a range of time periods. Those more experienced stakeholders offered the following advice in response to some of the barriers/challenges identified in the literature, and how, what might have been barriers in the past, are not necessarily challenges now. Data quality and data biases Rather than relying upon known analytical approaches, stakeholders should consider alternative ways to use the data; if the way that the data has been collected is not ideal, there may be other ways to use it: “Even if you could communicate that zeros are important, it still doesn’t take away that fact that it\\'s not very interesting for the volunteer. … there are statistical techniques that you can use to even out patchiness of recording” (Scientist, university, data only) Peer review/mistrust and politics Issues relating to buy in from colleagues and respect for citizen science activities often involve being able to influence colleagues and institutional objectives. One practitioner who has been involved in a long-term citizen science initiative indicated: “OK. I think it’s really important that your project has a high profile internally and I think you have to keep plugging away at that. So whether there are key individuals that you realise you need to get on board, you need to invite them to a volunteer day, you need sit down with them and share some results, or if you’ve had a brilliant press moment, share that. So you can be strategic, you can pick off the few people that you feel really need to get it and invest time in them. There’s also the broader brush approach, most organisations have internal communications, whether that’s a staff email or a noticeboard or whatever, but just Geoghegan et al. 2016. 72 to keep plugging away. Every time you have a success or you find something interesting, make sure you share it. And, obviously, being a strong personal ambassador for that project in whatever way. So I think it’s a drip feeding approach, just keep showing value. And, also, to use the language that that particular person will recognise is very, very important. So if you, if they are particularly inspired by resilience or, I don’t know, media opportunities to see or email sign ups, whatever it is that inspires them, that’s the language you need to talk in, I think. I’ve found that over the years.” (Practitioner, science, engagement) A scientist stakeholder indicated that universities were now very keen on citizen science as a result of the metrics used to measure research quality: “For [our citizen science project], what really helped us was that it was held up … as an exemplar of research impact … if research impact is going to define 40% of your funding as a university and citizen science projects have got four star rankings then any university worth their salt I think will sit up and take note.” (Scientist, university) In one instance, a UK university has committed core funding to citizen science: “Some of it of course does, we, we’ve been quite fortunate with [our university] in that they have, with the roles that started out as just funded, externally funded, the [university] has taken on all the core roles, and they’ve been incorporated into the different faculties and units where they’re sitting, because … the projects are not just one faculty or one unit.[...]. So for example, a few years ago my role was made permanent and more centred.[...]our previous vice chancellor was very keen on [our activities], and I remember having to send updates to his office every month or every couple of months because he loved to pull this thing out … and when you have leaders at certain levels who take this on board, that helps, that will help.” (Practitioner, education) Geoghegan et al. 2016. 73 Chapter 9: Technology in citizen science Chapter highlights \\uf02a Technology has transformed the potential of citizen science, making it possible to collect and analyse large quantities of data, and share that data \\uf02a Citizen science participants continue to use a range of online and traditional paper-based technologies \\uf02a Long-term participants can be slow to convert to online recording and data submission and stakeholders need to approach this with sensitivity, particularly as data quality is usually very high \\uf02a Stakeholders must not assume that all citizen scientists have access to technology or are familiar with how to use particular technologies \\uf02a Technology has the potential to increase the number of stakeholders involved in citizen science 9.1 The current place of technology Over the last 20 years or more, technology has transformed the ways in which: (1) research, monitoring and surveillance has been conducted; (2) data are analysed; and most importantly, (3) data can be shared (Haklay 2015a). As one stakeholder noted: “[our project] wouldn’t have worked in 1980, it only works in 2016” (Practitioner, science, engagement). This has had a profound effect on the rise of citizen science and partnership working between individuals, communities, agencies and other organisations, and the ability to work with members of the public in professional science projects, long-term monitoring and community- led science (Roy et al. 2012). There are many ‘digital natives’ who now live their lives via their devices, drawn to things that allow them to use their smartphone. People are receiving information about their geographic location and now want more information, and even want to contribute more. Digitization is not just an easier method to collect data, but also a significant motivator for encouraging people to engage (see also section 4.2 on motivations in online citizen science). As a result, technological innovations have meant that citizen science and the associated activity of biological recording have moved away from the use of: “a piece of paper on which you had a tiny column to fit something in, or going back to your computer and putting things into a relational database. No, it becomes a lot more user friendly, so [technology] makes a difference and it takes away the onerous part of getting data moving. … because we’ve got tech and because we’ve got cunning ways of analysing things, we can get more information than we ever thought we could from a wider group of people but also from a wider set of ways of gathering information” (Scientist, monitoring, policy). Furthermore there is an expectation by some members of the public, and stakeholder institutions, that technology will be integral to any citizen science project: “I think that is the expectation that people have is that how they record is going to be … a lot more digital than it used to be. … I don’t think interest in wildlife in, and nature in general is a barrier that we’ve really come up against, I think there’s a huge amount of interest and people who like watching, even just the birds in their garden, the wildlife that they see around them. … I really don’t think we’ve exploited everyone for data that we could do, … I don’t think it’s like, oh no … there’s, no one else is interested in nature. I think there’s a huge amount of people who are Geoghegan et al. 2016. 74 interested in nature and it’s just finding ways of convincing those people that telling somebody about it, what they see and making it easy for them, is the challenge for us” (Practitioner, science, engagement). Yet, it is important to remember that some of those long-term, expert volunteers are still only using traditional methods for data collection and submission, and it is important not to alienate them: “There are still some people who have to use bits of paper because they just can’t get on with technology, and we have to have support alongside to make sure that those key individuals, or motivated individuals, are still able to feedback without tech” (Scientist, monitoring, policy). This has led many stakeholders to spend considerable time encouraging a digital switchover: “… you get people who are obviously completely computer literate, but because they’d started off recording on paper forms, it’s all about habit forming isn’t it? What people are used to doing they’ll carry on doing, and a survey that starts off online is one thing, but then a survey that started off on paper, it’s actually very hard to move people over. … so I did a lot of prodding people and saying, you’re doing the [survey] online, why are you still doing [the survey] on paper, very nicely obviously, and it’s very important that people don’t get the impression that you’re marginalising them or excluding them, because of the way they prefer to record. So it’s very important for us on a lot of these surveys that we try and accommodate how people like to record, because if people think that you’re not valuing them, or you’re not valuing their data, then they get quite annoyed about it and could quite easily stop doing it, so it’s, we find that taking a bit of a hard line on this tends to not work very well” (Practitioner, science, engagement). In what follows we outline some of the barriers and challenges surrounding technology. 9.2 Challenges surrounding technology Whilst technology has been described as an important factor in the democratisation of knowledge and involvement of increasing numbers of people in science, stakeholders must proceed with caution: “I don’t think we’d be able to do half the stuff we’re interested in doing without the internet or mobile phones and stuff like that, so I think it’s really vital. We understand that this does exclude a proportion of the population from doing it but it’s a very easy way of being able to engage with large numbers of people. So it’s like a … bit of a balancing act between the two really isn’t it?” (Scientist, monitoring, policy). Projects need to be clear on the appropriate technologies to use, their participant’s ability to access those technologies and the associated technological abilities required to achieve a successful outcome (Roy et al. 2012). As one stakeholder respondent suggested: “different people are comfortable less or more with different technologies in play” (Practitioner, community). As a result, project leaders must not assume that all of their participants have access to technology or are comfortable with using it. One of our stakeholders gave this example of technologies in action: “… we use the tools which are appropriate for the people that we’re working with and the overarching objective of what people are trying to achieve here. So just using air quality as an example, the tools to be able to collect data and the methodology, we have it on pen and paper, people can write it down, they’ve got a survey sheet, but we’ve also built a mobile application so that people can do it digitally and it streamlines the process a little bit more if they have that technology and they’re comfortable with using it. The data, in terms of mapping it, we’ve got an Geoghegan et al. 2016. 75 online interactive mapping system whereby communities can put their results on the map, but in the same token we will also use a more traditional GIS and we will map the data on behalf of communities if, again, they don’t feel that they have the capacity or ability to do that themselves. So it’s really thinking about, OK, what is suitable for a specific community in a specific context, what, do we need the pen and paper, do, are we, is it OK to just use mobile application devices? What, yeah, what works in a, in one case will not necessarily work in another, and in some cases you may use a whole arsenal, a whole suite of different technologies to be able to deliver something meaningful” (Practitioner, community). This respondent works with many communities in the UK and beyond, and raised several issues that were not considered by any of our other stakeholder participants. Importantly, stakeholders should not assume that their participants: (1) have access to the internet; (2) have time (e.g. working full-time, single parent); (3) are able to follow strict data collection protocols; (4) have access to a mobile phone data plan; and (5) will be comfortable with web-based solutions. The following excerpt from our interview brings these points home: “Well I mean there are always going to be barriers, and it depends on what tools you’re using as part of the process, but if we’re sending call outs via Twitter and email and our website, you’re already assuming that somebody has access to the internet, that they’re digitally literate, and so forth. So you’ve already lost a segment of the population just in the nature of which, and then you also take things into consideration like time. Certain demographics have more time than others. I\\'m a mother of two but if I\\'m a mother of three and I have two jobs and what have you, I really don’t have the time, or maybe even the inclination to get involved in these kind of things because I\\'m far too busy chasing my tail and just trying to survive. And that’s where I think some of the citizen science, bigger citizen science programmes are really exclusive because the level of, so time, when I say time it’s, some of the protocols, data collection protocols are really quite rigorous, and I think a project that I was involved in when we were trying to collect noise readings and really get space time coverage would require people to go out and commit how much time to actually following this rigorous protocol to make sure that there’s measurements taken in every grid at these different times of the day. So you’re assuming already that people have time. Then you’re assuming that people have a mobile phone data plan whereby they’ve got unlimited data so they can upload and download, and all of these types of things. Then you start to lose people. If … I’ve got a Pay As You Go then I’m not going to be using my Pay As You Go credit to be taking readings and things like that using my mobile phone. So I think, yes, there are barriers. We try, we’re not perfect but we tried to keep things as simple as possible so that they can be inclusive, and we don’t require, we don’t rely solely on web based solutions so that there is still the face to face, paper, pen, pencil ways in which people can get involved” (Practitioner, community). Geoghegan et al. 2016. 76 9.3 Technologies currently used by stakeholders In addition to the technologies involved in communication and feedback, such as websites and email, the following technologies were identified in our stakeholder interviews: Method Benefits and challenges Traditional Pen and Paper (e.g. printed booklets, laminated sheets) Low barrier to entry Free post cost Technological GIS Mapping Geographical precision Data Analysis Programmes (data sharing too) Sophisticated analysis and tailored feedback Apps Identification guide, GPS device, description, photos Development time can be expensive Speed of data submission Easier data collection (e.g. broad scale data across UK, particularly from rural areas often unmonitored) Photographs to allow for improved verification Online data submission Quick, interactive, participants own their data, understand it more, why you are asking particular questions Sensors Facilitates wider public involvement regardless of specialist expertise (see air quality case study below Table 38 Technologies, benefits and challenges for stakeholders 9.4 The rise of sensors: an air quality case study Our stakeholder interviews with those representing science, policy and practice relating to air quality identified the availability of low cost sensors as an important area in which the potential of technology was growing. One practitioner commented: “the technology is advancing with regards to sensor networks, wireless networks and low cost sensors and devices to enable more people to participate in these kind of monitoring exercises, but there is still limitations with regards to the accuracy of some of these devices” (Practitioner, community). Whilst some projects have adopted sensor networks, these are still in the minority and the cost of sensors remains prohibitive for many citizen science initiatives. However, one scientist working in air quality monitoring and policy explained, the benefits go beyond data quality: “there’s a lot more low cost sensors, or the technology for low cost sensors is improving. So I think there’s a huge potential, that there’s this public awareness and concern about air quality, and the technology is such that there could be more monitoring and other opportunities for citizen science in the field of air quality. Not just, you could just think we’ll be much more au fait with this, but there’s the benefits that could bring by getting people involved and more aware. The crossover to ecosystems and, and maybe it’s a slightly different story in terms of biodiversity impacts and human health impacts, you’ve got slightly different exposure routes, etc, but nevertheless if they give us a general awareness of pollution, it will drive policy in the right directions for both human health and biodiversity” (Scientist, data only). Geoghegan et al. 2016. 77 Furthermore, the same participant, who uses citizen collected data, but is not involved in citizen science projects, suggested technology might be one way in which stakeholders may become more involved: “I think it’s actually in terms of kind of technology increasing, and low cost sensors and things and getting better, well as they develop there’s an opportunity for a better understanding of the spatial patterns of pollution exposure. And that’s always useful because we rely on currently quite expensive monitoring done on just a few, relatively few sites across the UK, and haven’t got anything in between, so anything that can help with that would be good. ... Yeah, so that’s kind of the exposure side of it, and then we’re interested in the impacts. The trouble we face is that you can use indicators such as lichens, and hence the example we’ve already discussed, but the impacts are fairly, they’re chronic and they’re not terribly easy to spot. Gone are the days where you used to have a huge visible injury on vegetation has gone, because there’s been acute exposure to everything. So there’s quite a subtle shift, which was really hard for even experts to go out and say yeah, that’s nitrogen causing that change on that site. So that limits what we can do with citizen science, or even sort of staff within the agencies in terms of interpreting changes and what’s driving them, that’s down to big physical exercises. Which then you go back to looking at vegetation data more broadly, not necessarily gathered for nitrogen deposition impact assessment, but can be useful, and that goes back to what we were just talking about before where we used their surveillance data” (Scientist, data only). For a detailed review of technology available for citizen science, please read this section in conjunction with Roy et al.’s 2012 report, where they highlight methods for data collection, visualisation, data management, crowd-sourcing, and virtual communities. Geoghegan et al. 2016. 78 Chapter 10: Evaluation of citizen science Chapter highlights Monitoring and evaluation are vital parts of project management as they help to ensure the project is meeting its goals, the needs of participants and the funder’s requirements Stakeholders recognise that evaluation is necessary and useful to improve the project and understand its impact Evaluation can relate to data and scientific outcomes, data collection processes, and successful citizen scientist participation Barriers to evaluation relate to resourcing, with evaluation and monitoring becoming desirable rather than essential aspects of any citizen science project Some stakeholders felt evaluation might bombard participants Common methods include: surveys; training activities; project team meetings There is a focus on ‘outputs’ and ‘reaction’ based evaluation and less on ‘outcomes and impact’ or ‘learning, behaviour and results’ In this chapter, we discuss the purpose and value of evaluation and monitoring to the management of successful citizen science projects. As suggested in our desk-based study and our online survey with citizen scientists and environmental volunteers, feedback and communication are regarded as integral to the success of any project, particularly in terms of encouraging continued participation and a good volunteer experience (Chapter 5). Furthermore, our review of the literature surrounding stakeholder motivations indicated that we need greater understanding of how scientists think the aims, motivations and benefits of their projects can be achieved and evaluated. Based upon our literature review and stakeholder interviews, successes in terms of data quality appear to be tested much more frequently than aims related to participants. While many projects claim they have aims beyond science, such as education, increasing scientific literacy and encouraging behaviour change, they do not appear to be doing anything active within the project to meet these objectives or test whether these aims have been met. As highlighted by Shirk et al. (2012), this needs to be an active not passive process for which a deliberative design and intentional opportunities to learn and reflect are needed. A common evaluation framework for citizen science projects could assist with encouraging people to think about this when designing projects. In what follows, we first discuss some of the key principles of evaluation. In order to highlight the varying and inconsistent uptake of evaluation, we move to the findings of our stakeholder interviews relating to the question: “How do you evaluate the impact of your projects?” We conclude with resources for stakeholders to access around evaluation good practice and our model derived from the evidence in this report for the stage-by-stage inclusion of motivations and evaluation 10.1 Existing work on evaluation and monitoring Monitoring and evaluation are vital parts of project management as they help to ensure the project is meeting its goals, the needs of participants and the funder’s requirements. Evaluation offers a way of assessing the value of activities in terms of their outcomes or impacts. It involves collecting information about the activities, characteristics and outcomes of a project in order to judge its worth, improve its effectiveness and/or inform decisions about the future (Patton 2002). There are other definitions of evaluation, but they all tend to emphasise that the primary purpose of evaluation is to improve and inform practice (Clarke Geoghegan et al. 2016. 79 1999). Allied to evaluation is monitoring, which involves collecting numerical data about, for example, the numbers, ages and genders of people taking part in activities (RCUK 2005). Much of this data tends to be collected as part of daily administration (Easton 1996), and can include records of numbers of people attending events, downloading materials, and so on. These numbers are sometimes divided into active beneficiaries (e.g. those who request information, attend events or participate in the project), and passive beneficiaries (e.g. those who attend an event and pick up a leaflet which describes the project, or who listen to a radio programme, or visit a website). There are two main types of evaluation – formative and summative. Formative evaluation is that which is carried out during the lifetime of the project in order to provide information about how to improve it (Patton 2002), whilst summative evaluation is conducted at the end of the project to help assess whether the project has been successful or not (National STEM Centre 2009). Projects can be evaluated in many different ways, but outcome-based approaches are particularly popular, and funders often encourage recipients to use an outcome-based approach to evaluation (Ellis and Gregory 2008). Outcomes can be defined as changes that occur as a result of the project, and it can be helpful to use a ‘logic model’ to think about what these changes might be. This is where the evaluator defines the inputs (resources), activities, outputs and outcomes of programmes, and then quantifies these different elements (Easton 1996). An example of a completed logic model is shown in Figure 5 (see also Shirk et al. 2012 for logic models within citizen science). Outputs are the immediate results, such as numbers attending. Outcomes are the short to medium term changes that occur as a result of the programme, whilst impact can be defined as the vision: the hoped for change that takes place over a longer term (Patton 2002). Thinking about project outcomes is a vital first step for any project evaluation, and can also be used when designing projects (Shirk et al. 2012). Figure 5 Logic model of evaluation, using example of tree planting (from West 2014) The programme’s success can be measured through one or more of the Outputs, Outcomes and Impact components. Once project staff have decided what outcomes they expect to achieve through their project, then methods can be designed to measure them. Outcomes occur over different timescales, and evaluating these requires different approaches. The Kirkpatrick evaluation model, which is commonly used in business and industry training settings, categorises evaluation into one of four levels; Reaction, Learning, Behaviour and Results evaluation, with each level giving increasingly detailed data about the impact of programmes on participants that is more time consuming to collect (see Figure 6). Reaction evaluation looks at participants’ initial responses to participation, Learning evaluation looks at changes in understanding or awareness, Behaviour evaluation considers whether people modify what they do after participation, and Results evaluation tracks long-term impacts on measurable outcomes (Kirkpatrick 1996, RCUK 2005). All projects should conduct Reaction Inputs • The initial investment e.g. staff, trees, land Activities • What is done e.g. tree planting activity Outputs • Immediate results e.g. numbers attending Outcomes • Changes that occur as a result e.g. people learn how to plant trees Impact • Longer- term vision e,g. other projects start to plant trees Geoghegan et al. 2016. 80 evaluation (to find out whether the participants enjoyed themselves and how the experience could have been improved), and ideally some Learning evaluation (to find out what participants have learnt), but Behaviour and Results evaluation are often too time consuming for small projects to conduct and can require external expertise (RCUK 2005). Figure 6 Kirkpatrick model of evaluation (from West 2014) Practitioners and participants in projects may have different interpretations of the outcomes that occur from projects (West 2015). This is important because mismatches between the expectations of the participants and the objectives of project staff or the reality of the role can contribute to high turnovers of volunteers (Measham and Barnett 2007). This highlights the importance of discussing expectations and goals with participants where possible. 10.2 Current status of stakeholder evaluation activities We asked all stakeholder interviewees involved in citizen science projects, whether they evaluate their activities. This evaluation related to: \\uf0b7 data and scientific outcomes: “the formal means of assessing a citizen science project is whether you can accept or reject your null hypothesis” (Scientist policy, monitoring); \\uf0b7 data collection process: “start with a user group … and then focus groups in the definition of the approach and the project objectives and as well during the project to see if we are … on target and after the project to actually do the wash up and lessons learned of what has worked, what hasn’t worked as well in looking at … what future projects would [do]” (Scientist, university); \\uf0b7 citizen participation: “feedback from the volunteers” (Policy, evidence), “Motivation and how it is for the individuals” (Science, policy), “what they like, did they enjoy participating, was it easy, did they enjoy the feedback” (Practitioner, science, engagement) Returning to the evaluation models identified in our literature review in section 10.1, the stakeholders we interviewed described a focus on evaluating outputs over outcomes and impact, and on reaction rather than learning, behaviour or results. The stakeholders were more familiar with evaluating the scientific outcomes of the citizen science project than the experience of their participants, with the following methods being identified: Geoghegan et al. 2016. 81 Surveys: online, paper questionnaires, ad hoc and formal “We do send out surveys from time to time. We’ve done one quite recently on the paper newsletter, what we send people, is it what you want? Are you telling us, are we telling you what you’re interested in? We also ask questions like, what motivated you to start? What motivates you to continue? Those kind of questions.” (Practitioner, science, engagement) Training activities: through training or in their words ‘checking’ the science “So are people identifying their species correctly? Are they doing the protocol in the way that they have been asked to, and continue development of that? … There’s also checking that people are going where they’re saying they’re going at the time that they’re going. And you can only do that by asking and you can only do that by, another way would be to look for anomalies, so when you analyse the data, if something’s an outlier, you want to know why. So our partners will then often phone people up and say, or communicate with them in some way and just say, well, could you just talk me through what you were doing there? You don’t say, you’ve got it wrong, but just try to understand why somebody’s different from the majority of other people around them. There are various ways of just checking up on the quality of what’s coming through.” (Scientist, policy, monitoring) Other Evaluation and monitoring online surveys (ad hoc and regularly); feedback forms; behavioural change based upon project-specific questionnaires; training activities; twitter statistics; talking to colleagues and reminding each other of the aims and objectives, refining our priorities, sharing what’s successful; learning evaluation and legacies of participation Table 39 Current modes of citizen scientist evaluation used by projects Yet, our interviews revealed that whilst evaluation is regarded as important to the success of citizen science projects, it is rarely undertaken with participants: \\uf0a7 “We don’t do any formal evaluation … to be honest it tends to be in my head” (Practitioner, science, engagement); \\uf0a7 “[I] have done evaluation on a couple of projects that we’ve undertaken but I am really bad at doing evaluation, and it’s something that I need to do more” (Practitioner, community); \\uf0a7 “This is something that we’re trying to sort of improve the process of because I think a lot of public engagement is done on quantitative metrics. So bums on seats as it were. How many people have been processed?” (Practitioner, science, engagement); \\uf0a7 “But as an organisation whose primary aim is to provide evidence to support policy and management what we’ve judged our projects on is whether they’ve actually delivered datasets that can be of use” (Scientist, policy, monitoring); and \\uf0a7 “I would say it’s probably the weakest bit of what we’ve done so far” (Scientist, monitoring, policy). Several challenges relating to resourcing, time, and inclination were identified, as well as a focus on what Kirkpatrick describes as ‘reaction’ evaluation: 1. More confident dealing with data than people: several respondents suggested that they felt more comfortable evaluating the success of the science element of citizen science (“obviously we evaluate data that comes in” (Practitioner, science, engagement)) rather than evaluating the volunteer experience. As a result, the evaluation may lie with other project partners and/or non-scientists. One scientist/policymaker respondent talked about a project they were partnered on: “For them it was difficult, as it is for us, because we’re not social scientists, so we have to think of how the best way is to ask that. So my only way I was able to help them at that stage was to say, go and ask [our organisation] because they’re big and they know what they’re doing, and they’ve done it before. So I know that [our Geoghegan et al. 2016. 82 organisation] have done it several times. … So it’s something that we’re aware of but also aware that we’re not the experts” (Scientist, policy, monitoring). 2. Responsibility for evaluation: not all respondents were sure whether evaluation had taken place on their projects, indicating that this may be a role assigned to a particular individual and that not all team members are involved in or privy to evaluation information. Another respondent highlighted how they knew they needed to do evaluation, but it wasn’t always at the top of their priority list (see also quotes above). 3. Timing of the evaluation: several respondents indicated that evaluation had taken place but only at the end of the trial phase, with no further formal evaluation of the project. One policy respondent indicated evaluation is critical at the pilot phase, for example “do you have a sufficient number of volunteers to give you a sample size that gives you a robust trend at whatever level you want to do?” (Scientist, policy), but evaluation is often overlooked due to funding/resource constraints once the project is live. 4. Funding availability: The above example was also linked to the funding availability as once a project is operational it has to be ‘self-sufficient’ (Scientist, policy). A practitioner about to launch a citizen science project highlighted how at the current stage “the survey evaluation is not actually something that we’ve given much thought to at this stage” (Practitioner, science, engagement), yet as the literature in the next section reveals, evaluation must be considered at all stages of the project. Time and money are key barriers to evaluation: “… evaluation takes time and money, if you’re going to do it properly, and if you’re doing a research project you design evaluation, in your funding application you have a whole work package on evaluation, and you’ll have people that are contributing to that work package and you’ve got a sum of money or resources designated for that. We are very much a small organisation, almost running hand to mouth in a sense, so we don’t always have the luxury of having, well we never have the luxury of having the finances or the resources to really undertake evaluation seriously. So I have done it where we’ve been commissioned for a couple of bits of work, I have, and not in any depth but started with an opening questionnaire with the participants and then one at the end just to look at the distance travelled and see what’s changed through their involvement. But yeah, we’re really bad at not really doing it, and it’s just, yeah.” (Practitioner, community) 5. Information overload for participants: As mentioned in the section of communication and feedback, it was suggested by the stakeholders we interviewed that evaluation might overload the participants. Some stakeholders make the conscious decision not to bombard participants, particularly when other aspects such as training often have to take a higher priority in the volunteers’ time: “when people signed up we said that we would only send them a maximum number of emails because of, we were just aware that we didn’t want to continually bombard people. But partly just through some anecdotal evidence that people just feel, like in the same way that when we were doing our engagement work in the shopping centres, people just thought we were chuggers, they thought we were trying to sell them something, and I think that that’s, there’s almost a scepticism when you get an email from us in your inbox, so what do they want now? (Scientist, education) Geoghegan et al. 2016. 83 10.3 Support for evaluation There are several guides available to help with project evaluation. \\uf0b7 Guide to Citizen Science: developing, implementing and evaluating citizen science to study biodiversity and the environment in the UK – research leading the new and existing practitioner through the project development and delivery by Tweddle et al. (2012). A copy of the guide is available from www.ukeof.org.uk. \\uf0b7 User’s Guide for Evaluating Learning Outcomes from Citizen Science – Cornell Lab of Ornithology have created a guide specifically for citizen science projects. Phillips et al.’s guide (2014) is useful for designing and conducting evaluations, with examples of best practice. \\uf0b7 Guides that have been produced for public engagement more widely may also be helpful for those wanting to evaluate citizen science projects: RCUK (2005): “Evaluation: practical guidelines”, produced by the Research Councils UK was designed for researchers wanting to evaluate public engagement, but gives helpful insight into some of the key terminology around evaluation and gives practical tips for evaluation. \\uf0b7 The National Co-ordinating Centre for Public Engagement also has some useful web pages designed to help researchers evaluate their public engagement activities, with a brief introduction to developing an evaluation plan http://www.publicengagement.ac.uk/plan-it/evaluating-public-engagement 10.4 Our design for evaluation Based upon the literature and our stakeholder interview findings, we have designed the following stage-by-stage guide for the inclusion of evaluation in any citizen science project. Evaluation begins pre-project and informs all stages of the process through feedback, improvements and formative and summative evaluation. Figure 7 Stage-by-stage inclusion of evaluation Geoghegan et al. 2016. 84 Chapter 11: Conclusion Citizen science plays an important role in delivering environmental data at local and national scales, and can form the basis of scientific research, as well as evidence for policy and management. Citizen science is also an important way of connecting people with nature, and has been used to help organisations communicate the importance of their work in the area of nature conservation. This study was designed to explore the motivations of environmental- based citizen science participants and stakeholders from ‘science’, ‘policy’ and ‘practice’ communities. We have outlined here the motivations, benefits and barriers surrounding citizen science for these groups, as well as discussed the evolving nature of the term citizen science and the increasing role of technology in data collection, analysis and communication. Our research also revealed the centrality of feedback and communication to successful citizen science, particularly as a key motivator for citizen scientists and environmental volunteers, and the associated importance of evaluation and monitoring. In this chapter we draw out the key findings of our study, and offer our recommendations and proposals for further work in this area. 11.1 Key findings Our research reveals: 1. Citizen science does not have ‘one’ definition for all stakeholders, as a result the term has been used by stakeholders in a range of different ways to meet the different needs of many different projects. For example the term citizen science may be used by organisations externally to brand their recording activities, but the activities themselves remain firmly within the category of traditional biological recording with the involvement of ‘amateurs’. 2. Citizen science and biological recording are intimately linked through the activities participants are asked to undertake and the subtle shifts in how projects are branded (as suggested above). As a result participants do not always associate their involvement to a particular subject area, instead being drawn to a particular species or set of activities. Stakeholders are aware of this difficulty, but acknowledge this allows them to access a range of audiences. 3. Policy priority areas, such as pollination, air quality, weather & climate change and tree health, were not as visible to citizen science participants as initially predicted. Citizen scientists preferred to identify with particular species, for example butterflies, moths and birds. 4. Motivations must be understood in order to successfully recruit volunteers and maintain the projects in question. Altruistic motivations were dominant in our survey of both citizen science and environmental volunteering participants, specifically ‘to help wildlife in general’ and ‘to contribute to scientific knowledge’. 5. ‘Sharing enthusiasm’ and ‘enjoyment’ were identified as additional motivations in the ‘other’ category by citizen science and environmental volunteering respondents. The open-ended answers provided by survey respondents revealed the importance of emotion and emotional attachments as intrinsic motivations towards initial and continued participation. 6. Environmental volunteers shared the same primary motivations as citizen science participants suggesting that citizen science projects may appeal to many environmental volunteers. Although it must be acknowledged that our study revealed considerable overlap between the projects represented in our survey by those who self-identified as Geoghegan et al. 2016. 85 doing citizen science and those doing environmental volunteering. Thereby reinforcing the difficulties surrounding the use of the term. 7. Only 1 in 4 of our citizen science respondents felt their motivations had changed over time, with the figure being 1 in 5 for environmental volunteering respondents. However, of those that did feel their motivations had changed they revealed that knowing they were making a contribution to scientific knowledge, sharing their knowledge and a stronger concern for conservation were important. This finding was linked to a movement from passive to more active roles, including leading projects and testing protocols. 8. Dispositional and organisational variables remain integral to the continued participation of volunteers in citizen science. Of those who answered the question regarding motivations and satisfaction, only two volunteers were dissatisfied, however this did not dissuade their continued participation, in fact they remained encouraged to do more. People hold competing and contradictory views, highlighting the complexity of the social world and people’s involvement in citizen science. 9. Feedback and communication are vital to both citizen science and environmental volunteering respondents. Feedback was the single-most cited reason for remaining involved in a citizen science project, followed by the linked reason of knowing their participation had made an impact or contribution. 10. Environmental volunteers want to do more volunteering. However, our survey identified a number of barriers to participation, namely: over-committed already; lack of time; advancing age; bureaucracy; family; knowledge; inclination; health; job; and the weather. 11. Stakeholders readily acknowledged the importance of feedback and communication to continued participation. The majority of our interviewees did offer feedback. However, some stakeholders identified the associated time commitment and resourcing issues as potential barriers to successful, reliable, and regular communication. 12. Experienced practitioners in citizen science suggested that feedback and communication must be immediate, specific to the locale or individual, interpretable, and offer online and offline options. 13. Stakeholders wear a number of different hats, ranging from institutional roles involving funding, using, running, facilitating, and advocating citizen science, to personal commitments to individual projects and volunteering in their spare time. Stakeholders represent a diversity of policy priorities, engagement activities, managerial responsibilities and personal passions for the inclusion of civil society in science and decision-making. 14. Personal experiences of citizen science, biological recording and environmental volunteering influence stakeholder involvement in citizen science. Stakeholders from science, policy and practice communities are also citizens and many are citizen scientists – volunteering, recording, and contributing data in their spare time. Their participation in such activities gives some of them a purpose in their professional lives and a commitment to valuing the activities of the citizen scientist and amateur naturalist. 15. Stakeholder motivations matched those identified in the existing literature on scientist motivations for citizen science involvement. However, our research revealed a shift in the stakeholder motivations being identified as citizen science has become more widely- accepted. The motivational range has expanded to incorporate those ways in which participants might benefit from citizen science, and not just about collecting data. As a result, a new motivational area has emerged, namely the category of personal satisfaction. Geoghegan et al. 2016. 86 16. Stakeholders identified changes in their institutional and personal motivations for citizen science over time, involving increases in: institutional commitment to citizen science; technology; bottom up approaches; public engagement; media reports; and sophistication of activities. 17. Our research revealed that recognition by stakeholders of the motivations of participants and incorporating this into the design of projects can increase participation. However, different motivations needed to be met depending on whether the citizen science project was contributory, collaborative or co-designed. 18. Existing literature largely focuses on the barriers affecting scientists and their involvement in citizen science. Our research incorporated barriers and challenges affecting those in science, policy and practice. 19. Interviews revealed ‘mobilising and maintaining citizen science projects’ (related to funding and time) as a common barrier across stakeholder groups. However, distinctions can be drawn between science and policy/evidence stakeholder barriers around data quality and biases and practitioner barriers relating to survey design and over reliance on technology. Whilst many barriers and challenges were identified, so were solutions. 20. Technology has transformed the potential of citizen science. Whilst increasing numbers of current and potential citizen science participants are driven by technology, stakeholders need to acknowledge that not all participants will have the same level of access to technology or knowledge about how to use it. 21. Technology has the potential to increase the numbers of stakeholders involved in citizen science. 22. Evaluation and monitoring were identified in the literature and in our stakeholder interviews as integral to the success of any citizen science initiative, whether the focus is on data quality, data collection or participant motivations. However, stakeholders in all groups identified resourcing issues as a barrier to evaluation, making it desirable rather than essential to their project. 23. Stakeholders described a focus on evaluating outputs over outcomes and impact, and on reaction rather than learning, behaviour or results. Stakeholders were more familiar with evaluating the scientific outcomes of the citizen science project than the experience of their participants. 11.2 Recommendations Based upon our study, we recommend: \\uf0a7 Stakeholders maintain their links with, learn from, and share their good practice with other national, European and global citizen science networks; \\uf0a7 Stakeholders should raise the profile of traditional recording/amateur naturalist communities in their citizen science initiatives, and where necessary celebrate the distinction between biological recorders and citizen scientists; \\uf0a7 Stakeholders need to consider how their citizen science projects can move from contributory to collaborative and co-designed projects, to enhance the participant experience; \\uf0a7 Stakeholders without access to academic journals need to keep up to date with the academic literature. In particular, we are thinking about the work of social scientists and the principles they apply to understanding the world. Other resources of this Geoghegan et al. 2016. 87 nature include conference programmes and websites containing research links. Umbrella organisations might be able to collate this data. \\uf0a7 Project websites are not using citizen science language. Organisations need to reflect on whether they are branding their citizen science projects as citizen science internally and/or externally, and whether some potential participants hold negative connotations around the term and would prefer biological recorders. Stakeholders need to be clear on the language they are using, specifically when and where. \\uf0a7 Stakeholders should approach existing environmental volunteering projects that may have links with a particular location, species or activity to help widen the audience for citizen science participation. \\uf0a7 Feedback and communication are vital to successful citizen science. Stakeholders should refer to our quick-list (Table 32) on what encourages and discourages participation in citizen science. \\uf0a7 Projects should involve an element of critical self-reflection to understand the competing stakeholder motivations around personal satisfaction and the project aims and objectives. Stakeholders exhibited varying levels of: enjoying their work; personal commitment/enthusiasm; equity and self-determination for participation; fulfilling career objectives, ambitions, building on previous education; generating impact for people’s lives; working with unpaid experts and harnessing their enthusiasm for science. \\uf0a7 Stakeholders must consider the motivations of their participants, and the motivations of other stakeholders engaged in the project. \\uf0a7 More work is required to develop a hub for resources on what works and what does not in citizen science. The UKEOF’s Understanding Citizen Science and Environmental Monitoring should be recommended for troubleshooting problems in the first instance (Tweddle et al. 2012, Roy et al. 2012). \\uf0a7 Social science research and evaluation is vital to the development of successful citizen science initiatives. However, some stakeholders and citizen science participants feel bombarded with surveys and interviews. Stakeholders interested in using these methodologies need to carefully plan and structure their activities to avoid overwhelming respondents. 11.3 Proposed further work (1) Evaluation Our research revealed the need for further work to support projects to implement evaluation, particularly beyond a focus on outputs. We recommend a series of one-day training workshops to draw on existing activities and the experience of practitioners from other fields, such as public engagement. (2) Longitudinal study of citizen science participants The open-ended answers to our online survey revealed the commitment many citizen science participants have to their activities, however, it missed out the details of what it is really like to participate in a citizen science project (whether contributory, collaborative or co-designed). Our telephone interviews with stakeholders revealed the richness of an ethnographic approach. As we have learnt volunteer motivations change over time and that people hold different motivations – therefore the need for longitudinal studies and extensive evaluation (see chapter 10) is vital. It is not our suggestion that UKEOF seek to fund this, but that stakeholders collaborate with social scientists to seek funding for this activity. This is particularly important for impact, practice, science and policy, as well as identifying future generations of potential citizen science and biological recording communities. (3) Contributory, collaborative or co-designed The respondents to the online study had largely taken part in contributory citizen science projects. As citizen science takes an increasingly participatory turn, it is important to focus to Geoghegan et al. 2016. 88 a greater extent on the motivations of participants in projects that have been co-designed and co-evolved with their participants. (4) Global environmental challenges and data needs Whilst it was not a key aim of our research, the literature revealed a need to understand how motivations differ in/between developed and developing nations. The potential for using citizen science in developing countries has received little attention. However, understanding and overcoming the challenges of doing citizen science in these different cultural contexts is important because these countries tend to be characterised by high biodiversity which is likely to be threatened by economic transition or environmental change and where financial resources are lacking to conduct large-scale monitoring (Loos et al. 2015). Geoghegan et al. 2016. 118 Appendix 3: Survey open-ended answers Responses to: (1) what encourages involvement in citizen science? (2) What makes participation worthwhile? What encourages involvement? In their own words… Feedback Good feedback to make me feel like it was worth my time; future evidence to show my data are being used and was useful; To know how the data is being used; Knowledge that my efforts have been useful to future research/conservation; Some response; academic papers, contributions to scientific knowledge; Some evidence of the data being used to extend scientific knowledge and this informing policy change; See the end result, the report, hear about the findings and whether this has resulted in changes being made; To know that data is of value to organisations and others and that it will be used to aid conservation / further research / education etc.; Valued feedback; Receive some feedback even if it is the first year of the project. Data analysis can wait but a general outline of coverage and initial observation sis very welcome; Feedback; To see results used in papers; To see how it has contributed to scientific knowledge; Positive feedback and seeing the results published; Evidence that the data collected is being used and is achieving something positive for environmental management; To see the results somewhere; Survey results update; Response from the organisers; Feedback relevant to the project, updates on the project etc.; A report of the species monitored/surveyed on yearly basis; Notification that my data has been used in the project; evidence that the data are useful or contributing to a cause (even just a map will do) - and on iSpot – identifications; Would like to receive the full outcomes or reports; some feedback that the data have contributed toward something worthwhile; so I\\'m not someone needing feedback, although I like to see the results; See how BTO use the final project data to report bird health and population change; Seeing reports published; being able to see my data in the context of the wider findings; need to know that the data has been acted on and feedback given - for JLbees there was alot of very helpful feedback and a feeling of belonging to the web community. A ref was supplied to a published paper; Feedback, a sense of involvement, and that results of project are used to inform policy makers; Feedback of results even if just general; Feeling that the information is of value and is used; feedback of data results; The knowledge that you are helping other people and the environment in general - derived from positive and informative feedback; Feedback; Just a confirmation that my record has been accepted; TO KNOW THAT THE INFORMATION WILL BE USED; feedback on survey at national level, and how my input has contributed; accepted records and requests for future assistance; To know what I\\'ve done is of use; To know my data is being used; To feel that my sightings information and site data have been recognised and used; Feedback and reporting; Access to combined results; That the data has been used by the organisation collecting it; data analysed and reported; To be sure that my data is contributing towards the aims of the project; Mapping, Report, Thanks/Acknowledgement for each batch of data submitted; The knowledge the info obtained is being used',\n",
              " '7 Part 1. Introduction to citizen science 1.1 What is citizen science? Citizen science is the involvement of volunteers in the collection and/or analysis of data. Various definitions have been applied to citizen science, including: “Volunteers collect and share data that can be analyzed by scientists, project participants, or both” (Cornell Lab of Ornithology 2013a) As this definition suggests, there are many different types of citizen science, and projects can be contributory (i.e. led by professionals and to which members of the public contribute), or can be much more strongly shaped by the participants themselves (collaborative and co‐created projects) (Bonney et al. 2009). For many projects involving environmental monitoring, the contributory model of citizen science is most relevant, leading to a specific definition of citizen science useful for this report, and which was used in a recent review of citizen science in environmental monitoring (Roy et al. 2012): Citizen science in environmental sciences is the “volunteer collection of biodiversity and environmental information which contributes to expanding our knowledge of the natural environment, including biological monitoring and the collection or interpretation of environmental observations” (UK‐EOF 2011). We note, however, that these definitions are working terms rather than fixed entities. Recent discussion has resulted in a preference for the descriptive, but lengthy, term: public participation in scientific research (PPSR). According to the Cornell Lab of Ornithology (2013) PPSR “includes citizen science, volunteer monitoring, and other forms of organized research in which members of the public engage in the process of scientific investigations: asking questions, collecting data, and/or interpreting results”. This diversity within ‘citizen science’ is confirmed by a recent systematic review of environmental citizen science showing the range of citizen science projects (Roy et al. 2012). Also, participants can be involved to different extents in shaping and guiding the questions addressed, the data collected and its interpretation and application (Bonney et al. 2009). An additional source of potential confusion concerns the definition of participants as ‘volunteers’ or ‘members of the public’. It raises the question of the limits of citizen science; how ‘expert’ do ‘expert volunteers’ need to become before the definition of citizen science does not apply to their project? Although we are aware of these issues, we do not consider them in detail and take a broad approach to considering contributory citizen science for environmental monitoring in this report. 8 1.2 Why is citizen science so popular? Citizen science “offers a means of doing substantial, thoughtful public outreach and of tackling otherwise intractable, laborious or costly research problems” (Gura 2013). It is an increasingly popular approach to undertaking research; many new projects are being promoted each year and there are an increasing number of publications based on the data collected by citizen scientists (Gura, 2013). We believe that there are several important reasons why citizen science has become so popular in recent years: 1. Excellent engagement. Citizen science provides a means for people to engage with science and their environment. Although people’s motivation for taking part in citizen science varies considerably, participants often describe it as fun. Citizen science can provide a way for people to feel they are contributing to something important. The engagement people have with nature through citizen science is a potentially powerful communication tool that enhances people’s appreciation of nature (for more detail see Common Cause for Nature: http://valuesandframes.org/initiative/nature/) 2. Resource‐efficient data collection. Citizen science provides the potential to collect data at much larger spatio‐temporal extents and much finer resolutions than would otherwise be possible. Even if the data could be collected via other means, citizen science can be a very cost‐efficient way of collecting the data. This is particularly so if the value of the public engagement aspect of citizen science is taken into account. 3. Technological advances make promotion and data collection increasingly straightforward. Over the past decade, advances in technology, especially in communications technology, have made it easy to set‐up and promote citizen science projects. Data collection, via websites or smartphones, is now a standard approach and relatively cheap to set up. Feedback to participants can be provided quickly and easily. 4. The data can be trusted. Increasingly, citizen science projects are incorporating data validation and verification steps ensuring the data are of known quality. Sometimes, only verified data is accepted, so the data are known to be correct. In other projects a sample of the data is verified in order to assess the quality of the data so that error or bias can be taken into account in the analyses. Both approaches provide trustworthy data. The results of many such projects have been published in the scientific literature, and citizen science data is also used in national indicators. 5. Volunteer involvement in science has a long history, but can now be included under the umbrella term of citizen science. Therefore, many types of volunteer‐led monitoring that have been undertaken over the past decades, can now be ‘re‐branded’ as citizen science, and 9 considered as this single entity. The term ‘citizen science’ can thus be applied to activities that pre‐date the term, such as systematic surveys by expert volunteers (e.g. the UK’s Breeding Bird Survey, whose pre‐cursor started in 1962) or unstructured recording of the presence of species (e.g. the long‐term datasets on species’ occurrence collated by the Biological Records Centre (Preston, Roy, & Roy 2012)). Citizen science can appeal to many diverse audiences, and different types of citizen science can appeal to different types of audience, e.g. expert volunteers, interested community stakeholders or members of the general public. Often the focus for citizen science is caricatured as engaging any member of the public via mass media, i.e. the sort of ‘mass participation’ project that anyone can take part in anywhere. However, this is not the only type of citizen science (Roy et al. 2012) and different types of projects can be extremely successful in engaging with more targeted audiences, e.g. working with volunteer experts (Grove‐White et al. 2007). 1.3 Citizen science and its role in long‐term monitoring One of the distinctive scientific roles for citizen science is its use in long‐term monitoring. The reason for this is that motivated volunteers can be very committed, permitting the long‐term collection of data in widely distributed locations. If resources are available to support the volunteers then large‐ scale monitoring projects can run for decades (e.g. the UK’s Breeding Bird Survey and the UK Butterfly Monitoring Scheme). 1.3.1 Citizen science can provide high quality data Although the data from citizen science projects can vary in quality, if it is collected appropriately and subject to quality assurance, then the data can be eminently suitable for regulatory purposes. For example, 7 of the 26 UK headline biodiversity indicators are reliant on volunteer‐collected data (Defra 2012), and monitoring of water courses in the USA is undertaken by volunteers according to the United States Environmental Protection Agency (EPA) protocols to meet regulatory requirements (Nerbonne & Nelson 2004), although as yet these data are not combined across different projects to provide national indicators of water quality. 1.3.2 Citizen science requires investment and resources It can be tempting to think that citizen science is a cheap way of fulfilling all large‐scale monitoring needs. This is certainly not the case. Many subjects are not suitable for citizen science: for example, data may not be able to be collected safely or accurately, or the subject is not appealing to volunteers. Additionally, although citizen science data may be free at the point‐of‐collection the annual support for citizen science projects providing data for UK headline biodiversity indicators still amounts to approximately £100K per project per year (Roy et al. 2012). However, citizen science 10 may be very cost‐effective. Citizen science need not replace professional monitoring, where this currently occurs. We consider that professional and citizen science monitoring are not mutually exclusive. Citizen science could be most effective when augmenting and complementing professionally‐collected data. For example, the value of systematic monitoring of a wide range of receptors at one site could be greatly enhanced by unstructured recording of a few receptors at a much larger number of sites. Alternatively citizen science data could provide point sampled ground‐ truthing of remote sensed data. It is important to note that modern analytical techniques (e.g. hierarchical modelling) mean that data from different sources can potentially be combined within single analyses. 1.3.3 Motivations of volunteers need to be understood Especially when considering long‐term monitoring, the role of volunteers needs to be understood. There can be tensions between the motivations of volunteers and the goals of the organisers (Nerbonne & Nelson 2004) which need to be understood for the project to be successful. The participants may have the expectation that local action will result from their data, but for many reasons this may not occur, or even be intended, so communication needs to be clear. Much work has been done on the motivations of volunteers, e.g. citations in Roy et al. (2012). 1.3.4 Advantages and disadvantages of a citizen science approach Often citizen science data are not collected in a systematic, structured way and so the analysis of the data and interpretation of the results can provide a challenge. Therefore, citizen science projects should not be entered into lightly. However, there are many benefits in undertaking citizen science. Advantages and disadvantages of a citizen science approach are summarised below: Advantages of a citizen science approach \\uf0b7 By getting people to be hands‐on with data, it engages them with important issues, including the complexity of the issues of concern and the challenge of monitoring impacts. \\uf0b7 It can help to build trust in organisations. \\uf0b7 It can be a cost‐efficient way of gathering data, especially at large spatio‐temporal extent and fine spatio‐temporal resolution. That is, the cost of acquiring suitable data ‘professionally’ is more than the cost of supporting volunteers to acquire these data. \\uf0b7 For long‐term monitoring, committed volunteers could provide a more reliable way of gathering data, less subject to the vagaries of agency funding than professional monitoring. \\uf0b7 It can permit many more simultaneous observations than would otherwise be possible. \\uf0b7 Where rare but significant events are noteworthy to members of the public (e.g. diseased wild animals, otters killed on roads, landslides etc.) it can permit the reporting of these 11 events across large spatial‐temporal extents, whereas using paid surveyors to report such events would not be practical. \\uf0b7 It need not be restricted to what people can see; people can use sensors, or they can collect samples for analysis by volunteers or by professionals. \\uf0b7 Many potentially interested people are willing to be directed and to be ‘useful’. Collecting data gives them purpose and helps them feel involved – thus encouraging commitment. \\uf0b7 By allowing lots of people to each undertake small or simple tasks (i.e. ‘crowd sourcing’, often of tasks that are simple for humans to undertake but difficult for computers, such as image recognition), it can provide a means of analysing large datasets for properties that cannot be picked up by an automated process and have so much data that it cannot realistically be achieved by a smaller number of people. \\uf0b7 Even the most unlikely subjects can be made engaging by applying the creativity and imagination of communicators – therefore almost any subject is potentially suitable for a citizen science approach. \\uf0b7 In some cases the expert amateur could have superior skills to the professional – this is particularly the case when surveying for and identifying plants and animals. Disadvantages of a citizen science approach \\uf0b7 It may be more efficient (and cost‐effective) to undertake systematic sampling with paid professional surveyors. \\uf0b7 Data acquisition becomes reliant on a resource that is outside of your control. That is, citizen science is most suitable where data cannot be collected any other way (i.e. you are not diverting resources from currently adequate monitoring), or where the data will be useful but not essential. \\uf0b7 Providing feedback to volunteers can be costly, in terms of time, but has to be maintained for the life of the project in order to motivate participants. \\uf0b7 The expense in providing secure infrastructure for data acquisition (e.g. online databases and web interfaces, or smartphone apps) can be relatively high. \\uf0b7 There can be tensions between the motivations of volunteers and the needs of the organisers. People take part because they are motivated through interest, curiosity, fun or concern. In the authors’ experience, people appear not to be motivated to take part because they are told they ought to, or because it is for someone else’s good. 12 \\uf0b7 Often citizen science data (especially mass participation) is ad hoc in its collection (i.e. the times and locations of samples are not subject to statistical design), so can require complex analytical approaches or may not be suitable for purpose for which it was intended. \\uf0b7 Data quality may be variable, so its suitability for scientific research or regulatory purposes needs to be carefully evaluated. The risk of not having adequate data to meet regulatory requirements (e.g. data is not collected over suitable time periods, or patterns of data collection change over time). \\uf0b7 For long‐term surveillance, either considerable commitment by individual volunteers, and/or a long‐term commitment to recruiting volunteers, is required. \\uf0b7 For long‐term surveillance, organisers of citizen science need to have a long‐term commitment to supporting and retaining volunteers (e.g. through training, mentoring, providing feedback, refreshing materials etc.) \\uf0b7 Volunteers need to be recruited. Therefore knowledge of your audience and what may interest them is essential, but it can still be challenging to ‘pitch’ the project to people, especially the media. \\uf0b7 The success of recruiting participants may depend on the reputation of the recruiting organisation. Government agencies may find recruiting harder, either because people trust them less than charities or universities, or because people believe that the activity should be supported with public funds, but working with partner organisations may lead to greater success in recruiting volunteers. 1.4 The aim of this report In this report we aim to provide a decision framework to help guide people who are considering whether a citizen science approach is suitable for their science or regulatory needs (e.g. a question that needs to be answered, or a feature of the environment whose condition needs to be monitored and/or reported upon). In this report, our primary focus is environmental/biological monitoring in freshwater and terrestrial environments, but we have created the decision framework in such a way that it will be applicable to citizen science in general. In so doing we aim to guide people: (1) to discover whether citizen science is suitable for their proposed project, and; (2) to discover what type of citizen science is most appropriate for them to adopt. We believe that the decision framework will help people to more clearly understand the potential opportunities and limitations of citizen science. This is necessary because there are so many different types of citizen science projects (Roy et al. 2012) and not every type is suitable for all situations. Therefore, for someone with a question which needs to be answered, or with a 13 monitoring need to be addressed, it can be daunting to consider whether citizen science can be used and, if so, what sort of citizen science project should be adopted. In this report we do not repeat the “Guide to Citizen Science” (Tweddle et al. 2012) which was written specifically to consider environmental and biodiversity science, or the Citizen Science Toolkit (Cornell Lab of Ornithology 2013b). We recommend that interested people consider the decision framework in this report and then, if they conclude that a citizen science approach is worthwhile, consult the Guide to Citizen Science (Tweddle et al. 2012) and follow the advice given. 14 Part 2. The decision framework In this part of the report we present the decision framework, to assist in the selection of a citizen science approach. We also include some preliminary questions and subsequent thoughts to advise on the suitability of citizen science. 2.1 Precursor to the decision framework: before you even consider citizen science We recommend that before seriously considering citizen science, it is important to consider six aspects: 1. the clarity of your question/aim 2. the importance of engagement 3. the resources available 4. the spatio‐temporal scale of sampling 5. the complexity of the protocol and 6. the motivations of participants. The suitability of a citizen science approach is summarised in Fig. 1, and expanded in the remainder of this section. Fig. 1. Six broad areas to consider prior to using the decision framework to assess the suitability of citizen science to your circumstances. 15 2.1.1. The clarity of the aim/question Citizen science is at its best when it is specific, i.e. when the question being addressed is precise. For citizen science to be effective the data need to be fit‐for‐purpose, therefore the purpose needs to be clearly defined. For many projects, e.g. where citizen science contributes to primary scientific research, the aim is well‐defined. It can be phrased as a testable hypothesis, leading to very effective citizen science (Silvertown 2009). However, citizen science can also contribute to environmental surveillance and monitoring. It can do this either by providing a spatial snapshot of the current state of a pressure or an assessment of change over time. In such cases, citizen science can contribute to an indicator of environmental change. In such cases, and as with the development of any indicator, it is important that there is an empirically‐tested and unique cause‐and‐effect pathway from the pressure to the indicator (Fig. 2). Key question: do you have a precise and clearly‐defined aim for your citizen science? Example: The Conker Tree Science project (www.conkertreescience.org.uk) sought to address two hypotheses relating to the invasive horse‐chestnut leaf‐miner, Cameraria ohridella, namely: does the (1) level of damage caused by the moth caterpillar and (2) the parasitism of the moth caterpillar increase with the length of time that the moth has been present. Both hypotheses were communicated clearly to participants at the start of the project. These hypotheses were tested with the data that were submitted via the project website and smartphone app. Fig. 2. Idealised role of citizen science in contributing to an official indicator of an environmental pressure. For any indicator each of the steps should be empirically quantified and there should be a unique link between the pressure and the indicator (or such a link should be determinable once confounded factors have been taken into account statistically). Pressure Of interest to agency Receptor Impact on receptor Assessed, e.g. with citizen science data to provide an: Indicator 16 2.1.2 The importance of engagement Engagement is an important component of citizen science but engagement alone is not citizen science. Perhaps you have an important message to convey but with no need to gather data. There are many examples of engagement working really well to raise awareness of a particular issue by communicating with many people without it being citizen science. However, perhaps you have an idea for engagement which could be quite simply extended to actively involve people in gathering relevant data either for your end‐use or for the benefit of the citizen science community. You could collaborate with others to successfully combine engagement and data gathering to answer a question of shared interest. Citizen science can provide an effective way of engaging people in complex scientific concepts while gathering valuable, high quality data. Indeed as people commit to the citizen science project, it is possible that the engagement aspect will be more successful – the citizen science acts as a highly participatory way for people to engage (Common Cause 2013). So if you are considering engagement think whether you can get more from the initiative by encouraging people to contribute through citizen science (i.e. asking a genuinely interesting scientific question, or gathering data for a genuinely useful scientific need). Increasing the awareness around a particular issue is extremely important and there may be no need for people to gather data. In which case keep it simple and invest in excellent engagement alone. Key question: Can you extend your engagement activity into meaningful and relevant citizen science? Example: The killer shrimp, Dikerogammarus villosus, arrived in England in 2010. Recognising the threat of this invasive non‐native species the Non‐Native Species Secretariat, in collaboration with a number of partners, developed an attractive biosecurity campaign (Check, Clean, Dry) targeted at people using high risk sites for watersports. The priority was to inform people of the potential for inadvertently spreading this species through movement of contaminated watersports equipment and how simple biosecurity methods could reduce the risk. Engagement and raising awareness of key users of high risk sites was critical to the biosecurity approach. However, the Check, Clean, Dry campaign also included a link to a website and associated e‐mail address for reporting sightings of the killer shrimp; many people have used these systems for reporting not only killer shrimps but other species potentially of concern. 2.1.3 The resources available Citizen science can seem an attractive approach because it is considered to be low cost. However, citizen science can require extensive resources. Indeed policy‐relevant citizen science projects in the UK typically cost more than £100k per annum for coordination and supporting volunteers (Roy et al. 2012). It is important to consider what resources will be required to run your initiative effectively. 17 Will you need a website? Will you need an online database? And, if so, can you use existing technology, such as Indicia (a database toolkit for biological observations developed by the Centre for Ecology & Hydrology; http://www.indicia.org.uk/ ), to meet this need? Will you need to provide supporting resources such as guidance notes or specialist equipment? If many resources are needed to adequately support a project and the cost of this is seemingly prohibitive then you could consider collaborating with other providers or consider using open‐source software, which may bring the costs down. The ‘Guide to Citizen Science’ (Tweddle et al. 2012) includes more detail about the sort of resources required for setting up and running a citizen science project. Key questions: Do you have sufficient resources available to ensure you can support your volunteers for the entirety of the project? If not, can you collaborate and share resources, which might also reduce duplication of effort? Example: The Open Air Laboratories (Opal project) http://www.opalexplorenature.org/ developed a wide range of resources to support citizen science activities on air, water, soil and biodiversity. A network of partners with a range of expertise worked together to develop attractive and accessible resources. 2.1.4 The scale of sampling Citizen science is particularly effective at addressing questions that require a large‐scale approach, especially across large spatial scales (by engaging many volunteers simultaneously). It is also useful when considering a very long‐term approach, in which volunteer‐led monitoring can remain consistent through peaks and troughs of funding cycles (although long‐term citizen science does require a long‐term commitment from the organiser), or when data is required at fine resolution (especially temporally). Citizen science could potentially work extremely well for both extensive large‐scale and intensive small‐scale studies. However, where there is a need for data across a large spatial scale it is important to consider whether information is needed from particular sites or whether an ad hoc approach will suffice. Site remoteness is also a consideration, as it may be difficult to persuade people to travel to these sites of interest. Key question: Do you need lots of people (or volunteer time) to achieve your aims? Example: Biological recording has a long history in Britain and the enthusiasm of people to contribute wildlife observations for many taxonomic groups is inspirational. The UK Butterfly Monitoring Scheme (UK BMS) http://www.ukbms.org/ involves a network of volunteers walking transects and recording butterfly abundance. The data contributes to one of the Defra biodiversity indicator assessments and enables trends to be reported on a regular basis. In recent years the UK BMS has been extended to include the Wider Countryside Butterfly Survey which includes random 1 18 km squares across the country, recognising the importance of recording in a wide variety of habitats. In 2012, the butterfly species present and their respective abundances were recorded by volunteers in more than 700 squares. 2.1.5 The complexity of the protocol Perception of citizen science is skewed by citizen science projects promoted through the mass media, which are typically those involving many people in gathering simple data (e.g. an observation or a single measurement) through so called “mass participation” projects. Simplicity is often key to the success of mass participation citizen science projects. Typically, a relatively large number of people will be prepared to get involved with an initiative that does not demand much time or expertise but as the complexity of the protocol increases then the number of participants is likely to decrease, even though the value of the data may increase. If you require the use of a complex protocol then ensure you provide sufficient support for participants and the protocol must be thoroughly tested (Tweddle et al. 2012). A complex protocol is likely to result in fewer people participating, but there may still be sufficient to provide enough data. Never presume too much of a volunteer; their time is given freely and they are not obliged to provide data. Ensure that you consider the motivation of your participants and maximise their enjoyment and satisfaction in taking part. This includes supporting their understanding of the importance of their record, and so requires you to provide feedback to participants. Feedback should ideally include an immediate response (e.g. a ’thank you’ for the record which could be automated or personal) and more considered feedback (e.g. an end‐of‐year report for volunteers). Key question: Is your protocol practical for volunteer involvement? Are you expecting too much from the volunteer community? Example: More than 40 000 people have submitted records of the harlequin ladybird, Harmonia axyridis, to the UK Ladybird Survey www.ladybird‐survey.org. It probably takes people less than 5 minutes to observe and record this species. By contrast, very few people got involved with the Ladybird Parasite Survey which involved a much more complex protocol than submission of a harlequin ladybird record. The data from both surveys has proved invaluable and, despite the mismatch in volume of data, both have provided insights into the ecology of this invader. 2.1.6 The motivations of participants People will get involved and continue to stay involved in a project for many different reasons, and these reasons will vary between people and can change over time (Rotman et al. 2012). It is important to consider people’s motivations. Progression in a project can be important for them to be motivated. 19 In terms of initial involvement, different projects will resonate with people in many different ways. Successful projects may resonate because of a sense of place (“it is my river”), a sense of community (“I can take part with my children”), a pre‐existing interest (“I’ve always like butterflies”), a sense of discovery (“I had no idea that…”), being part of a narrative (“I’m taking part with others …”) or a sense of jeopardy (“my trees are under threat”). This does not mean that the focus of your proposed study has to already have popular appeal, because even unlikely subjects can be communicated in such a way that they resonate with people. People also need a ‘trigger’ or prompt to make a record. Ideally triggers that will prompt involvement should not be too common (otherwise people feel overwhelmed and disengaged) or too rare (otherwise people will forget to participate), unless the event is rare and spectacular (e.g. a dead swan or a landslide). Often subtle changes to a question can make the trigger clearer and the data more useful. For example, asking people to report the health of garden birds is too general, while asking people to report sick birds in their garden or to report the health of garden birds on a particular day may be more successful. Remember that it is irrelevant how important you think an issue is – it is how it resonates with potential volunteers which will determine how motivated they are. Key question: Does your project resonate with potential volunteers, and are there clear and appropriate triggers for people to make records? Example: The Riverfly Partnership was a project created by anglers concerned about the quality of rivers and supported by organisations for which this information was important. This project resonated with new participants because it concerned places that were important to them, and empowered them to seek improvements in water quality (because their data could have impacts on management of the river). This feeling was enhanced through jeopardy, because the rivers were potentially vulnerable to pollution events which, without the data, would go undetected. The partnership successfully worked to create a larger sense of community among participants across the country, so supporting the efforts of individuals. There is a structured calendar for monitoring (e.g. monthly) so there is no specific trigger that people respond to in to provide observations. 2.1.7 Types of data Two current stereotypes of citizen science are (1) ‘mass participation’ citizen science, in which the media promote a project for anyone to take part anywhere, and (2) expert volunteers committed to making observations that provide long‐term surveillance. However, there are many other types of citizen science that could be considered: Observations are not the only contribution citizen scientists 20 can make, they could take samples instead. In some cases people have a vested interest in the results and are even willing to pay for analysis of the samples. You need not be restricted to considering observations that people can make directly; many sensors exist for people to record things that they otherwise cannot directly observe (e.g. radiation) or cannot otherwise quantify (e.g. temperature or noise). These sensors can use the capabilities of smartphones directly, or be developed as sensors that plug in to the internet, especially via mobile phones. This is likely to be an increasing field of activity, though if people begin passively dispersing sensors (e.g. air pollution sensors in cars) then it will raise debate as to whether it has enough engagement to be classed as ‘citizen science’! Similarly, it is possible to harvest social media, such as Twitter, for information which is contributed (albeit inadvertently) by a member of the public. This uses the information in the public domain, but does not engage with people to collect it. One of the strengths of citizen science is having a widely dispersed pool of volunteers. However, you do not need to restrict your thinking to tasks that require people to go out and make observations; for some projects people can get involved via the computer. There are many tasks that are difficult to automate, but easy for humans, e.g. pattern recognition, and if these can be divided into small tasks then the problem can be ‘crowd sourced’. Key question: Have you considered different types of citizen science, including crowd‐sourcing, collecting physical samples, citizen sensor networks, harvesting social media etc. Examples: In the State of the Oyster project participants collect shellfish samples and pay for laboratory sampling for harmful bacteria in order to receive information on their shellfish, while the results contribute to an overall understanding of faecal contamination of the sea water: http://wsg.washington.edu/mas/ecohealth/state_of_oyster.html. In Radiation‐Watch, members of the public funded the development of a sensor which is now available commercially to plug into a smartphone and provide real‐time data on radiation levels (e.g. in Japan after radiation leaks): www.radiation‐watch.org. Zooniverse and Crowd Crafting (PyBossa) are both platforms for the development of crowd sourced projects: www.zooniverse.org, http://crowdcrafting.org/. Forest Watchers invites people to contribute their intelligence to interpret satellite imagery and so quantify deforestation: www.forestwatchers.net. Geocloud in Oak Mapper harvests Flickr and Twitter to map potential records of sudden oak death in California: http://www.oakmapper.org. 21 2.2 The decision framework We have created the decision framework to provide guidance whether citizen science is suitable for you and, if suitable, which type of citizen science you should consider. 2.2.1 How to use the decision framework Using the decision framework with an extremely clearly defined question The decision framework is presented as a dichotomous key. You can use the decision framework with a specific aim/question that was been extremely clearly defined in advance. This was our initial expectation when developing the decision framework. You can then simply work through the questions in the decision framework to discover the suitability of citizen science for your proposed project. In this case the decision framework is used as a formulaic decision‐making tool. As we developed the decision framework, we became increasingly aware that it was unlikely that a person would have such perfect clarity in their aims. It is therefore unlikely that the decision framework will be used so inflexibly. Using the decision framework interactively In constructing the decision framework, we discovered a second, more practical and more productive use of the decision framework. In this case, you have a fairly well‐defined question/aim (see section 2.1.1) and you can develop, refine and clarify your aim by using the decision framework as an interactive tool. We anticipate that by working through the decision framework, you will face questions that you have not previously considered. The decision framework allows you to see the outcome of your decisions at each point in the decision framework. Using the decision framework could: \\uf0b7 Raise questions that you have not previously envisaged, thus broadening what you considered possible and so revealing the potential of a citizen science approach; \\uf0b7 Ask questions that you have not previously clarified, thus helping you refine your overall question, so making it more precise; \\uf0b7 Allow you to see the likely impact of each decision on the suitability of citizen science for your proposed project. We believe that the decision framework will be most‐productively used in an interactive way, rather than a formulaic way. 22 2.2.2 The decision framework for citizen science Part 1 of the decision framework. 23 Part 2 of the decision framework. . 24 Part 2 of the decision framework (continued) 25 Notes on the decision framework 1. Here we use the term ‘crowd‐sourcing’ to describe the sort of tasks that can easily be distributed for people to do on their own terms, especially at the computer. This is ideal for tasks that require human intelligence for problem solving or pattern recognition. Sometimes projects can be broken down to separate out‐of doors observations from a crowd‐sourced (computer‐ based) component, thus permitting people to be engaged with the crowd‐sourced components even when they are unable to make observations outside. 2. Safely does not mean risk free. Risk can be reduced with appropriate training but risk assessment is always needed for citizen science projects. 3. Limitations to a sensor being ‘available’ for public use include it being too complex or too expensive. However technological advances may quickly make sensor approaches affordable and tractable. Sensors could be made available by providing them free, or making them available to purchase (a form of ‘crowd funding’ of the project), hire or borrow. 4. We use the question about repeat visits rather than ‘long‐term’, because monitoring can be long term but collected by multiple people (from the same site or from multiple sites). Our distinction here makes clear an emphasis on volunteer retention, not just recruitment. 5. Short‐term, single‐site projects can be ideal to engage with people and provide education, but are less suitable for citizen science. ‘Bioblitzes’ (recording as many species as possible on a site in one day) are short‐term, single site projects; their scientific value is due to the presence of experts, but they have an important role in public engagement with nature. 6. ‘Anywhere’ means people do not have to travel to somewhere specific to take part, though they may need to be in a suitable habitat. Clearly, there is a judgement to be made for each circumstance and each intended audience whether locations could be viewed as ‘anywhere’. For example, depending on the audience ‘large rivers’ or ‘arable fields’ could be argued either way (most people are not near large rivers or spend time in arable farmland, but equally, a lot of people will visit large riversides, and many people could choose to visit arable farmland). Equally, a project requiring a visit to ‘woodland’ might require a special trip, but many people could choose to make that trip easily. 7. There are relatively few citizen science examples of trying to incentivise the visiting of sites (as is done with geo‐caching), but there is potential for this. 8. Usually not suitable for citizen science due to a mismatch between the intended audience and the ease of reporting. 9. Mass participation projects can be ideal in gaining a ‘snap‐shot’ overview of the state of something. Its success can rely on being featured in the mass media; alternatively it can take advantage of breaking stories in the news, in which case rapid response is necessary. You need to think clearly about the prompt for involvement (why would someone take part?), and whether sample sizes will be sufficient. Asking people to record something too infrequently is not ideal because they may forget the prompt to report it (unless it is very memorable). Asking people to record something too frequently (e.g. all sightings of a common animal, or 26 reports of river quality) is not ideal because there are too many prompts to record, hence it becomes too overwhelming and reduces motivation to submit reports. Making these observations more structured is an alternative (e.g. report your local river quality each month), but this comes under the sections regarding ‘long‐term’ surveillance. 10. Usually not suitable for citizen science due to a mismatch between the intended audience e.g. the general public and the accessibility of the project. 11. Engaging with wide audiences to undertake something reasonably detailed is one of the classic examples of citizen science. Key questions for projects organisers is why people would get involved – what is the prompt to get involved now rather than later (and potentially forget to take part), and why people would take part a second time – what are the incentives for continued engagement? Such a project definitely needs sufficient (i.e. substantial) investment in supporting resources and in recruitment. 12. It can be more successful to work with people who already have expertise (and interest) in the subject, e.g. working with birdwatchers to undertake surveys, rather than trying to recruit people who do not already have an interest in birds. 13. This question is important because although there may be a regulatory desire to collect data in a certain way, if the intended volunteer participants are not amenable to that approach then pushing ahead with the project has a high chance of failure. However, by working with the intended participants you could work collaboratively to develop a project that is acceptable for the intended participants. 14. If you require long‐term large‐scale monitoring by volunteers but do not have a ready pool of willing expert volunteers then you need to think carefully about their incentive to be involved. 15. For this long‐term surveillance, you need to demonstrate a longterm commitment to the project to fully engage with volunteers. 16. For this long‐term surveillance, the issue of working in collaboration with your intended audience is really important (see [14]). 17. This question is about the audience that you have identified. Groups of potentially interested people are often people who have a vested interest in the outcome of the surveillance, e.g. local action groups, or anglers concerned about river quality, mountain walkers concerned about invasive plants etc. 18. A key question that you need to consider is why someone would start to get involved and why they would continue to be involved. 19. A key question here is whether you have the commitment to provide sufficient resources for long enough. Training participants requires time and investment. You could have quite high drop‐out rates, but this approach has the potential to produce some really committed volunteers. 27 2.3 Final thoughts on the use of the decision framework for citizen science If you have decided that citizen science may be useful, we strongly recommend that you refer to the ‘Guide to Citizen Science’ (Tweddle et al. 2012) to help you consider the steps in actually setting up a citizen science project. Below we summarise a few important aspects that you should consider, which are largely based on the Guide. 2.3.1 Stop and check: is citizen science appropriate or would it be better to use an alternative approach? Citizen science can be effective and excellent, but it is not always optimum for data collection: the advantages of a citizen science approach do not outweigh the disadvantages in some circumstances. If you have decided that citizen science may be useful to you, we would encourage you to stop and consider once again whether citizen science is the best way to proceed. Undertaking systematic surveying with contracted professionals may be more effective because it allows you to stipulate the sampling requirements. You will gain data sufficient for your needs and can specify the spatio‐ temporal extent of sampling in response to formal investigation (e.g. knowledge of rates of change or seasonal variation, or a formal analysis of statistical power). With citizen science, a great risk is that you have insufficient public participation to gain adequate data for your needs. See Pocock & Evans (in press) for discussion considering the competing aspects of citizen science versus professional monitoring. While citizen science will have the added benefit of allowing people to engage with their environment, this could also be undertaken with a separate programme of public engagement. By separating the data collection and the public engagement with the environment, these two activities could each excel and proceed in parallel. 2.3.2 Resources: the organisers’ time Resources are needed at all stages of projects: in the set up and design, in the running of the project, and in the reporting phase of the project. Although citizen science can be very cost‐effective, it is not free, and has to be resourced properly. The most important resource is people’s time, especially for communicating with participants (via email, via the media, through blogs etc.). Time is also required when setting up projects, and there has to be sufficient lead time to perfect protocols, set up databases and websites etc. During the running of the project it is important to ensure websites continue to operate well (website links work, databases work, blogs are updated etc.). Project organisers (or other staff) need to be able to commit time for the life‐time of the project, and in larger organisations it is essential to recognise this, including the value of maintaining the same staff where they have public‐facing roles 28 in order to build rapport and trust with participants. The enthusiasm of people involved in organising citizen science projects is vital, and within a larger organisation this should be highly valued; there are many examples of citizen science project blogs that have been launched with great excitement but have rapidly ceased to be active or updated. Resources also need to be provided for the analysis, interpretation and communication of results. Often the analysis of citizen science data is complex and while the analytical approach should be planned before the project is started, undertaking the analysis and communicating the results to participants, and the general public (if appropriate), still requires resources. 2.3.3 Resources: infrastructure and data protection Infrastructure is an important aspect of citizen science, particularly the use of online databases, visualisation and feedback. Although web developers can set up bespoke databases, there are many examples of mature technologies for databases and for visualisation (Roy et al. 2012). Broadly these can be divided into: 1) bespoke technologies that are designed for a specific purpose and audience (e.g. NatureLocator smartphone apps http://naturelocator.org/ and the online databases of many extant citizen science projects), and 2) adaptable template‐type platforms where the project leader can modify the content within the bounds of the fixed parameters of the platform. The template‐ type platforms have the advantage of ease‐of‐use for content management by the project team , but may lack sufficient flexibility to allow content and design to be targeted to specific audiences (e.g. Epicollect http://www.epicollect.net/ for mobile applications, PyBossa http://crowdcrafting.org/ for crowd‐sourcing, CitSci.org http://www.citsci.org/ for data collection and visualisation, Ushahidi http://www.ushahidi.com/ for crowd‐sourced mapping, OpenTreeMap http://www.azavea.com/products/opentreemap/ for mapping trees). Some platforms do provide a hybrid of these approaches; allowing instances of the technology to be incorporated into local websites etc., but providing tools and code that can be repurposed, e.g. Google code (http://code.google.com/) or Indicia (a database toolkit developed by the Centre for Ecology & Hydrology http://www.indicia.org.uk/, which is purpose‐designed for the collection, visualisation, verification and sharing of biodiversity data and could, with adaptation, be used for the collection of environmental data as well). We strongly recommend that data are stored in a way that makes it easily accessible and easy to share. Often open‐source tools can be used to reduce costs, though we recommend the use of fairly mature and well‐supported technologies. Data protection needs to be considered when storing personal data online. It may be possible to overcome this by not collecting any personal information, but this limits the potential for 29 communication with people and personalised feedback. Advice must be sought to make sure that any online data storage in the UK complies with the Data Protection Act. 2.3.4 Quality assurance and verification One of the key aspects of data collected by citizen science projects is that to be useful it needs to be ‘of known quality’. ‘Known quality’ can be either ‘guaranteed to be accurate’ or having had quantified the degree of error or bias. One of the most cost‐efficient ways of ensuring high data quality is to have thoroughly tested protocols (Tweddle et al. 2012) so that errors of interpretation can be identified and errors in measurement/identification can be quantified. For some projects, the only data that is accepted is that which is guaranteed to be correct. In such cases, records may only be accepted if there is accompanying information (e.g. a photograph). Alternatively, accompanying information may be only required for unusual records (e.g. extreme measurements). This conservative approach may result in the discarding of genuinely interesting data points, so should be undertaken with care. For other data, random error and bias are two reasons why data are of less‐than‐perfect quality. Random error will increase the ‘noise’ in the data (for example, inaccuracy in making counts), thus making it more difficult to accurately discern signals in the data. However, most error is likely to be some form of bias, in which the error is systematic (e.g. people tending to over‐estimate counts) and this can vary due to many different factors, including people’s level of experience. This bias needs to be quantified and explicitly accounted for in the analysis. One hidden, and so often overlooked, source of error is the interpretation of an absence of records. People are most likely to record the presence of something, rather than record its absence, which may create systematic bias in the data. 2.3.5 Participant safety Although citizen science should only be considered if it can be undertaken by volunteers safely, no activity is risk‐free. Therefore risk assessments should be undertaken and sources of risk in the instructions to participants should be removed, as far as possible. The risk, and its reduction, should be communicated clearly and succinctly to participants. The level of support and training will influence the types of risk that are acceptable. For example, when assessing water quality, members of the general public might be asked to make observations from the bankside only (e.g. as with the Algal Bloom Pilot project http://www.fba.org.uk/algal‐bloom‐pilot‐project), while actually wading in the water might be deemed to be acceptable if personal training was provided (e.g. as with the Riverflies Partnership (http://www.riverflies.org/). 30 2.3.6 Communication Communicating with the target audience is clearly a vital aspect of citizen science. Communication via the mass media is appealing for many organisers of citizen science. Promoting projects in this way can be effective (if the intended audience is the general public). However the risks are high; whether stories are picked up by journalists depends on their perception of the interest of the story, and whether they are ultimately reported upon will partly depend on circumstances outside of the project organiser’s control (e.g. other news items on that day). We would recommend exploring alternative, more stable, routes of communication in addition to (or instead of) relying on the mass media. For example, communication to potential target audiences could be via organisations’ newsletters. Social media (e.g. Twitter and Facebook) has opened up new opportunities for promoting projects and communicating with participants; it can allow communication to be targeted to potential audiences, but also provides the opportunity for promotion to be enhanced via ‘word‐ of‐mouth’. Workshops and training sessions can provide invaluable face‐to‐face contact with project participants. Varied approaches to communication will ensure projects are promoted in a way that meets the requirements of the diverse range of potential participants. It is also important to consider what and how you communicate (Common Cause 2013). Not only do you need to communicate the ‘why?’ and ‘how?’ of your project, but you should also communicate the ‘so what?’. For some projects, participants might expect action in response to their observation but this may be beyond the scope of the initiative e.g. getting littered water courses cleaned on their behalf. For some other projects, participants might be asked to collect data that leads to a response they find unacceptable, e.g. eradication of an attractive but invasive non‐native species. It is important to consider and address people’s expectations early in the project. 2.3.7 Legislative implications There is growing awareness of the legal policies and codes of conduct that may be relevant to citizen science. These include data ownership and intellectual property, privacy, legal compliance and liability. For example, what are the implications of volunteers being asked to report notifiable diseases on someone else’s land, both on the landowner and the agency given the task of responding? If the citizen science data leads to action by regulatory authorities, then are the data sufficiently accurate and robust? If the citizen science data is used to derive an indicator on which government or agencies commit to act, then are the data sufficiently accurate and robust? Is the promoting organisation responsible if someone is injured while participating in citizen science? These are complex issues, so for more information we recommend you consult the guide to best practice in this area, recently published by Bowser et al. (2013). 31 Part 3. Citizen science and environmental pressures relevant to SEPA In this part of the report, our remit was to consider environmental pressures relevant to SEPA. However, much of the content will also be relevant to other organisations active in environmental/biodiversity monitoring, although they will need to take in account their own circumstances and requirements. 3.1 Is citizen science useful to SEPA? As we have determined in the report so far, citizen science is a potentially valuable tool for organisations such as SEPA. When monitoring the impacts of environmental pressures, citizen science may be a very suitable approach. However, for some other impacts, citizen science may not be suitable. Here, we consider the environmental pressures relevant to SEPA; assess the likely impacts of the pressures and provide a commentary on the likely suitability for a citizen science approach in monitoring these impacts. We note that the suitability of citizen science can depend substantially on the specific aim of the project. Two proposed projects could differ dramatically in their suitability for a citizen science approach depending on subtle differences in their aims. These subtle differences would be exposed when using the decision framework. Perfectly specifying the question in advance is unlikely to be the way in which the decision framework is used (as we highlighted in Section 2.2.1). This, therefore, makes it difficult to apply the decision framework to potential citizen science projects without detailed knowledge of SEPA’s priorities, policies and practice. Therefore, instead we have provided general guidance about the likely suitability of citizen science in each case. The pressures that we have considered are those relevant to environmental monitoring in Scotland as detailed in Appendix 1 of the Scottish Environmental Monitoring Strategy (The Scottish Government 2011) supported by the Coordinated Agenda for Marine, Environment and Rural Affairs Science (CAMERAS) initiative. However, many of these pressures are relevant across the world in countries from across the economic spectrum. 3.1.1 Using citizen science as part of a monitoring strategy In our commentary below (Section 3.2) we assess the potential for a citizen science approach, but we want to re‐iterate our comments in Section 1.3.2 that professional and citizen science monitoring are not mutually exclusive. (We define a ‘professional’ as someone who is contracted, and usually paid, to collect the data. They will have the expertise, or have been trained, to collect data to a standard deemed acceptable to the contracting organisation.) There is considerable scope for a minimum level of monitoring to be conducted ‘professionally’ but for it to be augmented extensively with a citizen science approach. There are many ways in which professionally‐collected data can be 32 augmented by a citizen science approach. Citizen science could add generality by sampling in a much wider range of sites, albeit less frequently. Also, citizen science could add precision by sampling much more frequently than professional monitoring in a few key sites. 3.1.2 Developing indicators from citizen science data Typically the collection of data for indicators needs to be supported in the long‐term for it to be useful. The support of participants over a long time period is challenging and can be expensive. For example, 7 out of 26 UK headline biodiversity indicators rely on citizen science data, at an approximate cost of £100, 000 per indicator per year (Roy et al. 2012). In each case these data are based on structured data collection with a systematic design, which makes them statistically rigorous. Although the 26 UK headline biodiversity indicators include aspects of environmental change in general (e.g. water quality or pressures from climate change), each indicator that relies on volunteer‐collected data depend on people’s observations of wildlife (e.g. birds or butterflies) and the participants are mainly recruited from groups already engaged with wildlife recording (e.g. birdwatchers). This ensures that the volunteers already have a high degree of commitment and expertise. All projects leading to the development of UK headline biodiversity indicators have grown incrementally, with a relatively long phase of testing (often several years), and are run by established conservation organisations. In contrast to this approach, mass participation can be an excellent way to gather a ‘snap shot’ of the state of an aspect of the environment with extensive spatial coverage. When considering the development of environmental indicators, it is also important that participants collect data for which there is a known and quantified link to the pressure of concern. 3.2 The citizen science approach to monitor environmental pressures Up to now in this report we have considered how to make a decision whether citizen science might be useful. We have introduced broad issues that make a citizen science approach more or less suitable (Section 2.1) and we have provided a decision framework that we expect will be used interactively to allow the suitability of a citizen science approach to be assessed (Section 2.2). Here we provide a commentary on whether a citizen science approach is likely to be suitable in monitoring a wide range of environmental pressures (summarised in Table 1, overleaf).',\n",
              " \"1 Chapter 1: Introduction 1.1 Overview and development of online citizen science Citizen science is a collective term for projects that engage both professional scientists and non-scientists in the process of gathering, evaluating and/or computing various scientific data (Kostadinova 2011). It has been around for over a century, and its development can be linked to the ‘professionalisation’ of science that began in the late nineteenth century, when science began to emerge as a distinct occupation (Vetter 2011, Miller-Rushing et al., 2012). Citizen science projects are often ‘top-down’ researcher-led initiatives, where professional scientists enlist the help of volunteers to either collect or evaluate data usually after a brief period of training (Marks 2013). In some cases, this data can lead to the production of scientific publications or it may help to inform public policy (Irwin 1995, Stilgoe 2009, Roy et al., 2012). Citizen science projects may also have a ‘bottom-up’ approach in which citizens influence the selection of scientific research topics, especially in the area of environmental activism (Irwin 1995, Ottinger 2010, Conrad and Hilchey 2011). In 1995 Irwin used ‘citizen science’ to refer to the involvement of citizens in addressing local environmental issues that relied on the collection and analysis of scientific data (Irwin 1995). The term was later used and adapted by Bonney and co-workers at the Cornell Laboratory of Ornithology to describe wider opportunities for non-specialists to become involved in real-world scientific research in a variety of different scientific disciplines (Bonney et al., 2009, Riesch and Potter 2013). These two distinct approaches to citizen science have been described as citizen-led co-created projects with local community groups on the one hand, and scientist-led participation initiatives that are 2 open to all sectors of society on the other (Roy et al., 2012). This research focuses on the latter. Scientist-led citizen science projects can have multiple aims and purposes and be utilised in a variety of settings, both small and large-scale. A significant proportion of these projects have used volunteers to collect ecological, biological or environmental data (Silvertown, 2009, Wiggins and Crowston, 2011). This data can be collected from a variety of geographical locations and over time in order to track phenological (seasonal or life-cycle) changes in wildlife, bird migration patterns, or more recently, biological or environmental markers of climate change (Devictor et al., 2010, Howard et al., 2010, Mayer 2010, Yaukey 2010). Given the geographical and temporal scale, such projects would be difficult, if not impossible, without the contributions of citizen scientists. As well as fulfilling a specific research aim, citizen science projects can play a role in informal science learning and potentially increase scientific literacy (Cooper et al., 2009, Kloetzer 2013). Some scientists who set up citizen science projects have used them as opportunities to educate and engage non-specialists (Bonney et al., 2009, Silvertown 2009, Gura 2013). Citizen science projects may also have the potential to produce partnerships between scientists and non-scientists, and introduce non-scientists to the scientific research process (Field and Powell 2001, Bonney et al., 2009). With the expansion of the Internet and a greater availability of digital tools, there has been a rapid growth in citizen science projects over the past decade or so (Hand 2010, Wiersma 2010, Gura 2013). Improvements in information and communication technologies (ICT) have made it possible for scientists to manage projects, recruit and communicate with volunteers, collate data, and disseminate research findings more 3 widely (Newman et al., 2011, Könneker and Lugger 2013). These technologies have also made it possible for prospective participants to get involved. 1.2 The impact of digital and communication technologies on citizen science During the past two decades, developments in ICT have changed the way scientists work in a number of ways (Holliman 2010). This is most notable in the creation and integration of new knowledge between different research areas or disciplines (Scanlon 2013). Digital technologies have also influenced how scientists communicate with one another, and how they communicate with those ‘outside’ the scientific community (Borgman 2007, Scanlon 2013). For example, online sharing of data has facilitated scientific collaboration; and the rise of ‘open notebooks’, online repositories, and open-access journals has aided the dissemination of scientific results (Grand et al., 2010, Cranshaw and Kittur 2011, Nielsen 2012). Scientists are able to communicate more widely with interested non-specialists through websites, blogs, podcasts and through social media (Birch 2010, Kouper 2010, Blank and Reisdorf 2012). Some maintain that the development of ‘Web 2.0’ technologies which facilitate participatory data sharing and the production of user-generated content, can begin to blur the boundary between professionals and an increasingly informed online public, and that this may have important consequences for the way scientific knowledge is generated (Lievrouw 2010, Stodden 2010, Nielsen 2012). This rise in digital science and the expansion of new avenues of communication has been referred to as ‘open science’ or ‘Science 2.0’ (Burgelman et al., 2010, Nielsen 2012, Könneker and Lugger 2013). While these are somewhat ambiguous terms, with 4 conflicting opinions regarding definitions, this phenomenon could be generally described as a trend towards the increased connectivity between scientists, and an increased capability for non-scientists to access science and the scientific community. Open science may allow for greater transparency, as well as greater opportunities for non-specialist participation (Catlin-Groves 2012, Grand et al., 2012, Mansell 2012, Czerniewicz 2013). The growth and impact of digital technologies, has been accompanied by an increase in the accuracy and productivity of scientific instrumentation and data storage technologies. This has led to what has become known as the ‘data deluge’, as scientists in some disciplines now acquire, store and mine huge volumes of digital data (Hey and Trefethen 2008, Creighton 2010, Clavin 2013). The increase in data-intensive science has also been referred to as ‘e-science’, and it requires new tools and techniques to organise, filter, share, re-use, recombine and analyse (McFedries 2011, Mayer-Schonberger and Cukier 2013). For example, the Large Hadron Collider generates approximately 15 petabytes 1 of data per year when in operation, and the Large Synoptic Survey Telescope which will be in operation in 2022, will produce 100 terabytes 2 of data every night (Hey and Trefethen 2008, McFedries 2011). The developments in ICT seen since the mid-1990’s (in particular, the expansion of the Internet), along with the ‘data deluge’ has resulted in some important changes in the way scientific research is carried out (Neylon 2011). Furthermore, these developments have also had implications for citizen science and in the type of opportunities available for both scientist and citizen scientist (Wiggins 2010, Kostadinova 2011, Prestopnik and Crowston 2012). 1 A petabyte is one million gigabytes. 2 A terabyte is one thousand gigabytes. 5 By the late 1990’s, some scientists had realised that they would never be able to analyse all of their data on their own, and devised new ways to enlist the help of those outside of their academic institutions (Nov et al., 2010, Schawinski 2011). This has led to what some have referred to as the ‘crowdsourcing’ of science, where interested members of ‘the public’ can help to analyse data produced by instruments that they would not normally have access to (Qadir 2013, Uchoa et al., 2013). Scientists too, now have access (via the Internet) to many thousands of potential participants in their projects, and are able to accomplish more than was previously thought possible (Hand 2010). As a result of these developments, some citizen science projects are conducted entirely through the Internet and participants help to analyse large sets of data that has been provided by the project scientists. These projects have been referred to as online citizen science (Holliman and Curtis 2014). 1.3 The rise of online citizen science One of the first online citizen science projects to emerge from this mixture of abundance of data, and the expansion of the internet was SETI@home 3 (Anderson 2004). In 1999, scientists from the University of California, Berkeley, asked members of the general public to volunteer their idle PC processing capacity to analyse data produced by radio telescopes searching for signs of extra-terrestrial intelligence. Hundreds of thousands of people have taken part in SETI@home 4 , and its software platform has been adapted for use in a number of other citizen science projects involving the analysis of data packages by PCs and games units around the world (Anderson, 2004). These ‘distributed computing’ projects as they have become generally known, provide a venue where 3 SETI@home project homepage: http://setiathome.ssl.berkeley.edu/ 4 This website provides daily statistics on the output and the number of participants in SETI@home and other distributed computing projects: http://www.teamocuk.co.uk/index.php?s=8f315d852c601368eb111539388a9393 http://setiathome.ssl.berkeley.edu/ http://www.teamocuk.co.uk/index.php?s=8f315d852c601368eb111539388a9393 6 interested individuals can become involved in scientific research via the Internet (Carroll et al., 2005). Shortly after the introduction of distributed computing projects such as SETI@home, a new project appeared that asked participants to take a more active part in the analysis of scientific data. This was a small experimental project created by three NASA scientists in 2000 that used volunteers (nicknamed ‘clickworkers’) for scientific tasks that required “human perception and common sense” (Kanefsky et al., 2001). The tasks did not require a scientific background and involved the identification and classification of craters on Mars from images taken by NASA’s Viking Orbiter. Stardust@home 5 was the next major project to emerge that enlisted the help of volunteers to take an active role in the analysis of scientific data via the Internet (Westphal et al., 2006). In 1999 NASA launched the Stardust Mission, in which particles from Comet Wild 2 were collected in the spacecraft’s special aerogel collectors which were parachuted back down to Earth after the completion of the mission. In addition to comet particles, the aerogel collectors may have also captured inter-stellar star dust. In 2006 a project was set up in which participants searched images of the aerogel for signs of inter-stellar star dust. Several thousands have taken part, and the Stardust@home project continues today. Projects such as Clickworkers and Stardust@home were the inspiration for many of today’s largest online citizen science projects such as Galaxy Zoo 6 and Planet Hunters 7 (Keel 2010, Schawinski 2011). These projects have been referred to as ‘distributed thinking’ projects, and participants help to classify, annotate or transcribe scientific data (Holliman and Curtis 2014). 5 Stardust@home project webpage: http://stardustathome.ssl.berkeley.edu/ 6 GalaxyZoo project webpage: https://www.zooniverse.org/project/hubble 7 Planet Hunters project webpage: https://www.zooniverse.org/project/planethunters http://stardustathome.ssl.berkeley.edu/ https://www.zooniverse.org/project/hubble https://www.zooniverse.org/project/planethunters 7 Online citizen science has also been referred to as ‘citizen cyberscience’ (Grey 2009, 2011) or ‘virtual citizen science’ (Wiggins and Crowston 2011, Reed et al., 2013). Unlike more ‘traditional’, ecology-based citizen science projects, in which participants help to collect data, these projects are conducted entirely online and the participant analyses data that is provided by the project scientists. Participation in citizen science can thus take place within the comfort of one’s home, on the way to work, or wherever there is access to the Internet (McDermott 2011). Online citizen science projects have enabled many thousands of interested individuals to become involved in authentic scientific research from anywhere in the globe with internet connectivity (Bohannon 2005, Carroll et al., 2005, Alexander 2008, Bohannon 2009, Hand 2010). There are now well over one hundred online citizen science projects to choose from, both in distributed computing and in distributed thinking 8 (Grey 2009, Haklay 2011a, 2011b). More recently (since 2008) several projects have emerged in which scientific research problems have been re-packaged into online multi-player games (Cooper 2011, Kawrykow et al., 2012, Rowles 2013, Curtis, 2014a). These citizen science games have attracted thousands of participants, and have, like other online citizen science projects, experienced some research success. Indeed, a growing number of online citizen science projects have produced significant results that have been published in the academic literature (Lintott et al., 2009; Khatib et al., 2011, Kawrykow et al,. 2012, Schwamb et al., 2013). These more successful projects have also attracted a fair degree of attention from journalists and science communicators, which can in turn help to increase the number of individuals taking part (Adams 2012, Borrel 2013, Hodson 2013). 8 SciStarter.org is an online repository for citizen science projects. This is a link to a list of current online citizen science projects: http://scistarter.com/activity/10-Exclusively%20online http://scistarter.com/activity/10-Exclusively%20online 8 1.4 Gaps in our understanding of online citizen science projects Despite the growing number of projects, the many thousands of participants, and the potential for significant research results, some aspects of online citizen science projects are not fully understood. For example, who takes part in these projects? Do they appeal to certain individuals or groups? Some projects have hundreds of thousands of registered participants. Do they all participate in the same way, or to the same extent? Are there other aspects of participation that are important such as online social interaction? What motivates individuals to get involved, and can these motivations change over time? The motivations of the scientists who set up online citizen science projects, and the ways in which they can become involved with communities of online volunteers are also poorly understood. An in-depth investigation of online citizen science projects would help to provide an insight into these issues and contribute to a greater understanding of this growing online phenomenon. Detailed information about patterns of participation and motivation to participate, would also be useful for scientists considering setting up such a project, and could be helpful in creating projects that attract and retain participants. Since the introduction and development of online citizen science projects, it has become evident that there are different types of projects, with specific types of tasks and user interfaces. Consequently, a small number of researchers have attempted to classify online citizen science projects. Haklay (2011b) proposes a division of online citizen science projects into ‘volunteered computing’ projects and ‘volunteered thinking’ projects. The former are considered to involve more ‘passive’ participation (the participant’s computer or games unit is doing all of the analysis), while the latter involve more of a direct cognitive input from the participant (such as classifying the shape of a galaxy). 9 Haklay adds a third category: ‘participatory sensing’, where mobile phones are used to sense the environment (e.g. noise levels, air pollution), or to record and report ecological observations. This data can then be collated and used for research purposes. For example, BirdLog 9 is a mobile phone app where users can log sightings of specific bird species. This data is used by scientists studying migratory patterns and geographical distribution of birds. Participatory sensing mainly involves data collection rather than data analysis, although there is potential for these apps to be used more widely by citizen scientists in their own research endeavours (Paulos et al., 2008). For the time being however, participatory sensing appears to have more in common with ecological, contributory projects, than with projects like Folding@home or Galaxy Zoo. A more appropriate third category for online citizen science projects may be citizen science games (Holliman and Curtis, 2014). While the participant contributes through a stylised games interface, they are involved in the direct analysis of data or in creative problem solving (Cooper et al., 2010, Kawrykow et al., 2012, Lee et al., 2014). Unlike some of the tasks required in ‘volunteer’ or ‘distributed’ thinking projects, the games themselves can be quite difficult to learn, and have extended levels of tutorials that teach the player about the game tools and objectives (Cooper, 2011, Andersen et al., 2012, Lee et al., 2014). In some citizen science games, the task can be quite abstract, and appear removed from the underlying science (Kawrykow et al., 2012). Thus, they offer a distinct approach and ‘package’ from many distributed thinking projects. I have developed a typology based on the work of Haklay (2011b) for use in this thesis that incorporates citizen science games as a third category (Table 1.1). 9 BirdLog was developed by the Cornell Laboratory of Ornithology http://ebird.org/content/ebird/news/birdlog/. http://ebird.org/content/ebird/news/birdlog/ 10 Table 1.1 Proposed typology of online citizen science projects Distributed computing Participants donate their computing power for the analysis of large volumes of data e.g. SETI@home, Einstein@home, Folding@home Distributed thinking Participants take part in classification tasks, annotation of objects, or the transcription of data (such as scientific log books or field notes) e.g. Stardust@home, Galaxy Zoo, Old Weather, Planet Hunters, Eyewire Citizen science games Players help to solve a scientific research problem through a games interface. e.g. Foldit, EteRNA, Phylo All three classifications are connected in that they can be accessed or played wherever there is an internet connection, and by the fact that the participant does not have to provide any data themselves. Thus, all three categories are mediated through technology (Holliman and Curtis, 2014). This classification also relates to the general level of difficulty of the project task. Distributed computing projects are easy to install and run, although they often offer participants the opportunity to learn more about the underlying science or interact with other participants online. Distributed thinking projects require a greater cognitive input from the participant and some of the tasks may require a small amount of training or practice. However, tasks need to be relatively straightforward in order to attract participants (Parsons et al., 2011, Ponciano et al., 2014). Citizen science games appear to have the greatest level of complexity and some require considerable training before the participant is able to make a useful contribution (Andersen et al., 2012). 11 It has become clear over the course of this research, that the area of online citizen science is developing rapidly, and the above typology may well need to be re-visited in the future. This typology does help however, to make sense of the current state of online citizen science (as of 2014), and has been used as a foundation for this research, particularly in the selection of projects to study. Thus, in order to explore a range of online citizen science projects, a project from each ‘type’ has been investigated in detail. The aim has been to explore certain aspects of participation, as well as the motivations that initiate and sustain participation. I have considered both the citizen scientist volunteers, and the scientists and developers who set up and manage online citizen science projects. Exploring multiple projects allows comparisons to be made and also permits an inquiry into how (or if) aspects of participation and motivation are related to types of project tasks, or to the type of project. This thesis will therefore focus on the following research questions: 1. Who participates in online citizen science projects? 2. What motivations initiate and sustain participation in online citizen science projects? 3. How do motivations vary between different types of online citizen science projects and their associated tasks? 4. How and why do project participants interact online? 5. How can contribution to online citizen science projects be characterised? 6. How do participants perceive their role in the project? 12 The following projects were selected as the focus of this research Folding@home (a distributed computing project), Planet Hunters (a distributed thinking project) and Foldit (a citizen science game). These projects will be described in detail in Chapter Four. 1.5 Organisation of the thesis In addition to this Introduction, the thesis is presented in eight further chapters. Chapter Two presents a review of the literature and places this research within the context of previous work. This also includes a consideration of other types of online peer production such as open source software and the production of open content. Gaps in our understanding of how and why individuals participate in online citizen science are identified and six research questions are proposed. Models relating to motivation and patterns of participation are considered. Of particular relevance are Preece and Schneiderman’s (2009) ‘reader-to-leader’ framework and Haythornthwaite’s (2009) model of ‘lightweight and heavyweight’ modes of peer production. Chapter Three discusses the methodology and methods used to address the research questions. A mixed methods approach that utilises online surveys, semi-structured interviews and participant observation is outlined, as is the predominantly qualitative analytical approach. Chapter Four provides a brief background for each of the three online citizen science projects that have been selected as a case study. The research objective, project task, as well as other project parameters (e.g. opportunities for interaction, and the numbers of active participants) is described. This information has been collected through my observations and participation in the three projects. Each project section will conclude 13 with an overview of my experience as a participant, and create a context within which to consider the results of the surveys and interviews. The results for each selected project are presented separately and the findings from online surveys, and semi-structured interviews are outlined. The projects are presented in the order in which their investigation commenced. Chapter Five presents the Foldit results, Chapter Six presents the Folding@home results, and Chapter Seven presents the results from Planet Hunters. Chapter Eight presents a comparative analysis of these findings. The projects are compared within the context of the research questions, and within the overarching themes of who participates, why they participate, and how they participate. Possible explanations for the findings are discussed, and the relevance and utility of the models outlined in Chapter Two are considered. Chapter Nine concludes this thesis by examining the contribution of this research in detail and in relation to the research questions. Potential limitations of this study are considered, and future avenues of related research are proposed. A general timeline of all research activities is presented in Figure 1.1. 15 Chapter 2: Review of the Literature 2.1 Introduction This literature review will focus on aspects of participation in online citizen science projects. It will consider previous work that has examined the demographic characteristics of those who take part in online citizen science projects, and a small body of work that examines how citizen scientists contribute to projects and how they interact online. I will also review the current literature focussing on motivation to participate in online citizen science, and also in other types of voluntary activity that may be of relevance to this research. My research questions will be presented within the relevant sections of this review. As online citizen science is a newly-emerging area, there are multiple sources of information in relatively disparate fields. My search for relevant literature has utilised mainly electronic means of retrieval allowing for multiple searches across disciplinary boundaries. I have used the search engines of the Open University online library catalogue and also the University of Cambridge Library. I have made use of Google Scholar and set up electronic alerts for a number of key words such as ‘citizen science’, ‘distributed computing’ ‘crowdsourcing’ and the names of the three projects I have investigated. These alerts have enabled me to keep track of new publications throughout the research period. Key journals in areas of interest (e.g. science communication journals, journals in disciplines where citizen science is carried out, and some computing journals) as well as a number of relevant blogs by researchers in related areas were identified and regularly monitored. In addition to electronic searches, I have undertaken ‘hand searches’ of the literature (browsing titles upon the shelves) at the Open University, and many useful references have ultimately been found within the bibliographies and 16 references cited by other researchers working in related areas. The literature has been constantly monitored throughout the period of study and more in-depth reviews were undertaken approximately every 6-8 months (see Chapter One, Figure 1.1). The content of online searches and the alerts has changed throughout the course of my study, as the research questions and areas of focus have evolved and developed. 2.2 Main features of online citizen science Online citizen science projects have a website that serves as the public interface of the project 10 . They provide background information such as the specific research aims, and educational material so that participants may learn more about the related science. Many online citizen science projects contain forums where participants can interact with each other and (in some cases) with the project scientists or developers. Online forums enable participants to discuss the project, share problems they may be having with the tasks, offer help to others, or ask questions of the scientists. Some project forums also have areas where more general topics (unrelated to the science or the project) can be discussed such as current affairs, or hobbies and interests 11 . Having an ‘area’ that allows interaction between citizen scientists, and between citizen scientists and the project scientists, can provide opportunities for co-operation and collaboration between project participants relating to project tasks (Paulus 2005). Online citizen science projects consist of tasks that have ‘granularity’, that is, the work consists of much smaller units that can be easily distributed among the participants (Nov et al. 2011). For example, a unit could be an individual work ‘package’ in a distributed computing project, or the classification of a single object in a distributed thinking project. 10 Examples of project homepages can be seen for Foldit http://fold.it/portal/; Cosmoquest http://cosmoquest.org/; and Old Weather https://www.zooniverse.org/project/oldweather. 11 This area of the SETI forum is devoted to discussion about politics: http://setiathome.berkeley.edu/forum_forum.php?id=23. http://fold.it/portal/ http://cosmoquest.org/ https://www.zooniverse.org/project/oldweather http://setiathome.berkeley.edu/forum_forum.php?id=23 17 Individual tasks, may not take very long to complete which means that participants can make a contribution whenever they have a small amount of free time or are in between other activities. This ability to make small contributions has been referred to as ‘microvolunteerism’ and has been key in the success of a number of online citizen science projects as it allows individuals the flexibility to vary the amount they contribute during any one visit, and to tailor their contribution according to other commitments (Paulos et al. 2011). In many cases, the tasks given to citizen scientists are straightforward and do not require any pre-requisite academic qualifications in science. By keeping tasks relatively simple, projects may be more accessible to a wider variety of individuals with differing levels of interest or ability (Parsons et al. 2011), and also increase the likelihood that the work carried out by citizen scientists is accurate and reliable (Cohn 2008). However, the complexity of the task can vary between different online citizen science projects and training on how to perform the task may be required through an online tutorial. Feedback can also be given to participants when they are taking part in the project and in some cases ‘dummy’ tasks are given to participants in order to remind them of the original task parameters 12 . In citizen science games, the required task can be quite complex, and project scientists and developers in some games (such as Foldit and EteRNA) have put together a series of tutorial puzzles that introduce the tools and concepts behind the game (Andersen, O'Rourke et al. 2012). How the level of task complexity relates to aspects of participation such as participant numbers, level of contribution, motivation to participate and how participants interact online has not previously been investigated in much detail and will be explored in this research. 12 In Spacewarps ‘dummy’ tasks are given at intervals to participants (see: http://spacewarps.org/) http://spacewarps.org/ 18 In more ‘traditional’ citizen science projects where the participant is involved in data collection, the accuracy of data provided by citizen scientists has been much discussed in the literature (Cohn 2008; Dickinson, Zuckerberg et al. 2010; Newman, Crall et al. 2010). How reliable citizen scientists are in the tasks assigned to them in online citizen science projects, has not been discussed as widely. However, some online citizen science projects have tried to address this problem by having more than one citizen scientist complete each task. For example, in projects where citizen scientists are involved in classification tasks (such as Galaxy Zoo and many other projects in the Zooniverse), more than one individual will classify the same object (Schawinski 2011; Schwamb, Lintott et al. 2014). The ‘majority vote’ is in operation here, and the classification with the greatest level of agreement is accepted. This is more readily achieved in projects where there are hundreds or thousands of participants. 2.3 Who takes part in online citizen science? Citizen science has on occasion been written about within the context of open science, and with a move towards a greater ‘democratisation’ of science – suggesting that scientific research is accessible to anyone who wishes to take part, even if they lack the formal educational qualifications (Könneker and Lugger, 2013, Nielsen, 2012, Stodden, 2010). However, there is little data available about who is taking part in citizen science projects. Information regarding the demographic characteristics of participants in ecology-based projects must be gleaned from a small number of published studies that provide limited information about sub-samples of the overall population of project participants (Rotman et al., 2012, Trumbull et al., 2000). One of these studies (Rotman et al. 2012) noted a greater percentage of male volunteers (57%) in a sample of their participants. A study by Trumbull et al. (2000) observed that volunteers generally had an 19 interest and positive views toward science, and that 70% of a sample of their participants were educated to at least an undergraduate degree level. There is slightly more demographic data available for online citizen science projects and Table 2.1 summarises the demographic information currently available from seven projects (SETI@home, 2006, World Community Grid, 2013, Estrada et al., 2013, Holohan and Garg, 2005, Krebs, 2010, Raddick et al., 2010, Reed et al., 2013). Much of the information about the demographic characteristics of those taking part in online citizen science projects, are from distributed computing projects. Table 2.1: Demographic data of citizen scientists obtained from 7 published studies Author / year Project and sample size Demographic details of sample Holohan and Garg, 2005 Various distributed computing projects including SETI@home and GIMPS (Great Internet Mersenne Prime Search) n=323 98.4% were male, and most aged between 26 and 49. 70% based in USA and Canada, and 24% based in Europe. SETI@home team, 2006 SETI@home distributed computing project, n=142 000 92.74% are male, and 61% were aged 20- 39. Krebs, 2010 malariaControl.net distributed computing project, n ranges from 693 -1097 56% of participants were based in Europe and 33% in North America. Most were aged between 20 and 50. 87.8% were male (n=693). Most of the survey participants were IT professionals. Estrada et al., 2013 Docking@home distributed computing project n=739 80% were male, and most males were aged between 31and 35. Female respondents were aged mainly between 46 and 55. Small representation of ‘ethnic minorities’. World Community Grid member study, 2013 World Community Grid collection of distributed computing projects, n=15 627 90% of sample was male, and most have a “technical knowledge base”. Most aged between 25 and 44. 36% work in information technology. Reed et al., 2013 Zooniverse projects, n=199 67.3% were male, with a mean age of 40.7. Most based in USA or UK. Many had a college degree (119 participants provided this info on education). Raddick et al. 2013 Galaxy Zoo, n= 10 708 82% are male, and the mean age is 43.2 with no clear age trends. Most respondents are from North America and Europe. Over half have at least a bachelor’s degree. 20 These studies suggest that online citizen science projects may attract more male than female participants. Previous attitudinal research carried out in the UK suggests that men may be more engaged with science than women, taking a greater interest in science- related issues and participating in more science-based activities (RCUK, 2008). The underrepresentation of women in computing-related academic disciplines and professions has also been well documented (European Commission, 2012, Camp, 2012, Klawe et al., 2009), and this may account for the small proportion of female participants in distributed computing projects. Two of these studies have also shown that participants may be well educated with significant proportions having a tertiary-level education (Reed et al., 2013, Raddick et al., 2013). The survey of World Community Grid participants illustrated that many have a ‘technical knowledge base’ and work in IT-related professions (World Community Grid, 2013). Overall, there is little information regarding those who participate in distributed thinking projects, and no information regarding those who play citizen science games. However, the degree to which the samples in these studies are representative of the wider group of project participants is not always clear. In some of these studies, the total number of project participants is not reported. This is the case for the Holohan and Garg study (2005), The World Community Grid survey (2013) and for the SETI@home survey (2006). There is also a discrepancy in how overall project numbers are reported. For example, in the Estrada et al. study, the total number of registered participants is provided (estimated to be 27 000 in Docking@home), while Krebs (2010) gives the number of active participants (estimated to be 10 000 for MalariaControl.net). Raddick et al. (2013) report that the total number of registered users of Galaxy Zoo was in the region 21 of 175 000, while the study of Reed et al. (2013) does not specify which Zooniverse project study participants were recruited from. The concepts of ‘registered’ participants and ‘active’ participants will be considered in more detail in Section 2.5 and in Chapter Four, as it is likely that that this distinction is of relevance to online citizen science projects. For example, what proportion of registered participants actively contribute to a project, or regularly undertake project tasks? Previous work exploring other online communities has demonstrated that not all those who register an initial interest in a project will become active contributors (Kittur et al., 2007, Shirky, 2009, Preece and Schneiderman, 2009, Clow, 2013). While the existing demographic data is of interest, a greater understanding of who participates in these projects may help to shed light on whether online citizen science can offer increased opportunities for wider participation in scientific research, or whether their appeal is more restricted. More information on participants may be of interest to those thinking about establishing an online citizen science project, particularly if they have educational or public engagement goals as well as research goals. The lack of data in this area, as well as the potential relationship with other aspects of participation (such as the level and type of activity in a project, or the motivation to participate), have formed the basis of the first research question. Research question 1: Who participates in online citizen science projects? This research will aim to add to the existing body of data by extending the range of online citizen science projects that are explored in this way, and by asking a wider range of questions of participants. In addition to data relating to age, sex, educational background, and occupation, I will also explore general level of interest in science and 22 participation in other science-based activities, something which has not been considered in previous work relating to citizen science. 2.4 Motivation to participate in online citizen science projects For anyone setting up an online citizen science project, it is important to know how to attract and retain participants if the tasks are going to be completed and the research goals are to be achieved (Reed et al., 2013, Rotman et al., 2012). For example, what makes a project attractive and interesting for a potential participant? How can interest and participation be maintained? Understanding participant motivation is key to the long-term success and sustainability of a project. Hundreds of thousands of individuals have registered to participate in citizen science projects, and they do so without any monetary re-numeration. It is essentially a type of volunteerism (Sproull, 2011). Research has been carried out on the motivations of those who volunteer generally and carry out charitable work (Houle et al., 2005, Hustinx et al., 2010), while a small number of studies have examined the motivations of those who take part in commons-based peer production such as open-source software production and editing for Wikipedia (Hertel et al., 2003, Kuznetsov, 2006). A consideration of this work may help to shed light on why people take part in online citizen science projects, and may provide some frameworks with which to consider this area further. 2.4.1 Motivation and volunteerism Volunteering can be defined as discretionary behaviour such as assisting, comforting, sharing and co-operating intended to help people other than oneself (Wilson, 2000). It has been further described as prosocial, non-obligated by family or friendship, and situated within an organisational context (Sproull, 2011). A number of frameworks have been developed to explore motivation for voluntary behaviour. Two in particular (see 23 Section 2.4.5) have been considered in relation to citizen science (Rotman et al.,2012, Raddick et al., 2013). Work by Clary et al. (1998) suggests that people are motivated to volunteer because it fulfils certain functions that reflect important features of self and identity. For example, people choose an activity because it allows them to express the values that are important to them. In this model, volunteering serves six potential functions. 1. Values: volunteering allows individuals to express values related to their altruistic and humanitarian concern for others. 2. Understanding: volunteering provides an opportunity for new learning experiences and to exercise knowledge and skills that may otherwise go unpractised. 3. Social: volunteering provides opportunities to be with one’s friends and peers, or engage in an activity that is viewed favourably by others. 4. Career: volunteering may provide career-related benefits (i.e. the development of new skills, leadership opportunities). 5. Protective: volunteering may protect the ego from negative features of the ‘self’, and may serve to reduce guilt over being more fortunate than others, or to address one’s ‘personal problems’. 6. Enhancement: this function involves a motivational process that centres on the ego’s growth and development (e.g. some people volunteer for reasons of ‘personal development’). Volunteers who serve in roles that match their own motivations will derive more satisfaction and enjoyment from their service and are more likely to continue (Clary et al., 1998). 24 After studying ‘community involvement’ Batson et al. (2002) concluded that motives arise in a given situation and are a function of the values of the individual and the nature of the situation. Furthermore, motives can change over time – often quite quickly (Batson et al., 2002). They define motives as “goal-directed forces induced by threats or opportunities related to one’s own values” (p. 430). A goal can be either ultimate or instrumental. An ultimate goal is the valued state the individual is seeking to reach, while the instrumental goals are sought as they act as stepping stones to one’s ultimate goals. These are related to four different underlying drivers: 1. Egoism: where one acts to increase one’s own welfare. 2. Altruism: the ultimate goal in this instance is the increased welfare of others, apart from oneself. The source of this motivation is empathic emotion and extends to a group where one may be a member. Not all groups invoke equal empathy. 3. Collectivism: when an individual is motivated by the increased welfare of a group or collective. This is directly focussed on the ‘common good’, but may be limited to one’s own ‘in-group’. 4. Principlism: where one is motivated by the ultimate goal of upholding some moral principle such as justice. Different motivations interact and do not always work in harmony, and motives to promote the welfare of self or group can undercut or compete with one another. 2.4.2 Motivation and other types of activity There is a substantial body of work on motivation. While some has focussed on general volunteering, some work has been carried out in the area of formal education by Ryan and Deci (2000, 2009). This work has been applied to disciplines other than education, and has informed a previous study of motivation to take part in a distributed computing 25 project (Krebs 2010). Their work is based on self-determination theory which defines the natural tendency to learn and develop as something that is influenced by the inner world of drives, needs and experiences (Ryan and Deci, 2009). In order to be motivated, an individual needs to be moved to do something. These motivations can be either intrinsic or extrinsic (Ryan and Deci, 2000). Intrinsic motivation involves carrying out an action because it is inherently interesting or enjoyable. When intrinsically motivated, an individual is moved to act for the fun or challenge of an activity, rather than because of external ‘prods’, pressures or rewards. Extrinsic motivation is engaged when doing something leads to a separable outcome such as a reward, or a desirable reaction from a significant other to whom they feel (or would like to feel) a connection (e.g. family, peer group, society). Extrinsically motivated behaviours are not always inherently interesting and must be prompted. The motivation to join social movements (e.g. the peace movement in the 1980’s) has been explored by Klandermans (2003), and has formed the basis of a theoretical framework that has been considered by one previous study exploring motivation to participate in online citizen science projects (Nov et al., 2011b). In this model there are four main types of motivations. 1. Collective motives, where someone is motivated to join a movement because of the importance they attribute to the project’s goals. 2. Norm-oriented motives, where participants are motivated by the expectations or reactions of significant others such as family, friends or colleagues. 3. Reward motives, or the benefits that one can gain as a result of participation such as making friends or gaining reputation. 26 4. Collective identification, when individuals identify with the social group and its practices. These frameworks have been applied in some of the previous work exploring the motivation to participate in online citizen science projects, and will be considered in greater detail in Section 2.3.5. The use and relevance of motivational frameworks to this study will be explored further in the analysis and discussion of the data (Chapters Eight and Nine). 2.4.3 Motivation and commons-based peer production A small number of researchers have likened some online citizen science projects (particularly distributed computing projects) to other types of online collaborations such as the production of open-source software, or the production of open content such as Wikipedia (Benkler and Nissenbaum 2006; Shirky 2009). These types of projects have become known as ‘commons-based peer production’, a term first coined by Benkler (Benkler 2006). Commons-based peer production is made possible by the Internet and involves the collaboration of large numbers of people to provide information, knowledge or cultural goods without relying on economic factors or an over-riding management structure or hierarchy. Commons-based peer production is also highly granular and smaller tasks are allotted to distributed participants. Tight-knit online communities working towards a common purpose may emerge (Kreiss, Finn et al. 2011). A small body of work exists in relation to the motivations of those who produce open source software, and those who write articles for shared content websites such as Wikipedia. Like online citizen science participants, open-source software writers and Wikipedia contributors are not generally paid to produce content and their motivations for doing so have been attributed by researchers in this area to elements of both altruism 27 and egoism (Hars and Shaosong, 2002, Chang and Yang, 2009). Some of this work on common-based peer production has also utilised some of the motivational frameworks considered by researchers looking at online citizen projects. Studies examining the motivation of Wikipedia writers / editors have shown that they tend to be motivated by more altruistic reasons, which are often based upon the belief that information and knowledge should be freely available to anyone (Forte and Bruckman, 2005, Kuznetsov, 2006, Nov, 2007). Unlike open-source software, Wikipedia has no established public recognition system that reflects individual contributions (Schroer and Hertel, 2009). However, the history of every article is available and authors often claim ownership of their articles and keep lists of their contributions. Within communities of Wikipedia editors, this may serve as a system of recognition and is therefore an extrinsic motivation (Ciffolilli, 2003). Schroer and Hertel (2009) looked at Wikipedia as a social movement and used the framework of Klandermans (2003) to characterise motivations to participate. While Wikipedia does not focus on political protest, there is a common underlying philosophy associated with the goal of free knowledge for everyone (Nov, 2007). Open-source software contributors on the other hand, are often motivated by more egotistical concerns such as establishing a reputation as a competent coder and the securing of employment opportunities (Hars and Shaosong, 2002, Lakhani and Wolf, 2005). The development of open-source software involves a review system that is similar in some ways to the academic peer review system, and software is released only if it is deemed good enough by the reviewers (Oreg and Nov, 2008). However, altruism and an ideology centred on the free provision and access to software solutions are also strong motivations for many (Hertel et al., 2003, Lakhani and Wolf, 2005, Oreg and Nov, 2008). 28 Like Wikipedia, open-source software has parallels with other social movements, and a strong ideological core has also been observed within these communities (Hertel et al., 2003). Some recent work on OpenStreetMap, a website where participants provide local geographic information that can be shared and edited, suggests that motivation to participate varies between those classified as ‘casual’ mappers, and those considered ‘serious’ mappers (Budhathoki and Haythornthwaite, 2013). Casual mappers were motivated by a belief in the general principle that mapping data should be freely available, while more serious mappers (those defined as ‘core’ and repeat contributors) were more orientated to community, learning, local knowledge and career motivations. The observation that motivation can vary depending on type of contribution is of interest, and may have implications regarding factors motivating participation in online citizen science projects (Crowston and Fagnot, 2008). 2.4.4 Motivation and citizen science Despite the fact that contributory ecology-based citizen science projects have been around for many decades, and that hundreds of projects have been undertaken, there are only a small number of studies that have explored why people are motivated to participate. Two of these studies are based on conservation volunteers, and both found a desire to help, and interest in conservation issues were important motivators for participants (Bradford and Israel, 2004, King and Lynch, 1998). One of the more comprehensive studies to look at motivation and citizen science not only explored motivations for joining a number of conservation-based projects, but also explored motivations for remaining with the project (Rotman et al., 2012). Using the motivational framework of Batson et al. (see Section 2.3.1) as a guide, Rotman et al. 29 found that motivation was dynamic and temporal in nature. For example, an important primary motivator was a personal interest in the project combined with an interest in gaining something from the project (such as skills). These initial motivations are based on egoism, in that there is something to be personally gained from participation. As participants became more involved with the project over time, secondary motivations became more important. For example, factors relating to community involvement, and the opportunity to develop a better understanding of conservation issues were key in sustaining their involvement with the project. These motivations are based more on collectivism and altruism (Batson et al., 2002). Another important secondary motivator was recognition and attribution, and participants wanted to be given the appropriate credit for the work they had done. The temporal nature of motivation is of relevance considering that many citizen science projects (both traditional and online) can continue for many months or even years (Lintott et al., 2008, Cooper, 2011). Those setting up citizen science projects must therefore take into consideration these changing motivations during the design process and when formulating the project tasks (Prestopnik and Crowston, 2012). Another observation of note in this work was that motivations could be affected by the attitude of the project scientists towards the citizen scientists. For example, some participants saw their involvement in the project as an opportunity to improve and extend their knowledge of specific habitats and species. This motivation appeared to be stronger where volunteers had greater contact with scientists in the field, and when educational opportunities were facilitated and encouraged. This is the only study located, where the motivations of the project scientists has been considered (Rotman et al., 2012). This group also concluded that Batson’s model of motivation did not translate well 30 for citizen science projects as the projects are complex, involve many different tasks. However, the model of Batson et al. (2002) is of relevance here as it highlights the temporal nature of motivations. 2.4.5 Motivation and online citizen science While the motivations behind general volunteering, participation in commons-based peer production and ecology-based citizen science projects may provide some insight as to why people take part in online citizen science projects, it is important to note that these projects occur within a context which may be quite different. Online citizen science could be viewed as more opportunistic (e.g. there is flexibility with regard to time and place of participation unlike working for a charity, or collecting ecological data), and the projects may be more accessible in that they do not necessarily require specialist skills or knowledge to make a contribution (unlike writing a piece of open-source code, or an entry for Wikipedia). Participants in online citizen science projects are usually working within the research protocols and procedures imposed upon them by the project scientists, and do not organise themselves as in open software or open-content communities (Reed et al., 2013). All of these factors may have an influence on motivation; therefore, it is important to consider studies that have specifically looked at online citizen science. Studies that have reported findings relating to motivation of participants in online citizen science studies are listed in Table 2.2 along with the methodology employed to collect data. Four of them have focused on distributed computing projects. 31 Table 2.2: Studies exploring motivation in online citizen projects Author / year Project Data collection method Holohan and Garg, 2005 Distributed computing projects including SETI@home and GIMPS (Great Internet Mersenne Prime Search) Online qualitative Survey (n=37) Online quantitative survey (n=323) Nov et al., 2010 SETI@home Online quantitative survey (n=274) Krebs, 2010 malariaControl.net and other distributed computing (BOINC) projects Online quantitative survey using Likert scales (MalariaControl.net, n=1097; BOINC projects, n=408) World Community Grid member study, 2013 World Community grid collection of distributed computing projects, n=15 627 Online quantitative survey (n=15 627) no details of survey format, followed by 6 focus group sessions (n=unknown) Nov et al., 2011a Stardust@home, n=139 Online quantitative survey using Likert scales Raddick et al., 2010 Galaxy Zoo, n=22 Qualitative Interviews (via Skype or instant messenger), plus the confirmation of motivations in 826 forum posts Raddick et al. 2013 Galaxy Zoo, n= 10 708 Online quantitative survey using Likert scales How representative these sample sizes are is unclear, and the same issue that relates to the number of ‘registered’ vs. the number of ‘active’ participants as outlined in Section 2.3 in relation to Table 2.1 (demographic characteristics of citizen science participants) is of relevance here. Four of these studies are also included in Table 2.1 (Holohan and Garg, 2005, Krebs, 2010, World Community Grid, 2013, Raddick et al., 2013). The earlier study by Raddick et al. (2010) forms the basis for the later study, and is also drawn from a sample of 175 000 registered Galaxy Zoo participants. The studies on SETI@home (Nov et al., 2010) and Stardust@home (Nov et al., 2011a) do not report the total number of either registered or active participants, although the Stardust@home survey was sent to participants who had been active during the previous 30 days, which is perhaps an attempt to reach this group of active participants. 32 Holohan and Garg explored motivations in SETI@home and GIMPS (Great Internet Mersenne Prime Search) 13 participants as part of a wider study that looked at online collaboration (Holohan and Garg, 2005). Results relating to motivation suggested that survey participants were mainly motivated by making a scientific contribution, followed by the competition with other participants (those who take part in distributed computing projects are awarded points for each work unit their computer processes). Other important motivators were the social aspects of participation (interaction with other participants and the sense of community) and the opportunity to gain greater technical knowledge about computers. Nov et al. (2010) conducted a survey of 274 randomly selected SETI@home participants. They linked survey findings on motivation to actual levels of participant contribution as determined by their activity logs (Nov et al., 2010). They considered intrinsic and extrinsic motivations, and also whether these motivations were ‘self-oriented’ or ‘project oriented’. They found that ‘self-oriented’ factors relating to personal enjoyment and enhancement of reputation were important motivators, but that this was not statistically related to contribution levels. However, being affiliated to a team was positively related to contribution levels, and this result may suggest that being part of a social structure is important to these participants (Nov et al., 2010). This study is the only one identified so far that tries to link motivation with contribution levels. However, this is only measured in terms of work units completed (and the resulting number of points). This thesis will explore the relationship between motivation and contribution in more detail, and will also consider other types of contribution in addition to the completion of project tasks, such as participation in online discussions and forums (see Section 2.6). 13 The Great Internet Mersenne Prime Search uses distributed computing to find Mersenne prime numbers. A Mersenne prime is a prime of the form 2 P -1. Forty-six have been discovered so far (http://www.mersenne.org/). http://www.mersenne.org/ 33 Krebs (2010) looked at the motivations of participants in MalariaControl.net, a project where participants’ computers run epidemiological models of malaria infection. She also looked at the motivations of a smaller group of participants who participate in a number of other BOINC 14 distributed computing projects (Krebs, 2010). Using a previously developed list of 10 potential motivations (that were either intrinsic or extrinsic), Krebs found that wanting to contribute to a community (also referred to as ‘solidarity’), and getting involved in a particular cause were the most important motivators for MalariaControl.net participants. These two motivations were also the most important for the group of general BOINC participants. Less important motivators for MalariaControl.net participants were more extrinsic motivators relating to enhancing professional experience, networking, learning and knowledge sharing. These more extrinsic motivations have been found to be of greater importance in studies looking at those who write open-source software (Hertel et al., 2003, Lakhani and Wolf, 2005, Oreg and Nov, 2008). This study also suggests that recognition of a volunteer’s contribution is important and many respondents stated that it was important to provide tangible rewards (e.g. points). Respondents further highlighted the importance of feedback and communication from the organisers, and knowing what went on behind the science. Survey feedback also emphasised the importance placed on the social component of the project as some participants like to interact with others. However, the degree of interaction between participants, or between the participants and the ‘organisers’ was not explored in any detail. Nor was motivation was linked to level of contribution. 14 BOINC stands for Berkeley Open Infrastructure for Networked Computing. BOINC software (or ‘middleware’) is now used in most distributed computing projects. It is based on the software originally developed for SETI@home. 34 Motivation to participate was explored in a recent survey of participants in the World Community Grid, a collection of distributed computing projects overseen by the IBM Corporation that focus mainly on humanitarian issues such as cancer epidemiology, the search for disease biomarkers, and computational analysis of potential drugs for HIV/AIDS (World Community Grid, 2013). This survey of over 15 000 participants found that the main reason respondents participated (69%) was to support scientific research with goals that they believed were important. The next most important reason (cited by 58%) was to make use of their unused computing power. The latter motivation may be a reflection of the fact that a large proportion of respondents have a ‘technical knowledge base’ (over a third of respondents worked in information technology). The fact that all the results generated through the World Community Grid are publicly available was also important to some respondents. Similarly to the MalariaControl.net study, World Community Grid participants also wanted more information about the impact of their contributions and regular updates from the scientists and researchers involved in the projects. These studies examining distributed computing do not take into account the involvement of communities of computer hardware enthusiasts, sometimes known as ‘overclockers’. Overclockers build and design their own computers for maximum work output, and often use distributed computing programmes to test or benchmark their machines (see Chapter Four, Section 4.2.3). These communities are important contributors to distributed computing projects and may account for over 50% of project outputs (Bohannon, 2005). This community has not been well-researched and their motivations may well be different to other participants. I will consider the contribution and motivations of overclockers in my own research on a distributed computing project (Folding@home). 35 In addition to studies on distributed computing projects, work on participant motivations has been carried out on two distributed thinking projects: Stardust@home and GalaxyZoo. The same group involved in exploring motivations in SETI@home participants, has also explored the motivations of Stardust@home participants and related them to levels of contribution (Nov et al., 2011a). On this occasion, a framework based on Klanderman’s framework of voluntary participation in social movements (see Section 2.3.1), was used to categorise motivations. Survey results of 139 participants showed that ‘collective’ motives (i.e. the importance attributed to the project goals) and ‘intrinsic’ motives (i.e. the enjoyment of taking part) were the most commonly cited reasons for taking part. Intrinsic motivation was also found to be associated with ‘participation effort’ and the more the participant enjoyed the experience of taking part in Stardust@home, the more they contributed. However, most respondents (over 100) participated for less than two hours a week. The appropriateness of Klandermans model was not discussed in detail nor was the extent to which participation in online citizen science projects could be considered as a parallel to participation in a social movement. In a further analysis, this group compared the motivations of these participants in Stardust@home with a larger sample (1843) of SETI@home participants, and explored whether task granularity was related to motivation (Nov et al., 2011b). They defined low granularity tasks as more ‘passive’ involving less participant input, and this included running a distributed computing programme. Higher task granularity was defined as more ‘active’ and would include tasks such as image classification or analysis (such as the Stardust@home task). Results of this study suggest that task granularity is positively 36 correlated with motivation levels, but that additional research was needed to determine the direction of this relationship. Examining motivation in relation to actual contribution and to task granularity has not been considered by other research groups, and the findings of Nov et al. may have implications for the designers and managers of online citizen science projects. This research will further explore the relationship between motivation and contribution, and will consider other ways in which a participant may contribute to a project in addition to the main project task. Raddick et al. (2010, 2013) have carried out two studies exploring the motivations of GalaxyZoo participants. GalaxyZoo was one of the first distributed thinking projects and involves the classification of galaxies according to a number of easy-to-recognise physical characteristics. In the first study, the group interviewed 22 participants to explore their motivation for participating. Instead of using an existing framework to consider motivations they used a grounded theory approach and devised a list of 12 important motivators such as ‘astronomy’, ‘community’, ‘contribute’ and ‘discovery’ based on the interview feedback (Raddick et al., 2010). One important observation was that each participant had several reasons for taking part in the project. In this small group of participants, an interest in astronomy, and the desire to contribute to the project were among the most important motivators. Feedback to a forum posting asking why individuals were taking part were also examined, and the same two motivations were also found to be the most important among the 826 responses. The list of 12 motivations derived from participant feedback during the first study was further explored using a larger group of participants. A more detailed follow-up survey was constructed utilising Likert scales, and over 10 000 GalaxyZoo participants responded 37 (Raddick et al., 2013). The most important primary motivation for participation was making a contribution to science (this amounted to nearly 40% of the responses), and an interest in astronomy (just over 10% of responses). The possibility of making an important discovery was also important for about 10% of respondents. This small collection of studies has shed some light on what motivates online citizen science participants, but there are no detailed studies that examine the motivation of those who take part in citizen science games such as Foldit or EteRNA. Although a small study looking at the potential of games to attract participants to citizen science, found that four Foldit participants were attracted to the game because they were interested in science more than they were interested in games (Iacovides et al., 2013). The number of citizen science games has gradually increased over the past five years, and one game in particular – Foldit, has had some success in terms of scientific advances directly resulting from the work of project participants (Eiben et al., 2012, Khatib et al., 2011b). A growing number of research groups and learned societies are exploring the use of science-based video games for informal science learning and public engagement (Curtis, 2014b). Also, the proportion of the population in the UK and USA who play video games is growing (Anderson and Rainie, 2012; IAB, 2011). Therefore, more work examining the motivations of those who take part in this type of project would be of interest, and may illuminate what approaches, features and designs work well in attracting and retaining participants. Using elements of video games, or ‘gamifying’, has been applied to other types of online citizen science projects (e.g. Eyewire 15 ) with some 15 Eyewire is a project that enlists the help of volunteers to help map the 3D structure of neurons. It uses elements from video games both in its graphical interface, and in its use of competition and leader boards (see http://blog.eyewire.org/about/) http://blog.eyewire.org/about/ 38 success. Through a detailed examination of the citizen science game Foldit, this research aims to address this gap in knowledge. There has been some consideration of whether participant motivations remain constant over time, and if the factors that draw an individual to a project, are the same that sustain their interest over months or years (Rotman et al., 2012). This area is of interest, and has implications for those considering setting up an online citizen science project and for a project’s sustainability. However, most of the detailed work in this area was carried out in contributory ecology-based projects (Rotman et al., 2012) and further exploration of this phenomenon is needed in online citizen science projects, particularly as previous research has shown that some online citizen science projects have a high attrition rate (Nov et al., 2011b, Ponciano et al., 2014). My research explores motivation to remain with an online citizen science project as well as the motivation to join. In addition to the motivations of citizen scientists, the motivations of professional scientists and developers who set up online citizen science projects are not well understood. Only one study has been found that has considered the motivations of professional scientists to take part in citizen science projects, and this was the study of Rotman et al. (2012). They surveyed and interviewed scientists involved in ecology-based citizen science research and found that many saw these projects as a way to facilitate large-scale data collection. Furthermore, many of the scientists preferred to have volunteers limited to the role of data collectors, while they asserted themselves as the leaders of the research. While data collection was the primary motivation, the secondary motivations were more altruistic, and scientists saw these projects as a vehicle for educating members of the public. The authors of the study state that understanding the motivations of both scientists and citizen scientists, may help to facilitate “broader, 39 sustainable, and more inclusive collaboration between scientists and volunteers” (Rotman et al., 2012, p 225). To explore motivations in a wider variety of online citizen science projects, and to build further on the work of Rotman et al. regarding motivations that sustain participation, the following research question will be addressed. Research question 2: What motivations initiate and sustain participation in online citizen science projects? Not only will the motivations of citizen scientists be explored, but also the motivations of scientists and developers who are involved in setting up and managing these projects. The views of this group have not been previously explored in online citizen science projects. The utility of the motivational models outlined for considering motivation to participate in online citizen science projects will be considered in Chapter Eight. Only one previous study (Nov et al., 2011b) has compared the motivation to participate in two online citizen science projects. This research will compare motivation to participate in three different projects. Motivation to join as well as motivation to remain participating in a project will be compared. The work of Nov et al (2011b) on Stardust@home and SETI@home found that task granularity may be related to motivations to participate (Nov et al., 2011a). This is a notable finding, and a further exploration of this phenomenon in relation to the complexity of the project task and the availability of other related tasks may help to further understand motivations for participation. To explore this finding further the following research question will be addressed. 40 Research question 3: How do motivations vary between different types of online citizen science projects and their associated tasks? In order to address this question the nature of the task associated with each project, as well as other opportunities for contribution have been considered in detail. 2.5 Interaction in online citizen science projects Observations of online citizen science projects have shown that interaction between participants can occur in a variety of settings, often in online forums, or synchronous internet relay chat 16 . Project blogs written by scientists can also provide a venue for interaction between the project scientists and the citizen scientists, and participants are able to comment on new posts 17 . The topics that are discussed can vary widely. For example, new participants can ask those who are more experienced questions relating to the project task or about the project more generally 18 . Participants may also use them as tools to interact more generally with each other. Previous research has shown that participating in online communities and interacting with other participants can be an important motivator for some participants in online citizen science projects (Holohan and Garg, 2005, Raddick et al., 2013). Jennett et al. (2013) refer to ‘sociability’ in online citizen science projects, and suggest that participants may be motivated to take part because of the social interaction with other participants, or with scientists. They state that this interaction may help foster feelings of community and that there is potential for many different types of sociability to develop given the right project platform, although how participants interact within online citizen science 16 Internet relay chat (also known as IRC) facilitates transfer of messages in the form of text. Applications are based on a client/server model of networking and are used to enable synchronous communication in a group setting as well as between two individuals. 17 The Galaxy Zoo Blog: http://blog.galaxyzoo.org/. 18 The GalaxyZoo forum has examples of this: http://www.galaxyzooforum.org/index.php?PHPSESSID=omd860f8o896cr5ev2jqof37r0&board=13.0. http://blog.galaxyzoo.org/ http://www.galaxyzooforum.org/index.php?PHPSESSID=omd860f8o896cr5ev2jqof37r0&board=13.0 41 projects has not been described or explored in much depth in other studies (Holohan and Garg, 2005, Mugar et al., 2014). Holohan and Garg (2005) explored interaction between participants within two distributed computing projects (SETI@home and the Great Internet Mersenne Prime Search). Through surveys and interviews with participants, they found that social interaction with other participants was one of the most important aspects of involvement and a strong motivator for over half of respondents. They also claim that interactions were stronger within the various project teams as participants worked together to generate the most points and improve their ranking on the project leader board. However, little detail or analysis was presented relating to these interactions. Recent work by Mugar et al. (2014) has explored interaction between citizen scientists relating to learning the project task on two different online citizen science projects, Planet Hunters and Seafloor Explorers. They state that in order to sustain groups in the long- term, newcomers to the group need to learn how to be effective participants. Their work details how new project participants learn about the tasks by examining ‘work in progress’ through the online discussion threads, and by asking more experienced participants for advice. This is known as ‘legitimate peripheral participation’, a term first used by Lave and Wenger in their work on ‘communities of practice’ (for example in skilled trades such as midwives, tailors or butchers) (Lave and Wenger, 1991). This work describes the process of development from newcomer to an accomplished insider, as community members become more familiar with the tasks and practices of that community. In a community of practice, joint sense-making and problem solving enhances the formation of interpersonal ties (McLure Wasko and Faraj, 2005). Members, or practitioners, develop a shared repertoire of resources, experiences, tools and other 42 ways of addressing recurring problems (Wenger, 2006, Hanson-Smith, 2013). Over the past decade or so, further thinking about communities of practice has extended the meaning of the term to include online communities and interactions (Makriyanni and De Liddo, 2010, Hanson-Smith, 2013). Such communities of practice may be of relevance to online citizen science projects, and could result through sustained interactions between participants. While interaction between project participants may generate ‘sociability’ it may also form the basis for co-operation or collaboration between participants. Co-operation and collaboration are two distinct processes. The main difference is whether the tasks are divided up and individually completed (i.e. co-operation), or whether they are completed together through dialogue (i.e. collaboration) (Paulus, 2005). Co-operation involves a division of labour and possibly task specialisation, and individuals take responsibility for their part of the final product. In contrast, collaboration is a co-ordinated, synchronous activity, in which there is a shared concept of a problem and a process of shared creation (Dillenbourg, 1999). Much previous work on collaboration and co-operation has come from the literature on education, and has focussed on the role of collaboration and co-operation in learning both in ‘real’ and ‘virtual’ environments (Dillenbourg, 1999, Holliman and Scanlon, 2006, Lai, 2011, Paulus, 2005). Online citizen science projects are occasionally referred to as ‘collaborations’ between citizen and professional scientist, although there is, as yet, little documented evidence exploring how (or if) participants in these projects collaborate, or co-operate to achieve the research aims of the project. While there are many opportunities to interact with others taking part in an online citizen science project, there is little information relating to what proportion of participants take 43 part in these interactions. This research has explored interaction between participants in greater detail. For each of the selected projects in this research, data has been collected relating to who takes part in online interaction, how they interact and why they interact. Interaction has been considered with regard to motivation to participate, and within the wider context of how participants contribute to online citizen science projects. Opportunities for collaboration and co-operation have also been explored. The following research question focuses on this area. Research question 4: How and why do project participants interact online? Not only has interaction between citizen scientists been considered, but also the interaction between citizen scientists and those who manage the projects (scientists and developers. 2.6 Contribution to online citizen science projects Initial observations of online citizen science projects has highlighted the fact that while some projects can have tens of thousands (or even hundreds of thousands) of registered participants, only a small proportion actively contribute to a project. And in some cases (e.g. Stardust@home) the numbers of participants decreases rapidly over time (Nov et al., 2011a). The concept of ‘registered’ and ‘active’ users, while alluded to in some previous work (Krebs, 2010, Luczak-Rösch et al., 2014), has not been explored in any detail. It is hoped that this research will shed some light on this phenomenon, and the numbers of active vs. registered participants for the three selected projects are estimated in Chapter Four. Observations have also demonstrated that there are other ways to get involved in a project in addition to the main project task. Some citizen scientists get involved in 44 moderating forum discussions, translating project content, helping new participants learn the project task, and writing content that can support a project such as FAQs or project wikis. Little work has been carried out that explores the different roles that may be available to project participants, and to what extent citizen scientists become involved in them. A small number of studies exploring Zooniverse projects have considered different levels of contribution. A study of participants in Old Weather, a project that involves the transcription of archived Navy logs, refers to two different ‘types’ of participant: high contributors and low contributors, or ‘dabblers’ (Jennett et al., 2014). High contributors are more engaged by the social and competitive features of the project, but make up a small percentage of the overall number of participants. Most contribute on a much smaller scale, ‘dabbling’ in the project often for a short period of time. They are less likely to become involved in the some of the social features of the project. Another study that examined the pattern of participation in a number of Zooniverse projects found that there was a community of highly active users who, in addition to making contributions through the completion of the project tasks, also made the most contributions to the online discussions, thus becoming a ‘core community’ (Tinati et al., 2014). A similar finding was made by another group examining 10 Zooniverse projects (Luczak-Rösch et al., 2014), and by a group looking at participation in Galaxy Zoo and the Milky Way Project (Ponciano et al., 2014). This pattern of ‘uneven’ contribution has been explored in detail in relation to other online communities. While hundreds of millions of people use the Internet, only a small fraction of them move from just reading content, to become contributors of user- generated content (Ciffolilli, 2003, Brake 2014). Some contributors move beyond this 45 individual effort and become collaborators and form connected networks with others with a particular focus (e.g. a Wikipedia article, or an online game wiki). Of this group of collaborators, an even smaller number of participants may become involved in activities such as helping novices, or establishing and enforcing community policies (Kittur et al., 2007, Brandtzaeg and Heim, 2009, Makriyanni and De Liddo, 2010). Preece and Schneiderman (2009) have described this pattern of participation in more detail in their ‘reader-to leader’ framework (Figure 2.1), and it describes the journey that some individuals make from reading content, to contributing content, to collaborating with others, and eventually becoming a ‘leader’ of the community. While the number of ‘readers’ may be great, the number of individuals moving to each successive stage rapidly decreases (Preece and Shneiderman, 2009). Whether this pattern of participation can be observed in online citizen science projects has not been considered in detail, and a further exploration of this framework may help to illuminate the nature of contribution in online citizen science projects. This framework goes beyond a quantitative description of how much a participant contributes, and considers the types and nature of the tasks at each stage. Some online citizen science projects may eventually result in the establishment of an online community of practice perhaps consisting of a small highly-motivated group of ‘collaborators’ and / or ‘leaders’. In such groups, participants would be expected to become more involved and adept at the project task, or perhaps become involved in other project-related tasks such as moderating forums, teaching new participants, developing a project wiki resource, or directly providing feedback and recommendations to the project scientists. An exploration of this framework in online citizen science projects may also be useful in a consideration of online interaction. For example, there 46 may be little interaction between ‘contributors’, but more substantial interaction between ‘collaborators’ or ‘leaders’. Figure 2.1: the ‘reader-to-leader’ framework (Preece and Schneiderman, 2009) An alternative framework for exploring patterns of contribution in online citizen science projects can be found in the work of Haythornthwaite (2009) who has examined peer production communities in detail, and describes contributory behaviours as either ‘lightweight’ or ‘heavyweight’, and carried out by either ‘crowds’ or ‘communities’. In lightweight peer production, individuals can easily contribute, and there is usually a large set of participants (the crowd) who provide minimal additions to the endeavour as a whole. The ‘rules’ of contribution are defined by authorities or owners of such projects, and participants are not expected to play a role in determining the direction or the project as a whole. Participants do not need to make long-term contributions, nor do they need to interact with others. They are free to dip in and out when time or inclination allows. Haythornthwaite uses distributed computing projects as an example of lightweight peer production. 47 In heavyweight peer production success depends upon a critical mass of contributors (the community) who make significant time investments to the project and who interact with other participants in order to sustain the community. There are learned norms of interaction and language which are indicative of community membership. Outsiders or novices can be easily identified. In some cases, the participants determine the goals of the project. Haythornthwaite uses the academic community as an example of heavyweight peer production. While some endeavours can clearly be defined as either lightweight or heavyweight, there are some examples where there is some overlap. For example, Wikipedia demonstrates both lightweight behaviour from the crowds who edit and update articles, and heavyweight behaviour from the small community of editors who act as gatekeepers and decide which articles remain. Table 2.3 outlines the main features of lightweight and heavyweight peer production. This framework appears to be relevant to online citizen science as it encompasses a range of tasks and behaviours, some of which are readily observed in a number of projects. It also affords some flexibility as projects may exhibit characteristics of both lightweight and heavyweight behaviours, which may be relevant to specific project tasks. This thesis explores contribution from a quantitative perspective including the identification of active participants and a consideration of the level of contribution (e.g. how many hours a week do individuals devote to a project). It also explores the diversity of roles and tasks that are available to participants in each of the three selected projects and to understand what citizen scientists do while they are participating. 48 Table 2.3 Main features of lightweight and heavyweight peer production (Haythornthwaite, 2009) LIGHT HEAVY Contribution type, granularity and authentication Atomistic, independent Connected, revised, negotiated Addressing uncertainty, explicit knowledge Addressing equivocality, tacit knowledge Rule-based contribution Negotiated contribution Delimited contribution attributes Variable contribution attributes Single form defined by authority / owner Multiple forms defined by and authenticated by group consensus, norms Pooled interdependence Reciprocal interdependence Individual to group focus Anonymous Attributed History of contribution unnecessary History of contribution important for group Open membership, low threshold to entry Review, gatekeeping to join, high effort for membership Two-tier hierarchy: authority and contributor Multi-tier hierarchy: novice to expert, newbie to experienced Independent, repetitive , discrete contributions Continuing, contingent, norms-based contribution to product and process Recognition, reputation, reward Quantitative recognition mechanism Qualitative recognition Internally relevant to the individual application or the arena of contribution Internally relevant, permeable to field of interest Quantitative measures of contribution to product Internal: judgements of contribution quality, expertise re field of interest Peer review judgements of contribution to products and process The ‘reader-to-leader’ framework, and the ‘lightweight-heavyweight’ framework for peer production will be used to consider the level and types of contribution observed in the three selected projects, and will be explored in the analysis of the findings in Chapter Eight. Thus, the following research question will be addressed. Research Question 5: How can contribution to online citizen science projects be characterised? A further aspect of participation, which has been little explored, is how participants (both citizen and professional scientists) view their contribution and involvement in online citizen science projects. For example, do they feel they are actively involved in scientific research? The application of the ‘reader-to-leader’ framework may provide some insight 49 here also, and participants may view themselves as contributors, collaborators or leaders (or perhaps, none of these things). The final research question explores this area in more detail. Research Question 6: How do participants perceive their role in the project? A greater insight into the experiences of participants in this respect may also increase our understanding of how these projects are sustained over time, and why individuals participate. 2.7 Conclusion Online citizen science projects have increased steadily in number over the past decade (2004-2014). Hundreds of thousands of individuals have participated in these projects, and new projects continue to appear. Furthermore, there have been significant scientific developments as a result of the efforts of citizen scientists and these have been published in a number of high-profile science journals (Khatib et al., 2011a, Khatib et al., 2011b, Lintott et al., 2009, Lintott et al., 2008, Schwamb et al., 2013, Wang et al., 2013, Schmitt et al., 2014). This analysis of the literature has identified a number of gaps in the understanding of online citizen science projects, particularly relating to who participates, why they participate and how they participate. While previous research may have explored a single aspect of participation (most notably motivation to participate), there has been less consideration with regard to how these aspects of participation may be inter-related. While the research questions aim to address all of these aspects of participation, the subsequent analysis will aim to adopt a more holistic approach to understanding online citizen science projects, and explore how these aspects of participation may be connected. This research will also examine how these aspects of participation vary 50 between different types of online citizen science project, as there has been very little comparative work carried out in this area so far (Nov et al., 2011b). The following research questions will be addressed. 1. Who is participating in online citizen science projects? 2. What motivations initiate and sustain participation in online citizen science projects? 3. Do motivations vary between different types of online citizen science projects and their associated tasks? 4. How and why do project participants interact online? 5. How can contribution to online citizen science projects be characterised? 6. How do participants perceive their role in the project? Three separate projects have been investigated which have different types of tasks, and different online platforms. One project from each of the major ‘types’ of online citizen science project (See Chapter One, Table 1.1) have been selected: Folding@home (a distributed computing project); Planet Hunters (a distributed thinking project); and Foldit (a citizen science game). These projects will be described in greater detail in Chapter Four. While I have examined three specific cases, some of the findings may be relevant to other online citizen science projects, and may provide some practical information for those considering setting up or managing a project of their own. 79 In Folding@home I chose to email individuals who were members of two important sub- communities of participants who make up the active group of project participants. One of these communities in particular (computer hardware enthusiasts), has not been investigated in any detail previously (see Chapter Four, Section 4.2.3) and this study provided an opportunity to look at these participants in more detail. Thus, in the case of Folding@home a structured sample of 60 participants was emailed. Structured sampling is used to explore the views of a particular group or sub-set of a population (Holliman, 2005). A total of 15 Folding@home participants agreed to be interviewed. Using this approach meant that only active participants across the three projects were included in the subsequent analysis. During the thematic analysis of this interview feedback, it became clear that after 8-10 interviews there were few, if any, new codes generated. This was the case in all three projects. Getting access to project scientists proved to be a lot more difficult than getting access to citizen scientists. In some cases my requests for interviews were not responded to, and in one project (Foldit) it took nearly 18 months before I was able to talk to key members of the project team. Many of the individuals concerned are busy running both research labs, as well as co-ordinating citizen science programmes, and it is often difficult to find the time to schedule interviews. All of these projects have small management teams ranging in number of from three to six, so the number of interviews carried out with scientists and developers is significantly smaller than the number of citizen science interviews. Table 3.8 summarises the number of interviews for each project and how they were conducted. 143 Chapter 5: Foldit results This chapter is the first of the three results chapters and it presents the findings from the Foldit player’s survey (see Appendix B) and a thematic analysis of the interviews with 10 players and three members of the project team. 5.1 The online survey results In total, 37 players responded to the survey. While this is a small number it represents 12-18 % of the active playing community (see Chapter Four, Section 4.1.3). However, their views may not be representative of all Foldit players, and this must be taken into consideration when interpreting the results of the survey. Survey results have been broken down into four main areas of interest that have direct relevance to the research questions: demographic characteristics (RQ 1); patterns of participation (RQ 5); motivation and reward (RQ 2), and interaction with other players (RQ 4). Not every respondent answered every question. For the results of each individual survey question presented, n=37 unless otherwise stated. 5.1.1 Demographic characteristics The majority of respondents were male (78%) and most (68%) were aged over 40. The over-representation of men has been observed in other online citizen projects (Holohan and Garg, 2005, Krebs, 2010, Raddick et al., 2013, World Community Grid, 2013, Estrada et al., 2013). Fourteen (38%) of the respondents were from the US, and 18 (49%) were based in European countries. The rest were from Canada, New Zealand, and Thailand. Figure 5.1 illustrates that this group of respondents is very well educated. Seventeen respondents (46%) had an undergraduate degree, and nine (24%) had a postgraduate 144 degree. Very few (6 individuals) had only the equivalent of a high school education, although 2 of this sub-group of respondents were still attending high school. Figure 5.1 Highest educational level attained The majority of those that had a university education (both undergraduate and postgraduate) were qualified in a STEM subject (science, technology, engineering and mathematics) (see Figure 5.2). Figure 5.2 Highest educational qualification in a STEM subject 15% 3% 12% 47% 23% high school / A levels HND / technical college diploma some undergraduate course completed undergraduate degree postgraduate degree 5% 13% 3% 11% 43% 22% 3% no formal qualification high school / A levels HND / technical college diploma some undergraduate course completed undergraduate degree postgraduate degree online / self-taught 145 The occupations of respondents are listed in Table 5. 1. There is a high representation of those in the IT sector. One respondent reports their occupation as biologist. Table 5.1 Occupation or profession of respondents (previous occupation given if retired or unemployed) IT – related 13 Student 5 Economist 2 Gardener 2 Librarian 2 Biologist 1 Teacher 1 Conservationist 1 Clerical worker 1 Factory worker 1 Car mechanic 1 Inventor 1 Sales 1 Sports instructor 1 Publishing 1 Translator 1 Not given 2 5.1.2 Background interest in science and participation in related activities In addition to participation in Foldit, I was interested in whether respondents regularly play any other computer games. A number of print and online media articles written about Foldit make much of the fact that ‘gamers’ are helping to solve scientific problems (Bourzac, 2008, Gross, 2012), and the Foldit team appear to highlight this packaging of Foldit as a game when promoting it to external audiences (Burke 2012, Cossins, 2013). Twenty-two of the respondents (59%) stated that they did not play any other games apart from Foldit. 146 Whether respondents had a wider interest in science was explored by asking them if they had taken part in other science-related activities during the previous year, this included participation in other citizen science projects. Nearly half of respondents (18) had taken part in other citizen science projects and Figure 5.3 illustrates which ones they had participated in during the past year. Only two respondents stated that they were still currently involved with other citizen science projects. Figure 5.3 Participation in other citizen science projects (the number of participants participating in each project is at the top of the bar) Figure 5.4 illustrates that the majority of respondents (35 respondents) had participated in a number of other science-related activities during the previous year. Only two respondents reported not taking part in any. Watching scientific television programmes and reading popular science books were the most common activities listed, followed by listening to scientific radio programmes and visiting science centres or museums. 8 6 4 3 1 1 1 147 Figure 5.4 Participation in science-related activities during the past year Perhaps the most notable feature of the Foldit demographic data is how well educated the respondents are, and the fact that the majority have a tertiary-level education in a science, technology or engineering. 5.1.3 Patterns of participation Just over a quarter of respondents (10) had been playing Foldit for less than six months (see Figure 5.5). Seven respondents (19%) had been playing for over three years. This sub-group of long-term players contains some (approximately seven) of the Foldit core group, the small group of participants who are highly committed to the game, and have reached a high level of proficiency (see Section 4.1.3 in Chapter Four). 29 22 18 15 10 8 5 3 2 2 2 1 1 148 Figure 5.5 Length of time respondents have been playing Foldit Only six respondents (16%) described themselves as ‘beginner’ level, while 16 (43%) described themselves as ‘intermediate’ and 15 (41%) considered themselves to be ‘advanced’ players. These levels were not based on any particular criteria, but gauged how the respondent viewed their own standing within the game. Thus, this sample is skewed towards those who consider themselves to be relatively proficient in the game. Foldit players have the option to join a team. And among this group of respondents, 23 individuals (62%) belonged to a team, 11 (30%) defined themselves as ‘solo’ players, while 3 (8%) stated that played both within a team and as solo players. Figure 5.6 illustrates how long (on average) survey respondents report playing Foldit per week. Many within this group of respondents spend a considerable amount of time participating in Foldit. 27% 30% 5% 11% 8% 19% < 6 months 6-12 months 12-18 months 18 months - 2 years 2-3 years > 3 years 149 Figure 5.6 Number of hours per week (on average) respondents play Foldit Almost half of respondents (18) are playing Foldit for 15 hours or more a week. Only 11% of respondents (4 individuals) are playing for fewer than 4 hours per week. Given that the majority of respondents are in full-time employment or education, this could be considered a relatively high level of commitment to the project, particularly if this has taken place over many months or years. Question 6 asked respondents about their participation in the Foldit online community via the internet relay chat and the website forum. Thirty-one respondents (84%) stated that they interact with other players, with nearly a third of respondents (12 individuals) participating for more than 5 hours a week on average (see Figure 5.7). Of this group of respondents, 3 individuals specified that they tend to read posts and content rather than contribute input of their own. 3% 8% 24% 16% 49% < 2 hours 2-4 hours 5-10 hours 11-15 hours 15 hours or more 150 Figure 5.7 Number of hours per week respondents participate in online discussions (n=31) Most of the respondents specified that this interaction takes place during the game within the various chat windows. Therefore, this aspect of participation does not necessarily add any more time to their total project participation. However, some participants take the time to post in the online forum, or take part in one of the scientist’s or developer’s chats. 5.1.4 Motivation and reward Respondents were asked why they decided to play Foldit. This was to address the first part of RQ2 concerning motivations that initiate participation in an online citizen science project. The responses to this question are illustrated in Figure 5.8. Practically all of the respondents gave more than one reason for trying the game although a few motivations were dominant. 33% 29% 3% 3% 32% <1 hour 1-2 hours 2-3 hours 3-4 hours 5+ hours 151 Figure 5.8 Why respondents decided to participate in Foldit Clearly, one of the key motivations for beginning to play Foldit is the opportunity to make a contribution – to science, to medicine, to the development of new drugs. Twenty-two respondents (59%) stated that this was one of the reasons that they began playing Foldit. Thirteen respondents (35%) are motivated by a background interest in science, which may be related to the fact that many have undergraduate and postgraduate qualifications in science or technology-related subjects, and take part in a number of science related activities in addition to Foldit. The intellectual challenge attracted 10 respondents (27%), while eight (22%) tried Foldit after their curiosity was aroused. Only a small number of participants (3) were attracted 22 13 10 8 4 3 3 2 2 2 2 1 1 1 152 to Foldit as an opportunity to learn something new, while the same number of respondents was drawn to Foldit because it was a computer game. Respondents were asked what they liked best about Foldit. This question was used to elucidate the motivations that sustain participation in the project. The responses to this question are illustrated in Figure 5.9. Figure 5.9 What respondents like best about Foldit Again, making a contribution to research is important for 14 respondents (38%) and may motivate their continuing participation in Foldit. The interaction with others and sense of ‘community’ was mentioned by 13 respondents (35%). Learning something new and developing new skills were important for approximately a quarter (9) of the respondents. Only three respondents mentioned specifically that they thought Foldit was fun to play, although the enjoyment of playing can be inferred from other comments – particularly 14 13 9 8 8 3 3 2 153 those that refer to the community aspect of the Foldit. Once involved in the game, the importance of the community aspect of participation becomes more apparent. Perhaps working closely with others toward a useful scientific goal lends an air of seriousness to Foldit, and may explain why there are fewer comments referring to the ‘fun’ in the game. Research into computer games has generally found that players are strongly motivated to play if they find the game enjoyable or fun in some way (Williams et al., 2008, Yee, 2006, Yee, 2007). Question 17 explored the concept of rewards in more detail and asked respondents if they thought Foldit players should be rewarded for playing, and if so, what would be the most appropriate way. Overall, 35 respondents provided an answer to this question. Of this group, 23 (66%) stated that they did not think that Foldit players should be offered an extra incentive. The view that rewards could harm the cooperation within the Foldit community was expressed by several respondents. “More concrete rewards would probably increase the incentive, but would likely also reduce cooperation, so it’s a double-edged sword.” The potential commercial spin-offs of Foldit work and who should benefit from this was raised by three respondents, mainly among the nine who had stated that they thought Foldit players should receive a specific reward for their time and effort. Three other respondents would like more public (and specific) recognition. “Yes. At the very least by naming each individual who, by playing the game and coming up with a solution, contributed to a breakthrough, in publications. Have these people share in patent revenues and other sources of income.” 154 The responses to this question reinforce the observation that the respondents are attracted to the game in order to make some sort of contribution to science, or the advancement of scientific knowledge (and the societal and personal benefits that they may bring), rather than for any tangible rewards. The importance of making a contribution has been observed in previous studies exploring motivation to participate in online citizen science projects (Holohan and Garg, 2005, Raddick et al., 2010, Krebs, 2010, World Community Grid, 2013). The view (of most respondents) that a financial incentive of some sort would have a negative impact on the Foldit community and reduce cooperation indicates that these players are focussed upon the long-term outputs of the game, and the potential societal benefits of the results. 5.1.5 Interaction with other participants Question 16 asked players to describe their interactions with other players in Foldit. Of the 29 players who responded to this question, 25 provided responses that were interpreted as positive and they described interactions as friendly, supportive, helpful, enjoyable – even ‘world changing’. “I count my team mates as friends and treat them as such. Being in a small team we have got to know each other very well. In the wider community other players are always treated with respect and helped where possible. Generally everyone is very good and we tend to get on well with each other.” “So far it has been pretty good. I feel heard and respected for my interactions for the most part. With so many people and so many cultural back grounds it's not always easy, but it can be so much fun”. “Sociable, friendly, warm, supportive, intelligent.” 155 These respondents refer to talking “off-topic” with fellow team members, and discussing more personal topics. Foldit players come from a variety of countries and cultural backgrounds, which has to be taken into consideration when participants interact with one another – especially on global chat. Five of the respondents stated that they didn’t really interact very much, mainly because they are new to Foldit, or don’t have the time. Discussions between Foldit players can be quite technical at times, with players using scientific terms relating to the science of protein folding, or technical terms relating to coding recipes. Therefore, new players may not feel confident enough to make a contribution for some time. In total, only four of the respondents gave answers that could be interpreted as negative. One respondent described their interactions with other players as “insubstantial”, while there are privacy concerns for one respondent. “Privacy concerns keep me from sharing much personal info on the Foldit site.” Despite this small number of more negative comments, interactions reported by these Foldit players appear to be very positive and supportive, and friendships have been formed between participants, particularly those within the same team. As outlined in Chapter Three (Section 3.4.1) a quantitative analysis of the survey results was attempted, but the number of respondents was too small to permit any meaningful statistical tests. 5.2 Interview feedback The results of the online survey informed the interview schedule by highlighting areas that required further investigation including motivation, the skills required to play Foldit and the role of the project team. The results of the interviews have been subjected to a 156 thematic analysis using the approach of Braun and Clarke (2006). This approach has been described in greater detail in Chapter Three (Section 3.4.2). The major themes will be presented for both the interviews with Foldit players (n=10) and with members of the project team (n=3). 5.2.1 Interviews with Foldit players Emerging themes from the player interviews were identified through a process of inductive analysis. Nine of the interviews were carried out via email, and one via Skype. Table 5.2 details the emerging themes from the interviews with the citizen scientists 56 . Table 5.2 Emerging themes from interviews with Foldit players The ‘right stuff’ The essential qualities of a good Foldit Player, and how the high barriers to participation have led to the establishment of a small playing population. Qualities considered to be the ‘right stuff’ have a direct bearing on other aspects of player experience, and therefore, on other emerging themes. Learning how to play Foldit Player experiences and strategies involved with learning the basics of Foldit, and how they develop their own unique approach to playing the game. How participants have adapted to the game as it has evolved since its launch. The Foldit community The type and strength of online community interaction and some of the associated issues. The relationship between the players and the the Foldit team of scientists and developers. Why I play Foldit Factors that have motivated players to begin playing and remain commited to the game. Aspects of the game that are enjoyable for players. Science as a game Some of the issues involved in repackaging a scientific research problem as an online game, and how effective this approach is from the players perspective. The ‘right stuff’ This is one of the most recurrent themes that emerged from the data as it appears to have a bearing upon several of the other themes. It relates to the concept of an essential 56 Interview questions are listed in Appendix D. 157 skill set that is required to be a good Foldit player. One respondent referred to this as the “right stuff”. This wider skill set can be described as comprising of intellectual attributes such problem- solving skills, intellectual curiosity, and pattern recognition skills. It may also comprise of science-related skills including a background interest in the science, or a formal qualification in a STEM subject. However, the attributes most commonly referred to by interviewees were personal or character attributes such as perseverance, obsessiveness, patience, ‘people skills’, determination, and dedication. Several of the interviewed players stated that it is a combination of these qualities that makes a successful Foldit player. “By far the two attributes that help Foldit players are an obsessive personality and scientific inquisitiveness. With these, a player will eventually figure out strategies that help him to do well.” (Respondent FD1) Respondents also spoke of a “state of mind” or an “attitude” that was required by successful Foldit players. “More than knowledge or skill, the game requires a state of mind. The game requires patience because you can play for a long time without result of any kind.” (FD5) This attitude appears to be a combination of character traits such as patience, perseverance and a degree of obsessiveness. Approximately half of these interviewees are within the Foldit ‘core’ community, and these individuals are playing for several hours every day, as well as managing Foldit teams or moderating player discussions. 158 Even among the committed Foldit players interviewed, there is an acknowledgement that Foldit is a very difficult game to play, it can take up a lot of time (and financial resources), and some of the puzzles can be extremely challenging. “…participating in Foldit not only means having your computer running, but it’s really working. I spent hours constructing molecules…” (FD2) One respondent spoke about the misrepresentation of Foldit by news media, and the fact that new players had unrealistic expectations of the game. Ultimately, they weren’t prepared for the level of difficulty and commitment that authentic scientific research entails. “Many new players see this game through other media, see the words ‘play for science’ and jump in. They start the tutorials, not really paying attention and asking for help instead of actually reading what is presented to them, and then ask ‘am I curing cancer or AIDS?’. Then they realize it isn’t as glamorous as they thought and go back to their Call of Duty or Left for Dead. There are a few who persevere though, and start the road to actually contributing to the ‘collective knowledge’.” (FD9) The responses from the interviewees suggests that the Foldit project team have to cast their net far and wide in order to attract this small number of people who have the right background interests, technical skills and personality attributes. Correspondingly, those with these attributes may be more motivated to play Foldit. Learning how to play Foldit In a sense, the learning process is a test or a filter that determines whether an individual has the ‘right stuff’ to carry on playing the game, and players make decisions during the learning process that affect the way they subsequently play the game and interact with 159 other players. It is recommended by the Foldit team that all new players complete the set of tutorial puzzles. In addition to the tutorial puzzles, there is a player-constructed wiki that contains advice for new players as well as simulations of the tutorial puzzles. If one gets stuck, then they can view the solution via the wiki. It was generally acknowledged by the players interviewed (even those that have gone on to become very highly ranked players) that learning how to play can be difficult and time-consuming. “Learning to play Foldit was quite agonizing…it took me 17 days to complete the tutorials on my outdated Windows XP workstation.” (FD4) There is also a sense of people learning to play through trial and error and by “fumbling around”. “I learnt by taking the long road; reading the bubbles; making mistakes; resetting, and understanding each tutorial. As for the main puzzles, it was, once again, the long road. I looked at the wiki but I mainly just got on with it.” (FD9) While learning by doing, and finding one’s own way through the tutorial puzzles is important, there is also a sense of the importance of help from other players. Most of the interviewees (8) stated that they also received help from other players through global chat, team chat, the wiki, or from a specific person. “The people on global chat were eager to help. Most of my experience I got from the help of my (former) team members.” (FD3) However two players talked of teaching themselves, or preferring to figure out the game without any help from others. “I tend to be a ‘lone wolf’, so to speak and I like to figure things out for myself.” (FD9) 160 Not only does a player learn the basics of the game tools, they also start to develop playing strategies and preferences. The player learns what types of puzzles are more appealing, where their strengths lie, and to what extent they want to work with others. One interviewee described Foldit as a game where you have to find your own rules. “Foldit players don’t have the same background and follow different paths and mental representations to find one good solution….you have to let people find their own way.” (FD5) One subject that was raised by several interviewees related to their attitude toward helping other players learn how to play. Two interviewees stated that they did not help new players due to a belief that a new player needs to find their own way in the game. “I do not help others to learn. The really good ones will know how to learn themselves. Helping makes them lazy, it is an everyday sight in the chatroom. Appreciation for help is not often expressed. I do not waste my time and energy on those who do not have the right stuff’.” (FD8) Other interviewees expressed an interest in helping new players, and actively contributed to the wiki, or to online discussions. “I’m more than willing to help others who ask for assistance (I think I’m a bit of a control freak, but don’t tell anyone).” (FD9) An important part of learning to play for some players involves seeking out information relating to the science of protein folding, and most interviewees have increased their scientific knowledge through their involvement in the game. This information can be through informal channels, such as researching protein folding on the Internet, from other players, or from the information that is provided on the Foldit website. For one 161 interviewee, Foldit inspired him to return to formal education in order to understand the related science in more detail. “I knew nothing about chemistry or microbiology or proteins or protein structures before joining Foldit. As a result of my participation I went back to school and audited two classes in chemistry and organic chemistry to get a better understanding of the forces at work.” (FD1) The learning process is not restricted to those entering the game for the first time, and the game has changed since its launch in 2008. One of the most significant developments has been the increasing automation of some of the tasks. Instead of instigating puzzle moves ‘by hand’ one at a time, a sequence of moves can now be programmed using the scripting language Lua (see Section 4.1.1). These programmed sequences are known as recipes, and their application means the game can be speeded up and can be played even while the player is not physically present. Some of the players interviewed had strong opinions about this development, and it is evident that this change has had a significant impact on the game. Many players have learnt how to code in Lua and develop their own recipes, while others have remained ‘hand-folders’. Ultimately, Foldit players use many different approaches in learning how to play. The game is not static and players must evolve with the game. The difficulty associated with learning how to play, and the complexity of the game, keeps the barrier to entry high, and ensures that only those with the ‘right stuff’ become long-term folders. The Foldit Community The success of Foldit can be directly attributed to its community of participants. Foldit players come together to work in teams, or through the global chat facility on the game interface. They share ideas and recipes while ultimately advancing the goals of the 162 project. All of the participants interviewed interact with the wider community of active players, and for many this is one of the most important and enjoyable aspects of their involvement. Even players who do not belong to a team interact with other players on global chat, or help new players. Some interviewees talked about the pride they have in the Foldit community and in its achievements, about the friendships they had formed and about the respect they had for their fellow team mates (one respondent referred to his team as his “folding family”). The ability to work both cooperatively and collaboratively with others is a core ingredient of the underlying player skill set. “I like seeing how far we, my group and the whole community, can push the puzzles. I have made several friends in the Foldit community.” (FD6) “Being in a group, team play and contributing to the benefit of the group is important to me.” (FD1) One interviewee mentioned that the Foldit community is a good example of the ‘Pareto Principle’ in operation. Also known as the ‘law of the vital few’, this phenomenon is observed when the bulk of the work in a community is carried out by a minority of its members. “The Pareto Principle is alive and well on Foldit. A few folders make a huge difference to the rest of us.” (FD4) The unequal contribution of project participants has been observed in all three projects that have been investigated as part of this research (see Section 4.1.3, Chapter Four), as well as in other online communities (Ciffolilli, 2003, Kittur et al., 2007, Preece and Schneiderman 2009). It appears that some project participants have noticed this distinct pattern of participation also. 163 In addition to the interaction between players, there is some interaction with the scientists and developers from the project team. I asked the players how and if they interacted with the project team, and what they thought their wider role was within the project. Most respondents regard the management team as responsible for the overall co-ordination of the project, to supply participants with new puzzles, and to make sure the efforts of the players have some useful application. Two players stated that receiving feedback from the project team to the issues raised by the players is very important, yet one was slightly critical of the effectiveness of the response of the project team. “I offered a few ideas and posted a little on the forum….however, little positive reply. I would have liked a pat on the back once in a while to keep me going.” (FD2) Another interviewee expressed a desire to know more about how the contribution of players was advancing the science of protein folding. “It is a bit frustrating that it is not very clear what the contribution to science is as a result of our efforts. “ (FD3) Several players had little or no interaction with the management team, and did not really give them too much thought during their participation in the project. “To be honest, I don’t give much thought to them. They are ‘over there’ so to speak. If they pop into chat to ask something, I will respond, but mainly I see them as the fixers of problems, or the givers of news. And the disher-outer of puzzles.” (FD9) One of the respondents is a former number one ranked player, and he was the only interviewee in regular contact with the key project managers. 164 “I am in direct contact with Firas and Seth whenever I need contact with them or they with me. That should not change”. (FD8) The ties between the players, particularly those on the same team, appear to be more developed and closer than those between the players and the management team. There also appears greater evidence of collaboration between players, than between the project team and the players. However, there is evidence of collaboration between some members of the core group and the project team, and this can be observed within the transcripts of the developer and scientists’ chats 57 . Why I Play Foldit While the Foldit community is an important general theme that has emerged from the results of the player interviews, it is also an important motivation to play. For seven interviewees, being part of a diverse community with a shared goal is one of the most enjoyable aspects of their participation. This motivator has also been found among Galaxy Zoo participants (Raddick et al., 2010) and in participants in distributed computing projects (Holohan and Garg, 2005). Five interviewees stated that working with others towards a common goal, and getting to interact with participants from all over the world are key motivators for their individual participation in the project. “Being in a group, team play and contributing to the benefit of the group is important to me.” (FD1) “I have made several friends in the Foldit community.” (FD6) A few of the interviewees expressed a desire to help, and had been personally affected by a disease caused by protein mis-folding (such as Alzheimer’s). Foldit may be able to shed 57 This transcript of a developers chat shows players and developers working on a problem relating to the in-game scoring system: http://fold.it/portal/node/991799. http://fold.it/portal/node/991799 165 more light on such diseases in the future, and many wanted to be actively involved in helping to find a cure. “A family member fell ill with dementia and I wanted to help by searching for drug treatments. There were none and I stumbled upon Foldit.” (FD8) “I was diagnosed with relapsing-remitting MS about 4 years ago. So I am interested in medical research, particularly oligo-dendrocites- the cells that re-build myelin.” (FD10). Despite the more serious side of Foldit and its potential applications, several respondents stated that playing Foldit is a lot of fun, and a source of enjoyment. “The game is great fun. And it is nice that it might help science.” (FD3) Another important motivation is the opportunity to take part in authentic scientific research, and to make a contribution to a project with tangible outcomes. The fact that this could be achieved without a scientific qualification was very important to one of the interviewees. “..the real point is that Foldit simply allows us folks without the proper CVs, and would crawl over broken glass to participate given half the chance, an opportunity to do this stuff. It’s that simple.” (FD4) Some interviewees enjoy the intellectual challenge of the game, and enjoy the opportunity to develop their skills. “I am also intellectually curious about the progress the Foldit approach to solving this problem has taken….I now have a cursory understanding of the forces at work and some theories about what might make a successful strategy.” (FD1) 166 Community and motivation are closely related within this small population of Foldit players, as is the desire to make an intellectual contribution to the project. Motivations are also in turn, influenced by the personal attributes, characteristics and interests of the players (such as perseverance, creativity, scientific inquisitiveness), linking this theme with the pre-requisite of the “right stuff”. Discussions relating to why people play have also highlighted another important issue: how participants view their contribution to Foldit, and if they feel as though they are ‘doing’ scientific research. How a player views their contribution may have a direct impact on how long they stay committed to a project and the quality of their work. Among the players interviewed, there are differing views on individual contribution, as well as differences of opinion regarding whether participating in Foldit constitutes carrying out scientific research. Most of the interviewees clearly felt that what they were contributing, or indeed, participating in scientific research. “It does feel like I’m contributing to “SCIENCE”! Even if the details are a little nebulous.” (FD9) “I feel like I’m doing core uncredentialed science when I’m doing Foldit, which I strongly feel is a valuable adjunct to science done by trained scientists.” (FD4), One interviewee described the role of Foldit players as similar to that of other ‘support staff’. “We play a part. I don’t think we are doing science….it became scientific when it is analysed and prove to be true or useful by scientific methods. We help scientists to solve a problem like many other technicians useful in scientific research: the one who makes the 167 instruments or takes care of animals in a lab for example or the communication team…all these people are useful parts of the scientific work but are not scientists.” (FD5) Several respondents were less sure about their contribution to science. “For me it doesn’t feel like I’m doing science. It is a game and that’s also the idea the team want to promote.” (FD7) “I’m not sure how much contribution I’m making as I’m not in the top 20.” (FD10) Regardless of the degree to which interviewees felt they were ‘doing’ scientific research, some use language that is associated with scientific methodology and speak of developing and testing their “theories and assumptions”; “theorycrafting” with other players; and suggesting directions that the project may take. Many also confidently use technical terms associated with the science of protein biochemistry, and are able to hold productive and well-informed discussions with members of the Foldit team (some of the top ranking players in this small sample, work closely with members of the Foldit team). For five of the interviewees, the opportunity to get involved in science is what draws them to Foldit, the degree to which they see themselves as participating in science varies from one player to another. It is unclear from the interview data why this may be the case. From a closer examination of the demographic characteristics of the interviewees, it does not appear to be related to player rank, level of involvement in the project or to their level of formal science education. Their perceived role in the project could be related to their understanding or interpretation of what it means to be a scientist and to conduct scientific research, however, this was not explored in any detail during this research. 168 Science as a Game The games interface is a powerful draw for several players, and the system of points and ranking is a motivating factor for two of those interviewed. “I liked the ranking system, it motivated me. Apart from that, the score on each puzzle also motivated me to try and get the highest score.” (FD2) “I play by score; I have no real idea what I am doing, I just follow the score.” (FD6) However, for one of the players, the competitive nature of the game may possibly interfere with the overall aims of the project. “The players within the teams have collective knowledge in the true sense of the phrase, but their driving goal is, I feel, one of getting the highest score, rather than designing an actual working protein. (FD9) Another respondent talked about the conflict between cooperation and competition in the game, and also made reference to the approach taken by ‘gamers’ – those he felt were more motivated by score instead of the scientific objective of creating a stable protein. One interviewee took exception to Foldit being called a game, and felt that it trivialised the efforts of the participants. “Folding is work, hard work, NOT gameplay. I am involved in scientific research. I approach it as such. If this really was just a game I would stop today and dedicate my intellect, time, money and determination to a more worthy cause.” (FD8) 169 However, this tension does not appear to be an issue for other interviewees and one respondent described Foldit as “the synthesis of crowdsourcing, scientific discovery and play.” Packaging a scientific research problem as an online game may not be without its problems, and the potential ‘conflict’ between co-operation and competition may be an important consequence of this approach to online citizen science. This is encapsulated in the feedback of one of the interviewees who made the distinction between ‘gamers’ (who are motivated by score and rank) and ‘other’ types of participants (who are motivated by the scientific goals of the project). An examination of other citizen science games may shed further light on this issue. 5.2.2 Interviews with members of the Foldit team Three interviews were carried out with members of the Foldit team 58 . All three of these individuals are software developers for the project (including one of the original developers) who work with both the Foldit playing community and with the biochemists at Professor David Baker’s lab at the University of Washington where outputs of the game are utilised. Since being interviewed, one of these individuals has since left the project and the other two developers I interviewed were happy to be paraphrased. Transcripts from these interviews was analysed in exactly the same way as all other interviews (Braun and Clarke, 2006). This was the outcome of numerous attempts over an eighteen month period to speak to the scientists who are involved in the project. Table 5.3 details the emerging themes from the interviews with the Foldit developers. 58 Interview questions are listed in Appendix F. 170 Table 5.3: Emerging themes from interviews with Foldit developers The role of the project team The perceived roles and responsibilities of the developers in the Foldit project team. Retention and recruitment of players The issues surrounding retention and recruitment of players in Foldit as a consequence of its relatively high threshold for participation. Characteristics of the Foldit community The defining features of the Foldit community of players as viewed by the developers. How they work together, the development of smaller communities and specialisations. Community relations The issues that arise when working with a diverse group of volunteers. View of the player’s motivations Why do the project team think that people take part in Foldit? What are the perceived benefits for the volunteers? The role of the project team While the project developers are not scientists directly utilising the results, they have played a crucial role in the design, development, and implementation of the game (Cooper et al., 2010, Cooper, 2011). The developers invest a great amount of time in liaising with the player community and in responding to technical issues that arise from time to time. From my observations of the player forum, and the transcripts of the developer’s chats, a player (particularly a member of the core group) is as likely to come into contact with one of the developers as they are with one of the scientists from the Baker Group. While the developers I interviewed are not scientists, two of them stated that one of the main roles of the project team was to help advance scientific research. “The main role of the Foldit management team is to advance Foldit for the benefit of scientific research.” (FDPT1) 171 Liaising with scientists of the Baker lab is clearly an important aspect of the development team’s remit and two of the developers spoke of the need to work with them on a regular basis to discuss results, and what the players were doing. Scientists and developers also work together to explore new areas of research that could potentially be turned into a Foldit puzzle. While the developers do not have a scientific background, they have inevitably learned a fair amount of biochemistry through their involvement with Foldit. In addition to liaising with the scientists, the project team must also interact with the community. One developer stated that they work very hard to listen to all the feedback, and make changes where they can. Another developer stated that they must also put the needs of the scientists and of the research across to players. “[project members] have done incredible amounts of work to help the community understand the needs of the scientists and how they are helping science.” (FDPT1) However, this same individual went on to state that they occasionally use too much scientific or technical jargon, and that they may need to work more to make the science behind the project more accessible to players. “I think we can do a better job at explaining what it is exactly people are doing at any given time.” (FDPT1) In addition to working with the project scientists, the developers also need to ensure that Foldit is fun and enjoyable for those who are committed players and that it can attract new players (Yee, 2006, 2007). Recruitment and retention One of the developers is actively involved in communicating and promoting Foldit. There have been numerous news articles written about Foldit since it was launched in 2008, as 172 well as features on radio and television programmes. The appearance of these items, particularly on radio and television often result in an increase in the number of registered players and people taking a look at the game. I observed increased activity on the game and in global chat after Foldit was featured on a national US radio feature, and on a US television documentary. However, as there is a lengthy learning process associated with Foldit, most of this interest does not always manifest itself as a significant increase in the playing community. According to two of the developers interviewed, most people are ‘lost’ in the tutorial puzzles, and each successive puzzle has fewer players than the last. The developers are able to observe where, and at what stage there are the biggest number of losses, and work to try and address this. Foldit is a complex game, and the tutorial puzzles teach potential players about the project tools, so there aren’t always changes that can be easily made. There appears to be a general acceptance among the developers that Foldit will not appeal to all who take a look at it. One developer spoke of focussing on the current core community, and keeping the game interesting for these individuals. Characteristics of the Foldit community The developers stressed that the community of players is crucial to the success of the project. One of the advantages of opening this research up to the wider community is that it fosters diversity in the approach to the protein puzzles. Participants bring different skills and problem solving experience to the game which has been beneficial to the scientists at the Baker Lab. One of the developers stated that it is this diversity that has helped the game to advance scientific research. 173 “I think the main benefit of working with ‘citizen scientists’ is the diverse set of knowledge and skills you have at your disposal.” (FDPT1) There is also an acknowledgement that the skills of the Foldit community have surpassed the developers when it actually comes to playing the game. They have developed skills, new tools, and strategies over the past five years (for example, the development and coding of ‘recipes’, the automated sequences of moves), and the project has evolved as a result. While the remit from the scientists from the Baker Lab may determine the overall research goals of the project, some members of the community have a degree of influence in determining how those goals are met by suggesting particular approaches via the developers’ chats, and by highlighting problems with the project software. The developers referred to the small group of core players, and there was some agreement with my estimate of the number of these individuals (20). “There is a small group of players who contribute many hours to Foldit and that group tends to stay fairly static.” (FDPT1) This group in particular has accumulated many skills over the years, and have been responsible for one of the more notable changes in the game, the development of recipes. The impetus for this development came entirely from the players, and the developers assisted them in developing this new way of playing. This has changed the nature of the game for many, and according to one of the developers has added a new layer of competition to the game, as individuals work to create the ‘best’ recipes. The discussion about recipes led to a consideration of the competitive element of the game and how this interplays with the desire to co-operate with other players and work towards the ultimate goals of the project. While players strive to design the best recipes, 174 they will also share these recipes with members of their own teams, or with the rest of the Foldit community. The same can be said of the solutions to the puzzles which can also be shared. One of the defining features of the Foldit community that emerged from these developer discussions was that competition and co-operation sit side by side. Not all Foldit players use recipes when they are playing, and some remain ‘hand-folders’ and carry out all of the necessary moves and tweaks manually using the set of tools provided by the developers. One developer referred to different “ideologies” that have emerged within the community, as some players prefer and adhere to different strategies of playing. The developers try to accommodate these different playing approaches, and will sometimes set puzzles where recipes can’t be used, and have to be completed by hand-folding alone. The Foldit community has organised itself into smaller groups and teams. Not all players belong to teams, but some of the ‘solo’ players within the core group still interact and work with other players in the global chat window. The developers noted that there are always opportunities to interact with other players, and to help others along the way. Community relations There can be substantial interaction between the team of developers and the Foldit community particularly with the core players (e.g. through developers chats, and the ‘Feedback’ area of the website), and at times, this is one of the more challenging aspects of being involved in the management of the project for the developers. However, maintaining this level of interaction has led to significant developments and improvements in the game. 175 While having a diverse approach to problem solving is beneficial to the project in terms of problem solving, in can also lead to problems. “That same diverse set of people has a very diverse set of opinions on what is the ‘correct’ path when it comes to issues that arise……what we think may be best for the community may not be what some of the community thinks is best for the community.” (FDPT1) This developer spoke of spending time dealing with community relations, and of his attempts at dealing with the community in a sensitive manner, and the fact that some of the players could be “easily offended”. Another developer stated that some of the interaction between the developers and players could become quite difficult at times and that one had to not take feedback personally and should consider what issues were actually being raised in such exchanges. Two of the developers spoke about not being able to please all of the players all of the time. The importance of interacting with the community, rather than controlling or managing them, was stressed by one of the developers. “I don’t think the players need to be actively managed….I think active involvement in the community helps immensely.” (FDPT1 The same developer spoke about the appointment of a community relations manager in Foldit, and how that was helping to “give them a voice” and enabling them to get more involved in what the developers do on a day-to-day basis. View of the player’s motivations Why people become involved in Foldit and commit significant amounts of time are clearly of interest to the project management team. All of the developers had clear views as to why they thought people got involved. 176 “One of the main driving forces is the altruism of players, many want to advance biochemistry and see Foldit as way to do so. Some players have loved ones who are afflicted by diseases that have or may be addressed by Foldit.” (FDPT1) The desire to contribute to science was also considered to be a main motivating factor by the other two developers. Being able to contribute was thought to provide the players with a sense of fulfilment. Foldit was also described as a “game with a purpose”, and players could play knowing that they aren’t just wasting their time, but are actually making meaningful contributions to scientific research. Two of the developers spoke of the fact that players hear about Foldit through “science-based” channels such as popular science magazine articles, in the science or technology sections of newspapers, or science-based websites, so it naturally attracts those that are interested in science. The competitive, or gaming, aspect of Foldit was felt by one of the developers to be one of the most important draws for participants. The competitive element also meant that even if you weren’t doing that well as an individual player, your team could be doing well in the team rankings. However, another developer stated that he didn’t think this aspect of the game was important to everyone. “The competitive aspect of Foldit is also a major selling point. I don’t think it is the primary factor for many players, but it is a hook that keeps people more involved in the community.” (FDPT1) Many players remain with Foldit because of the community, and because they make friendships with other players. 177 “As Foldit is a large community, many subgroups have formed. These groups can be more personal and intimate, fostering another reason to come back, to meet and talk with your new found friends in the group.” (FDPT1) These groups also make it possible for players to help others and co-operate while working on the protein puzzles. This is also an enjoyable aspect of the game – perhaps one that appeals more to those who aren’t so motivated by the competitive aspect according one of the developers. One of the developers spoke about the importance of being aware of all the different motivations for participation in Foldit, and how these factors needed to be taken into consideration during the design of the game. Despite the fact that these developers do not directly use the results generated by Foldit players, they also have motivations behind their participation. One developer wanted to work with those who were external to academia. “I wanted to work on a project that was not only being used for research purposes, but also had a wider reach among the non-research community.” (FDPT1) One of the other developers who had had a major role in the development of Foldit, was more enthusiastic about the use of games in scientific research, and was clearly keen to explore this in his work with Foldit. He spoke about how public perception of computer games had changed in recent years, and that there was a greater appreciation of the other areas (apart from entertainment) when games could be useful, such as carrying out scientific research. 178 5.3 Summary of results The feedback from the survey illustrates that this group of respondents is predominantly male, aged over 40, well-educated, and with a background interest in science. These findings have revealed more about the characteristics of some of the active playing group and help to address RQ1 (Who participates in online citizen science?). Most of those surveyed are regular, committed participants who spend at least fifteen hours a week playing Foldit. Within this sample there are some members of the core group of players, who serve as ‘leaders’ in this community (Preece and Schneiderman, 2009). Thus, the survey feedback has provided a quantitative measure of participation and has helped to address RQ5 (How can contribution to online citizen science projects be characterised?). The survey has also provided information on how participants interact online (RQ4). The majority of respondents play in teams, and regularly use internet relay chat to communicate with, and work with other players. Respondents are motivated to begin playing because they want to make a contribution to science, and have an interest in science. They also like to develop new skills and some like the competitive aspect of the game. Making a contribution to science keeps individuals playing Foldit, as does the enjoyment they derive from interacting and being part of a community. These findings have helped to address RQ2 (What motivations initiate and sustain participation in online citizen science projects?), and have demonstrated some similarities with the findings of other studies (Holohan and Garg, 2005, Raddick et al., 2010, Krebs, 2010). As in the research by Rotman et al. (2012), motivations of players are dynamic and can change over time, and the importance of the Foldit community emerges as an important motivator for sustained participation. 179 The interviews with players have also highlighted the importance of having the ‘right’ combination of skills and personality traits. For example pattern recognition skills, intellectual curiosity, and an interest in science, combined with patience and perseverance. This suggests that those with these skills are more likely to be among the group of active and core players (RQ1). Feedback from the player interviews also highlighted the importance of the community, and how help from other players can be of importance during the process of learning how to play. The latter observation has been observed in previous work on how participants in Zooniverse projects learn about the project tasks (Mugar et al., 2014). More detailed information regarding the interaction between players, and between players and the project team has been highlighted (RQ4) and interview feedback confirms, as noted in my observations (Chapter Four, Section 4.1.5), that Foldit players exhibit a degree of independence from the project team. How participants perceive their role in the game (RQ6) has been explored. While a few players consider themselves to be actively participating in scientific research, most felt that they were providing more of a supporting role to the project scientists. Feedback from the developer’s interviews suggests that there is a core group of players who make a significant contribution to the project. These individuals are extremely skilled and their abilities in the game have overtaken those of the project team. The game is very difficult to learn and most individuals who take a look at Foldit do not progress beyond the first few tutorial puzzles. Given the high threshold to participation, recruitment and retention of players has to be addressed on a continual basis. These factors are relevant with regard to motivating and sustaining participation. Relations with the Foldit community can sometimes be challenging, but the need for regular communication and engagement was recognised. This feedback provided some insight 180 into how players and members of the project team interact (RQ4), and illustrates that Foldit developers aim to take account of, and to incorporate player feedback. The developers saw a real potential of computer games as a basis for citizen science projects which motivated their involvement in Foldit. This data, along with my own experience as a player demonstrates that this is a complex and challenging game that appeals to individuals with specific skills and interests. A small group of active players invests a great deal of time in the game, are committed to the scientific objectives, and has developed their knowledge of protein biochemistry. The project community is tight-knit, with friendships forming among the players as they co- operate and collaborate on the science puzzles. Members of the project team rely on and value the input of players, not just to achieve the scientific objectives of the project, but also to improve and develop the game. Some players are very influential in this respect and work closely in partnership with the Foldit developers. 181 Chapter 6: Folding@home results This chapter presents the findings from the Folding player’s survey (see Appendix C) and a thematic analysis of the interviews with 15 participants and two members of the project team. A detailed analysis and a comparison with the other two projects will be presented in Chapter Eight. 6.1 The online survey results This survey had the greatest number of respondents (407) which may be due to the fact that this project has the largest community of active participants out of all three projects. The higher response may also be due to the fact that the link to my survey was independently shared on a number of Folding@home team web sites and other discussion forums (e.g. overclocker forums) by some of the survey respondents. However, as outlined in Chapter Four (Section 4.2.4), the actual number of participants in Folding@home is difficult to ascertain, and various estimates range from 27 000 to 100 000. Therefore, this sample is still a relatively small proportion of the total number of participants, and represents approximately 1.5% of participant numbers (if the lower estimate is taken). However, it may represent of a greater percentage (4%) of the more active community of overclockers and hardware enthusiasts, which has been estimated to number 10 000. The results of the survey have been broken down into four main areas: demographic characteristics; patterns of participation; motivation and reward; interaction with other players; and views regarding contribution. For the results of the each individual survey question presented, n=407 unless otherwise stated. 182 6.1.1 Demographic characteristics The majority of respondents (63%, 255 individuals) were under the age of 40 (unlike Foldit). Most were based in the US, Canada and Europe. Only 11 respondents were from developing countries. However, one of the most striking results from this survey was the very small proportion of female respondents. Less than 2% were women (seven individuals in total). Of the studies that have looked at distributed computing projects, all have found that the majority of respondents to their surveys (usually over 90%) have been male (Anderson, 2004, Estrada et al., 2013, Krebs, 2010, World Community Grid, 2013). The disproportionate representation of men may be due to the appeal of distributed computing projects to hardware enthusiasts, and those with an interest (professional or amateur) in computing. While significant efforts have been made to improve the representation of women in IT-related industries, and in their uptake of computer science at school and university, women continue to be under-represented in this field (Commission, 2012, Camp, 2012, Klawe et al., 2009). This may have a knock-on effect regarding the numbers of women who are computer hardware enthusiasts and who take part in distributed computing projects, or even have an awareness of such projects. Of the seven women who responded to my survey, two are currently studying IT and electrical engineering, and two are IT professionals who describe their computing skills as advanced. Just over half of respondents (232) had a university degree, while approximately 26% (106) were educated up to high school or UK ‘A’ level (see Figure 6.1). However, 80 respondents (almost 20%) are currently students, which may explain the younger age 183 profile of this group of participants. Of those who had a university degree, the vast majority (82%) qualified in a STEM subject. Figure 6.1 Highest educational level attained by respondents The majority of respondents are in skilled professions. A significant proportion (37%, 150 individuals) stated that they work in an IT-related profession (see Table 6.1). A high representation of IT professionals was also observed in the World Community Grid survey (2013). There is also a significant representation (about 12.5%) from science or engineering-related professions. Table 6.1 Occupation / profession of respondents (n=403) IT Professional 150 Student 80 Business professional 43 Engineer (not IT) 26 Science / medical 24 Technical / mechanical 19 Unemployed (no previous occupation given) 16 Clerical / admin 8 Retired (no previous occupation given) 6 Retail 5 Education 5 Law enforcement / fire service 4 Military 3 other 14 23% 3% 14% 2% 35% 19% 4% high school (UK GCSE) UK 'A' levels / BTEC junior college (HND) some college course undergraduate degree postgraduate degree other 184 6.1.2 Background interest in science and participation in related activities Over half of respondents (212) had not participated in any other citizen science projects, and Folding@home was their first experience (see Figure 6.2). Nearly half (179) had taken part in other distributed computing projects with many respondents mentioning SETI@home and other BOINC projects such as the World Community Grid and Einstein@home. Participation in other types of citizen science project was limited with a small number (12) trying Zooniverse projects and Foldit. Figure 6.2 Participation in other citizen science projects * distributed computing Most respondents had taken part in a variety of science related activities in the previous year (Figure 6.3). Watching scientific television programmes and reading online science material were the most popular. Many read science magazines, and nearly a quarter had taken part in an amateur astronomy event during the past year. 212 179 6 6 2 2 1 185 Figure 6.3 Other science-related activities undertaken during the last 12 months 6.1.3 Patterns of participation Sixty per cent of respondents (244) stated that they had been taking part in Folding@home for over 2 years (see Figure 6.4). Sixty-one respondents (15%) were relatively new and had been participating for less than 6 months. I included an option for members of the Beta Testers, and members of the Professor Pande’s research group who were also participants. This was in order to help identify those individuals who may be making a greater contribution to the project. In total, 32 respondents are on the Beta Testers, and five are members of the Pande Group. 290 269 160 153 122 106 104 63 30 24 186 Figure 6.4 Duration of participation in Folding@home The survey included questions about participants’ involvement in Folding@home online discussions and forums. From reviewing the responses, it became apparent that interaction between participants is not just restricted to the ‘official’ Folding@home forum, but also takes place on the forums of some of the larger teams, and among other forums associated with overclocking and hardware enthusiasts (such as supplier and manufacturer’s forums). Sixty-five per cent of respondents (264 individuals) report using the ‘official’ Folding@home forum, while 10% of participants (40) report the use other forums (such as overclocking and enthusiasts’ forums). Five per cent of respondents (20) specified that they used both the official forum, and other discussion forums (see Figure 6.5). However, this figure may in fact be higher given the wording of the original question (“Do you participate in the Folding@home online discussions and forum…”), as some may have thought I was asking specifically about the official folding@home survey. Only 20% of (81) respondents stated that they didn’t participate, but again, this may be inaccurate as some of these respondents may actually participate in other forums and not the ‘official’ one. A closer inspection of the ‘no’ < 6 months 15% 6 months - 1 year 7% 1-2 years 9% > 2 years 60% Beta Tester 8% Pande Group member 1% 187 group reveals that 67 of the 81, are quite specific in their response stating that they were unaware of the forums, didn’t have the time or expertise to contribute, or simply wanted to run the software and not get any more involved in the project. Figure 6.5 Participation in online in Folding@Home discussions and forums (n=405) Respondents who participate in online discussions can spend a significant amount of time interacting with other project participants. Nearly 20% spend over 5 hours a week on forums. A closer look at this group suggests that some of this interaction occurs on the Folding@home discussion threads on overclocking websites (for example [H]ardOCP 59 , EVGA 60 , OverclockersUK 61 ). These forums provide a space for enthusiasts to help each other build better performing machines by sharing advice and expertise, and thus help participants increase the amount of points they accumulate on the project. However, 42% of respondents (170) who participate in forums do so for less than an hour a week. Most of the survey respondents (89%, 362 respondents) belong to a team and some respondents mention participating in team forums. Many of these respondents 59 [H]ardOCP Folding@home forum : http://hardforum.com/forumdisplay.php?f=32 60 EVGA Folding@home forum : http://forums.evga.com/tt.aspx?forumid=28 61 Overclockers UK DC forum : http://forums.overclockers.co.uk/forumdisplay.php?f=39 20% 10% 65% 5% do not participate in forums use other forums to discuss F@H (not offcial F@H forum) use 'official' forum use numerous forums to discuss F@H http://hardforum.com/forumdisplay.php?f=32 http://forums.evga.com/tt.aspx?forumid=28 http://forums.overclockers.co.uk/forumdisplay.php?f=39 188 (approximately 250) appear to belong to a team that is obviously related to an overclocking, hardware enthusiast, or gaming community. For example: Team EVGA 62 ; Maximum PC; and Team OCF (over clockers forum) to name a few. Some belong to national teams of hardware enthusiasts such as Dutch Power Cows and Hardware.no (Norway). 6.1.4 Motivation and reward Respondents were asked why they decided to participate in Folding@home. This was to address the first part of RQ1 concerning motivations that initiate participation in an online citizen science project. The responses to this question are illustrated in Figure 6.6. Figure 6.6 Why respondents decided to participate in Folding@home 62 EVGA is a hardware and motherboard manufacturer 207 128 74 68 27 21 16 12 11 2 189 Folding@home respondents usually had more than one reason for taking part in the project although the most commonly cited reason for participation (49%, 207 individuals) is to make a contribution of some sort. There appeared to be two distinct types of contribution: contributing to a ‘worthy’ cause; and making a contribution to scientific or medical research. Over a quarter of respondents (118) state that making a contribution to a worthy cause is one of the reasons they began participating in Folding@home. This is a distinct motivation to contributing to scientific or medical research, as many respondents view their participation as a type of a charitable donation – only they are donating their computer processing power instead of cash. Other respondents (89) refer specifically to making a contribution to science, and like to feel as though they are part of a wider research endeavour. The second most commonly cited reason for participation (31%, 128 respondents) was to fully utilise computing power, and this is a reflection of the presence of overclockers and hardware enthusiasts among this group of participants. Fifty respondents specifically mentioned their involvement with an enthusiast community in their response to this question, while many in this group made comments regarding minimal wastage of power or getting the most out of their machines. Another important reason that respondents participate in Folding@home is because they have had a personal experience of one of the diseases that is being researched through the project, either personally, or with a family member. Seventy-four respondents (18%) relate some experience of cancer, Alzheimer’s Disease, Parkinson’s Disease, or other degenerative disease, and through their participation in Folding@home, they address a need to take a more ‘active’ role in potentially beneficial research. 190 Few respondents state that they join the project for the community or for the competition, which is surprising given the involvement of overclocking community. However, in a previous study of distributed computing volunteers, the authors noted that respondents had ‘official’ reasons for taking part, which were often altruistic, and ‘unofficial’ reasons, which were often implied and usually related to the competitive aspect of participation and position on leader boards (Holohan and Garg, 2005). This observation may also be of relevance to Folding@home, and is also alluded to in the interview feedback (see Section 6.2.1). The next question was what they liked best about Folding@home. The responses to this question are illustrated in Figure 6.7. Figure 6.7 What respondents like best about Folding@Home Making a contribution either to scientific or medical research or to the project as a ‘worthy cause’ for approximately 150 (or 37%) of respondents, and the ease of using 150 75 54 47 39 37 26 22 18 15 8 191 Folding@home is appealing to many respondents, both within and outside the hardware community. Respondents who are more technically inclined talked about specific technical features of the project that they liked. Community and competition were mentioned more in relation to sustaining participation than in relation to initiating it, and approximately 10% of the respondents mentioned that these aspects were what they liked best about the project. Some of the communities referred to were those within the population of hardware enthusiasts or to members of the same Folding@home team. Respondents were asked whether they thought Folding@home participants should be rewarded for taking part, and if so, what would be the most appropriate way of rewarding them. More than half of the respondents (248) feel that there is no need to reward participants beyond what is already offered (e.g. points for completed work units and the chance to participate in a scientific research project), and that participation is its own reward. While a majority felt no extra reward was required for participation, over a hundred participants (25%) would like to see something extra offered to participants and made suggestions as to what that should be. These included virtual badges, better quality certificates (currently participants can download a certificate of participation at any time), tours of the labs for significant contributors, an annual convention for participants, discounts on related computing products, discounts on educational items, cash for reaching a certain level of points, prizes such as Folding@home T-shirts or mugs, or a ‘user’ of the day feature (this is done on many BOINC projects). However, the most commonly suggested reward (made by 31 respondents) was to be able to claim their 192 electricity costs as a tax rebate, in the same way that other charitable donations can be offset (this suggestion was specific to US participants). In addition to tax rebates, approximately 20 respondents indicated that they would like to see a greater acknowledgement of the contribution of participants in scientific papers. A similar number of respondents stated they would like to receive more information about the project. 6.1.5 Interaction with other participants Compared with other types of online citizen science projects, the opportunities for interaction between participants in a distributed computing project may appear at first to be limited. As a small number of respondents to the survey have pointed out that for them, participation simply involves downloading the software and forgetting about it. A few respondents were even unaware of the existence of the Folding@home forum. It is within the hardware enthusiast and overclocking communities that many participants are able to make significant contributions to the project. Their expertise (and willingness to shoulder higher electricity bills 63 ) has meant that the output of the project is significantly greater than it would be without their involvement. Where this extra contribution occurs, there are also opportunities for cooperation or collaboration between these participants as they strive to improve the performance of their machines. Survey respondents were asked: Do you have any interaction with other participants within the project, or with members of the Folding@home scientific team? If yes, how would you describe that interaction? 63 The amount of electricity consumed and subsequent costs will vary according to components and how many hours per day a computer is running Folding@home. This topic is commonly discussed on overclocking forums, as are ways to keep costs as low as possible. 193 A significant proportion of respondents (41.5%, 169 individuals) report little or no interaction with other participants. By ‘little’ this means occasionally reading the posts on the Folding@home forum, or other parts of the project website. Over 58% of respondents report some interaction with other project participants. However, not all of them describe this interaction (or specify where it takes place), but those that do, mainly report positive and helpful interchanges with other participants. Much of the reported interaction occurs within the context of a team forum, or on discussion forums that are frequented by hardware enthusiasts or overclockers. Over 100 (25%) respondents report involvement in such discussions. In addition to the team and overclocking forums, there is the ‘official’ Folding@home forum. Some respondents interact on this forum in addition to team and technical forums, while for some respondents, this is their only online venue for Folding@home- related discussions. Very few respondents report any direct contact with members of the project team. Those that do are usually members of the Beta Testers as they work together to make improvements to the project software. The Beta Testers will be explored further in Section 6.2.1. 6.1.6 Perceived contribution to Folding@home Question 12 on the survey asked whether participants felt as though they were involved in scientific research. In total I received 366 responses that went some way to addressing the question, although some of these were just ‘yes’ or ‘no’ answers. The responses have been classified into six broader categories (see Figure 6.8). 194 Figure 6.8 Do respondents feel as though they are involved in scientific research Over a third of respondents (35%, 142 individuals) do indeed feel as though they are involved in scientific research – particularly if they have been participating in the project for a number of years, or if they are a high-ranking participant. While some respondents feel unequivocal about their involvement in scientific research, approximately 22% of respondents (90) describe their involvement as very small, and view it within the wider context of a project in which many thousands of individuals are contributing. Twenty- eight respondents (7%) describe their role in Folding@home as a supporting one, and make a clear distinction between actually doing the research and enabling it or facilitating it. Fifty-seven respondents (14%) make the distinction between actually being involved in scientific research, and just making a contribution or donation. This group of respondents feel that they are merely donating their computing resources, or liken their involvement to making a cash donation to a charity. Approximately 11% (45) of respondents felt they 138 86 28 57 45 12 I feel involved in scientific research I am only a small part of a wider effort I am supporting & enabling the work of scientists I am just donating resources I don't feel involved in scientific research I don't know 195 were not involved in scientific research. Many of these respondents did not connect what they were doing with their computers with the process of doing scientific research. 6.1.7 Further quantitative analysis As outlined in Chapter Three (Section 3.4.1) a quantitative analysis of the survey results was attempted. The larger number of Folding@home respondents meant that some statistical analysis could be attempted. However, given the relative homogeneity of this group (particularly in relation to age and gender), only a limited amount of tests were undertaken. Chi-squared tests were carried out in order to explore possible relationships between participation in online forums (which was coded according to how many hours a week respondents participated) with the following: level of education; level of STEM education; time with the project; and profession (whether a participant was in IT or not). Only one of these tests produced any significant results. The amount of time a respondent spends on the online forums is related to how long they have been with the project. The longer they have been with the project, the longer they spend on online forums (Chi-Square = 45.85, P<0.001). 6.2 Interview feedback The results of the interviews have been subjected to a thematic analysis using the approach of Braun and Clarke (2006) which was covered in detail in Chapter Three (Section 3.4.2). The major themes will be presented for both the interviews with citizen scientists (n=18) and with members of the project team (n=2). The emerging themes will be outlined 64 . 64 The interview questions can be found in Appendix E. 196 6.2.1 Interviews with citizen scientists The online survey had identified the presence of two important sub-communities of participants in Folding@home: overclockers / hardware enthusiasts; and the Beta Testers. Members of these two groups comprise the active participants in Folding@home, and I wanted to find out more about them. Overclockers in particular are important contributors to distributed computing projects, yet very little has been written about them (Bohannon, 2005). Therefore, subsequent interviews of citizen scientists focussed on members of these two groups. During the analysis of the interview data, five themes were identified that were of relevance to all of the respondents, In addition, several themes were of importance to either the Beta Tester interviewees, or the overclocker interviewees. Table 6.2 outlines the themes from all 15 of the interview subjects. Table 6.2 Emerging themes from interviews with Folding@home participants What have I learned? What has been learned through participating in Folding@home, particularly with regard to the science of protein folding, and learning about the technical aspects of running a distributed computing project. Contributing to research Participants’ view of the way science is enabled by Folding@home. Openness and transparency, as well as opportunities to make a contribution to research. What I like about participating The more enjoyable aspects of participation and what keeps them participating in the project. The ‘mission’ Participants united in a community-wide effort to combat serious diseases. Personal experiences with the diseases that Folding@home investigates. Relationship with the project team Issues relating to the way the project team communicates with the community. 197 What have I learned? All interviewees were asked if they had learned anything though their participation in Folding@home. Approximately half stated that they had learned more about the science of protein folding either by seeking out and reading information, or more passively acquiring information after years of involvement in the project. “I have learned about protein folding and how studying it can help with research for cancer and other serious diseases.” (FH13) “Basic principles of protein folding structures. I admit most of the stuff is way over my head, but when you’re involved in something for so many years you automatically pick one or two things up.” (FH4) Several respondents outlined how they had learned more about distributed computing projects, and the potential these have to address real-world scientific problems. “Folding@home has taught me several lessons about how to use common technology for the greater good.” (FH8) “The project has put an interest of distributed computing in me; I am currently enrolled in ‘introduction to Parallel Computing’ on Udacity to learn more.” (FH10) Most of the respondents talk about what they have learned in relation to the technology (hardware expertise or software skills) associated with their involvement. All of these individuals are technically proficient, and are involved in computing either professionally or through their hobbies. For some of these participants, learning about computing is one of the reasons they became involved in Folding@home in the first place, so the opportunity to learn more about hardware, the use of GPUs in non-gaming contexts and programming in Linux (necessary for some aspects of overclocking) is important to them. 198 “It introduced me to Linux earlier than I would have found it….I’ve learned more about the various components of a computer.” (FH11) “One major aspect was my introduction to Linux. I never used it before and as I learnt about F@H, I realised Linux was quite useful for some aspects of F@H and eventually I learnt the basics of it….” (FH7) Three individuals mention other things that they have learned that are related to working within a larger community towards a common cause such as developing team-building skills, and being able to work with others. Contributing to research Most of those interviewed are motivated to participate in Folding@home to make a contribution to science. “As a child, I would dream of being a scientist but as I grew up, that dream just remained a dream. Thus, when the opportunity presented itself to me to help scientists find a potential cure to cancer, Alzheimer’s and other diseases, I was overjoyed and spent a considerable amount of time participating in the F@H project.” (FH7) “Having a distributed network to do the ‘dirty work’ for them is immensely helpful. It allows them to do more science ‘for free’ and also focus on analysing the data. We facilitate the science.” (FH9) Other views related to the science carried out by the Pande Group were also highlighted and the value the interviewees placed on the quality of the science became apparent. For example, several respondents felt that the work the Pande Group do is at the cutting- edge of science. 199 “Protein folding is at the cutting edge of science. I believe it will lead to many new drugs for diseases and possibly some cures.” (FH3) Several other interviewees highlighted the fact that the results of the research were open, and available for any other research group to use. “As the papers from folding are openly published for all researchers in the field to read, who knows how many accomplishments by other groups are inspired or aided by the result of folding at home.” (FH6) From comments like these, it is not only the opportunity to contribute to science that is important to some of these participants, but also the opportunity to contribute to a project that is open to other researchers and where the results are regularly published. This opportunity to contribute has been highlighted in previous studies of online citizen science (Holohan and Garg, 2005, Krebs, 2010, Raddick et al., 2010). What I like about participating For many of those interviewed (particularly those who describe themselves as overclockers), Folding@home gives them the opportunity to push their hardware to the limits, while also achieving something that is worthwhile. “All of the money, electricity and my time is going toward a good cause while at the same time I get to explore new hardware and configurations.” (FH6) “Rather than waste my time ‘online’ I have found a fun and educational way to spend my free time which has a nice balance of work and play.” (FH7) Learning about new technology and the application of hardware is enjoyable for several interviewees. 200 “You learn about how new technology is helping scientists reach goals which were once deemed impossible.” (FH7) Another respondent liked the fact that there is some degree of transparency regarding the results of their efforts. “I have never been a fan of senselessly throwing money at a cause because I never know where the money is going. For all I know, the money I donate goes to buying packs of paper. By donating my processing power, I know exactly what my contribution is doing.” (FH9) Most respondents enjoy being involved in a larger community working towards a common goal. “I fold to be part of a greater community in the Folding@home world – in this case, lately, as a member of the EVGA folding team where I have developed several years worth of friendships.” (FH 8). “The people on this team are great folks – always supportive and willing to help solve issues regardless of what the problem is. If they don’t know the answer, they’ll help dig and find it. Along with the teammates, just the general folding community.” (FH9) “I do enjoy interacting with others that are interested in this area.” (FH10) This enjoyment can stem from their involvement with the Beta Testers, their own team, or just the general Folding@home community. The ‘mission’ One of the respondents referred to the fact that he was involved in Folding@home because he had a “mission” and a “purpose”. 201 “Until everything we are fighting is gone, I will continue to fold.” (FH9) This is a reference to the diseases that the Pande Group seek to understand as part of their research. In the online survey, approximately a fifth of respondents stated that they had been personally affected by the illnesses that Folding@home investigates. This personal connection to these illnesses was also referred to by the majority of the interviewees emphasising that for some participants, the long-term goals of the project are of great importance. “Some of my closest relatives have been affected by some of the conditions researched by Folding@home. I wish to contribute in whatever way I can to improve our medical knowledge of these conditions…” (FH12) “I have experienced first-hand the effects of diseases like cancer and Alzheimer’s. I have lost family members to these diseases and have a good friend that has cancer, who I met through the folding community.” (FH13) “My grandparents were affected with diseases that PG [Pande group] research, particularly Parkinson’s and Alzheimer’s, and I would sincerely hope that the PG’s efforts will lead to a cure.” (FH5) “One day when / if folding@home data helps to eradicate few deadly diseases, I will proudly be able to tell everyone, that I helped finding a cure for this or that disease.” (FH4) Even for those who do not mention a personal experience of the Folding@home diseases, the long-term research goals of the project are recognised as being important. 202 Relationship with the project team Five of the interviewees who belong to the Beta Testers report some contact with members of the project team as they work to test new project software and identify any bugs. While most of those interviewed from the overclocking / enthusiasts’ community do not report any direct contact with the project team, some take an interest in project developments and in any change of policies or project parameters. As some overclockers invest significant amounts of time, energy and money (in the form of hardware and electricity) in Folding@home they like to maintain a level of involvement and keep up to date with the project team through the blog, and numerous folding forums. Practically all of those interviewed had some opinion about the project team, and particularly how the team interacted with the folding@home community. Several respondents wanted to see better general communication with the folding community. “The biggest thing is increased communication. What I really want to know is what is currently useful to the scientists and where they think the project is heading. It all comes back to knowing that the work we are doing is useful.” (FH6) “More behind the scenes content. While donors only get to see the final product, it would be nice to see what happens within the labs.” (FH7) The importance of communication in an organisational setting has been explored in the literature (Tourish and Robson, 2006, Salem 2008), and work on an online community that produces film reviews has also shown that communication should focus on the value of the contribution of community members (Rashid et al, 2006). One individual also stated that the scientists could be terse at times and sound “authoritarian”, and that this was most likely the result of them not having enough time to keep up with the folding 203 community on a regular basis. A feeling of an “us” (the folders) and “them” (the scientists) was also articulated by another individual. “They have a history of not being too understanding with the concerns of the contributors. This slightly elitist attitude is kind of a turn off. With folding it feels more like there are people in charge (like your boss at work) who hear what you say but don’t always seem to be listening.” (FH13) A few of the comments about the project team were quite negative. However, it is very likely that some interview feedback was influenced by a change in the BigAdv (Big Advanced Folding) points initiative (see Chapter Four, Section 4.2.6). These changes were announced in December 2013, and I carried out these interviews during the first few weeks of January 2014. A few comments directly referred to the BigAdv changes, and these respondents generally felt that changes were made without any consultation with the community, and felt undervalued as a result. “…whenever change occurs in the project, how the change affects users are not really the primary focus. I would argue that too many people end up feeling badly about their donation. They feel that they are not valued and end up quitting and bad mouthing the whole project.” (FH2) How points are awarded has become a contentious issue for several of these participants and is an important consideration for Folding@home as well as other distributed computing projects that rely on the efforts of overclockers. The reaction to BigAdv also suggests that some participants are highly motivated by points and by measuring the performance of their machines, something that wasn’t so obvious in the online survey results. Holohan and Garg (2005) in their work on distributed computing projects 204 identified ‘official’ reasons for taking part which were more altruistic, and ‘unofficial’ reasons which were based on pointes (see Chapter Two, Section 2.7.5). It may be that I am seeing a similar emergence of ‘official’ reasons for participating (such as making a contribution) and ‘unofficial’ reasons, which are more influenced by extrinsic (points) reward and competition with others. The Folding@home Beta Testers The Beta Testers are participants who have suitable computing experience who help make improvements to the software. The support they contribute is in addition to the professional software developers employed by the project. There are approximately 30 individuals in the Beta Testers (as of 2014). Seven members of the Beta Testers took part in the follow-up interviews. I asked these individuals specific questions about their involvement in this sub-community of participants. Three separate themes emerged from their feedback (Table 6.3) Table 6.3 Emerging themes from Beta Testers interviews Folding@home resource The role of the Beta Testers as a valuable resource for the project. Main tasks and responsibilities associated with membership. Motivation for involvement Why did these individuals decide to become members of the Beta Testers? Benefits of involvement ‘Perks’ or advantages to being involved in this group. The feedback from the interviews suggest that the Beta Tester is an important resource for the project. It allows the project managers to tap into the pool of technical expertise among the folding community and to exploit the diversity of hardware and operating systems in use. Sometimes Beta Testers members have hardware that the project team 205 doesn’t have access to. This makes the Beta Testers invaluable for testing new software, and cuts down on the issues and failures when new software is released. Furthermore, this help is given voluntarily, and the Beta Testers are unpaid testers who work alongside the professional developers employed by Folding@home. One interviewee described the Beta Testers as: “Necessary to critical…the computer code and science need to be sorted out.” (FH1) Another stated that many of the improvements brought about by the project team would not have been possible without the help of the Beta Testers. “Without it, f@h would not be where it is today. We would not be able to use our hardware to the fullest.” (FH4) Some of the individuals I interviewed had been in the Beta Testers for several years, two had been involved for over 5 years. Individuals join the Beta Testers in order to share their expertise with the rest of the community and to help out the developers in the project team. “I am helping others with like-minded values. Everyone there is trying to help improve folding beyond just folding…I would argue that I participate far more answering questions and testing than I do actually folding. The being helpful is the more significant reason.” (FH2) The fact that contributing to the Beta Testers is time consuming does mean that many Beta members do not have as much opportunity to earn points through actually folding. The same respondent also stated: 206 “They are not personally gaining from beta testing. If points are the currency of folding, they take less because when something fails they get far less points. They are the group that cares enough about the project to make it better.” Despite the fact that members of the Beta Testers can lose points by helping with the testing and developing work, interviewees also highlighted a number of benefits or privileges to be involved in this group. In particular, they get access to “cutting edge” software developments from Stanford, and get to see new developments before other members of the community. “…the cutting edge aspect of it. Getting to see things beforehand is exciting.” (FH3) “I love testing the cutting edge stuff from Stanford and being able to see what is coming before everyone else. Because after a while when you are folding the same old project it gets kinda repetitive. So every time the Pande group announces something new, I get excited.” (FH4) Another benefit of being involved in the Beta Testers is that members get to interact more with the project team, and sometimes get a glimpse of the inner-workings of the project. “There is more direct feedback from the scientists in PG [Pande Group] when participating in beta trials, and there is the opportunity to provide input into how details of the software can be shaped.” (FH5) “I really like the closer interaction with the researchers. Getting some insight into the inner workings of folding and providing them with valuable feedback is very fulfilling.”(FH6) 207 Some of the Beta Testers talk of the fun and enjoyment they have had while being involved in testing. “Building and learning about new hardware has been a blast. I hope to continue being useful to researchers and I currently plan to continue being an active folder and member of the beta team for the foreseeable future.” (FH6) Overclockers Eight overclockers took part in the follow-up interviews. Specific questions were about their involvement in this sub-community of participants. Three separate themes emerged from their feedback (Table 6.4) Table 6.4 Emerging themes from overclocker interviews An important project resource The importance of the overclocker community with regard to their technical expertise, and overall contribution. Role of this community in dissemination. Features of the overclocking community Characteristics of the community e.g. helpfulness, love of technology and competitiveness. The work within overclocking teams, cooperation and collaboration. Motivations Why do individuals become involved in overclocking and why do they contribute to Folding@home? Much of the ‘work’ in Folding@home is completed by this sub-group of participants and their continued involvement ensures computational power for the project group at Stanford (see Chapter Four, Section 4.2.3). In addition to this processing power, members of this community also contribute by bringing a broad range of technical skills to the project. These skills and knowledge are often shared with other enthusiasts to help them build better machines (usually via team or supplier forums), or by providing advice on what products to buy. Overclockers also share their knowledge more widely 208 with other folders on the Folding@home forum. This community also “spreads the word” about Folding@home, and individuals actively recruit others to Folding@home teams through the overclocking forums and networks, particularly when there are competitions. “The community [overclockers] as a whole is not only spreading the word of Folding@home through their various competitions, but by using Folding@home as a tool to measure their overclocks against, they push the work volumes for the program.” (FH8) “There’s a few ways the community contribute. Probably the most important one is technical help, either for new contributors or longtime users…..The contests that some teams run also help the project.” (FH11) “Overclockers / hardware enthusiasts tend to have multiple PC’s and powerful hardware. As a result they produce a much greater amount of points and completed work units that the average F@H user. They are also the type of people that are regularly present on the forums and will help out other users.” (FH13) The above comment refers to the helpfulness of those in the overclocking community, and this characteristic has also been referred to by several other interviewees. One respondent also described this community as “collaborative and innovative”. “In general these are people who share common interests and are very involved in computer hardware. I like how knowledgeable most people are.” (FH11) The generosity of this group was also referred to by one interviewee. “People can be quite generous in a number of ways. There are members of OCAU [Overclockers Australia] including myself who have laid out thousands of dollars in hardware for dedicated ‘folding rigs’ that just sit there 24/7 crunching numbers.” (FH15) 209 Overclocking has a highly competitive aspect to it (Bohannon, 2005), and Folding@home (as well as other distributed computing projects) provides a way of testing an individual’s skills and knowledge. “I see a lot of competition within the overclocking community as a whole – This is a community that naturally gathers those who love competition and / or those who are obsessed with getting the most out of a piece of equipment, much like those who tune their cars engines for maximum performance.” (FH8) “We all share our thoughts and brag to one another about out points per day (PPD).” (FH15) Most of the individuals interviewed had been overclocking for several years at least. Several spoke of the enjoyment they derive from being part of a community of like- minded individuals and of the opportunity to learn more about computer hardware. One overclocker described himself as an “eternal student of technology”. Feedback from the survey and interviews illustrate that some overclockers are drawn to Folding@home because it allows them to push their equipment and compete with other overclockers. Observations of the overclocker forum discussions also support this. Others either initially, or after some period of time with the project, begin to see the value of the work of the project group and some interviewees have become committed to the goals of the project. One interviewee referred to this community as being in “two camps”. “Generally, I see this group in two camps: A. Those that are folding because of their hatred of disease and wanting to eliminate it from the face of the earth. B. Those that fold because they want to build the best computer they can and tweak and twist it to get every 210 last drop of performance. I started out in camp #2…but the more I learned about what I was actually getting into, I’ve migrated to camp #1.” (FH9) One interviewee was not aware of the science behind Folding@home when he first joined the project. “When I first found out that the program was actually doing science and not stress testing, I looked straight into it and was amazed that we were able to simulate such things.” (FH15) Such statements suggest that motivations in this community are dynamic and can change over time, and support the findings of Rotman et al. (2012). While the technical aspects of Folding@home may be a stimulus for initial involvement, more altruistic motivations may operate once participants begin to understand the benefits of the research (Clary et al.,1998). 6.2.2 Interviews with project team members Two members of the project team were interviewed, including the founder of the project and an individual who has been closely involved with the project for many years. This individual has worked with both the scientists and the project developers on the software. Timing of these interviews coincided with the proposed changes to the BigAdv points scheme, and this has influenced the feedback to some extent 65 . Table 6.5 details the emerging themes from the interviews with the Folding@home project team. 65 The list of questions for project team members can be found in Appendix E. 211 Table 6.5: Emerging themes from interviews with Folding@home project team The role of the project team The perceived roles, responsibilities and tasks of the Folding@home project team. Motivations of the project team Motivations that led to the establishment of Folding@home and those that sustain continued involvement. View of participant motivation Why the project team think that people take part in Folding@home and some of the perceived benefits for the volunteers. Interaction with ‘donors’ The nature of the interaction between members of the project team and project participants, and some of the issues that can arise The role of the project team From this feedback, it became clear that there are numerous roles within the project team. Some roles are focussed on the science, others relate to maintenance of the project software, while others are responsible for communicating with the community of active participants. The two individuals interviewed have quite different roles within the project. The founder of Folding@home wrote much of the initial project software, and is now responsible for the overall management of the project. “I’ve done about everything. In the beginning, I managed and wrote some of the code. Now it’s management of the project as a whole.” (FHPT 1) When asked what the main role of the project team is, or should be, he stated that it was: “To do the best science we can. People are donating a vast resource to us and in the end, we will be judged by the impact of our scientific output.” (FHPT 1) 212 The other interviewee is now one of the dedicated moderators on the Folding@home forum, and also plays a role in addressing and prioritising software bugs that gets identified by the community. “I consider myself an expert in all things that can go wrong from the perspective of the general public (donor-community). (FHPT 2) This individual is also involved with much of the day-to-day interaction with project participants and provides an interface between the project team and the community of participants. “I have important abilities that help to bridge the gaps between Donors and Scientists. I spend several hours most days along with similar support from other Moderators / Administrators.” (FHPT2) There was an awareness of the diversity of the roles in the project, but also an acknowledgement that some roles do not always get the time they require, such as advertising the project. Motivations of the project team Folding@home was set up as an “ambitious scientific research project”. It requires significant computing power, and these needs are best addressed through a distributed computing framework. “We realized that there was an opportunity to do something grand. We had the algorithmic ideas in place and needed the raw horse power to push it forward’” (FHPT 1) This individual also spoke about the research output of the project, and that his work had been “fundamentally advanced”. 213 The second interviewee also spoke about the research output of the project, and that the impact of this work was something that motivated his continued involvement in the project. “It’s truly dedicated to the betterment of mankind (specifically the health of mankind)” (FHPT 2). He also enjoyed the intellectual challenge of his involvement, and some of the friendships that he made as a result. “It satisfies one personal goal: to be mentally challenged to learn more and to figure things out. It satisfies another personal goal, to do good for mankind. I’ve developed a number of good friendships, though that wasn’t part of my original goals.” (FHPT 2) This response demonstrates that other aspects of participation (e.g. forming friendships) can arise or develop over time, and may not have played a role in motivating their initial involvement in the project. View of participant motivation The interviewees were asked directly why they thought people became involved in Folding@home. Both felt that there were numerous motivations in operation. “For some it is the science – our goals and the ability to make a contribution. For some, it is the scale of the whole project and the interest in computers and what can be done with them.” FHPT 1 The above makes reference to the community of overclockers and hardware enthusiasts. This individual also acknowledged the social aspect of participation, and that this may important for some. 214 “I think the ability to connect, whether that be to make a difference, to do science, or to connect with other people – these are all important areas.” (FHPT 1) The fact that many participants had been affected by the disease being researched by the Folding@home scientists was referred to. “Most everyone has known someone who has been touched by the diseases being studied.” (FHPT 2) The same interviewee also stated that the fact that the project had resulted in a number of publications and was not profit driven may also be important to participants. “The multitude of publications prove that it is good science. It’s not driven by a profit- oriented company but contributes to the open uses of scientific advancements.” (FHPT 2) These responses corroborate much of the feedback on motivation provided by participants who took part in the online survey and interviews. Interaction with ‘donors’ The final theme to be identified in the interview feedback relates to how members of the project team interact with Folding@home participants (often referred to as ‘donors’). Issues that can arise as a result of these interactions were also identified. The founder of the project stated that they aim to get ‘involved and engaged’ with the community of participants. However, he also added: “…there are so many demands on my time, and there are so many donors to connect with.” (FHPT 1) This sentiment has been expressed by most of the project scientists who have been interviewed, as they try to balance the demands of carrying out research, with the 215 demands of managing large numbers of citizen scientist volunteers. Apart from these statements, this interviewee was reluctant to discuss interaction with participants in any great detail. Given the level of hostility directed towards the project team (and this individual in particular) observed on the Folding@home forum and several of the other folding forums in the wake of the changes made to the BigAdv points system, his reluctance to discuss these topics is perhaps understandable. The controversy surrounding BigAdv has highlighted some of the problems associated with working with a large community of volunteers, many of whom feel that their contribution to the project is expressed in terms of the points awarded to them. Some participants felt undervalued as a result of these changes, and by the fact that they were not consulted beforehand. The other interviewee who is a forum moderator is involved to a much greater extent in interactions with participants. “We maintain a forum environment that is conducive to constructive discussion, fight forum spam etc. in a way that properly fits the title of Moderator / Administrator.” (FHPT 2) This role of ‘interface’ between the project team and the participants is important in such a large community, and there is some recognition that the scientists don’t always have the time to do this. “Donors need to know that somebody is listening to them and supporting them, some of which I can do, a role that should not be overly burdensome to the scientists.” (FHPT 2) When asked about interaction with the folding community, he stated that most of his interactions were brief and tended to focus on a specific problem. There are some challenges however. 216 “Some [participants] are very knowledgeable and some are not. In brief interactions, it’s difficult to know which. It’s easy to incorrectly assume my audience is at one level when they’re really at a different level, making communications difficult.” This comment may help to explain the few negative comments that were made by participants in the online survey about the official Folding@home forum, and highlight some of the difficulties in managing asynchronous communication (James and Busher, 2009a, Abawayj 2012). 6.3 Summary of Results The feedback from the survey illustrates that this group of respondents is almost entirely male and mainly aged under 40. They are generally well-educated with a background interest in science and technology. Over a third (150 individuals) are in IT-related professions, and many (80) are currently students. This survey has highlighted the importance of an active community of overclockers and hardware enthusiasts, and has helped to address RQ1 (Who participates in online citizen science?). This group of active participants interacts online through Folding@home, team forums, and on forums that are linked to groups of overclockers and hardware enthusiasts. The longer a participant has been with the project, the longer they spend participating in online forums and discussions. These observations have helped to address RQ 4 (How and why do project participants interact online?) and RQ5 (How can contribution to online citizen science projects be characterised?). Over 200 respondents are motivated to take part in Folding@home so that they can make a contribution, either to science, or to a ‘worthy’ cause, and 74 respondents report that they, or someone close to them, has been affected by one of the diseases being 217 investigated by the project. One of the most commonly cited motivations for taking part is to fully utilise computing power and this is indicative of the involvement of the sub- community of overclockers and hardware enthusiasts (Bohannon, 2005). Respondents maintain their involvement with the project because they like making a contribution, and because getting involved is relatively straightforward. This feedback has helped to address RQ 2 (What motivations initiate and sustain participation in online citizen science projects?), and also highlights the importance of making a contribution when considered within the context of previous research (Holohan and Garg, 2005, Raddick et al., 2010, Krebs, 2010, World Community Grid, 2013). Over a third of respondents (35%, 138 individuals) feel they are making a contribution to science, while just over 100 respondents felt they were making only a small contribution, or enabling the work of the scientists. Over 50 respondents felt that they were just making a donation of their computing power, similar to a cash donation to a charity. This feedback has helped to address RQ 6 (How do participants perceive their role in the project?). Interviews with overclockers, and members of the Beta Testers has revealed that these groups of participants are important contributors to the project in terms of their computing output, and level of technical expertise they bring to the project (RQ 5). Some overclockers invest significant amounts of money in their computers and in running them which is of great benefit to Folding@home. Both of these communities see their contribution as crucial to the project (RQ 6), and most report deriving enjoyment through their involvement in these groups (RQ 2). However, there were some significant issues highlighted in these interviews relating to their relationship and perceived treatment by the project scientists (RQ 4 218 Interviews with two members of the project team reveal that advancing the science of protein folding and accomplishing “something grand” were the main motivators to setting up Folding@home (RQ 2). For one member of the project team, being able to contribute to improvements in human health were important, as were developing friendships and good working relationships with others. There was an acknowledgement that the efforts of participants have significantly advanced this area of science. However, recent issues associated with changes in the way points are awarded to high-level participants has clearly had an impact on the relationship with the community of participants. Managing a community of many hundreds or thousands of participants has been challenging for the project team during the time when much of this data was collected. This data, along with my own experience as a participant demonstrates that in order to make a significant contribution to this project, a degree of technical knowledge is important. The community of overclockers have made a substantial contribution to Folding@home, and there is evidence on the numerous forums that participants are committed to the long-term project goals, and will readily offer support to other members of the community. The changes relating to BigAdv have provided an insight into the relationship between the project team and the community of active participants, and highlight the importance of participants feeling valued by the project team. 219 Chapter 7: Planet Hunters Results This chapter presents the findings from the Planet Hunters survey (see Appendix D) and a thematic analysis of the interviews with 18 citizen scientists and four members of the project team. 7.1 Online survey results In total, 118 individuals responded to this survey. Approximately 160 000 individuals have registered with Planet Hunters. However, the number of participants actively contributing to the project is estimated to be in the region of 300 (see Chapter Four, Section 4.3.2). The results have been broken down into four main areas: demographic characteristics; patterns of participation; motivation and reward; and interaction and contribution. These correspond to the areas being investigated and to the focus of the research questions. With each result, n=118 unless otherwise stated. In the bar graphs, the number of participants in a specific category or providing a response is on the top of each bar. 7.1.1 Demographic characteristics The majority of respondents to the survey were male (74%, 87 individuals), with a relatively equitable distribution among the different age groups. Most respondents were well educated with 65% (77) having either an undergraduate or postgraduate degree (Figure 7.1). Most of these qualifications (75%) were in a STEM subject. 220 Figure 7.1 Highest level of educational attainment Most participants are from developed countries. Nearly half (57) were based in Europe, 43 were in the USA and Canada, and 10 were based in Australia, New Zealand or South Africa. Other countries represented included India, Japan, Argentina and Venezuela. Table 7.1 illustrates the professions represented by this sample of Planet Hunters participants. There is a relatively high representation of IT professionals (14), business professionals (13) and students (12). Eleven respondents work in the sciences. high school / GSCE 18% UK A levels / BTEC 7% Junior college/HND 6% undergraduate degree 41% postgraduate degree 24% still studying 2% other 2% 221 Table 7.1 Profession of respondents IT / software professional 14 Business professional 13 Student 12 Work in the sciences 66 11 Engineer 11 Skilled trade 8 Educator 6 Administrator 6 Retired 6 Artist / creative 6 Unemployed 4 Sales 2 Civil Servant 2 Librarian 2 Leisure industry 2 Medical profession 2 Police 2 Retail 1 Home-maker 1 Gardner 1 Hospitality sector 1 Pilot 1 Translator 1 Again, one of the most notable features of the demographic data is the predominance of male respondents. According to one of the Zooniverse scientists, a skew towards male participants is found in a number of Zooniverse projects (Lintott, 2014) 67 . Furthermore, astronomy is a subject that has been male dominated both in the professional and amateur arena 68 . According to a report published for the Royal Astronomical Society, only 28% of UK university lecturers in astronomy and 7% of astronomy professors are women (McWhinnie, 2011). The largest UK amateur astronomy association, The British Astronomical Society, estimates that fewer than 10% of their members are women 66 Survey responses included professions such as lab technician, meteorologist, physicist, chemist, scientific researcher, science writer, geologist, and “scientist”. 67 Zooniverse users do not provide any personal data such as age or gender. Zooniverse scientists have estimated the proportion of male and female participants by looking at the names on email addresses. 68 Statistics relating to women in professional astronomy have been compiled by the International Astronomical Union (IAU): http://iauwomeninastronomy.org/statistics/international-stats/. http://iauwomeninastronomy.org/statistics/international-stats/ 222 (Bowdley, 2009). Given these figures, it is perhaps unsurprising to find more male than female participants in an astronomy-based citizen science project such as Planet Hunters. 7.1.2 Background interest in science Almost this entire group had taken part in other science-related activity during the past year. Figure 7.2 illustrates the types of activities and the numbers of respondents reporting taking part in them during the past year. Figure 7.2 Other science-related activities undertaken during the last 12 months Perhaps not surprisingly, given the subject of the Planet Hunters project, over 80 individuals (68%) had taken part in an amateur astronomy activity during the past year. In addition to their participation in Planet Hunters, many survey respondents (80, 68%) report participating in other citizen science projects (Figure 7.3). Half (59) have taken part in other Zooniverse projects such as Galaxy Zoo, Old Weather, Moon Zoo and Planet 105 103 81 75 69 69 56 33 17 15 5 1 223 4, while a smaller proportion (20%, 24 respondents) have taken part in distributed computing projects (mainly SETI@home). Figure 7.3: Other citizen science projects that respondents have participated in (n=93). Just under a third of respondents reported that they first heard about Planet Hunters via the Zooniverse. Most respondents had first heard about Planet Hunters via an online article or link. 7.1.3 Patterns of participation A number of survey questions were posed to ascertain how individuals participate in the project. For example, I asked how many hours a week they spent on Planet Hunters, and whether they contributed to the online community. Many of the respondents had been with the project for some time, with 60% (71) participating in the project for more than one year. Figure 7.4 illustrates how much time they spent (on average) per week taking part in Planet Hunters. 59 24 5 2 2 1 3 224 Figure 7.4 Number of hours respondents spend participating in PH per week (on average) Seventy-five per cent of respondents (88) report spending fewer than 2 hours per week classifying light curves. A much smaller number of individuals (8) participate for more than 5 hours a week. According to the Zooniverse Team, many Planet Hunters have a casual pattern of participation. Participants often classify light curves when they have a bit of spare time, and the average time of a participant session (based on user logs) in Planet Hunters is 11.9 minutes (Simpson, 2013). Forty-nine respondents (41.5%) do not participate in the online discussions in any way, meaning that they neither read or post content, and focus on classifying light curves. This included posting content via the Talk function, and the online discussion forum. Forty-three respondents (36%) stated that they read content only, while only 26 (22%) within this group had ever posted content. According to one of the Zooniverse scientists, the number of people posting content in this group is a much higher percentage than they would generally expect (Lintott, 2013). Data provided by the Zooniverse team on the number of Talk comments posted on Planet Hunters, suggest that nearly a third of the total number Talk comments have been posted by only 48 participants (Raddick, 2013a). < 2 hrs 75% 2-4 hrs 18% 5-7 hrs 5% 11-15 hrs 2% 225 This is a similar finding to that of Tinati et al. (2014) who also found that small numbers of participants contribute to online discussions in other Zooniverse projects. For those respondents that do take part on the project discussion forums (either reading or posting), 90% report spending less than 1 hour a week there, while a very small number of individuals report spending more than 2 hours a week on the discussion forums. This small group also report spending more time participating in Planet Hunters per week, and may well number among the project’s core group (See Chapter Four, Section 4.3.2). 7.1.4 Motivation and reward Questions relating to motivation were open-ended, and content analysis was used to categorise the responses into important themes. This process has been described in greater detail in Chapter Three (Section 3.4.1). I asked respondents why they decided to try Planet Hunters and the responses were coded into 13 different categories illustrated in Figure 7.5. Figure 7.5 Why respondents decided to try Planet Hunters 56 40 26 11 10 9 7 6 6 6 5 3 226 Most respondents gave more than one answer to this question, but the most commonly cited reasons for trying Planet Hunters (47%, 56 respondents) was a background interest in the science. After a prior interest in the science, the next most popular reason for trying Planet Hunters was to make a contribution to scientific research (34%, 40 respondents). These motivations were also observed to be of importance in previous studies of Galaxy Zoo participants (Raddick et al., 2010, Raddick et al., 2013), although this observation did not inform my own coding scheme. The possibility of making a discovery was a big attraction for 26 respondents, and seven individuals in this group of respondents have been involved in the discoveries of Planet PH1b and Planet PH2b (Schwamb et al., 2013, Wang et al., 2013). For a smaller group of respondents (6%, 7 individuals), the fact that Planet Hunters is a Zooniverse project was important. Many respondents take part in other Zooniverse projects, and there are similarities in the layout and format which may make Planet Hunters feel more familiar to some participants. Only 5 (4%) respondents stated that the opportunity to learn was one of the reasons that they began to participate in Planet Hunters, although the desire to learn more about exoplanets may be part of the ‘interest in the science’ motivation category. In order to find out what kept participants involved in the project, they were asked what they liked best about Planet Hunters (Figure 7.6). 227 Figure 7.6 What respondents liked best about the project Again, being able to make a contribution to science was one of the most important features of Planet Hunters for over a third (39) of respondents, and one of the things that kept them contributing to the project. The possibility that a new discovery may be around the corner continues to remain an important motivator for about a quarter of respondents (29). The accessibility of the project and the user-friendly interface are also important features of the project for over a fifth (21) of respondents. The concept of a ‘community‘ surfaces in the responses to this question, and 11 respondents (9.3%) stated that the Planet Hunters community was one of the things they liked best about participating. In order to gain a further insight into participant motivation, respondents were asked whether they thought Planet Hunter participants should be rewarded for their participation in some way (Figure 7.7). 39 29 21 11 9 9 8 4 4 2 1 228 Figure 7.7 Do you think you should be rewarded for your participation in Planet Hunters? (n=110) Many participants (42%, 46 individuals) wanted to be recognised for any discovery they might make, which they already are through co-authorship and acknowledgement on scientific publications (Wang et al., 2013, Schwamb et al., 2013, Schmitt et al.,2014). Over a third of respondents (41) felt that participation was its own reward, and that adding an extra incentive would change the nature of the project. A minority of respondents (16%, 19 individuals) felt that an extra reward may lead to a greater numbers of participants, or that those who contribute more than other participants should receive something extra. A number of suggestions were made including t-shirts, vouchers, certificates, ‘virtual’ badges, discounts and lab tours. 7.1.5 Interaction and contribution Respondents were asked to describe their interactions with other participants through an open-ended question. Sixty-seven respondents answered and their answers were coded into 3 separate categories which related to whether they read content or posted content also (Figure 7.8). yes -general recognition for contributions or discovery 42% no, participation is enough 37% don't know 5% yes 16% 229 Figure 7.8 How would you describe your interaction with other participants? (n=67) The lower response rate for this question could have been due to the fact that the remaining 55 respondents do not interact with other participants in any way. In the previous question relating to how much time they spent on the online discussion per week, it was evident that nearly half of respondents do not spend any time at all on the project Talk or discussion boards. Of the 67 that answered this question, 35 reported no interaction at all with other participants. Twelve respondents stated that they only read the content on the online discussion, so again, had minimal interaction with other participants. Only twenty individuals actually post content, and respond to the comments and/or posts of others. This roughly corresponds with the feedback relating to how much time respondents spent on Talk or the discussion boards. The majority of respondents to the survey appear to focus on analysing light curves, and do not actively take part in the online Talk or discussion boards. One respondent made the following comment: no interaction 52% minimal interaction (read only) 18% some interaction (incl posting) 30% 230 “I have had no interaction with other participants. In fact, with the way the site is currently set up, unless you go looking for other people, you wouldn’t know that there was anyone else around,” Discussions via the Talk function are only prompted after a participant has classified a light curve, which may not appeal to the casual participant who does not wish to discuss individual objects in detail (see Chapter Four, Section 4.3.3). Furthermore, a participant can only access the more general discussion boards by going into the Talk function, so unless a participant has used the Talk function before, they may not be aware of the presence of these more general discussion boards . View of contribution How participants view their own contribution to the project was an issue that was highlighted in the follow-up interviews with Foldit players, and I was interested in exploring this with Planet Hunters participants also. The online survey contained the question: ‘How would you describe your contribution to Planet Hunters? Do you feel as though you are involved in scientific research?’ All 118 respondents provided an answer to this question and most (82%, 97 individuals) felt they were involved in scientific research to some degree. For the majority of these respondents this contribution was felt to be quite small. However, there was a sense of respondents feeling part of a wider, group effort. Those who had been involved in the discovery of a new exoplanet (7 of the respondents) expressed a sense of pride at having helped in these discoveries. For most of the respondents, Planet Hunters does not necessarily make them feel at the forefront of scientific research, but more as helpers. One respondent likened the role of 231 participants to lab assistants. This differentiation between the analysis of light curves by citizen scientists, and the further evaluation of that data by professional scientists may relate to views and beliefs of the respondents about what actually constitutes science and scientific research (i.e. basic classification tasks are not as important as more in-depth analyses). 7.2 Interview feedback The results of the interviews have been subjected to a thematic analysis using the approach of Braun and Clarke (2006) which was outlined in Chapter Three (Section 3.4.2). The major themes will be presented for both the interviews with citizen scientists (n=18) and with members of the project team (n=4). The emerging themes will be outlined. 7.2.1 Interviews with citizen scientists Table 7.2 lists the emerging themes from the interviews with the citizen scientists Table 7.2 Emerging themes from interviews with citizen scientists Interest in astronomy Prior interest in astronomy, including formal and informal study, involvement in amateur astronomy, and communication activities relating to astronomy. Science fulfilment The desire to be more connected to science and the opportunities Planet Hunters presents to get involved in scientific research. What I like about Planet Hunters Aspects / characteristics of the project that were admired by respondents, and perhaps motivate their involvement in the project. What have I learned? What participants have learned about the science behind planet hunting, as well as learning about citizen science projects more generally. Contribution and participation How respondents view their efforts in the project. What respondents feel about the tasks they are asked to perform and how active respondents are in the project. Getting more involved in the project, and in data analysis. Interaction and communication How respondents interact (or do not interact) with other project participants, and with the project scientists. Views regarding the project interface. 232 Interest in astronomy One of the most common themes to emerge from the interview data, and one that was expressed by nearly all of the interviewees was a prior interest in astronomy. Most of the respondents stated that they had had an interest in astronomy since childhood. “I have been interested in astronomy since I was a young kid and that interest remains to this day.” (PH2) “There was always at the back of my mind a basic interest in the stars and planets and a curiosity about all things of that type.” (PH3) Not only have most of those interviewed expressed an interest in astronomy, but just over half have also stated that they have some involvement in amateur astronomy either through their membership with local amateur groups, or making observations with their own telescopes. Several respondents even report conducting their own research in astronomy. “I am an amateur astronomer and have always been interested in our solar system in particular. I have even written a few blogs in regards to astronomy, especially to put some flawed ideas and theories to rest by doing a lot of research on my own.” (PH1) One participant used their involvement in Planet Hunters as a way to further their own independent research in astronomy. “..to make contact with those who have skills I’d like to tap into for my own research (esp. statistical analyses).” (PH5) Most respondents report reading about astronomy online (particularly though the NASA website or Astronomy Picture of the Day) and through the news and magazine articles. 233 About a third of respondents, report following key projects and scientists through social media sites such as Facebook and Twitter. Over half of respondents reported taking part in other astronomy-related citizen science projects – particularly other astronomy projects in the Zooniverse such as Galaxy Zoo, Space Warps, Moon Zoo and the Andromeda Project, and other online astronomy projects such as CosmoQuest (a classification-based project that maps craters on the moon and on Mercury) and SETI@home. Two respondents stated that they had some formal qualifications in astronomy. One of these had a BSc in astrophysics and was also a fellow of the Royal Astronomical Society (the learned body for astronomers in the UK), although employed as a software engineer. The other respondent had returned to formal education and was studying an astronomy module as part of an undergraduate science degree. A few other respondents had undergraduate degrees in physics or mathematics. However, none of the interview respondents was a professional astronomer, or worked in a related scientific discipline. Science fulfilment As well as an interest and involvement in astronomy to varying degrees, most of the respondents expressed a wider interest in science. Indeed, over half of the interviewees have a formal undergraduate-level qualification in science in areas such as physics, oceanography, medical sciences, computer science, chemistry and geology. Despite a general interest in science and formal academic qualifications, none of the interviewees report working in scientific research or in a scientific field. Several have in fact, expressed regret at not being in a science-related occupation despite initially training for a career in science. 234 “it is sad to think that the time spent learning at school and university is generally wasted by many people like me who are forced into other fields and occupations. I would like to do more in the sciences, and I think this is a great way to start.” (PH10) Participating in projects like Planet Hunters therefore provides a way for those with an educational qualification, or interest in science who work in unrelated professions, to reconnect, or remain connected, with science in some way. Participation in online citizen science projects may help to fulfil a need to maintain this connection with science, and many of these individuals used the interviews to express gratitude to the project organisers for providing them with this opportunity. One respondent left school at an early age with no formal qualifications, and stumbled upon Planet Hunters during retirement. With growing experience and confidence, this respondent eventually co-discovered a new exoplanet. “Working on this project has really stretched my horizons….it has been great fun and given great satisfaction, the fact that I have been credited with the discovery of a new planet added to the experience.” (PH3) Participation in projects like Planet Hunters also provides this experience for those with an interest in science, who may have no, or few, formal educational qualifications in the subject. What I like about Planet Hunters Respondents generally do not refer to motivations as such, but talk more generally about what they like, or what they find appealing about Planet Hunters. Other motivations may be referred to more indirectly, or in their responses to some of the other questions. The feedback from the interviews generally agreed with the results obtained in the online 235 survey in the questions relating to motivation and reward. For example, most of the interviewees were drawn to Planet Hunters because of an interest in astronomy, or a more general interest in science. A few of the respondents liked the fact that they are able to make a meaningful contribution to scientific research in some way. “…I keep participating because it is good to be actively taking part in science.” (PH2) “Love astronomy and science and it’s great to get the chance to be part of some real science.” (PH11). One of the most appealing aspects of Planet Hunters for those interviewed (as in the survey) was the possibility of discovering a new exoplanet, and being the first to make such a discovery. Three of the interviewees had been involved in the discovery of an exoplanet. “Looking for new planets was appealing in that it is such an exciting thing to be part of and perhaps be one of many who can have their name associated with a new find.” (PH2) “I kept participating in Planet Hunters because I knew that in order to increase my odds of finding an exoplanet I would have to go through a lot of data.” (PH14) “I thought what a cool job that is, to hunt for planets…..and immediately understood the quest.” (PH1) About a third (six) of those interviewed stated their admiration for the goals of the project, and that they liked the idea of breaking down, or crowdsourcing, the task. “There are lots of projects around and we’ve barely scratched the surface in getting people involved. The idea that you can take a huge problem and break it into a billion tiny pieces is pretty neat.” (PH8) 236 “A great way to crowdsource data – and people like to feel involved in large projects even if they don’t have the aptitude to be in that science field but have an interest.” (PH12) Five interviewees stated that taking part in the project was fun or enjoyable, and one individual stated that taking part provided an ‘escape’. Another stated that it helped them to unwind after a stressful day at work. What have I learned? Interview participants were asked what they had learned through their involvement in Planet Hunters. Most of the answers to this question outlined what the respondents had learned about the science associated with extra-solar planets, supporting previous findings that participation in citizen science can play a role in informal science learning (Evans et al., 2005, Jordan et al., 2012, Jordan et al., 2011, Price and Lee, 2013). However, none of the respondents stated that the opportunity for learning was what they liked about taking part in Planet Hunters, or indeed, cited it as a reason for joining or remaining with the project in their feedback to the other interview questions. There is a suggestion that some respondents may have learnt a bit more about the scientific research process. The tasks associated with Planet Hunters could be considered representative of ‘typical’ scientific research. For example, in order to make a discovery a lot of data must be processed, which at times, can be quite repetitive and may take many months. “To do well, they require a dedication of time and effort to grasp not only the tools but also the concepts.” (PH5) “Reminded me about the grind involved in checking samples and collecting data!” (PH6) 237 Those involved in exoplanet discoveries may get more of an insight into the various stages in scientific research and in dissemination of results. One respondent who had been involved in the discovery of an exoplanet talked about her experience sharing her findings with others and how she discussed her potential discovery with other project participants. This may have demonstrated to her the importance of verification, and of seeking out the opinion of one’s colleagues or peers. A further insight for those involved in discoveries may involve an increased appreciation for the ‘end-products’ of science, and the collaborative nature of the effort required to produce results, and to produce publications – knowing that they are one of many involved in a single discovery. “I have already been credited with the discovery of a new planet…It was a real thrill to know that I was one of only about 40 non-professional astronomers in the world to be credited with that distinction.” (PH3) Not only have some participants been acknowledged in publications or co-authors in papers, but some have found themselves talking to journalists about their experiences, as well as to other scientists and science communicators. This has given them direct experience of communicating about scientific research. “I was the first one to flag up a possible exoplanet candidate. She [Meg Schwamb, member of the project team] also asked me if I was available for an interview….Stephen Craft was writing a piece for Scope (an online publication at MIT) on my recent success….Then post-graduate students contacted me in relation to a documentary film 238 project. The Royal Observatory Edinburgh and the Physics department St Andrews commissioned a film for the Edinburgh International Science Festival this year 69 .” (PH1) “And I also had my 15 minutes of fame when a quote of me in The Guardian got picked up by the Belgian press, and I got inundated by phone calls and interviews for a day! My confidence in speaking to the press has obviously increased as well. And also, since I am not a native speaker, my English has improved a lot.” (PH13) Clearly, the majority of Planet Hunters participants do not get this opportunity, and the three interview respondents who have made a discovery have classified thousands of light curves each, and have been involved with the project for some time. Contribution and participation The responses relating to how participants view the task, as well as responses regarding how much time they spend on Planet Hunters in comparison to other activities, have helped to build up a picture of how this group of participants view their contribution and what they think of the tasks. The task itself is perceived by most respondents to be straightforward if a little repetitive. “I think they are simple tasks, they don’t take long and can be easily done if focused. Watching the light curves can be tedious but the fact that there is something out there waiting to be discovered is very rewarding.” (PH1) “The tasks are easy enough and not over taxing.” (PH2) 69 This film ‘Close Distance’ can be viewed here: http://www.roe.ac.uk/vc/content/wywhexoplanets/close- distance.html. http://www.roe.ac.uk/vc/content/wywhexoplanets/close-distance.html http://www.roe.ac.uk/vc/content/wywhexoplanets/close-distance.html 239 A few respondents have stated that they would have liked more feedback from the project scientists while they were learning to classify light curves, and stated that at times they were not confident that they were classifying the light curves correctly. “…I feel like there was not enough examples and tutorial / training – I often wasn’t certain what I was looking for or doing and I note that many other participants felt the same way in the community forum.” (PH12) “Sometimes I’m nervous when I’m not sure about the light curves being right.” (PH15) Three respondents expressed a desire to go further in the project and to get more involved in the analysis of the data. There are some basic tools in Planet Hunters that allow participants to more closely examine the light curves, and some committed and more technically skilled participants have shared resources for conducting certain types of further analysis, for example, using the raw data to calculate planetary diameter 70. Two of the respondents mention carrying out their own research in astronomy, although neither specified whether this was using the data from the Planet Hunters project. One respondent talked of her desire to actively encourage other project participants to become more involved in the project. “I also have an interest in fostering, encouraging, etc. as many of my fellow Zooites to go beyond classifying: get involved with data analysis, testing hypotheses etc.” (PH5) This respondent also actively seeks out fellow participants who can help her with her own research using the Planet Hunter data. 70 This page on the Planet Hunters Discussion Board provides links to tools for further examination of the project (Kepler) data that have been created by project participants (accessed 21/04/14). http://talk.planethunters.org/discussions/DPH1014lr5?sub_board_id=planet_hunters_resource_library http://talk.planethunters.org/discussions/DPH1014lr5?sub_board_id=planet_hunters_resource_library 240 The amount of time interviewees spent on Planet Hunters varied greatly. Four stated that do not currently participate due to lack of time, while three stated that they contribute for approximately half an hour a week. Only a couple of interviewees stated that they spend a bit longer participating (1-2 hours per week). Only one respondent described Planet Hunters as their main interests, and that they spent as much time as they could classifying light curves. Three stated that their participation was seasonal – either related to work schedule, or that they participated in the summer months more, when it was harder to look through their telescopes. One respondent stated that he moved from project to project and may not participate in Planet Hunters all the time. “I also only analyse data for a month or two before becoming interested in some other project. With so many amazing citizen science projects available online it is difficult to find time to participate in them all. I tend to only participate in the projects I feel I can make the greatest contribution to.” (PH14) Just over half (10) of the interviewees spread their free time among a number of different projects on the Zooniverse collection of projects is of interest. Indeed, that is what some find so appealing about the Zooniverse projects. Despite the fact that some of the interviewees only participate lightly (or not at all) in Planet Hunters, or share their attention among several different projects, half (9) of the respondents stated that they felt the work they did on the project was in some way useful or beneficial to scientists. “The contribution a volunteer ‘citizen scientist’ can make to research is huge and we are all well loved by those in charge of the project.” (PH1) 241 “I think that they [the project tasks] are interesting, useful and of some great value to scientists who use our findings.” (PH3) The utility of the task assigned to citizen scientists is believed to be an important motivator by the Zooniverse scientists, and they have stated that it is important not to waste the time of those who participate in the Zooniverse projects 71 . Interaction and communication Interviewees were asked about their interaction with other project participants (including the project scientists), and what type of interaction was expected. Insights about interaction and communication were also gained from the responses to some of the other questions. Of the eighteen respondents, half have stated that they do not interact at all with other participants. “I don’t participate in the community, but I know it is there. I expected a community forum of some kind for guidance and discussion.” (PH12) Two of the respondents act as moderators on other Zooniverse projects, and so are more familiar with a number of the scientists, and with some of the more active participants both from Planet Hunters and from other Zooniverse projects. “Since I’ve been very involved in the various Zooniverse projects, I already ‘know’ some of the scientists or moderators and I recognize user-id’s from other projects. So for me, this feels very comfortable.” (PH13) These moderators were also somewhat critical of the Talk function in Planet Hunters, and the general set-up of the Planet Hunters online community, which is different to some of 71 The Citizen Science Alliance that oversees the Zooniverse states in a brief guide that projects should not waste participant’s time http://www.citizensciencealliance.org/downloads/zooniverse_guide.pdf. http://www.citizensciencealliance.org/downloads/zooniverse_guide.pdf 242 the other Zooniverse projects which have a more ‘traditional’ online forum set-up (such as Galaxy Zoo or Space Warps). “The only improvements I’d like to see are improvements in Talk (the ‘forum’ developed by the Zooniverse). As I understand it this is really helpful for the scientists, but for me as a user it lacks the easy use we had with the old fashioned forum on the first GalaxyZoo. I am a moderator on a couple of new Zooniverse projects that use Talk and we never have the same feel of a community there.” (PH13) Another respondent also made a comment relating to the tools available for interaction. “The time I want interaction is when I find an image I can’t figure out in the way desired. There are facilities for this on the sites but they are a bit cumbersome. They do not lead to group analysis really.” (PH4) There were very few comments relating specifically to interaction with the project scientists. One very active Zooite stated: “..after much strong involvement, project scientists have largely disappeared from the other astronomy Zooniverse projects….and I was pleasantly surprised to see how active Meg is (no sign of others though).” (PH5) One of the respondents who was no longer very active in Planet Hunters but had become active in the citizen science game EteRNA, made several comments about what he thought project scientists should be doing in their interactions with citizen scientists. “I think interaction with project scientists is very important. The EteRNA project has bi- weekly group chats with the developers and scientists and players have been able to make significant contributions to the functionality of the website and project. I think regular 243 interaction with project scientists helps participants stay motivated because it makes them feel like they aren’t working in vain and that they are important to the project.” (PH14) When talking about the project tasks, this same individual stated: “If participants who analysed more data were somehow rewarded by getting more recognition or interaction with project scientists I think it would help retain those who are the most dedicated.” From these interviews it appears that the overall interaction between the participants, and between the participants and project scientists is somewhat limited, unless that respondent has been involved in a discovery, or has been a moderator on one of the other Zooniverse projects. About a quarter of the respondents (4) were slightly critical (either directly or indirectly) of the current Planet Hunter interface for interaction and communication (the Talk function) and there is a sense that they would like more feedback from the scientists, particularly when it comes to uncertainties surrounding the project tasks. 7.2.2 Interviews with the project team Five major themes emerged from the interview feedback from the Planet Hunters project scientists. These are outlined in Table 7.3. 244 Table 7.3 Emerging themes from interviews with project scientists Benefits for scientists The positive aspects and outcomes that are a direct result of their participation in the project. The motivation of the scientists involved. Role of the project team The perceived roles and responsibilities of the project team in managing Planet Hunters. View of the volunteers’ motivations Why the scientific team think that people take part in Planet Hunters, and the perceived benefits for the volunteers. Varying levels of involvement The recognition that there are different levels of interest and engagement with the project. The acknowledgement of the contribution of core participants. Working with the participant community Some of the responsibilities and challenges associated with working with a large online community of volunteers. Benefits for scientists One of the main benefits for the Planet Hunters scientists is that they get help with research that relies on human observation. The tasks on Planet Hunters cannot be carried out with total reliability by a computer, and human participants have identified exoplanet candidates that have not been detected by the Kepler computer algorithms. “It’s science that you can do with the public that can’t be done by your computer algorithms.” (PHPT1) The involvement of thousands of volunteers also enables very large datasets to be analysed, and such datasets are becoming increasingly common in astronomy. One scientist talked of the difficulty in keeping up with all of the data output produced by Planet Hunter participants. One scientist spoke of citizen science as a new research tool, one that could complement other ways of doing research. 245 “I would say it’s transformed how I do my research. Citizen science is very much a tool we can use for large datasets, particularly in astronomy – and we’re heading towards more data.” (PHPT2) From the scientists’ responses, it appears that the primary motivation to participate in online citizen science projects like Planet Hunters, is that it provides an effective way to carry out, or ‘crowdsource’ a relatively straightforward analytical task that requires little training or supervision. Time that would have been spent by scientists on these tasks is now freed up for more in-depth analysis of the data. However, this is not the only motivation for their involvement, and it is evident that for most of the scientists involved, the opportunity for public engagement and outreach is also appealing. “I think it’s an interesting way to do outreach, I think it brings people into the scientific process and the actual organic component of it…” (PHPT2) When this scientist described outreach activities, it became clear that outreach for her involved telling people about her research and educating them about some of the underlying science. “We talk about what science we want to do, and how do we do that, and how do we educate the volunteers…… My goal when being on Talk has been to make sure that information is being disseminated properly.” (PHPT2) This scientist also saw Planet Hunters as enabling outreach at the same time as she carried out her research thus ‘killing two birds with one stone’. It also makes her consider different avenues of communication. 246 “…it’s made me more creative in how I do outreach. I can satiate doing outreach as well. Having live chats and things like that. We don’t need to do it, but I think we should do it….is there something new I can do with Planet Hunters volunteers….?” (PHPT2) It is evident that the scientists get some enjoyment out of their participation. “It’s fun! Both in terms of engagement and in terms of research.” (PHPT3) This same scientist also spoke of the enjoyment associated with the intellectual leadership involved in managing the project. Role of the project team One of their primary and underlying roles involves making sure that the project site is well maintained, and functioning. This requires that members of the project team liaise on occasion with members of the Zooniverse team, particularly those involved in the more technical aspects of the projects. It also requires that members of the project team have a presence on the project site, and are aware when there is a technical issue. “If something’s broken, I tend to be the first to know.” (PHPT2) Securing and maintaining funding is also something that is of importance to those involved in managing a Zooniverse project, and highlights the issue of sustainability in online citizen science. “One of the biggest problems on my desk is making sure that all these [Zooniverse] projects exist in ten years-time when citizen science is not sexy and almost all our grants have come to an end.” (PHPT3) 247 However, it appears that the main obligation perceived by the scientific team is to ensure that something is done with the results generated by citizen scientists. Most of the scientists emphasised the importance of not wasting anybody’s time. “People understand that we’re doing something with what they’re contributing as well. We can show them we’re doing something with the data.” (PHPT2) “…we’ve been entrusted with 18 million classifications so the main thing is we have to get as much science out of that as possible. I feel that the worst thing would be to waste people’s time. For Planet Hunters I feel very responsible that this happens.” (PHPT3) The project scientists also articulated a responsibility towards the community of Planet Hunters, and that they need to be present and available to answer questions, or disseminate information about the projects and the results. However, not all of the scientists get involved with interacting with the community to the same degree, and there are varying degrees to which scientists become embedded in the participant community. “I think engage with the community is the responsibility of the science team, but I don’t think everyone in the science team has to do that.” (PHPT3) One of the scientists I interviewed is heavily embedded in the Planet Hunters community and is highly visible both on the online discussion forum, and writes most of the entries on the project blog. “I probably check Talk 4, 5, or 6 times a day….My goal when being on Talk is to make sure that information is being disseminated properly, and that when someone asks a question, we’re answering it the way it should be answered….we need to be around, and trying to teach people what actual science is….sometimes you want to go the extra mile.” (PHPT2) 248 Taking the time to interact with Planet Hunters participants can be time-consuming, and many of those in Zooniverse scientific teams are early career researchers, so they are under “pressures to publish” and to produce results. View of the volunteer’s motivation All of the scientists interviewed had been sent a summary of the survey results, so we used some of the findings as a starting point for our discussion. One of the more interesting areas for the scientists was the exploration of the motivations of the Planet Hunters volunteers. The opportunity to get involved in authentic scientific research was considered to be important by the scientists (as well as participants), and was felt to be one of the defining characteristics of the Zooniverse projects. “…it’s always about the experience and I think for me it’s the authenticity….People get that they’re data and actual graphs – but they understand the abstract concept of looking for the signature of another world.” (PHPT2) “I think it matters in the long run that it’s real.” (PHPT3) That participants like to contribute or to help out was highlighted. “I think it’s the sense that ‘today I did this thing’. You see that in the way the users, certainly the advanced users, talk about the project. I think there’s a pride in it.” (PHPT3) “I think a lot of people might do it because they want to contribute, and this is definitely a solid way that they can contribute, and we can’t do it any other way.” (PHPT1) The scientists felt that the possibility of making a discovery was a key motivator for participants and an important draw of the project. 249 “We did worry if people would come and look at these graphs – are people going to come and look at light curves? They do – that’s because we have really good ‘bacon’, and our bacon is you can help find planets.” (PHPT2) The educational component of participation was raised by the community support manager, who has plans to increase the educational content of the Planet Hunters website. “So one of the things I’m really looking forward to is attempting to educate our users in Planet Hunters. Please come to the site and do the classification and also learn about exoplanetary physics while you’re doing it. From a base level up.” (PHPT4) The fact that the hunt for exoplanets is a new and exciting field was highlighted by one of the scientists, and the number of new exoplanet discoveries in the last 10 years has been significant 72 . Varying levels of involvement There is an acknowledgement among the Planet Hunters project team that participants demonstrate varying degrees of engagement with the project. Some limit their involvement to classifying light curves when they have the time or inclination, and there are those who want to go further into the analysis of the raw Kepler data. The existence of a small community of core participants was discussed and some of the scientists had worked directly with these individuals. “I have contact fairly regularly, at least a couple of times a week, with some of the users by email. So it’s discoveries they’ve made. I guess they coined the term ‘superusers’, 72 The official webpage of the Kepler Mission which lists the several thousand exoplanets discovered since the launch of the mission in 2009: http://kepler.nasa.gov/Mission/discoveries/. http://kepler.nasa.gov/Mission/discoveries/ 250 they’re the volunteers who monitor the site and collect things and whatnot. They’re really keeping the project going!” (PHPT1) “I think we have a group who have evolved out of the project interface who are just doing their own analysis…and I think that’s quite interesting. They talk to each other behind the scenes…I think there might be about 20….but they kind of have their own little hierarchical structure as well…although we still have a bit of authority over it…” (PHPT2) The importance of this group is acknowledged, however, while the project team want to support this group and offer the opportunity for greater involvement to other participants, it was also recognised that this is not what all volunteers want from their participation in Planet Hunters. “We talk a bit about a career in citizen science and that people should go from just clicking on stuff to writing papers and how they can reconnect to a professional. The problem with this type of rhetoric is that you end up sounding like you want everyone to go down that path, whereas I think a lot of our volunteers just want to have a hobby that takes a few minutes…and you can kind of get into trouble….” (PHPT3). I spoke to one of the scientists about the pattern of participation in Planet Hunters where most participants contribute very little, and a small number of participants do the bulk of the work. We discussed the graphic that had been produced illustrating the pattern of participation for Planet Hunters (Figure 4.19, Chapter Four). The scientists stated that this is a characteristic pattern of participation in many of the Zooniverse projects. “We have to cater to those who are among the little squares, and those who are among the big squares.” (PHPT3) 251 One of the key challenges for the project team is to manage the expectations and requirements of these different levels of participation. Working with the participant community While the value of online citizen science as a scientific research tool was made clear by the project team, a number of challenges associated with working with a community numbering in the thousands were also outlined. One of the issues highlighted was that it is time consuming to spend time interacting and communicating with the community of volunteers. Project scientists have full-time jobs doing scientific research, sometimes lecturing, and always involving the writing and publication of research papers. “If I were to ask scientists to go on to Talk once a week and find five things to say, most would say they’re too busy….I’ve always been reluctant to mandate ‘you must spend three hours on talk’ or what-have-you.” (PHPT3) The need to address other tasks and commitments has to be balanced with their obligations to the participant community, which suggests that these scientists draw a line between, or compartmentalise, these different obligations. It was evident that some of the scientists enjoyed this interaction more than others. One of the scientists did not interact with participants at all on the website, while another appeared to be highly committed to the community, and spoke about sharing the experience of scientific research. “It’s about sharing the news when you have it and about sharing the process as you go along.” (PHPT2) The same scientist spoke of project participants as ‘collaborators’, and the fact that this was their role demanded that time and effort be invested in communicating with them. 252 “I think you need to remember that you are dealing with people. These are not machines, and one of the things you have to recognise is you have to deal with that, and acknowledge and treat them as collaborators. I think it’s very important to bring them along on the ride.” (PHPT2) They went on to add, that once you start to view participants as collaborators that “things get easier”. Another scientist spoke about trying to make everyone happy during earlier experiences in other Zooniverse projects, and speaking personally to participants who may have been dissatisfied in some way to make sure everything was alright. However, such an approach could not be maintained, and this individual stated that they didn’t actually have a responsibility to make everyone happy. “I find the problems of running a citizen science project….[doesn’t finish sentence]. People are more complicated than galaxies!” (PHPT3) In recent months there appear to be some changes taking place on the project regarding how the Planet Hunters project team communicate with participants, for example, Facebook and Twitter are being used more widely to communicate and to disseminate results. I was able to discuss some of these changes with new community support manager, who confirmed that social media was going to be an area of focus not only for Planet Hunters but for other Zooniverse projects. They clearly felt that a greater use of social media would improve and facilitate communication between project participants and the project teams and ultimately make the community ‘happier’. When asked how they would know if the community was happy, they stated that there would be more classifications. 253 “Productivity and happiness heavily correlate I think. I don’t think people would be there otherwise.” (PHPT4) How ‘happiness’ would be gauged in the Planet Hunters community is unclear, but some economics-based research into happiness and productivity, has shown that individuals can be more productive, and produce better quality of work if they are happy (Zelenski et al., 2008, Oswald et al., 2009). Another point that arose during my discussions with project scientists that may also influence the relationship between the project team and the community is the fact that citizen scientists may have a different idea of what constitutes scientific research to those of the professional scientists. One scientist referred to the notion of “Hollywood science” where people had an unrealistic view of what actually constitutes scientific research, and perhaps as a result, had unrealistic expectations regarding the project, or of the project scientists. 7.3 Summary of results The feedback from the survey illustrates that this group of respondents is predominantly male, well-educated, with a background interest in science, and in astronomy more specifically (RQ1 – Who participates in online citizen science?). Most (75%) contribute quite lightly in that they do not spend much time each week classifying light curves, and half of those surveyed (58) do not interact at all with other participants. This feedback helps to address RQ 5 (How can contribution to online citizen science projects be characterised?) and illustrates that contribution to Planet Hunters can be casual occurring when respondents have some free time. Participation can be solitary, and an individual can participate without interacting with others. This pattern of participation has been observed in other Zooniverse astronomy projects (Ponciano et al., 2014, Tinati et al., 2014). Most (92) of the respondents felt they were making a contribution to scientific 254 research although many felt that rather than being at the forefront of scientific research, they provided a supporting role (RQ6 How do participants perceive their role in the project?). Respondents are motivated to try Planet Hunters because they have a background interest in the science and because they want to make a contribution to an authentic research project. This was found to be an important motivator for participants in Galaxy Zoo, another Zooniverse project (Raddick et al., 2010, 2013). The possibility that they may discover a planet also draws in many of the respondents. Most of these individuals continue to participate in order to carry on making this contribution to science, and hope that they will one day make a discovery themselves. The fact that the project task is relatively easy and the interface is accessible, are important in maintaining participation for a significant group of respondents. This feedback has helped to address RQ 2 (What motivations initiate and sustain participation in online citizen science projects?). The interviews with participants have highlighted the importance of motivations that were identified by the survey. They have also demonstrated that taking part in Planet Hunters may provide an opportunity for participants to reconnect with science, particularly for those who may have been formally educated in a science subject and who may have wanted to follow a career in science, thus identifying another important motivation for this group of citizen scientists (RQ 2). A small amount of dissatisfaction with the some features of the website is evident, and some interviewees display a lack of confidence in the accuracy and validity of their contribution. This may affect the motivation of some participants to continue their contribution to the project. Data from both the survey and interviews with citizen scientist volunteers suggests that participants do not generally interact with the project scientists unless they are forum moderators, or 255 have been involved in one of the exoplanet discoveries (RQ 4 How and why do project participants interact online?). Interviews with the project team suggest that Planet Hunters is seen as an important tool for carrying out research, with the secondary benefit of providing a vehicle for communication with the community of participants (RQ 2). Most of this communication is one-way from the scientists to the citizen scientists, but there is some evidence that a small group of participants actively engages one of the scientists with their observations and wider interests relating to the Kepler data (RQ 4). There is an acknowledgement that some participants want to go beyond ‘clickwork’, but that the needs of more casual participants have to be considered alongside those who want to go further in their analysis. Managing a large community of volunteers can be difficult, and communication activities must be fit into a busy professional schedule of conducting research. Planet Hunters involves a relatively straightforward and repetitive task than can be carried out with very little training or input from other participants (unlike Foldit). It provides a good example of ‘microvolunteerism’ in that interested individuals can participate whenever they have some spare time, and for a relatively short duration (Paulos et al., 2011). Those with a greater interest in the science, and in examining the Kepler data will be able to interact with other interested participants on the discussion boards and on Talk, and will also be able to interact directly with one of the project scientists, who are keen to bring on board enthusiastic citizen scientists, and will acknowledge their efforts in any resulting discoveries and publications (Schwamb, 2014). 256 Chapter 8: Discussion and comparison of results This chapter will integrate and discuss the findings of the three case studies, introducing a comparative analysis based on data produced by the different streams of evidence. I will explore explanations for the findings and consider them within the context of previous research (Holohan and Garg, 2005, Raddick et al., 2010, Krebs, 2010, Nov et al., 2011, Rotman et al., 2012), and theoretical models including motivational frameworks (Clary et al., 1998, Batson et al., Ryan and Deci, 2000 &2009), the ‘reader-to-leader’ framework (Preece and Schneiderman, 2009), and the ‘lightweight-heavyweight’ model of peer production (Haythornthwaite, 2009). Each research question will be considered in turn, with the emphasis on exploring evidence across the three case studies. 8.1 Who takes part in online citizen science projects? (RQ1) This question was addressed by data obtained from the online surveys of (citizen scientist) participants, and through my observations as a participant. One of the earliest and most important observations was that despite the fact that each project has many thousands of registered participants, only a small percentage of this population actively contribute. This was particularly apparent in Foldit and Planet Hunters, where my analysis documents approximately 300 active contributors from a pool of over 500 000 registered participants and 160 000 respectively. In Folding@home, a group of highly active contributors from the community of overclockers and hardware enthusiasts was identified. This group make a substantial contribution to the project through the modification of their computers and their technical knowledge (see Chapter Four, Section 4.2.3). This research has ultimately focussed on these groups of more active participants because of their importance to these projects, and because they are more accessible to the researcher though project forums, discussions and blogs. 257 The analysis of the data on active participation demonstrates that what constitutes an ‘active’ participant varies from project to project and depends upon the project task and the level of difficulty involved (Section 4.4). Some tasks are more demanding than others, and may require more regular and sustained participation, not only to complete the project task, but in order to maintain a degree of familiarity and competency with the project tools, particularly if they evolve over time such as in Foldit where participants have become a self-organising community, taking on specific roles. In distributed computing projects, such as Folding@home, pre-existing communities of overclockers constitute a significant proportion of the group of active participants (Bohannon, 2005), illustrating how the Internet can facilitate connections across networks. Neither the discrepancy between registered and active participants, or a consideration of what constitutes an active participant in a project, has been discussed in detail in previous research (Holohan and Garg, 2005, Krebs, 2010, Nov et al., 2011, Raddick et al, 2010, 2013). Furthermore, communication efforts by some of the scientists involved in these projects, as well as a number of news and magazine articles, refer to and emphasise the numbers of registered participants (Bonetta, 2009, McGonigal, 2010, Nielsen, 2012, Wiederhold, 2011, Savage, 2012, Cossins, 2013, Schwamb, 2014c). While these articles and publications help to generate interest in a project, they can also give the impression that there are more people contributing than is actually the case, and create an impression that many thousands are making important contributions to these projects, when in fact, the numbers are more modest. The observation that only a small proportion of those showing an interest in a project will go on to make an active and productive contribution, has been made in relation to other types of online projects and endeavours (Kittur, 2007, Preece and Schneiderman, 2009, Schroer and Hertel, 2009). 258 This pattern of contribution will be explored in detail in Section 8.5 in relation to RQ5 (How can contribution to online citizen science projects be characterised?). The online surveys have provided more information about the active participants in these projects, and include some demographic information that has not been explored in previous research, such as formal qualifications in STEM subjects, and participation in other science-related activities. I have also reported demographic information relating to participants in an online citizen science game. When considered together, the results from the three online surveys illustrate that these respondents have a number of demographic features in common. \\uf0b7 They are predominantly male (there were 45 female respondents from a total of 562, 8%). \\uf0b7 Respondents are mainly from developed countries (549 respondents, 98%). \\uf0b7 Most of the respondents are well educated with the majority having at least an undergraduate degree (335 respondents, 60%). A high proportion of those who are graduates have qualified in a STEM subject (81% of graduates, 272 respondents). Ninety-seven respondents are currently students. \\uf0b7 Almost one third of respondents work in IT-related professions (177 respondents, 31.5%). \\uf0b7 Most of the respondents are engaged with science in some way. Practically all (with the exception of a few) had taken part in other science-related activities in the previous year, and just over half (293, 52%) had taken part in other citizen science projects. 259 8.1.1. The predominance of male respondents Much of the previous work that has measured demographic characteristics of participants in online citizen science projects has found an overrepresentation of men (Estrada et al., 2013, Krebs, 2010, Raddick et al., 2013, World Community Grid, 2013). My survey data also found that the overwhelming majority of respondents to the surveys were male (this was especially true in the Folding@home group where more than 98% of the respondents who responded to the survey were male). There are a number of possible explanations for this pattern of participation. Some research has shown that men are more likely to engage with science than women and are more likely to take an interest in new technology and scientific developments (von Roten, 2004, RCUK, 2008). There may also be a disparity in what type of science appeals more to women than men, and some areas of science (particularly the physical sciences) attract less women than men professionally (Eurobarometer, 2010, Ivie and Tesfaye, 2012). For example, astronomy is a subject that has been male dominated both in the professional and amateur arena (McWhinnie, 2011, Bowdley, 2009). Previous research has also shown that public engagement events in astronomy also attract more men than women (Entradas et al., Curtis, 2013). It is perhaps unsurprising therefore, to find more male participants in an astronomy-based citizen science project such as Planet Hunters. However, both Foldit and Folding@home deal with research in biological science, which tends to have a better representation of women both in formal education and professionally (albeit at relatively junior levels compared with men) (Bowden, 2012, Howard Eckland, 2013). A recent UK study also showed that women tended to be interested in areas of science that were related to health and medicine (Ipsos-MORI, 260 2011). Both Foldit and Folding@home address the underlying mechanisms associated with some serious diseases such as Alzheimer’s, cancer and influenza, so the low proportion of women among the survey respondents was surprising. Some previous research has found that gender has an effect on internet use, and several studies have found that men tend to spend more time than women on the Internet (Jones et al., 2009, Helsper, 2010, Joiner et al., 2012). Not only do men tend to spend more time on the internet, but they are also more likely to use it for ‘entertainment’ purposes and to play games (Jones, et al. 2009, Helsper, 2010). These observations could also help to explain the higher proportion of men in the projects explored in this research, and increase the likelihood that men will learn about these projects via online articles or searches. However, the format of these projects is of importance, and the fact that Foldit is a game and Folding@home is a distributed computing project could influence their appeal to women. While the number of women who play computer games has grown rapidly over the past decade, they tend to play different types of games than men (Internet Advertising Bureau, 2011). For example, they do not play as many online multi-player games and they tend to play more games on mobile devices (Green, 2012). One study also found that due to less leisure time available, fewer women played computer games, and those that did, played them for much shorter bursts of time (Winn and Heeter, 2009). These effects of gender on game-playing habits could help to explain the smaller number of female Foldit players (Entertainment Software Association, 2012, Green, 2012). In distributed computing projects, all previously published research has shown that the majority of participants are male, and this includes studies with very large sample sizes (SETI@home, 2006, World Community Grid, 2013, Estrada et al, 2013). The appeal of 261 distributed computing projects to hardware enthusiasts and overclockers, and the lack of women among this community in the online survey (as well as on the overclocker forums that were observed), may explain the extremely low proportion of women in the Folding@home sample. Folding@home does not appear to be as widely promoted by the project team as Foldit and some of the Zooniverse projects, and most of the survey respondents report hearing about the project through websites and forums relating to computer hardware and overclocking. As a result, a smaller proportion of women may be hearing about the project. However, it has been harder to reach more ‘passive’ participants in Folding@home (those who just download and run the programme) and an investigation into these participants could possibly reveal greater female representation. To date, only a limited number of online citizen science projects have been explored (see Chapter Two, Tables 2.1 and 2.2), and the trend toward greater male participation may not be a feature of other projects. The results of this research could have implications for the wider accessibility and appeal of online citizen science projects, particularly distributed computing ones. Those setting up an online citizen science project may want to consider whether they want to appeal to a wider base of participants. The scientific discipline of the project, the project format, how and where project managers communicate about the project may have an influence on the gender profile of their participants. 8.1.2 Level of education and general interest in science Another notable feature of these three groups of respondents was how well educated they were and 60 % (335) had a university education. Of the remaining 227 respondents, 97 were currently studying. Two other studies on Zooniverse participants have also shown a high representation of university graduates among study respondents (Reed et 262 al., 2013, Raddick et al., 2013). Previous research on attitudes towards science has shown that those with a greater level of education are more interested and engaged with science (Ipsos-MORI, 2011, RCUK, 2008). Such individuals may consume more science- related content and come into contact with sites and publications which promote and discuss online citizen science projects. Many respondents (272, 48%) had formal qualifications in STEM subjects although there were very few respondents (36 in total) who were employed as scientists or were in medicine. These individuals bring some expertise to the projects, either through their knowledge of the related science (e.g. two of the Planet Hunters interviewees had degrees in astronomy), or knowledge of the ‘scientific process’ (e.g. the importance of rigorous data analysis, publication of results, collaboration etc.). Feedback from some of those who took part in the Planet Hunters interviews highlighted a desire to be (re)connected with science, and that their involvement in the project offered them to the opportunity to get involved in an area that was once of great interest or importance to them. Those with a formal qualification in a STEM subject have already demonstrated an interest in science and technology, so online citizen science games may appeal to these interests. The tasks associated with Planet Hunters and Folding@home were not especially complex, and there is no requirement for any previous knowledge or experience of scientific research in any of the projects investigated. However, Foldit is a difficult game to learn and play, and requires complex problem-solving skills and spatial awareness. Of the three projects, Foldit respondents had a greater percentage of graduates (65%), and a quarter of respondents were educated to masters or PhD level. The level of difficulty associated with the project task compared with the other two may explain this finding. 263 Practically all of the respondents demonstrate a wider interest in science and report taking part in science-related activities and reading scientific publications and online content (only three respondents reported not taking part in any science-based activities in the previous year). More than half (52%, 293 respondents) had taken part in other citizen science projects. This high level of engagement with science may be indicative of the appeal of online citizen projects to those who have been described previously as ‘confident engagers’ (Ipsos-MORI, 2011) or ‘fans of science’ (Priest, 2009). According to this research, such individuals are usually better educated, more affluent than other sections of the population, and have more positive attitudes towards science and scientific developments. In a recent UK study, 14% of the population fit into this classification (Ipsos-MORI, 2011). Among survey respondents, there were a high proportion of those in IT-related professions (177, 31.5%). This could be related to the presence of a large number of hardware enthusiasts and overclockers in the Folding@home sample. However, the other two projects also had significant numbers of IT professionals. This suggests that online citizen science projects may be more appealing to those who are confident in using computers, or who are more technically proficient. Such individuals bring skills which have been beneficial to these projects. This may be seen in the advent of ‘recipes’ in Foldit and the coding of moves with the Lua coding language, and in the contribution of the Beta Team in Folding@home. The most recent Oxford Internet Survey (2013) refers to confident internet users as ‘e- mersives’, and defines them as individuals who are comfortable and naturally at home in the online world and use the Internet for entertainment purposes, to meet people and to make their lives easier (Dutton and Blank, 2013). Based on survey and interview 264 feedback, as well as my observations, it is likely that this term accurately describes active participants in these three projects. 8.1.3 The appeal of online citizen science projects The projects that have been investigated as part of this research appear to appeal to male, well-educated and scientifically engaged individuals who are also likely to be confident with computer technology. The findings of this research appear to corroborate the findings of other studies (Holohan and Garg, 2005, Krebs, 2010, Nov et al., 2011, World Community Grid, 2013, Raddick et al., 2013), yet there remains a small sample of projects that have been investigated in any detail, and most of these have been distributed computing projects and astronomy-based distributed thinking projects (see Chapter Two, Tables 2.1 and 2.2). The lack of diversity in participants in some online citizen science projects may be related to a phenomenon known as ‘threshold fear’ (Gurian, 2005). This has been examined in relation to attendance at museums, art galleries and other public cultural institutions. It has been defined as the constraints people feel that prevent them from participating in activities that are targeted at them (Gurian, 2005, Simon, 2012). In the case of physical spaces there may be tangible impediments that prevent some people from attending or participating (such as location, or cost of entry), but there are also important socio- cultural factors such as gender, ethnicity, age, class background and personal history that influence who takes part in these activities, and what kind of experience they have if they do participate (Dawson and Jensen, 2011). In the case of online citizen science projects, these socio-cultural factors may also be of importance and some individuals may think that these activities simply aren’t meant for them. The scientists who are involved in Planet Hunters (and other Zooniverse projects) 265 have tried to ensure that the project task is relatively straightforward and have provided tutorials and other educational material. Tutorial training (in the form of intro puzzles) has also been provided by the Foldit project team. However, this may not be enough to increase the appeal of the projects to those outside of the ‘confident engagers’ group. In addition to where and how projects are promoted, there may also be design features that influence their wider appeal e.g. the overall appearance of the website and ease of navigation, and the presence of online forums or internet relay chat. Whether this is the case merits further research. One reason for the relatively small numbers of active participants observed may be unrelated to ‘threshold fear’, and may instead be related to something that has been referred to as ‘participation bandwidth’ (McGonigal, 2008). This refers to the total amount of time we have available for online activities. According to McGonigal, there are ever increasing numbers of social networks to join, new wikis to edit, new content to contribute and new games to play. We are exposed to more opportunities to contribute than we could possibly accept, and we only have so much time to contribute to online ventures. Therefore, online citizen science projects have to compete with other networks and online interests for the attention of those who are active online, particularly those who are ‘e-mersives’. The results of this research suggest that these three online citizen science projects have created opportunities for small groups of ‘confident engagers’ to become involved in authentic scientific research. These distributed volunteers have responded to a range of scientific challenges, and have self-organised into various project roles and teams to produce new knowledge. These findings also highlight the importance of making a 266 distinction between ‘registered’ and ‘active’ participants, and that what constitutes an ‘active’ participant, will vary between projects. 8.2 What motivations initiate and sustain participation in online citizen science projects? (RQ2) Table 8.1 presents a summary of the results obtained through the online surveys, and lists the most important motivations that initiate and sustain participation for citizen scientist volunteers in each of the three projects. The number of individuals who gave these responses is in brackets. In most cases, two or three motivations are predominant. Table 8.1 Summary of main motivations that initiate and sustain participation for citizen scientists Foldit (n=37) Folding@home (n=407) Planet Hunters (n=118) Motivations that initiate participation 1. Contributing to research (22). 2. Interest in science (13). 3. Intellectual challenge (10). 4. Curiosity (8). 1. Making a contribution (to research or a worthy cause)(207). 2. Fully utilise computing power / hardware enthusiast (128). 3. Personal experience of diseases being researched by project (74). 4. Admire goals of the project (68). 1. Interest in the science (56). 2. Contributing to research (40). 3. Chance to make a discovery (26). 4. Goals of project important (11). Motivations that sustain participation 1. Contributing to research (14). 2. Interaction with other players / community (13). 3. Developing skills / learning (9). 4. Opportunity to be creative (8). 1. Making a contribution (150). 2. Ease of use / accessibility (75). 3. Overall idea / ‘concept’ of project (54). 4. Competition (47). 1. Contributing to research (39). 2. Chance to make a discovery (29). 3. Easy to take part (21). 4. Community (10). 8.2.1 Initiating participation For all three of the projects making a contribution is one of the most important motivations for participation. This was mainly making a contribution to scientific 267 research, although Folding@home participants tended to make a differentiation between making a contribution to science and making a contribution to a ‘worthy’ cause. A background interest in science was also an important motivator for Foldit and Planet Hunter respondents. An interest in the science and the desire to make a contribution were also important in the two other distributed thinking projects that have been the subject of previous research (Nov et al., 2011, Raddick et al., 2010). However, a background interest in the science was not mentioned by as many Folding@home respondents, although in this group (given the involvement of overclockers) a background interest in computing can be considered a related motivator. A few motivations initiating participation varied between the projects. For example, the intellectual challenge of the game was important for Foldit respondents, as was their curiosity about the project (perhaps relating to the re-packaging of a scientific research problem into a game format). In Planet Hunters, the opportunity to discover an exoplanet was important for over a fifth of respondents. Although this is how the project is ‘marketed’, and its tagline on the Zooniverse website is ‘Find planets around stars’. In Folding@home, the involvement of overclockers meant that the opportunity to fully utilise computer processing power was an important motivator for a large proportion of these respondents. This was also found to be an important motivator for participants in the World Community Grid (2013). As Folding@home is concerned with research into the underlying causes of many significant diseases, a significant proportion of respondents stated a personal experience of a disease such as Alzheimer’s, Parkinson’s or cancer (either themselves, a friend or family member) as a reason for joining the project. While Foldit is also concerned with how protein folding may relate to disease, this was not given as a reason for participation 268 by many of the respondents to the Foldit survey suggesting that its format as a game may be more important when it comes to attracting new participants. Feedback from the interviews with participants (citizen scientists) in Foldit and Planet Hunters mirrored the results of the online surveys, and participants re-stated their desire to make a contribution to scientific research, and an interest in the background science. The latter was especially true for the Planet Hunter interviewees, many of whom took part in amateur astronomy activities. In Folding@home I spoke specifically to individuals who were involved in the Beta Testers and in overclocking. Those in the former group appeared to be motivated to join so that they can share their knowledge and expertise in order to make the project more successful. Those in the overclocking group wanted to make a contribution to the project, but also wanted to develop their technical knowledge relating to computer hardware. A small number also mentioned the competitive aspect of their involvement in Folding@home. The results from the surveys and interviews suggest that both the research goals of an online citizen science project and the subject area are important considerations for potential participants. A project may need to demonstrate that there is a relevant and useful research problem to be investigated and that participants can actually make a difference. Members of the Zooniverse team who I interviewed also stressed the need to effectively utilise the efforts of citizen scientists and not waste anybody’s time (see Chapter seven, Section 7.2.2). This utility of citizen scientists’ contributions may be demonstrated by promoting the discoveries that ‘ordinary’ citizens can make. For example, this has been done 269 successfully in some of the Zooniverse projects, where participants have made a number of serendipitous discoveries (Cardamone et al., 2009, Lintott et al., 2009), and in Foldit, where Foldit teams have been co-authors on important scientific papers (Khatib et al., 2011a, Khatib et al., 2011b). Folding@home has a link to all of the papers resulting from the project (including a summary of the findings for non-specialists) and lets participants know via the project blog when new publications appear. Making a contribution to good quality science was also alluded to in some of the follow-up interviews with Folding@home participants. That potential participants can make an important contribution appears to be key in the way some of these projects are pitched and communicated about both by those managing the projects, and also by those writing about them in magazine and news articles, both print and online (Bohannon, 2009, Bonetta, 2009, Borrell, 2013, Hand, 2010). Individuals are more likely to participate in a project that is also in an area of science that is of interest or relevance to them. While this was stated explicitly by a number of Foldit and Planet Hunter respondents in the surveys, interviews, and within discussion threads I observed though my own participation, it was more implicitly stated by Folding@home participants. The relevance of this project was illustrated in references by participants to loved ones who were suffering from the various diseases that are being investigated by the scientists associated with Folding@home. I also observed this strong motivation while exploring several of the overclocker communities, and many of these communities have discussion threads which have become virtual memorials to friends and family who have been affected (see Chapter Four, Figure 4.15). These individuals are motivated by the potential applications of this research and the relevance to their own lives. Previous research in citizen science, particularly work exploring communities and local 270 environmental issues, has also highlighted the importance that personal relevance plays in motivating participation (Irwin, 1995). 8.2.2 Sustaining participation For all three projects the opportunity to make a contribution to science is also one of the main motivations that sustains participation. Again, as in initiating participation, it may be that the utility of citizen scientists’ contributions need to be made clear to participants in some way if they are to continue devoting their time to a project. All three of the projects have made an effort to do this through the material they provide on the project website and in how the projects have been communicated and promoted. The value of citizen scientists’ contribution is also demonstrated in some instances through the acknowledgement of contribution on publications (Lintott et al., 2009, Khatib et al., 2011, Schwamb et al., 2013, Schmitt et al., 2014). While there appears to be a similar ‘hook’ of making a contribution for getting people to begin participating, the goals of the project, and the project task may mean that the subsequent experience for participants is very different from one project to the next. These differences will be explored in Section 8.3, when RQ 3 (How do motivations vary between different types of online citizen science projects and their related tasks?) is considered. 8.2.3 Models of motivation All of the reasons for taking part in the three projects, as well as what participants liked best about participating, were listed. These were then considered in light of the motivational frameworks that have been utilised by other researchers looking at participation in online citizen science. While no single framework could be applied to all of the observed motivations, the work of Ryan and Deci (2000, 2009) on intrinsic and extrinsic motivations was the most relevant to my findings, and many of the motivations 271 given by citizen scientists for their participation could be classified in this way. According to Ryan and Deci (2000), intrinsic motivation exists when an activity is enjoyable, or when it promotes feelings of fulfilment and competence. There is no need for an external reward because the activity is inherently interesting and fulfils some of our basic psychological needs such as feeling capable or useful. Extrinsic motivation is in operation whenever an activity is done in order to attain some separable outcome. However, extrinsic motivation is not merely a question of punishment or reward. It exhibits a range of expression that is related to the degree of autonomy experienced by an individual. This is also known as ‘self-determination theory’, which maintains that although an activity may not be interesting or appealing, it is personally endorsed in some way and the individual has a feeling of choice (Ryan & Deci 2009). This is in contrast to compliance, when an individual carries out an activity because of an external control (e.g. avoiding punishment). Motivations based on compliance are not relevant when it comes to participation in online citizen science, however, extrinsic motivation based on introjection, which is defined by Ryan and Deci (2009) as ego involvement or the desire for approval from others, and identification, where an individual has identified the value of the activity, do appear to be relevant. In the latter case, an individual may continue an activity they find dull or boring if they believe it may ultimately be of relevance or importance (this is evident in survey feedback from Planet Hunters participants, several of whom described the task as monotonous). While the approach of Ryan and Deci (2002, 2009) was relevant for many of the motivations for participation articulated by citizen scientists, some were not entirely explained by this framework. For example, one of the most commonly cited reasons for participation, the desire to help and to make a contribution to scientific or medical 272 research, is based on an altruistic motivation or empathy that has more in common with other types of more general ‘community-based’ voluntary behaviour as detailed by Clary et al (1998) and Batson et al. (2002). An altruistic desire to contribute to the ‘public good’ has also been observed among participants of other types of commons-based peer production such as writing open source software or Wikipedia articles (Kuznetsov, 2006, Nov, 2007, Oreg and Nov, 2008). In addition to altruism, another important internal motivation is the desire to work with and be a part of a community; to cooperate and collaborate, in this instance as part of distributed social group. This motive has been identified in previous work on those who write open source software, and has been linked to a basic human need for belonging (Hars & Shaosong, 2002, Maslow, 1943). Previous work on open-source software has also highlighted an important external motivator that relates to ‘expected future returns’ (Hars and Shaosong, 2002). Within the context of open-source software, this means that an individual may be rewarded for their involvement sometime in the future in the form of revenues from related products and services, or career advancement through marketing and showcasing their technical skills (Hars and Shaosong, 2002, Hertel et al., 2003, Oreg and Nov, 2008). However, none of these are guaranteed. There appears to be a parallel to this motivation among some participants of the online citizen science investigated here, particularly those who have a more personal stake in the outcome of the research. For example, many respondents to the Folding@Home survey stated that their involvement in the project was the direct result of a loved one (or they themselves) being affected by one of the diseases being researched by the Folding@Home scientists. Some of the respondents to the Foldit survey also expressed this sentiment. Many of these individuals also hoped that their involvement would result in the development of a cure or therapies for these conditions. 273 Again, there is no guarantee that any of these participants will see these things in the near future, yet this remains an important motivator. Such a motivation could also be considered ‘enlightened self-interest’. A framework, or classification system, has been developed that is based largely on the work of Ryan and Deci (2000, 2009) but also takes into consideration some of the motivations that have been identified in previous work on community volunteering, and participation in open source software. This framework (Table 8.2) seeks to incorporate all of the major motivations that have been articulated by respondents to the three online surveys, and build them into a model that represents a hierarchy of motivations, from high to low granularity. At the highest level (Level 4), motivations can be classified as either internal factors, which are rooted within the individual, or they can be classified as factors that are external to an individual (Hars & Shaosong, 2002). At Level 3, internal factors can be subdivided into intrinsic factors, altruism and community, while external factors can be divided into extrinsic factors and expected future returns. These can be further sub- divided (Level 2) into a number of elements that have been identified by Ryan and Deci (2000, 2009) as the components of intrinsic and extrinsic motivation. A further breakdown of altruism, community involvement and expected future returns, and their relevance to online citizen science projects has also been illustrated at this level. Level one represents the lowest ‘granularity’ of motivation. Respondents articulated motivations that were either Level One or Level Two 274 Table 8.2 Motivational framework based on feedback to online surveys by Foldit, Folding@home and Planet Hunters participants Level 4 Level 3 Level 2 Level 1 task granularity Internal Factors Intrinsic motivations Enjoyment Relaxing Visual appeal Fun Fulfilment Background interest in science Participation in authentic research Allows creativity Learning opportunity Competence Intellectual challenge Using skills Formal qualifications not required Different ways to contribute Altruism Making a contribution Contributing to scientific research Contributing to a worthy cause Helping scientists Community Interaction with others Work with others toward common goal Make friends External factors Extrinsic motivations ‘Ego enhancement’ (introjection) Points Rank Making a discovery wider recognition Positive feedback from scientists Identification Goals of the project are important Expected future returns Medical / scientific breakthroughs Research publications New drug therapies Cures Most participants in this research have expressed more than one reason or motivation for taking part in their project and they can incorporate both internal and external factors. This research has also shown that their motivations can change over time. However, internal motivations appear to be dominant in the case of online citizen science, as making a contribution (altruism), and an interest in the science (the intrinsic motivation of fulfilment) are the two most commonly cited reasons for participation. ‘Ego enhancement’ rewards in the form of points, rank and reputation appear to be of greater 275 importance to the community of overclockers who participate in F@H, and for some Planet Hunters participants who want the recognition associated with making a discovery. Whether this model is applicable to other online citizen science projects is of interest, and it may be of relevance to scientists and developers considering setting up an online citizen science project. Results from previous studies, particularly work on other Zooniverse projects (Raddick et al., 2010, 2013) and some work on distributed computing projects (Holohan and Garg, 2005) suggest that levels two through four may be applicable and able to describe the motivations reported by participants. However, this would require further investigation and discussion. 8.2.4 Motivations of project scientists and developers The motivations of those involved in setting up and managing online citizen science projects has not been considered in previous research in any detail. The scientists interviewed who are involved in Planet Hunters and Folding@home set these projects up in order to accomplish specific research goals based on the analysis of large volumes of data. In order to achieve their research aims in a realistic timeframe (years rather than decades) these scientists involve non-specialist volunteers. A precedent had already been set in the form of contributory, ecology-based programmes (Mayer, 2010, Dickinson et al., 2010, Roy et al., 2012) and the advance of digital technologies and the internet has provided the infrastructure that enables online citizen science projects. In the case of Planet Hunters, a digital infrastructure was already in place in the form of the Zooniverse. The developers of Foldit work with a team of biochemists to ensure that Foldit is guided by specific research aims and goals. The game was designed to take advantage of the diversity that exists in the problem solving abilities and game strategies of each individual player. The developers interviewed were motivated to get involved in Foldit because 276 they wanted to work with the wider community that were external to their research institution, and because they saw a potential for games in addressing specific scientific problems (Cooper, 2011). In appears therefore, that the primary motivator for the project teams I interviewed is to tap into the pool of citizen scientist volunteers to get help with scientific research. Citizen scientists help professional researchers to carry out tasks that would be highly time- consuming and repetitive, thus freeing them up for more in-depth analysis that requires more specialist knowledge and skills. They also carry out tasks that (at this point in time) cannot be carried out by computer algorithms, and tasks that rely on distinctly human approaches to problem solving. In this sense, citizen scientists are providing a free labour resource to these groups of scientists. However, scientists are also able to meet other professional objectives through their involvement in online citizen science projects. For example, the Planet Hunters project team see the project as a way to engage with their volunteers and in the words of one of the scientists “bring them along on the ride” - the ‘ride’ being an insider’s view of professional scientific research. The Zooniverse community manager spoke of developing informal educational programmes for Planet Hunters participants (something that Zooniverse projects have not tended to do in the past), and trying to encourage participants to learn more about astrophysics. While ‘engagement’ more often than not refers to educating volunteers, or telling them about the research process, some of the project scientists try and encourage them to become more involved in the research process and to start asking questions of their own. A recent project in the Zooniverse called ‘Galaxy Zoo Quench’ has tried to involve participants more closely in the research process and citizen scientists are able to get 277 involved with different levels of data analysis and collaborate on the preparation of a journal article (Simpson, 2013). However, this project is overseen by a group of professional astronomers, and they (not the citizen scientists) have formulated the research questions. Foldit scientists and developers also regularly engage with the players and hold regular online ‘chats’ that any participant can attend and pose questions to the project team (see Chapter Four, Figure 4.7). Members of the Baker Lab (including Professor David Baker) have been involved in the production of blog posts, podcasts and short films about the project that are regularly featured on the Foldit website. External presentations made by scientists have also been shared with players. However, the Foldit developers interviewed stated that because there are so many participants to engage with there isn’t always the time (or the human resources) to do this enough justice. Over time, and perhaps as a result of this, the Foldit community has become a self-organising research community, with a small number of players occasionally working with the project team. This sentiment was expressed by nearly all of the project scientists that were interviewed, and could also be seen as a reflection of how they view citizen science more generally. For some of the project scientists, a conflict between interacting with citizen scientists volunteers and carrying out their ‘day jobs’ is evident. This suggests that for some of these scientists, citizen science may not be considered a core research activity. This issue, as well as some of the other issues surrounding ‘management’ of large groups of citizen scientists, as well as the varying degrees to which scientists become ‘embedded’ in the project community, will be discussed in greater detail in Section 8.4.2. 278 8.3 How do motivations vary between different types of online citizen science projects and their associated tasks? (RQ 3) Motivations that initiate participation in online citizen science projects share a number of similarities between the three projects investigated, and most of the individuals surveyed become involved in order to make a contribution, and because they have a background interest in the science relating to the project. However, the nature of the project task and the goals of the project also influence other (less cited) motivations that initiate participation. For example, my research indicates that Foldit players are drawn to the game because of the intellectual challenge of the task, and because they are curious to see how a scientific problem has been re-packaged as a game. For Planet Hunters participants, the data indicate that they want the opportunity to make a discovery which they feel is important in some way. For many (74, 18%) Folding@home participants, they have some personal experience of the diseases that are being investigated by the Pande Group, and they want to be able to help the scientists find a cure, or develop new drugs. Where the projects show greater divergence, is in the factors that sustain motivation (see Table 8.1). This has implications for the design of projects, and in the setting of project tasks. While making a contribution sustains motivation in all of the projects, one of the key differences appears to be the importance placed on the interaction with other participants and the online community. For example, for Foldit players, the importance of interaction with other players was emphasised in both the online survey and in the follow-up interviews. My experience as a participant also highlighted the importance of the community and I observed a high level of interaction (see Chapter Four, Section 4.1.5). Players help each other learn to play Foldit, they collaborate and co-operate with 279 each other on the science puzzles, and many make friendships through their participation. This interaction also contributes to some of the other motivations that help to sustain participation such as the ability to develop skills, and to work creatively. This was especially apparent in the interviews where participants spoke of “theorycrafting” and collaboration, and their pride in the achievements of the community. The importance of this interaction is related to the difficulty of the project task, and through sharing different approaches and perspectives, better results may be achieved both individually and collectively. This interaction was emphasised to a lesser extent through the interviews with the Folding@home Beta Testers and members of the overclocking community. Some Beta Testers members spoke of the enjoyment they derived in working with others to solve some of the software issues the project faces. Overclockers referred to the fact that members of this community, while competitive, were also happy to share expertise and help others with technical issues. One of the appeals of Folding@home for many of these participants was that they could combine this interest with making a contribution to scientific research. Only a small percentage of Planet Hunters participants (10 respondents, 9%) spoke of a project community on the online survey, with half (9) stating in the interviews that they didn’t interact at all with other project members. The survey indicates that this is a solitary activity for a significant number of the respondents. The task on Planet Hunters is relatively straightforward and completed individually, although a participant may ‘flag’ an interesting item, or ask for advice about spotting a planetary transit on one of the discussion boards. In fact, the ease of participation keeps 21 respondents (18%) participating. Most of those who took part in Planet Hunters survey and interviews work 280 by themselves classifying the light curves, dipping in and out of the project when they have the time and inclination. It is this pattern of participation that is appealing for these individuals allowing them to ‘microvolunteer’ when time and inclination allows, while still enabling them to contribute to authentic scientific research (Paulos et al., 2011). Thus, the importance of ‘the community’ in sustaining involvement in a project appears to be related to the complexity of the project task, and the community is more prominent in projects, such as Foldit, where the task is difficult, and where new participants may need the help of more established participants in order to learn how to play, and among the Folding@home Beta Testers who collaborate with project developers to address software issues. The possibility of discovering a new planet remained an important motivating factor that sustained participation in Planet Hunters. The possibility of discovery was referred to by one respondent as the “thrill of the chase” and has also been noted as an important factor that retains participants in Galaxy Zoo. In Galaxy Zoo however, it is the possibility that the next image will be even more beautiful than the last (Raddick et al., 2010, Sproull, 2011). Another factor that was highlighted in the Planet Hunter interviews that may play a role in sustaining participation, was the opportunity to re-connect with science. This appeared to be important to those who had a formal qualification in science who were unable to pursue a career in science and for some who had been deeply interested in science during childhood. Planet Hunters may also offer a way back in to science more formally, and two of the Planet Hunters participants (as well as a few Foldit and Folding@home participants) had enrolled in formal education courses to improve their knowledge of the science associated with the project. Involvement in an online citizen 281 science project may also offer the opportunity for a professional scientist to become involved in another discipline. In Folding@home, motivations that sustained participation were related to the involvement of the overclockers and hardware enthusiasts. The fact that the project and project interface are easy to use was mentioned by 75 respondents, and in many cases this feedback was provided with a reference to overclocking activity. For other participants (47) the competitive element was important, and again, this aspect of participation in distributing computing projects appeals to hardware and overclocking enthusiasts. Opportunities for informal learning were not explicitly referred to by many of those responding to the survey or taking part in the interviews, and this applies to all three of the projects investigated. When asked directly what they had learned by participating in their project during the interviews, the majority stated that they had learned about the related science to some degree. However, this was not an important factor in sustaining participation, although learning about game-related skills helped to sustain participation for approximately a quarter (nine) of the Foldit survey participants. The results of this research show that the main factors that initiate motivation (wanting to make a contribution and an interest in the science) are similar between the three projects but that the less commonly cited reasons for participating are related to the project task, and to the overall goals of the project. Factors that sustain motivation show a greater divergence between projects, although making a contribution is still the most important motivation for remaining involved in all three projects, as are task-related motivations (such as discovering a new planet, or being able to combine participation with an interest in computer hardware or overclocking). The level of difficulty of the 282 project task influences the importance of the online community in sustaining participation. Thus, Foldit participants value this aspect of participation more than those participating in Planet Hunters and Folding@home, although the importance of a community is important to some Folding@home participants involved in overclocking or the Beta Testers. Scientists and developers considering setting up an online citizen science project need to consider the dynamic nature of motivations (Batson et al. 2002, Rotman et al., 2012), and that they are influenced by the design of the project (particularly any opportunities provided for interaction with other participants) and by the complexity of the project task. 8.4 How and why do project participants interact online? (RQ4) Interaction in online citizen science projects can occur between citizen scientist participants, and between citizen scientists and members of the project team. This interaction can take place in a number of online locations such as forums, blogs, and internet relay chats. Project participants were asked about interaction with others on the online surveys and during the interviews. I was also able to provide a partial, ‘snapshot’ assessment of the degree and extent to which participants interact via experience as a participant-observer. 8.4.1 Interaction between citizen scientists Of the three projects, Foldit players reported (via the survey) the greatest level of involvement in the online project community. Of this group of respondents, 84% (31 players) report regularly interacting with other players online. The ‘community’ appeared to be one of the most important features of Foldit, and players created and fulfilled a variety of project roles (see Chapter Four, Table 4.1) . Some Foldit players (both on the survey and during the interviews) spoke about the community with pride and warmth, 283 highlighting the social dimension of online participation. Players talked about the friendships they had made online and with fellow team mates. One player referred to his team as his “folding family”. General levels of interaction between Foldit players are also greatly enhanced by the presence of a synchronous ‘global’ chat window. In Folding@home, approximately 80% of survey respondents stated that they read or post in forums relating to the project. Through the interviews, the sub-communities within the Folding@home sample, the overclockers and the Beta Testers, also report regular use of online forums to discuss issues relating to their involvement in the project. Similarly to Foldit, comments were made by members of the overclocking community and the Beta Testers in Folding@home about making friends online, and about the enjoyment of working with others on technical problems. In comparison, a smaller proportion of Planet Hunter respondents take part in online discussions, and most tend to work alone while carrying out the project task. Approximately half report reading content on the online discussion boards or Talk function, but only 25 respondents (21%) have ever posted content. Data provided by one of the project scientists on the number of Talk comments posted on Planet Hunters suggests that nearly a third of the total number of comments made have been posted by only 48 different individuals (Raddick, 2013). Taking part in the Planet Hunters discussion boards does not have the immediacy of taking part in an internet relay chat conversation, or even taking part in one of the overclocker forums, which appear to have a relatively rapid response rate if one posts a comment. My experience in Planet Hunters was that responses to comments on the discussion board and on Talk can take a while, if indeed there are any responses at all. According to one of the Planet Hunters scientists, a small group of core participants works together and analyses the raw data from the NASA 284 Kepler project. She stated that these individuals privately email each other and one of the project scientists, and carry on their collaboration outside of the project infrastructure. A recent study of Zooniverse discussion boards has shown that responses to comments can take many hours (or days), and that this can affect the vibrancy of a community and how it is perceived by new participants (Luczak-Rösch et al., 2014). This lack of immediacy can make asynchronous online communication less dynamic (O’Connor, 2008, James and Busher, 2009). The absence of interaction between Planet Hunters participants was also apparent in the interview feedback, and comments referring the community (or rather the lack of it compared with other Zooniverse projects) were made by two individuals who had previously acted as forum moderators in other Zooniverse projects. Participant observation and the feedback from surveys and interviews indicate that the level of interaction between participants is related to the project task and certain aspects of project design. A high level of task complexity means that participants interact with other more experienced members of the community if they want to actively contribute to a project. This occurs when new participants are learning about the project task, and when participants work together on the task. This is the case in Foldit and to a lesser extent among overclockers and members of the Folding@home Beta Testers as they work together to address specific hardware and software issues. The project task in Planet Hunters is relatively straightforward in comparison to the Foldit science puzzles, and one can carry it out alone after reviewing the tutorial material provided on the website. The presence of teams also affects interaction between participants. In Foldit and Folding@home teams bring participants together to work on a specific project task, or to 285 compete against other teams. They can also develop their own sense of identity. Teams have their own webpages (some with their own ‘mission statements’ or approach to playing), forums (e.g. overclocking teams in Folding@home), or even their own internet relay chat channel (e.g. Foldit) where they work together in real-time. Teams on Foldit and Folding@home also have some independence from the project organisers, which may affect the dynamics and content of this interaction. The lack of teams, or an element of competition of Planet Hunters, reduces the opportunities for participants to come together, and interaction is limited to the Talk function, discussion board and project blog, all of which are overseen by the project managers and a small number of moderators. From my observations of Foldit, internet relay chat is a key facilitator of online communication between players. Not only does it play a role in establishing collaborations between players, but it is also important for the formation of online friendships, and more general sociability (Jennett et al, 2013). Internet relay chat has also been used on one occasion by members of the Beta Testers who collaborated to address a particular software problem. The lack of internet relay chat on Planet Hunters, and the lack of a ‘quick turnaround’ with regard to responses to online posts influences the amount of online interaction between citizen scientists, and may also limit the number of participants who are motivated to interact. 8.4.2 Interaction between project team members and citizen scientists The avenues used for communication (e.g. blog, forum, internet relay chat) and the degree to which members of the project team are ‘embedded’ and have a regular presence within the community vary between the three projects. However, the prevalence of more one-way communication from project scientists to citizen scientist 286 volunteers (particularly in Folding@home and Planet Hunters) reflects the fact that all three projects were initially organised in a top-down fashion (Mueller et al., 2012), and that volunteers were not directly involved in setting up the project or asking the research questions. This is in contrast to other types of citizen science project, where citizens, who may be concerned about an environmental issue, for example, determine the focus of the research including what questions are addressed (Irwin, 1995, Ottinger, 2010, Conrad and Hilchey, 2011). In Folding@home, there is some interaction between members of the project team and those in the Beta Testers (Chapter Four, Section 4.2.2). However, there were a number of points made on the online survey and the interviews that outlined a general desire for greater communication with the project team, particularly for greater visibility of the project scientists and more information about how the output of the project is used. The controversy surrounding the changes in the BigAdv points system highlighted some of these issues and some of the problems that can arise when small teams of project scientists try to communicate with many hundreds or thousands of participants, particularly when they relate to changes in the project (Chapter Four, Section 4.2.6). In Planet Hunters, there is one key scientist who is involved with communicating with participants via Talk, the discussion boards and the project blog. This same project scientist takes an active role in communication, and considers it to be an important component of their involvement in Planet Hunters. A second project scientist takes part in research-related discussions via email with a small group of participants directly, although these discussions are not visible on the project website (and none of this group took part in the survey or interviews). From observations of the project website and from survey and interview feedback, the number of citizen scientists who interact with the 287 project scientists is very small and limited to a core group of approximately 20 very active participants. During Foldit most of the interaction occurs during play via global and team internet relay chat. This is usually between players and seldom involves members of the project team. Communication with the project team occurs through the project forum, the project blog (on the Foldit homepage) and through real time online chats with members of the project team (see Chapter Four, Figure 4.4). These chats generally occur every two months and any player can take part. There are members of the project team who regularly communicate with the players and are visible on a regular basis. Interviews with Foldit developers suggest that the project team place a high level of importance on these interactions. As with Planet Hunters and Folding@home, direct interaction between members of the project team and the players is largely restricted to members of the core team. From interview feedback and my observations, it is evident that all three project teams have experienced some problems when communicating with citizen scientists. Some volunteers can be critical of changes made to projects or of other aspects of the research. Participants who have been with the project for many months or years, and invested a lot of their free time may develop strong opinions regarding how the project should be managed and what the priorities should be. To what extent the views of citizen scientists are taken into account, or how (and if) they are consulted when changes are made to a project may need to be considered at some point by project teams, especially if projects run for many months or years. This has been especially evident in Folding@home after the changes to the BigAdv points system was announced (Chapter Four, Section 4.2.6). 288 These observations raise issues relating to governance, and to what extent the participants are, or can become involved in influencing decisions relating to the project. The Folding@home project team responded to concerns about the changes to BigAdv, and stated their intention to involve participants more in future decisions relating to the project. Recently, the Zooniverse team have been reconsidering the Talk function (several Planet Hunters interviewees were critical of the vehicle for communication) and have asked participants to provide feedback. Of the three projects explored in this research, Foldit appears to take into consideration the views and experiences of participants to a greater extent, and players have been involved in helping to set the parameters of the game since its inception (Cooper, 2011). However, the Foldit team have recently experienced some negative feedback from players in response to recent changes to the project software 73 . Perhaps in responses to some of these issues, all three projects have recently appointed ‘community relations managers’, although their actual remit appears to vary between projects. Issues of governance may have implications in relation to the sustainability of online citizen science projects and on the motivation of participants. Consequently, arrangements may need to evolve over time, and some projects may benefit from greater partnerships between project team members and participants, particularly those with small numbers of active participants. Some individuals invest greatly in these projects (both in time and in financial resources), top-down governance may not remain sufficient for some of these heavily involved participants. Furthermore, other prospective participants may be put off by the lack of engagement with the project scientists. 73 Information about the changes to Foldit software: http://fold.it/portal/node/996871. Feedback from players regarding these changes: http://fold.it/portal/node/998292, http://fold.it/portal/node/998346. http://fold.it/portal/node/996871 http://fold.it/portal/node/998292 http://fold.it/portal/node/998346 289 Feedback from citizen scientist volunteers also suggests that some may have certain expectations regarding how much contact they will have with the professional scientists and become annoyed when these expectations are not met. This was apparent in some of the interview feedback from Planet Hunters participants. Project scientists may need to consider and explore the expectations of their participants and tailor their interaction accordingly, or be explicit about the role of the project team and how they intend to interact with volunteers. It also implies the existence of an unwritten social contract between the project scientists and the citizen scientists, or of a sense of mutual obligations. This mirrors more general discussions relating to ‘science and society’ in which science has a contract with wider society which is built on trust and a set of expectations of the one held by the other. Gibbons (1999) describes this contract as one where the production of scientific knowledge is seen by society as both transparent and participative. While this contract is presently ‘unwritten’ in the projects that were investigated, they could be considered and eventually ‘written’. The presence of material of this nature on a project website could help to increase trust and transparency, and may ultimately help to sustain participation. Some of the problems relating to interaction between project scientists and citizen scientists may stem from a lack of time or personnel. All of the scientists and developers interviewed were engaged in scientific research full-time, or were involved in other projects. Many of the project team members interviewed spoke of a conflict between doing their ‘day jobs’ and trying to manage large numbers of volunteers. Practically every scientist or developer I spoke to alluded to this and stated that it was impossible to please everybody all of the time. These comments also provide an insight into how these scientists view citizen science, in particular, the degree to which they (or their 290 institutions) value the contributions of non-specialists. For example, is citizen science merely an ‘add-on’ to their research, or is it fully integrated into their ‘day jobs’? This integration of citizen science into the ‘day job’ varies between project team members. For Foldit, it is evident from the interviews with developers that this work is central to their research activities, and observations of communication activities of scientists, as well as the research output, from the Baker Lab also suggest that this is case for the project scientists. For one of the Planet Hunters scientists, interview feedback strongly suggests that they view citizen science as central to their research efforts, and stated that this is where they saw their career continuing to develop in the future. For Folding@home, the view of the project scientist relating to this issue was difficult to ascertain, as they were reluctant to talk about their experiences in much detail. Recent work exploring the views of scientists involved in OPAL (Open Air Laboratories), a series of ecological and environmental citizen science projects in the UK, highlights both of these issues – the time available to engage with volunteers, and the value placed on citizen science generally (Riesch et al, 2013a, 2013b). Interview feedback from the scientists involved in this project suggests that despite the fact that many of the scientists involved viewed these projects as good opportunities to engage with volunteers, they often underestimated the amount of effort that this required (Riesch and Potter, 2013a). The findings from my research are similar to those from OPAL. What this research adds, however, is that while the three projects provide opportunities for public engagement with research, this is not the primary motivation for scientists to become involved, and the main motivation for scientists and developers in the three projects investigated here, is a desire to employ citizen scientists to help analyse their data. 291 8.5 How can contribution to online citizen science projects be characterised? (RQ5) Previous research into other online projects and communities has shown that there are a variety of ways in which participants can contribute (Brandtzaeg and Heim, 2009, Preece and Schneiderman, 2009, Haythornthwaite, 2009, Makriyanni and De Liddo, 2010). This research has shown that this is also the case in the online citizen science projects investigated. Participation in these projects can be considered quantitatively, for example, how long they have been contributing to the project and how many hours a week do they spend on project tasks. It can also be considered more qualitatively, for example, what else do they do apart from the main project task, are there other roles they may fulfil? 8.5.1 Contribution of citizen scientists Of all three projects, the group of Foldit players spend the most time per week participating in the project task, with almost half (18 players) spending at least two hours a day playing. The greater time investment of Foldit players is related to the fact that the task is more time consuming than the tasks in the other projects (perhaps with the exception of the efforts of the overclockers or Beta Testers in Folding@home). Completing a Foldit science puzzle may take several days. Learning the task in Foldit is also lengthy and complex, filtering out individuals who are less than highly committed to the game. The strength of the community (which also helps to sustain participation) and the fact that many collaborate and co-operate together on the science puzzles may contribute to the greater level of time commitment demonstrated by some Foldit players (Borda and Bowen, 2009, Preece and Shneiderman, 2009). 292 As well as quantitative differences in the way an individual participates, this research has also identified other project-related tasks. These tasks have largely been identified through participant-observation, and confirmed by survey and interview feedback. Table 8.3 lists additional ways that an individual can contribute to each project. Some of these extra roles require specific skills such as knowledge of computer hardware or software, or language skills (for translation). Table 8.3 Additional project tasks and roles Foldit Folding@home Planet Hunters Forum / internet relay chat moderators Forum moderators Forum moderators Translating web content Translating web content Translating web content (recent request for help with this by Zooniverse team) Team co-ordinators Team co-ordinators Teachers / instructors (help new players) Beta Testers member Technical experts (e.g. project software) Scripters (write ‘recipes’ ) Strategy specialists (e.g. hand folders, soloists and evolvers) Foldit has the greatest diversity of tasks and many of these have evolved as a result of the complexity of the task and the learning process. Most of the additional roles in Folding@home are related to co-ordinating the efforts of the Beta Testers and teams of overclockers. In the absence of teams, competition and with a relatively straightforward project task, the diversity of roles is reduced in Planet Hunters. Offering a diversity of tasks may help to sustain motivation in online citizen science projects, and this aspect of participation requires further investigation. 8.5.2 Patterns of participation and contribution Each project has a small community of very active participants, who are a smaller proportion of the population of registered participants. This research found that among 293 the groups of active participants, are smaller groups of highly dedicated volunteers, or ‘core’ participants. They are defined as participants who: \\uf0b7 go beyond the project task and take on other roles within the project (see Table 8.3); \\uf0b7 complete a far greater number of project tasks than other participants; and \\uf0b7 use their involvement in the project to ask their own research questions or undertake their own independent research (this can occur independently or with some help from the project scientists). In Foldit, these individuals have created and filled a range of roles in addition to dedicating many hours a week to the project. Some of these individuals have been with the project since its launch in 2008. In Folding@home, core participants were found among members of the Beta Testers, and among the more active teams and communities of overclockers. In Planet Hunters, there are a small number of individuals (estimated to number 20) who do much of the classification work. Some carry out their own analysis using tools provided by the scientific team or that they have developed themselves. These individuals are highly visible on the discussion boards, and two of them have been named as co-authors in several Planet Hunters publications (Wang et al., 2013, Schwamb et al., 2013). Small groups of core participants can create a ‘community of practice’ (see Chapter Four, Section 2.4), and members of these communities share a repertoire of resources, experiences and skills (Wenger, 2006). As a result, these groups have close inter-personal ties (McLure Wasko and Faraj, 2005). The ‘reader-to-leader’ framework The ‘reader-to-leader’ framework (outlined in Chapter Two, Section 2.5) is of relevance to the patterns of contribution observed in these online citizen science projects, and like 294 other online communities, there are decreasing numbers of participants with increasing levels of commitment and involvement (Preece and Shneiderman, 2009). This framework also illustrates that users don’t always progress from one stage to another, and there can be movement in both directions between the different levels of participation. Similar observations have been made in this research in relation to the three projects investigated. For example, in Planet Hunters and in Foldit, contribution can wax and wane as participants may not participate for a while but become more involved when they have free time. However, the ‘reader-to-leader’ framework does not make it clear that progression along this ‘continuum’ doesn’t necessarily mean that the participant no longer engages in other types of participation (e.g. a leader may still be a reader, and a contributor or collaborator). With this in mind, the framework has been adapted for online citizen science projects, and is based more on a Venn diagram that illustrates that level of contribution sits within a wider participatory context (Figure 8.1). Each project has a large number of registered users, in effect, the potential pool of contributors. From this group of registered users, a smaller number make a limited number of contributions, or contribute for a short period of time. These individuals have been referred to transient participants (Ponciano et al., 2014) and as “dabblers” (Jennett et al., 2014) and they contribute on a more ad hoc, or casual basis, spending short amounts of time on a project task when they have the time and inclination. Individuals in this group are less likely to interact with other participants (or project scientists), or get involved in other project roles. This pattern of participation was reported by most (75%) of the Planet Hunters survey respondents, and it has been observed elsewhere on the Zooniverse (Ponciano et al., 2014, Jennet et al., 2014). 295 Figure 8.1 Adaptation of the ‘reader-to-leader’ framework for online citizen science projects From the population of registered users, a smaller number of participants will become interested in the project and participate on a regular basis and perhaps over a longer period of time. These individuals are more likely to become involved in the more social aspects of the project, and in other project roles. The active playing community in Foldit is an example of this type of contribution. This group of active participants may show this level of commitment from the beginning of their involvement in a project, or they emerge from the group of more casual participants. Conversely, active participants may reduce their level of contribution and move to the group of more transient participants. This is illustrated on Figure 8.1 as a ‘transition zone’ and illustrates that there can be some movement between these two groups of participants. Several Planet Hunters 296 participants who were interviewed, spoke of varying their involvement in the projects based on the amount of free time they had available. Out of the group of active participants, will emerge a number of core participants. These citizen scientists are more likely to interact with each other and with members of the project team. They may work together either co-operatively or collaboratively. Core participants are more likely to get involved in other project-related tasks such as moderating forums, or mentoring new participants. Core participants do not emerge from the group of transient participants or ‘dabblers’, as they require an in-depth knowledge of the project and the related tasks, something that is more likely to be acquired during active participation. One thing that should be emphasised is that while different levels of contribution may exist, as well as different ways to contribute, some individuals will be quite happy to remain as more casual participants or dabblers. Not everyone has the inclination, time, or in some cases, the skills to be a core participant. This was especially evident in the feedback from Planet Hunter participants, and many liked the fact that they could dip in to the project when they had the time. Indeed, one of the Planet Hunters scientists made the point that both types need to be catered for, the dabbler and more committed participant, and this may well be one of the more challenging aspects of working with groups of citizen science volunteers. Lightweight and heavyweight peer production Another way of considering patterns of participation in online citizen science activities is through the lens of Haythornthwaite’s (2009) model of lightweight and heavyweight peer production (see Chapter Two, Table 2.3). The results of this research suggest that a continuum of lightweight and heavyweight behaviours (rather than a dichotomous 297 classification) may be more appropriate with regard to online citizen science, as projects exhibit a range of both types of characteristics. Furthermore, different types of behaviours may be observed within one project. Figure 8.2 illustrates a continuum of lightweight and heavyweight behaviours, and identifies the location of groups of project participants. In ‘lightweight peer production’, individuals can easily contribute, and there is usually a large set of participants (the crowd) who provide minimal additions to the endeavour as a whole. The ‘rules’ of contribution are defined by authorities or owners of such projects, and participants are not expected to play a role in determining the direction or the project as a whole. Participants do not need to make long-term contributions, nor do they need to interact with others. The more casual participants of Folding@home (e.g. those who just download and run the project software), would fall into this category, as would (to a lesser extent) the ‘dabblers’ of Planet Hunters (see Figure 8.1). In heavyweight peer production, success depends upon a critical mass of contributors (the community) who make significant time investments to the project and who interact with other participants in order to sustain the community. There are learned norms of interaction and language which are indicative of community membership. Outsiders or novices can be easily identified. This applies to the Foldit community (especially the core group), the Beta Testers and overclocker communities in Folding@home, and the small number of core participants in Planet Hunters. 298 Figure 8.2 Lightweight and heavyweight scales of peer production and position of the three online citizen science studies investigated in this research Findings from all three data streams of evidence used in this research have shown that citizen scientists are not a homogenous group. Both the ‘reader-to-leader’ framework (Preece and Schneiderman, 2009) and the work of Haythornthwaite (2009) enable a further consideration and illustration of this observation. These frameworks also help to place online citizen science within the broader context of open online contribution by illustrating that the patterns of contribution observed are not unique to online citizen science. 8.5.3 Contribution of project team members Interviews with scientists and developers suggest that there are a number of ways that members of the project team can contribute to online citizen science projects, although their main contribution is their scientific and technical expertise. In these examples, project scientists set the research parameters, decide the research questions and utilise 299 the output of the projects. Online citizen science projects also require the input of software developers, games developers, and website designers. Indeed, the technological input into these projects is of great importance, and contributes greatly to the potential success of a project (Prestopnik and Crowston, 2012). Project team members must also promote and communicate about the projects in order to recruit more participants. Thus, scientists and developers can also contribute by participating in these communication efforts. From interviews with project teams, and through participant-observation, it appears that the degree to which this occurs can vary greatly. Some project scientists and developers are very active in talking to external audiences and promote their projects at academic conferences or to journalists (e.g. Foldit, Zooniverse projects). Some appear to rely more on ‘word of mouth’ (Folding@home), and their involvement in promotion is on a much smaller scale. Members of the project team can also make a contribution through their interaction and engagement with members of the community of citizen scientists. Some participants are very interested in the science behind the project and some report enjoying the interaction they have with professional scientists. As outlined in Section 8.4.2, this research has shown that the extent to which this occurs, and how it occurs, can vary from project to project. Some project team members greatly enjoy this aspect of their involvement in online citizen science, while for others, it can be more challenging. 8.6 How do participants perceive their role in the project? (RQ 6) How participants view their contribution to online citizen science projects has not been previously explored in any detail. Views regarding individual contribution, particularly with regard to their involvement in scientific research varied greatly and a wide range of opinions were expressed. However, the majority of respondents (n=356, 63%) felt that 300 they were making a contribution to science, although the importance they placed on their individual contribution varied. Some felt very strongly that their contribution was important, if not vital. This was especially evident in the interview feedback with Foldit players and Folding@home participants who are members of the Beta Testers. “It [Beta testers] contributes a lot. Without it, f@h would not be where it is today. We would not be able to use our hardware to the fullest. In recent years Pande Group with the help of beta testers brought in huge amount of fantastic innovations, which improved project immensely.”(FH4) “I am involved in scientific research. I approach it as such. If this really was just a game I would stop today and dedicate my intellect, time, money and determination to a more worthy cause.” (FD8) Approximately a quarter of folding@home and half of Planet Hunters survey respondents felt their contribution was small but significant when pooled with the efforts of other volunteers. Foldit players tended to view their contribution more positively in terms of individual impact, while Planet Hunter respondents felt that their contributions were more of a “drop in the bucket”. This illustrates that the complexity of the project task may be related to one’s perception of contribution. Those who invest more time in learning a difficult task, and carrying it out may feel more strongly that they are making a more important contribution to a project than someone who occasionally spends time on relatively straightforward classification task. Projects that have a greater diversity of tasks and give participants the opportunity to get involved in other ways may also influence the way participants feel 301 about their contribution (this may have been important for Foldit players). However, the role of task diversity requires further investigation. How the members of the project team view or value the involvement of citizen scientists can influence how the volunteers feel about their contribution. For example, approximately a quarter (n=102) of Folding@home participants do not feel as though they are involved in scientific research, and 57 of this group felt they were just donating their resources (even though there were some overclockers and hardware enthusiasts in this sub-group) and likened their involvement to making a financial contribution. This view of donation rather than contribution could have been reinforced or promulgated by the fact that the project team refer to the Folding@home participants as ‘donors’. This is contrast to Planet Hunters where participants are referred to as collaborators. The fact that Folding@home participants are not acknowledged in publications based on the output of the project could also be of importance regarding this issue. However, only 20 survey respondents (5%) mentioned that they would like to see greater acknowledgement of participants on resulting publications. How an individual views science and what they think scientific research actually entails may influence views about their own participation in a project. One of the Planet Hunters scientists referred to “Hollywood science” implying that some of the project participants may not have an accurate grasp of what actually constitutes scientific research in the ‘real world’. While giving citizen scientists an experience of authentic scientific research has been cited as one of the benefits of their involvement in citizen science projects (Bonney et al., 2009, Cronje et al., 2011, Riesch et al., 2013a), this issue may require additional consideration. Many of the participants in this research have formal qualifications in STEM subjects which may increase their level of awareness of the scientific research 302 process. However, unless an individual is a professional scientist (and very few respondents in this study are), they may, over time, become less familiar with the processes involved in carrying out scientific research. From these results, the complexity of the task and the views of the scientific team appear to be more important. However, further research that explores the views of citizen scientists about scientific research would be of interest and help to address this question in greater depth. Project team members view their contribution in terms of the responsibilities it entails, and how well they achieve their goals. All of those interviewed stated that their main role was to ensure that valid and high quality scientific research was being carried out. The Planet Hunters scientists were keen to stress that they should not be wasting anybody’s time, and that they had an obligation to produce something meaningful from the efforts of volunteers. The developers of Foldit also spoke about their efforts to produce an entertaining game that would appeal to current and potential participants. The ‘quality’ of their efforts was linked to the number of scientific publications produced. Such comments not only reflect their views concerning their contribution to a project, but are also a reflection of the primary motivation for their involvement, which is to carry out and produce valid and useful scientific research. 8.7 Conclusions Overall, it appears that the three projects appeal to self-selecting groups of people. Respondents tend to be male, well educated (often in STEM subjects) and have an interest in other science-based activities. Why these projects may appeal more to men or to ‘confident engagers’ of science has been considered. Aspects of project design, subject content, and where the projects are promoted may influence who they ultimately attract as participants (Cooper et al., 2010, Jennett et al., 2014). Other socio-cultural factors, 303 such as demographic characteristics and socio-economic background, may also influence who is attracted to these projects (Dawson and Jensen, 2011). In all three projects, making a contribution to science and a background interest in science or computing were the main reasons that the citizen scientists initially joined. Factors sustaining participation showed greater variation between the projects and appeared to be related to the project goals and the level of difficulty of the project task. The motivations of citizen scientists are dynamic and can change over time (Batson et al., 2002, Rotman et al., 2012). Scientists primarily become involved in online citizen science projects in order to accomplish research involving the analysis of large datasets. Some also see these projects as an effective way to engage ‘the public’ and become more involved in communication activities. However, working with large communities of volunteers can be problematic for members of the project team, especially when changes are made to the project. Issues relating to governance and communication with large groups of volunteers have been highlighted, and demonstrate that communities of volunteers need to be supported, and may want to be consulted when changes are made to a project. Each project offers a number of opportunities for online interaction between participants. The presence of internet relay chat, and forums with a quick ‘turnaround’ of comments facilitate greater interaction between citizen scientists, and may help to sustain participation in projects where the task is complex (e.g. Foldit). A variety of roles were available to citizen scientists in each project, although a greater diversity of tasks was identified in Foldit. All three projects have groups of highly committed participants, or core participants, who do much of the ‘work’. They interact more with each other online than less-committed participants, and they also interact more with members of the 304 project team. This pattern of participation has been observed in several other online citizen science projects (Ponciano et al., 2014, Jennett et al., 2014). The groups of core participants can be considered as the ‘leaders’ of these project communities, as defined by Preece and Schneiderman (2009). However, this asymmetric pattern of participation means that scientists in all three of the projects have to cater for varying levels of interest and involvement. Most of the respondents felt they were making some kind of contribution to science, although views about personal contribution varied both between and within projects. Differences in the view of one’s contribution may be related to the complexity of the task, diversity of project tasks and the views of the project team regarding the work of citizen scientist volunteers. This analysis has attempted to draw together the main themes that have emerged from the three streams of data, and to identify explanations and frameworks which help to elucidate the findings. In addressing the research questions I have compared the three projects and identified points of similarity and divergence. Despite very different project tasks there are some important common features that relate to motivation to participate, and to the fact that there are different ways that citizen scientists can contribute. One of the most significant findings of this research has been that relatively small groups of citizen scientists are needed to make these projects viable. Despite attracting interest from many thousands of potential participants, only a fraction of these individuals will actively contribute. The commitment of these small groups of active and core participants makes these projects successful, and enable the realisation of the research goals. 305 Chapter 9: Conclusion A review of literature from a range of academic fields has highlighted a number of gaps in our understanding of online citizen science projects (Holohan and Garg, 2005, Raddick et al., 2010, 2013, Krebs, 2010, Nov et al, 2011a, 2011b, World Community Grid, 2013). This research has attempted to address these gaps by identifying and elucidating six research questions. A mixed-methods case study approach has been employed and a comparative analysis of three selected projects has been undertaken (Thomas, 2011a). Patterns of online interaction, contribution, and motivation to participate have been investigated in relation to both citizen scientist volunteers, and the scientists and developers who set up and manage the projects. Several theoretical models have been employed to examine motivation to participate in the selected projects that were originally derived from studies on general community volunteering (Batson et al., 2002), formal education (Ryan and Deci, 2000, 2009) and participation in open source software (Hars and Shaosong, 2002). Two other theoretical models, the ‘reader-to-leader’ framework (Preece and Schneiderman, 2009) and the ‘lightweight-heavyweight’ model of peer production (Haythornthwaite, 2009), have been used to characterise participation and contribution. The research questions have been addressed through findings from online surveys, semi- structured interviews, and through an analysis of my experience as a project participant. The findings have been presented in relation to the research questions in Chapters Five through Eight. 306 9.1 Contributions of this work There are several ways in which this research has made contributions to knowledge. In addition to the production of new knowledge relating to online citizen science (including demographic information), the inter-relationship between several parameters of participation (namely, motivation, contribution and interaction) has been explored. The typology of online citizen science projects presented in Chapter One (see Table 1.1) has been reconsidered, and a new typology based on project task (rather than project ‘type’) has been formulated (Table 9.1). Using a mixed-methods case study approach has resulted in several methodological insights that may provide a basis for future work in this area. Finally, the findings of this research have been used to generate some considerations for scientists and developers who may be thinking about setting up an online citizen science project. 9.1.1 Contributions to knowledge New knowledge has been generated over the course of this research which ultimately enhances our understanding of online citizen science projects. These contributions to knowledge have been grouped according to which research question they help to address, and can be summarised as follows. RQ 1 Who participates in online citizen science projects? \\uf0b7 While many individuals register to participate in a project, only a small proportion become active participants and contribute on a regular or sustained basis (see Chapter Four, Sections 4.1.3, 4.2.4, and 4.3.2). What constitutes an active participant varies between projects and is related to productivity and effort. The distinction between active and inactive is not absolute; these categories are blurred. In order to fully explore and understand online citizen science projects, 307 the role of active participants must be considered. This will inevitably entail identifying who these individuals are. The implication of this finding is that the organisers of online citizen science projects will need to attract a large pool of registered participants, in order to yield a more committed group of active participants. \\uf0b7 This research has added to the small body of demographic information about participants in online citizen science projects (see Chapter 5, Section 5.1.1; Chapter Six, Section 6.1.1; and Chapter Seven, Section 7.1.1). As in previous studies, I have found an over-representation of male participants (Holohan and Garg, 2005, Raddick et al., 2010, 2013, Krebs, 2010, Estrada et al., 2013, World Community Grid, 2013). Survey feedback has also highlighted that these groups of active participants are well-educated (often with a formal STEM qualification), have an interest in science more generally, and are confident users of ICT (see Section 8.1, Chapter Eight). These findings have implications with regard to widening participation in these projects, and those involved in setting up and managing them may wish to consider if they wish to attract a greater diversity of participants. RQ 2 What motivates and sustains participation in online citizen science projects? \\uf0b7 The findings indicate that, typically, citizen scientists begin participating in a project in order to make a contribution, and because they are interested in the science and goals of the project (see Chapter Eight, Table 8.2). Motivation to participate is dynamic and motivations that sustain participation can be different from those that initiate it. Participants can become de-motivated if they feel their contributions are not valued or if major changes are made to a project without 308 any consultation. Thus, decisions made by project teams relating to communication and governance, play an important role in sustaining motivation. \\uf0b7 Project scientists and developers are motivated to take part in online citizen science projects in order to get help with research tasks, or to explore the potential of computer technology (such as games) in solving scientific research problems. Projects enable scientists and developers to interact and engage with participants, and provide non-specialists with an authentic experience of scientific research. However, the degree to which these opportunities for engagement are taken up varies between the projects, and between different individuals within project teams. RQ 3 Do motivations vary between different types of online citizen science projects and their associated tasks? \\uf0b7 A desire to make a contribution to scientific research was the key motivator that initiated participation across the three projects. This supports the findings of previous research (Holohan and Garg, 2005, Raddick et al., 2010, 2013, Krebs, 2010). This research builds on previous work by demonstrating that motivations that sustain participation show a greater variation between projects, and are related to aspects of the project task, such as level of difficulty, the presence of competition, and opportunities for discovery (see Table 8.1, Chapter eight). Understanding the motivations that sustain participation, and the de-motivations that have the opposite effect, have practical applications for those wishing to sustain a project over time. 309 RQ 4 How do project participants interact online? \\uf0b7 The extent to which participants interact is influenced by the complexity of the project task. Difficult project tasks necessitate greater interaction between participants as they help one another learn about the project task, or work together to complete the task (see Section 8.4.1, Chapter Eight). The presence of technical features such as internet relay chat, facilitates synchronous communication between participants, and can help to create a dynamic online environment. Asynchronous forums enable communication between participants who are geographically and temporally dispersed. However, both of these vehicles for communication entail technical and infrastructure requirements for both the participants, and the project organisers. These communication channels also require moderation. The presence of a competitive element can also increase the level of interaction between participants as they assemble and co-ordinate themselves into teams. \\uf0b7 Regular communication with members of the project team is important for some citizen scientists, and there are a number of approaches that can be taken. In some cases, there may be a designated individual (e.g. a community support manager) who acts as the interface between the project team and community of participants. Interacting with communities of citizen scientist volunteers can be difficult at times, and some underestimate this aspect of their involvement, and the time commitment it entails. Issues relating to communication have highlighted the importance of governance in online citizen science, and illustrate that the degree to which volunteers are consulted as projects change and evolve over time, and can vary between projects (see Section 8.4.2, Chapter Eight). 310 RQ 5 How can contribution to online citizen science projects be characterised? \\uf0b7 Small groups of core participants emerge from the wider community of active participants who show a high level of commitment to a project. These individuals can co-operate and collaborate while carrying out a project task, and may ultimately create online communities of practice. These small groups of committed volunteers help to sustain these projects, often carrying out a large proportion of the project tasks, and filling a variety of roles (see Figures 8.1 and 8.2). \\uf0b7 Online citizen science projects can offer participants a number of ways to contribute in addition to the main project task (see Chapter Eight, Table 8.3). Other tasks identified through the course of this research include moderating forums, co-ordinating and managing teams, fixing and identifying software problems, instructing new participants, translating web material, and developing new approaches relating to the project task (such as writing ‘recipes in Foldit). \\uf0b7 Not all participants want to become core members, get involved in other project tasks, or even interact with other participants. It would appear that certain types of project task (such as the task in Planet Hunters) allow participants to participate more sporadically and in isolation, but still in a productive way (see Chapter Seven, Section 7.1.3). Online citizen science projects also create opportunities for these individuals to be involved in authentic scientific research. RQ 6 How do participants perceive their role in the project? \\uf0b7 There is some variation in how citizen scientists view their personal contribution to a project, and the degree to which they feel involved in scientific research. For some scientists and developers, their own contribution or degree of ‘success’ is 311 measured in terms of useful scientific output or results. There is also some variation in the way project team members view their relationship with citizen scientist volunteers and this is evident in the language they use to describe participants. For example, they have been referred to as ‘donors’ in Folding@home and as ‘collaborators’ in Planet Hunters. The language used to describe participants may be indicative of an existing ‘hierarchy’ within a project, and the extent to which there is a potential for a more meaningful and collaborative partnership to develop between scientists and citizen scientists. These findings suggest that online citizen science projects have been important in making scientific research more open for a number of distributed volunteers. These individuals have responded to the challenges presented by these projects, increasing their scientific and technical knowledge, and self-organising into various roles and teams in order to produce new knowledge (Khatib et al., 2011, Eiben et al., 2012, Schwamb et al., 2013, Wang et al., 2013). This increased openness of scientific research can be placed within the wider context of changes in ‘science and society’, as science becomes more transparent, contextualised, and participative (Gibbons, 1999, Miller, 2001, Bauer et al., 2007). Online citizen science projects have mirrored some of these changes enabling (and empowering) interested citizens to become more involved in the generation of new scientific knowledge. 312 9.1.2 The inter-relationship between contribution, motivation and interaction Through the course of this research, it has become evident that motivation to participate, interaction with other participants, and the type of contribution, are inter-related to some degree, and that these relationships are mediated by the project task. Figure 9.1 summarises these relationships, and is based upon the findings from the three data streams. Figure 9.1: Inter-relationship between contribution, motivation and interaction The nature of the project task (e.g. is it in an area of science that is of interest, or is the task considered appealing in some way) motivates participation in a project (A). The level of complexity of the project task influences the level of interaction between participants, and the greater the complexity of the project, the greater the likelihood that participants 313 will learn from other participants, or co-operate and collaborate with others (B). If the task has a competitive element, this will also facilitate interaction by encouraging the formation of teams (Foldit and Folding@home). The amount of time participants devote to a project and the diversity of roles available is influenced by the complexity of a project task (C), and projects with a complex task (such as Foldit) stimulate the development of a greater number of related tasks, and may require a greater time commitment from participants in order to learn and carry out the task (e.g. Foldit). Motivation, interaction and contribution are also inter-connected. A high level of interaction between participants, or with members of the project team can be a powerful motivator and the presence of an online community can help to sustain participation (D). Conversely, highly motivated participants may be more likely to interact with other participants, and want to work or share their experiences with others. Interaction with others may stimulate a greater level of contribution to a project (E) by creating a requirement for other project roles (e.g. moderation of forum or online chat) and by facilitating collaboration and co-operation between participants. The more a participant contributes to a project, the greater the likelihood that they will come into contact and interact with other participants, or members of the project team. Highly motivated participants may be inspired to make substantial contributions to a project, both in terms of their time commitment, and perhaps their involvement in other project roles (F). Being able to contribute to a project in number of different ways, may motivate an individual to sustain participation. If this contribution is felt to be of importance and is valued, then this too may motivate partipation. 314 The inter-connection between these aspects of participation has not been considered in previous research, and the elucidation of these relationships was made possible by a detailed examination and comparison of the three selected projects made possible by adopting a case study approach. 9.1.3 Methodological insights A mixed methods case study approach has been used to investigate three online citizen science projects. This approach allows a researcher to examine a phenomenon in depth and from a variety of angles (Gillham, 2000, Yin, 2003, Thomas, 2011a). While this approach is not ‘new’ and has been applied in other areas of study, it has not been used in previously published research in online citizen science. By utilising a case study approach, I have been able to investigate several aspects of participation in detail, including contribution, interaction, and motivation. It has also enabled an exploration of how these parameters may be inter-related both within and between projects (see Figure 9.1). The insights gained from this aspect of the research would not have been possible if only one facet of the projects was considered (e.g. motivation). Another advantage of using a case study approach has been that it has facilitated a comparison between three different projects. In each case, a standardised approach to data collection and analysis was employed, which has made the results more comparable across the projects. Online citizen science projects are well-suited to a case study approach. There are numerous data strands that can be examined in addition to the ones considered in this research such as transcripts of forum discussions, podcasts, and external press articles. This abundance of data also presented challenges, as I had to select the most relevant data strands that would help me address my research questions. 315 Most of the previous research on online citizen science projects has made use of quantitative surveys containing Likert scales (Holohan and Garg, 2005, Krebs, 2010, Nov et al., 2011, Raddick et al., 2013, World Community Grid, 2013). While this approach facilitates quantitative statistical analysis, my approach has been to utilise a greater number of open-ended questions in my online surveys. As a result, the responses that I have obtained from participants (particularly with regard to motivation) appear to have been more wide-ranging than those reported in previous research (see Chapter Eight, Table 8.2). This suggests that this approach may be able to encapsulate a greater diversity of responses and viewpoints which may not be identified in more prescriptive questionnaires (Jamieson, 2004, Pell, 2005). Using more open-ended questions on the first (Foldit) survey also helped me to identify another aspect of participation that I had not previously considered: how participants viewed their contribution to the project. This was subsequently included on the Folding@home and Planet Hunters questionnaire. Becoming a participant in all three projects has helped to provide a deeper insight into how participants interact online, who interacts, the identification of key and committed participants, and an understanding of what is actually entailed in the project task (DeWalt and DeWalt, 2002, Kawulich, 2005). It informed various stages of my research (see Chapter Three, Figures 3.3 and 3.4), and through participation in these projects, I have made contact with numerous members of the citizen scientist communities. Some of these individuals have been important in providing information about the projects including issues and developments which may have not been immediately apparent by just making observations of the project website (e.g. the controversy surrounding the changes to the BigAdv points system in Folding@home). The utility of this approach also 316 became apparent during my investigations of important sub-communities of participants such as overclockers, and the Foldit core group. Overall, using a mixed methods case study approach has permitted a close examination of the three projects, as well as a detailed comparison. Using different data streams has provided multiple sources of evidence with which to address my research questions, and has facilitated methodological triangulation. Using participant-observation as one of these data streams has provided an insight into each of these projects, and an appreciation of what participation involves, and the extent to which some individuals commit to these endeavours. I would recommend this approach to other researchers who are interested in investigating online citizen science projects, or other interactions mediated by, and through technology. 9.1.4 Revisiting the typology In Chapter One (Table 1.1) a typology of online citizen science projects was presented and used as the basis for the selection of projects as case studies. It consisted of three different ‘types’ of project: distributed computing, distributed thinking, and citizen science games. The findings of this research suggest that this typology should be re- considered and that it may be more appropriate to classify citizen science tasks rather than projects. Table 9.1 outlines three main types of task associated with online citizen science projects, a description of the task, the level of contribution or commitment required, and some key examples. 317 Table 9.1 Revised typology - online citizen science project tasks Type of project task Description of task Level of contribution Examples DISTRIBUTED COMPUTING Provision of computer processing power. ‘Passive’ participation, programme is downloaded and run. Folding@home, World Community Grid, SETI@home. DISTRIBUTED DATA ANALYSIS Classification, annotation, transcription of text, game interface. Participation can be transient (‘dabblers’) or more regular and sustained. Zooniverse projects, Stardust@home, citizen science games (EteRNA, Foldit and Phylo). DISTRIBUTED COLLABORATION Participants work together to complete the project task or other related tasks. Collaboration can involve members of the project team. Active and committed participation. Groups of core participants can create online communities of practice. Foldit core players, Planet Hunters core participants, Folding@home Beta Testers, overclocking teams in distributed computing projects. Distributed computing remains as a category of task, however, in this new typology, it refers specifically to a more ‘passive’ type of participation, where individuals simply run the project software. Distributed data analysis involves classification, annotation and transcription of data (as seen in the projects of the Zooniverse). It also includes online citizen science games as a type of distributed data analysis (albeit through a graphical games interface) rather than as a distinct category. Participation in distributed data analysis may be on a more transient basis (as demonstrated by many of the Planet Hunters survey respondents), or it can be more active and sustained. In distributed collaboration, participants can work together to address the project task, or a related project task such as improving the project software (Paulos, 2005, Cranshaw and Kittur, 2011). Those who engage in this task are usually more committed, and often includes core participants, and in some cases, members of the project team. Distributed 318 collaboration can occur in any online citizen science project, and in this research, it can be seen in the Foldit core group, the Planet Hunters core group and in the Folding@home Beta Testers. One of the main findings of this research has been that, as with other types of online projects or endeavours, there are different ways in which an individual can participate in online citizen science (Preece and Schneiderman, 2009, Haythornthwaite, 2009). Therefore, different types of task may be observed in one project. This typology takes this factor into consideration, and thus, an individual project may make more than one appearance in this typology. However, all types of project tasks are ‘distributed’ in that participants do not have to occupy the same geographical space or time zone (Holliman and Curtis, 2014). While this typology is based on research relating to three specific online citizen science projects, it can be applied to tasks associated with other current online citizen science projects (Jennett et al., 2014, Lee et al., Tinati et al., 2014), and may therefore be of utility to researchers in this area who are interested in aspects of participation and contribution. 9.1.5 Considerations for scientists and developers interested in online citizen science The number of online citizen science projects has increased significantly over the past ten years and thousands of individuals have been active participants between 2004 and 2014 (Roy et al., 2012, Gura. 2013). These projects have enabled scientists to process large amounts of data, utilise the diversity of human pattern recognition and problem solving skills, and on occasion, have resulted in contributions to knowledge (Lintott et al., 2009, Khatib et al., 2011, Lane et al., 2013, Schmitt et al., 2014). The findings of this research, 319 suggest that those interested in setting up such a project, including scientists, developers, and also experienced citizen scientists, could usefully consider some of the following. Varying levels of commitment Citizen scientist volunteers will show varying degrees of commitment to, and involvement with, a project (see Figure 4.19, Chapter Four). Some may want to participate occasionally, while others may seek greater opportunities to become involved in the research. A project interface therefore, needs to be able to cater for more transient participants (or ‘dabblers’), as well as those who become core contributors (see Chapter Four, Figure 4.19). Project task The level of difficulty of a project may affect the number of active participants (e.g. a more complex task may attract fewer participants), and may also affect the degree to which participants interact. More complex tasks may mean that new participants need help from others to learn them, or that participants may need to collaborate and co- operate in order to carry them out. Facilitating interaction Some individuals like to interact with other participants online and the presence of a project community can help to sustain participation. There are a number of ways that online interaction, both between citizen scientists, and between citizen scientists and members of the project team, can be facilitated. This can include forums, blogs, and synchronous internet relay chat. Introducing a competitive element, and enabling the formation of teams, can also promote interaction between participants. When and how members of the project team are going to communicate with participants needs to be 320 made clear. Communication with scientists can help to motivate and sustain participation, and make participants feel that their involvement is valued. Demonstrate the value of the efforts of volunteers Individuals take part in these projects and remain committed for a variety of reasons. Being able to make a contribution to science appears to be of importance, and project team members may need to demonstrate the utility and value of the efforts of citizen scientists. In addition to regular communication with citizen scientists, this can be achieved through the publication of research results and public acknowledgement of their effort (Lintott et al., 2009, 2013, Khatib et al., 2011, Schwamb et al., 2013, Lee et al., 2014). Volunteer citizen scientists value recognition for being productive, and this helps to sustain participation in a project. Investing time in the community Some projects attract many hundreds or thousands of registered participants. Interacting with these communities of volunteers can be demanding and should not be underestimated. Some participants want and expect some interaction with professional scientists as part of their involvement in the project, and scientists and developers may need to be explicit about how (and when) they interact in order to manage the expectations of the community. Supporting communities of volunteers is time- consuming, but it is important with regard to motivating and sustaining participation, and in promoting transparency. Governance Issues of governance should be considered in relation to the sustainability of the project, and in terms of the transparency of communication. Individuals who make a substantial personal investment (and financial investment in some cases) may want to become more 321 involved in decision making, or at least, be consulted when major changes are made to the project, as their involvement progresses over time. As scientists are initially involved in setting up these projects and in determining the research parameters, online citizen science projects appear to be mainly top-down in terms of their governance and organisation (Mueller, 2012). However, opportunities may arise for greater collaboration with citizen scientists, and this may help to motivate and sustain participation. 9.2 Limitations of this study While efforts were made to ensure that appropriate methods and methodological approaches were utilised, there are some limitations of this research. While the use of a case study approach has enabled a detailed consideration of the projects in question, a selection of three completely different projects may well have provided a very different picture regarding motivation, contribution and interaction. Different strands of evidence were considered in this research, and the methods adopted for each are subject to certain limitations (see Chapter Three, section 3.3.2). In total, three strands of evidence were considered: online surveys, semi-structured interviews, and participant observation. Alternative strands of evidence could have been considered in addition to, or instead of, the ones selected, e.g. project-related web content such as project wikis, FAQs, podcasts, transcripts of online ‘chats’ between and discussions (see Section 9.1.3). These may have contributed to a greater appreciation of online interaction and some of the alternative tasks that a small number of participants become involved in. However, the final selection of the three sources of data was driven by the focus of the research questions, as well as time constraints which necessitated the prioritisation of the analytical approach. 322 For each project, only a small number of participants took part in the surveys and interviews. It is unlikely that these groups of respondents are representative of the total population of registered participants. While this research has ultimately focussed upon the population of active participants, it is unable to provide information about the characteristics and behaviour of participants who are less active, or inactive. There is also the possibility that my sample of respondents was not representative of the wider population of active participants. This may be particularly true of Folding@home, where the sample was a much smaller percentage of the estimated population of active participants (which in itself, was difficult to estimate). The small number of survey respondents has meant that little quantitative analysis of the data was possible. Small sample sizes have made it very difficult to obtain any statistically significant results (see Chapter Three, Section 3.4.1). 9.3 Future work While this research has made contributions to knowledge regarding certain aspects of contribution and participation in online citizen science projects, there is substantial scope for further work in this area. This particular study has provided a snapshot into three different projects. It would be of interest to explore how they develop over time, how numbers of participants vary, and if a project has a natural lifespan. What factors contribute to the longevity (or otherwise) of an online citizen science project? This would help to contribute to discussions relating to the sustainability (and concomitantly, funding) of projects. Interviews with some Planet Hunters participants also suggest that personal involvement in projects waxes and wanes, and that after time, the more committed participants can desert a project and move on to something new. Of the three projects I have 323 investigated, Foldit would perhaps be the most likely candidate for a further exploration of this issue, as it is relatively easy to track the contribution and activity of individual players through the leader boards, player profile pages and participation in online chats with the project team. The contributions and experiences of a small number of project participants could be studied in detail over time. Given that this research has focussed on active participants, a further investigation into less active or inactive participants would be of interest. For example, why do those who show some initial interest in a project, then decide not to make any contribution? What factors play a part when an individual is making the decision to switch between projects, or withdraw? This may have implications for the design of a project (Newman et al., 2012). Interview and survey feedback from citizen scientist volunteers regarding their contribution to these projects has raised questions relating to their views about science and what constitutes scientific research. A further exploration of these views would be of interest, and may help increase understanding of motivations relating to ‘making a contribution’ and what participants’ expectations of their involvement may be. Understanding views about science could be useful to project managers and may in turn influence the design or interface of a project (e.g. how might a task be positioned within the overall research design), or how the project team interacts with participants. The presence of competition and the formation of teams have been observed to promote online interaction between participants. However, not all projects have this feature, and it would be of interest to explore whether the introduction of these features had an impact on interaction in a project, or influence how participants contribute to a project, 324 and their motivation to sustain participation. Table 9.2 lists some specific projects that could follow on from this research. Table 9.2 Future research projects Exploration of the sustainability of a project Foldit. Explore the numbers and composition of the active and core participants over time. How do the project team work to sustain numbers of participants? Is there a turnover of participants? Longitudinal study of participation and contribution Track the participation and contribution of a small number of active online citizen science participants (either from one project, or from several) over the course of 6 months – 1 year. What influences participation over time? Investigation of non-active registered participants. Foldit or Planet Hunters. Interviews with individuals who showed an initial interest in the project, but then decided not to become active contributors. Is this related to format of project, task, or absence of key motivations? What constitutes ‘scientific research’ for citizen scientists? Explore opinions and views of citizen scientists in order to ascertain how they view / define scientific research. What activities / processes do they think it encompasses? Where do they ‘fit in’? Further exploration of teams and competition in online citizen science projects. Planet Hunters. What impact does the introduction of these features have on interaction between participants, level and type of contribution, and motivation to participate? 9.4 Final reflections This thesis has examined key aspects of participation in online citizen science projects, namely, who participates, why they participate, and how they participate. It has considered the views and experiences of both the scientists and developers who set up and manage these projects, and the views and experiences of volunteers, some of whom contribute a significant amount of time and resources. Online citizen science projects offer volunteers an opportunity to become involved in authentic scientific research. Feedback from surveys and interviews with participants suggest that this opportunity is highly valued. Scientists and developers are able to carry out research with the help of 325 volunteers that may not have been possible otherwise. Significant contributions to knowledge have been made and communicated widely both in the academic literature and in the print and online press. While the numbers of active participants may be smaller than those who initially register, online citizen science projects have great potential to involve non-specialists citizens in the production of new knowledge. These volunteers must be supported, however, and future projects need to ensure communities of citizen scientists are made to feel involved and their efforts appreciated and acknowledged in ways that are meaningful to them. 370 10. Have you discussed Folding@home with your family or friends? What do they think of your participation? 11. What do you think are the benefits of online 'citizen science' projects such as Folding@home to you personally, and to society in general? 12. How would you describe your contribution to Folding@home? Do you feel as though you are involved in scientific research? 13. Have you recommended this project to others? 14. Do you have any interaction with other participants within the project, or with members of the Folding@home scientific team? If yes, how would you describe this interaction?\"]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_rdrive "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YG1oC1d_SJV"
      },
      "source": [
        "## Dataframe with text from articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqQi3jAuieyQ"
      },
      "outputs": [],
      "source": [
        "docs_acsguide = pd.DataFrame(list(zip(files_adrive, text_adrive)), columns = ['linkDrive', 'text'])\n",
        "docs_acsguide.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqew-b4HMPjH"
      },
      "source": [
        "### Pre-processing text from articles\n",
        "- Removing References\n",
        "- Removing Acknowledge paragraph\n",
        "- Extracting URL \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyFX1me1MvKl"
      },
      "source": [
        "### Removing all references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amstQ1HlCF7F"
      },
      "outputs": [],
      "source": [
        "# removing all the caracthers after the word \"References\"\n",
        "def removeReferences(doc):\n",
        "  doc = doc.split(\"References\", 1)\n",
        "  return doc[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pgdjZ0dKwDf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "7cfd4d86-5705-4f0d-c66c-374d984595fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5dedb1dbb72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# counting the ocurrences of the worrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocs_acsguide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count_ref'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"References\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_acsguide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# New text without references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocs_acsguide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no ref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdocs_acsguide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremoveReferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# checking no ocurrences of \"References\" in the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'docs_acsguide' is not defined"
          ]
        }
      ],
      "source": [
        "# counting the ocurrences of the worrd\n",
        "docs_acsguide['count_ref'] = list(map(lambda x: x.count(\"References\"), docs_acsguide['text']))\n",
        "# New text without references\n",
        "docs_acsguide['no ref']= docs_acsguide['text'].apply(removeReferences)\n",
        "# checking no ocurrences of \"References\" in the text\n",
        "docs_acsguide['count_ref1'] = list(map(lambda x: x.count(\"References\"), docs_acsguide['no ref']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc3eiGcYJFtw"
      },
      "outputs": [],
      "source": [
        "# droping unnecesary columns\n",
        "docs_acs_noref = docs_acsguide.drop(['text', 'count_ref', 'count_ref1'], axis=1)\n",
        "# rename the new column as \"text\"\n",
        "docs_acs_noref.rename(columns={'no ref': 'text'}, inplace=True)\n",
        "docs_acs_noref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnace_ZOAEiP"
      },
      "source": [
        "## Dataframe with text from reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YAdFVVFicMq"
      },
      "source": [
        "The reports came from different organizations and in different styles of writing, design and content. It was neccesary to remove pages that are not readeble by machines before get the text. Considering that the small amout of reports produce by organizations in this topic, the best approach is to do this task manually. \n",
        "\n",
        "This is the online tool used to remove pages from multiples PDF 🔜  \n",
        "https://tools.pdf24.org/en/remove-pdf-pages#s=1641833691789    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldI4rGB__a8c"
      },
      "outputs": [],
      "source": [
        "# creating the dataframe\n",
        "docs_rcsguide = pd.DataFrame(list(zip(files_rdrive, text_rdrive)), columns = ['linkDrive', 'text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-z_9GrGAVa1"
      },
      "source": [
        "## Creating the Final Dataframe for the analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA5wnwW5AbdK"
      },
      "outputs": [],
      "source": [
        "# final dataframe \n",
        "frames = [docs_acs_noref, docs_rcsguide]\n",
        "docs_csguide = pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwbMmP62wWH0",
        "outputId": "92449aeb-69c2-4d9d-cf24-029e55ec314c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 22 entries, 0 to 14\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   linkDrive  22 non-null     object\n",
            " 1   text       22 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 528.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "docs_csguide.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgjisO2MZM2A"
      },
      "source": [
        "## Creating a column with all the URLs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezCBXasUZVXf"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5qaa316gNhZ"
      },
      "outputs": [],
      "source": [
        "# extracting URLs method2\n",
        "patternLarge =r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJku0WPUE7ry"
      },
      "outputs": [],
      "source": [
        "# extracting URl method 3\n",
        "#checked in https://regex101.com/\n",
        "pat = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&\\/=]*)'\n",
        "docs_csguide['URLs'] = docs_csguide[\"text\"].str.findall(pat)\n",
        "docs_csguide['count_URLs'] = docs_csguide.text.apply(lambda x: re.findall(pat, x)).str.len()\n",
        "\n",
        "# https://stackoverflow.com/questions/62229836/how-to-extract-url-from-pandas-dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8zvzKHMojuu"
      },
      "source": [
        "## 😎 checking text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "Z8dQHg5nZMBz",
        "outputId": "42d0803c-9167-4a4d-9cd3-d46bb77a0119"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science RESEARCH PAPER A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Tina Phillips*, Norman Porticella†, Mark Constas† and Rick Bonney* Since first being introduced in the mid 1990s, the term “citizen science”—the intentional engagement of the public in scientific research—has seen phenomenal growth as measured by the number of projects developed, people involved, and articles published. In addition to contributing to scientific knowledge, many citizen science projects attempt to achieve learning outcomes among their participants, however, little guidance is available for practitioners regarding the types of learning that can be supported through citizen science or the measuring of learning outcomes. This study provides empirical data to understand how intended learning outcomes first described by the informal science education field have been employed and measured within the citizen science field. We also present a framework for describing learning outcomes that should help citizen science practitioners, researchers, and evaluators in designing projects and in studying and evaluating their impacts. This is a first step in building evaluation capacity across the field of citizen science. Keywords: learning outcomes; evaluation; informal science learning * Cornell Lab of Ornithology, US † Cornell University, US Corresponding author: Tina Phillips (tina.phillips@cornell.edu) Introduction Citizen science, defined here as public participation in scientific research, was originally conceived as a method for gathering large amounts of data across time and space (Bonney et al. 2009b). For decades or even centuries, citizen science has contributed to knowledge and understanding about far-ranging scientific topics, questions, and issues (Miller-Rushing et al. 2012). More recently, citizen science practitioners—those who conceive, develop, and implement citizen science projects—have sought not only to achieve science research outcomes but also to elicit learning and behavioral outcomes for participants ( Bonney et al. 2016; Phillips et al. 2014). Many proponents of citizen science argue that participat- ing directly in the scientific process via citizen science is an excellent way to increase science knowledge and literacy (Bonney et al. 2016; Fernandez-Gimenez et al. 2008; Jordan et al. 2011; Krasny and Bonney 2005); understand the process of science (Trautmann et al. 2012; Trumbull et al. 2000); and develop positive action on behalf of the environment (Cornwell and Campbell 2012; Cooper et al. 2007; Lewandowski and Oberhauser 2017; McKinley et al. 2016). While some projects have demonstrated achieve- ment of a few learning outcomes (see Bonney et al. 2016 for examples), most projects have yet to document robust outcomes such as increased interest in science or the environment, knowledge of science process, skills of science inquiry, or stewardship behaviors (Bela et al. 2016; Bonney et al. 2016; Jordan et al. 2012; Phillips et al. 2012). Several factors may account for the lack of demon- strated and measurable learning outcomes. First, the field of citizen science is still young. Few if any specific outcomes have been defined or described by the field, therefore, project designers may not have clear concepts of what types of learning they are attempting to foster. In addition, measuring learning requires dedicated time, resources, and expertise in conducting social science research or evaluations, which many citizen science pro- jects lack. As a result, citizen science suffers from a lack of quality project evaluations and cross-programmatic research (Phillips et al. 2012). The informal science learning community recently devel- oped guidance including tools and resources for evaluat- ing learning outcomes from participation or engagement in informal science education (ISE) activities (Friedman et al. 2008; National Research Council 2009). These tools and resources are relevant to the field of citizen science, because many citizen science projects operate in informal environments such as private residences, parks, science and nature centers, museums, community centers, after- school programs, or online. In addition, many citizen science projects are funded through ISE initiatives because the projects are expected to foster lifelong science learn- ing (Crain et al. 2014). Therefore, tools developed to meas- ure learning outcomes resulting from ISE can serve as logical starting points for evaluating outcomes of citizen science participation. Phillips, T, et al. 2018. A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science. Citizen Science: Theory and Practice, 3(2): 3, pp. 1–19, DOI: https://doi.org/10.5334/cstp.126 mailto:tina.phillips@cornell.edu https://doi.org/10.5334/cstp.126 Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 2 of 19 The objectives of the research presented in this paper were to determine and describe the types of learning outcomes that are intended by citizen science project developers, to examine the alignment of these outcomes with informal science learning frameworks and guide- lines, and to develop and present a new framework for articulating citizen science learning outcomes. We believe that the framework will help citizen science practitioners to design projects that achieve measurable learning. We also hope that the framework will facilitate cross-programmatic research to help the citizen science field show how its projects are impacting science and society. Our research further sought to determine the extent to which citizen science learning outcomes have been evaluated across the field, as a first step toward our overall goal of deepening evaluation capacity for the citi- zen science community. Citizen Science and Informal Science Learning The educational underpinnings of citizen science— particularly when involving adults—draw heavily from Informal Science Education (ISE), what Falk and Dierking (2003) refer to as “free-choice learning”—lifelong, self-directed learning that occurs outside K-16 classrooms. Two influential documents from the ISE field provided a starting point for our study. The Framework for Evaluating Impacts of Informal Science Education Projects (Friedman et al. 2008), supported by the National Science Founda- tion (NSF), was the first publication produced by the ISE field that described a “standard” set of learning outcomes (referred to as impact categories) that could be used to systematically measure project-level outcomes. (We will refer to this publication as the “ISE Framework” for the remainder of this paper.) A major goal of the framework was to facilitate cross-project and cross-technique comparisons of the impacts of ISE projects on public audiences. The five impact categories are: • Knowledge, awareness, or understanding of Science, Technology, Engineering, and Math (STEM) concepts, processes, or careers • Engagement or interest in STEM concepts or careers • Attitude toward STEM concepts, processes, or careers • Skills based on STEM concepts or processes • Behavior related to STEM concepts, processes, or careers A second document, Learning Science in Informal Environments: People, Places, and Pursuits (National Research Council 2009), focuses on characterizing the cognitive, affective, social, and developmental aspects of science learners. Termed the “LSIE strands,” these aspects of science participation include: • Interest and motivation to learn about the natural world • Application and understanding of science concepts • Acquisition of skills related to the practice of science • Reflecting on science as a way of knowing, participat- ing in, and communicating about science • Identifying oneself as someone capable of knowing, using, and contributing to science The authors of the LSIE strands noted that while the con- cepts originated in research, at the time of writing they had not yet been applied or analyzed in any systematic venue. The significant overlap between the LSIE strands and the ISE Framework’s impact categories is shown in Table 1. A third ISE document also contributed to framing this study. In 2009, an inquiry group sponsored by the Center for Advancement of Informal Science Education (CAISE) produced “Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education” (Bonney et al. 2009a), which was cre- ated as a “first step toward developing an organized methodology for comparing outcomes across a variety of Public Participation in Scientific Research (PPSR) projects” (p.20). This paper included a rubric of potential citizen science learning outcomes, based on the ISE Framework, and examined ten NSF-funded citizen science projects to assess whether they reported outcomes similar to those described in the ISE Framework. Figure 1: Participants in citizen science engage in a large number of activities such as designing studies, collecting and analyzing data, and disseminating project results. What do project designers hope that participants will learn from their participation? How are desired learning outcomes designed? How are they measured? Credit: No copyright. Pacific Southwest Region USFWS/ Flickr/Public Domain. https://www.flickr.com/photos/usfws_pacificsw/36220761671/ https://creativecommons.org/publicdomain/mark/1.0/ Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 3 of 19 One result of the CAISE report was a realization that citizen science practitioners were measuring project outcomes in varied ways, making it difficult for cross- programmatic research to study the collective impact of the field. Another result was the development of a project typology based on the level of participant involvement in the scientific process (Bonney et al. 2009a). This typology described “Contributory” projects that are researcher- driven, where participants primarily focus on data collection; “Collaborative” projects that are typically led by researchers, but may include input from participants in phases of the scientific process such as designing methods, analyzing data, and disseminating results; and “Co-created” projects that involve participants in all aspects of the scientific process, from defining a question to interpreting data to disseminating results (Figure 1). This typology allowed projects designed for different rea- sons and in different ways to be grouped to help research- ers understand common outcomes. The three documents described above served as foun- dations for articulating learning outcomes from citizen science participation, however, they lacked systematic empirical support. This study provides such support by ground truthing and applying the concepts within the ISE Framework, the LSIE strands, and the Bonney et al. (2009a) rubric to the field of citizen science. Methods and Results Our research used two sources of data—a structured review of citizen science project websites and an online survey of citizen science practitioners—to address the following three questions: 1) What are the learning outcomes that are intended or desired by citizen science practitioners, and to what extent do these outcomes align with those described by the field of informal science education? (Data Source: Website Review) 2) What is the status of evaluation of citizen science learning outcomes across the field? (Data Source: Online practitioner survey) 3) How are citizen science learning outcomes measured by different projects? (Data Source: Online practitioner survey) We also conducted a literature review to uncover definitions, descriptions, and elucidations of the learning outcomes that we identified through our research. We used the results of this review, along with our new under- standing of the outcomes desired and measured by citizen science practitioners, to develop a framework of common learning outcomes for the citizen science field. Intended Learning Outcomes To describe and understand the learning outcomes that are intended or desired by citizen science practitioners as they develop projects, we first identified individual projects by conducting a semi-structured search of the following citizen science portals: Citizen Science Central (citizenscience.org); InformalScience (informalscience. org); SciStarter (scistarter.com); Citizen Science Alliance (citizensciencealliance.org); and National Directory of Volunteer Monitoring Programs (yosemite.epa.gov/ water/volmon.nsf/). The last portal included 800 pro- jects, from which we sampled every fifth one. If a project Table 1: Comparison of NSF Framework and LSIE strands. NSF Framework Category LSIE Strands Knowledge, Awareness, Understanding: Measurable demonstration of assessment of, change in, or exercise of awareness, knowledge, understanding of a particular scientific topic, concept, phenomena, theory, or careers central to the project. Strand (2), Understanding: Come to generate, understand, remember, and use concepts, explanations, arguments, models, and facts related to science. Engagement, interest or motivation in science: Measurable demonstration of assessment of, change in, or exercise of engagement/interest in a particular scientific topic, concept, phenomena, theory, or careers central to the project. Strand (1), Interest and motivation: Experience excitement, interest and motivation to learn about phenomena in the natural and physical world. Skills related to science inquiry: Measurable demonstration of the development and/or reinforcement of skills, either entirely new ones or the reinforcement, even practice, of developing skills. Strand (3), Science Exploration: Manipulate, test, explore, predict, question, and make sense of the natural and physical world; and Strand (5): Participate in scientific activities and learning practices with others, using scientific language and tools Attitudes toward science: Measurable demonstration of assessment of, change in, or exercise of attitude toward a particular scientific topic, concept, phenomena, theory, or careers central to the project or one’s capabilities relative to these areas. Attitudes refer to changes in relatively stable, more intractable constructs such as empathy for animals and their habitats, appreciation for the role of scientists in society or attitudes toward stem cell research. Related to Strand (6), Identity: Think about themselves as science learners, and develop an identity as someone who knows about, uses, and sometimes contributes to science. Also, related to Strand (4), Reflection: Reflect on science as a way of knowing; on processes, concepts, and institutions of science; and on their own process of learning about phenomena. Behavior: Measurable demonstration of assessment of, change in, or exercise of behavior related to a STEM topic. Behavioral impacts are particularly relevant to projects that are environmental in nature since action is a desired outcome. Related to Strand (5), Skills: Participate in scientific activities and learning practices with others, using scientific language and tools. http://www.citizenscience.org/ https://informalscience.org https://informalscience.org https://scistarter.com/ https://www.citizensciencealliance.org/ https://yosemite.epa.gov/water/volmon.nsf/ https://yosemite.epa.gov/water/volmon.nsf/ Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 4 of 19 was listed on multiple portals, we included it only once. In total, 327 citizen science projects met our criteria for study inclusion: Being open to participation in the U.S. or Canada, having an online presence, and being opera- tional at the time of the search (2011). The complete list of databases and search terms used to locate citizen science projects is available in the supplemental material for this paper (Appendix A). From each of the 327 project websites, we gathered the following information: Project name, URL, contact information, general goal statements, learning objectives or desired outcomes (if any), and potential indicators of learning (if any). Nine percent of project websites did not describe intended learning outcomes (e.g., some projects stated their goals to be purely scientific in nature), but the remaining 92% of projects described at least one. We coded each goal statement and learning objective into one of the major categories outlined in the ISE Framework (knowledge, engagement, skills, attitude, behavior, other) and into sub-codes outlined in the assessment rubric by Bonney et al. (2009a). Several projects described multiple learning outcomes. In these cases, each distinct outcome was coded sepa- rately. For example, the Great Lakes Worm Watch states that its goal is “increasing scientific literacy and public understanding of the role of exotic species in ecosys- tems change.” Objectives are to “provide the tools and resources for citizens to actively contribute to the devel- opment of a database documenting the distributions of exotic earthworms and their impacts across the region as well as training and resources for educators to help build understanding of the methods and results of scientific research about exotic earthworms and forest ecosystems ecology.” The text from the goal statement and learning objectives (left) were coded into the outcomes categories on the right: • Increasing scientific literacy and public understanding \\uf0e0 content knowledge • Citizens actively contribute to the development of a database \\uf0e0 data collection and monitoring, data submission • Help build understanding of the methods and results of scientific research \\uf0e0 Nature of Science knowledge Results from our coding of project goals and objectives are presented in Table 2. They reveal that the number of aspirational learning outcomes for projects ranged from zero to as many as seven, with about 40% of projects includ- ing at least two. The majority of projects (59%) focus on influencing skills related to data collection and monitoring. Intended outcomes for these projects are often stated as “Volunteers gain data collection and reporting skills.” The second most frequently stated intended learning outcome (28% of projects) was understanding of content knowledge (e.g., “volunteers learn about macroinvertebrates and stream health”). The third most-common intended out- come, increased environmental stewardship—which typi- cally includes some type of behavior change (e.g., “engage watershed residents in protecting water quality”)—was specified by about 26% of projects. Other intended learning outcomes were mentioned much less frequently, Table 2: Count of specified learning outcomes as coded from 327 citizen-science project websites. Percentages represent the proportion of projects that described the stated outcome. Several projects stated more than one outcome. Stated Outcomes on project websites Count of projects stating outcome (N = 327) Percentage of projects stating outcome Data Collection and Monitoring 193 59% Content Knowledge 90 28% Environmental Stewardship 86 26% No Education Goal Specified 29 9% Attitude/Awareness 25 8% Nature of Science 20 6% Data Analysis 14 4% Interest in the Environment 13 4% Civic Action 12 4% Submitting Data 12 4% Interest in Science 10 3% Community Health 9 3% Communication Skills 7 2% Using Technology 6 2% Science Careers 4 1% Designing Studies 2 0.5% Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 5 of 19 including increases in knowledge of the nature of science, data analysis skills, interest in the environment, civic action, data submission, communication skills, use of technology, science careers, study design, and also shifts in attitude/awareness. Considering all projects for which intended learning outcomes were stated, each of the ISE Framework impact categories was represented, suggesting a strong alignment between learning outcomes desired for citizen science participants and those for participants in the informal science learning community more generally. Status of Citizen Science Project Evaluation To uncover the status of evaluation of citizen science learn- ing outcomes across the field, we conducted an online survey of citizen science practitioners in March 2011. Delivered via Survey Monkey™, the survey contained 25 questions, including 20 closed-ended questions with predetermined options including “other.” The remaining five questions were open-ended, providing text boxes for answers. Only one question, which asked respondents to classify their project according to the three-model typol- ogy of citizen science developed by Bonney et al. (2009a), required a response. Additional questions focused on the duration of the project, the approximate number of partic- ipants, and the type of training that participants received. Respondents also were asked if any type of evaluation had ever been conducted for their project; details about evaluations that were conducted; what learning outcomes described in the ISE Framework had been measured; and what other types of outcomes had been measured. The complete set of survey questions is available in the sup- plemental material (Appendix B). Following approval by the Cornell University Institutional Review Board (#1102002014), we sent an email invitation to potential respondents describing the goal of the survey and explaining that participation was voluntary and confidential. Two reminder emails were sent approximately two and four weeks following the initial invitation. An informed consent statement was included at the start of the survey. Potential respondents were recruited via the citizenscience.org listserv (citsci-dis- cussion-l), which anyone could join, and which at the time of recruitment had approximately 1,100 members. Not all members of the listserv were project leaders, and multiple list members likely represented a single project, making it difficult to know the actual number of projects repre- sented by listserv members. After the survey was closed, we made sure that all responding projects were included in the previously described website review, to obtain as much overlap between the two datasets as possible. The survey was completed by 199 respondents repre- senting 157 unique projects (some projects had multiple entries, in which case only the first entry was included; other respondents failed to include information about their project name, which was optional). All but ten of the 157 unique projects also were represented in the project web- site data. The remaining ten projects that responded to the online survey but were not in the website review were either no longer operational, not in the US or Canada, or did not have a web presence. The majority of projects (72 or 37%) had been operating from 1–5 years, and nearly half (49%) had fewer than 100 participants. Because most questions were optional, response rates varied for different survey items. Results revealed that of the 199 respondents, 114 or 57% had undertaken some type of project evaluation. More than half of the evaluations were administered by inter- nal project staff to measure project outcomes or impacts, mostly using data collected through surveys. About one third of respondents reported conducting post-only or pre-and posttest evaluation designs. Reasons for conduct- ing project evaluations included: Gauging participant learning; identifying project strengths and weaknesses; obtaining additional funding or support; promoting a project more broadly; and providing recommendations for project improvement. In addition to asking about pro- ject learning outcomes (described in the next section), the survey also asked what other aspects of the project had been evaluated. Two thirds of participants reported measuring satisfaction or enjoyment with the project, followed by motivation to participate (53%) and evalua- tion of project outputs such as numbers of participants, web hits, journal articles, and amount of data collected (44%). Other measured outcomes included scientific/con- servation (39%); effectiveness of workshops and trainings (38%); data quality (37%); community capacity building (23%); and social policy change (3%). Another open-ended question asked respondents “Please do your best to provide the name or description of any instrument (e.g., Views on Science and Technology Survey) used to collect evaluation data, even if you developed the instrument.” Of the 72 respondents to this question, only three had used a pre-existing, validated instrument. The majority of respondents had developed their own instruments in-house or had an external evalu- ator develop original instruments. A handful of respond- ents replied with “Survey Monkey” or some other data collection platform as opposed to describing an evalua- tion instrument. Some mentioned tools such as GPS units or calipers as instruments used by the project, while others stated that they did not understand the question. When asked about their overall satisfaction with their evalua- tions, more than half of respondents expressed agreement or strong agreement that evaluations were of high quality, that evaluation findings were informative to the project developers, that recommendations from the evaluation were implemented, that the project had improved as a result of evaluation, that they learned a lot about evalu- ation, and that they felt confident they could personally conduct an evaluation in the future. Survey respondents also were asked about aspects of the evaluation process for which they would like assistance. The highest priority was help with developing goals, objectives, and indicators, followed by creating or finding appropriate survey instruments, help with analyzing or interpreting data, and help with data collection. Participants also were asked what specific resources would be most helpful for conducting evaluations. The most common replies were a database of potential surveys and data collection instruments; sample evaluation reports from citizen science; examples of evaluation designs; and an entry-level guide for conducting evaluations. https://citizenscience.org Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 6 of 19 Finally, respondents were presented with a list of eight different online organizations that support or provide resources for evaluation and were asked how often they access them. Surprisingly, the majority of respondents had never heard of any of the resources or organizations. The only exception was citizenscience.org, which was used by 46% of respondents, but rarely (as opposed to frequently, sometimes, or never). These results show a range of evalu- ation efforts and a positive attitude toward evaluation and findings among citizen science practitioners, but also a need for more knowledge of and accessibility to evalua- tion tools and resources. Measurement of Learning Outcomes Respondents who reported having conducted evaluations (114 or 57%) were asked “For the most recent evalua- tion of your project, which broad categories of learning outcomes, if any, were evaluated?” Responses to this question were based on the ISE Framework broad impact categories. Aggregated results across all projects revealed that interest or engagement in science was the most com- monly measured outcome (46%), followed by knowledge of science content (43%). Behavior change resulting from participation and attitudes toward science process, con- tent, careers, and the environment accounted for 36% and 33%, respectively, of measured learning outcomes. Science inquiry skills (e.g., asking questions, designing studies, data collection, data analysis, and using technol- ogy) were the least commonly measured outcomes across all projects (28%). In an open-ended question about other types of learning outcomes, about 10% of respondents also described measuring motivation and self-efficacy or confidence to participate in science and environmental activities. Considering differences in categories of learning out- comes measured within project types, contributory projects (for which there were 69 respondents that had conducted evaluations) reported measuring interest in science most frequently (43%) and skills of science inquiry least frequently (18%). Two-thirds of all collabo- rative projects (N = 21) measured content knowledge, followed by interest (57%), behavior change (52%), and attitudes and skills (both 43%). Only nine survey respond- ents represented co-created projects that had conducted evaluations, and of these, skills of science inquiry were measured most often. Responses combined across pro- jects and separated among project types are summarized in Figure 2. Earlier in this paper we showed that a majority of citizen science project websites described intended learn- ing outcomes very similar to those in the ISE framework, although not always using the same language. Results from the online practitioner survey added to our “ground truthing” of the ISE Framework, as respondents described attempts to measure these same outcomes, albeit to vary- ing degrees. Open-ended responses highlighted the need to emphasize efficacy as an important learning outcome in citizen science. Survey respondents also made it clear that additional resources were needed to help formulate and measure learning outcomes. A Framework for Articulating and Measuring Common Learning Outcomes for Citizen Science In addition to synthesizing and comparing empirical results from our website review and practitioner survey to describe intended and measured learning outcomes, we used key word searches to conduct a review of more than 40 peer-reviewed articles focused on defining and measuring these learning outcomes. Our data and review facilitated a re-conceptualization or contextualization of several of the impact categories presented in the ISE Framework to make them relevant to citizen science, in Figure 2: Measured learning outcomes from online survey of citizen science practitioners who reported having conducted some sort of evaluation (n = 99). https://citizenscience.org Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 7 of 19 particular environmentally based projects. For example, some outcomes uncovered in our research, such as “skills of science inquiry,” map well to the categories and their definitions in the ISE framework. Other outcomes, such as “attitude,” required clarification. Our new framework is thus based on both empirical data and contributions from the literature and includes the following learning outcomes: Interest in Science and the Environment; Self- efficacy for Science and the Environment; Motivation for Science and the Environment; Knowledge of the Nature of Science; Skills of Science Inquiry; and Behavior and Stewardship (Figure 3). This framework should help citizen science practition- ers consider some of the more commonly desired and achievable learning outcomes when designing projects. However, we emphasize that no single project should try to achieve and/or measure all, or even most, of these out- comes, as doing so can set up unreasonable expectations for both the project and its evaluation. We also note that the framework is not exhaustive. Indeed, as citizen science continues to expand, new research will inevitably reveal other learning outcomes that are important to articulate and measure. Below we describe each outcome within the framework, highlighting how each has been explained in the broad educational field and also providing examples of how each has been used in published studies of citizen science. These outcomes are not hierarchical but, beginning with interest in science and the environment, build from and reinforce each other. Interest in Science and the Environment We define interest as the degree to which an individual assigns personal relevance to a science or environmental topic or endeavor. Within ISE, Hidi and Renninger (2006) treat interest as a multi-faceted construct encompassing cognitive (thinking), affective (feeling), and behavioral (doing) domains across four phases of adoption: triggered situational interest typically stimulated by a particu- lar event and requiring support by others; maintained situational interest, which is sustained through personally meaningful activities and experiences; emerging individual interest characterized by positive feelings and self-directed pursuit of re-engaging with certain activities; and well-developed individual interest leading to enduring participation and application of knowledge. Our defini- tion of interest is compatible with Hidi and Renninger’s (2006) later phases of interest development, which are characterized by positive feelings and an increasing investment in learning more about a particular topic. Interest in science is considered a key driver to pursuing science careers in youth (Maltese and Tai 2010; Tai et al. 2006) and sustained lifelong learning and engagement in adults (Falk et al. 2007; Hidi and Renninger 2006). Over time, this type of interest can lead to sustained engage- ment and motivation and can support identity develop- ment as a science learner ( Fenichel and Schweingruber 2010; National Research Council 2009). Further, interest is noted as an important precursor to deeper engage- ment in democratic decision-making processes regarding science and technology (Mejlgaard and Stares 2010). Figure 3: Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science. Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 8 of 19 Although interest is considered to be an attitudinal struc- ture (see Bauer et al. 2000; Fenichel and Schweingruber 2010; Sturgis and Allum 2004), equating interest with atti- tudes should be avoided because attitude is a very broad construct, encompassing related but distinct sub-constructs such as efficacy, interest, curiosity, appreciation, enjoyment, beliefs, values, perseverance, motivation, engagement, and identity (Osborne et al. 2003). Interest also has been used synonymously with engagement (Friedman et al. 2008), but as McCallie et al. (2009) point out, engagement has yet to be well defined and has multiple meanings within the literature, particularly in ISE. Citizen science projects, especially those for which repeated visits or experiences are the norm, can lend themselves to deeper and sustained interest in science and the environment, yet few studies have looked at inter- est as an outcome, and those that have find mixed results. Price and Lee (2013) reported increased interest in science among Citizen Sky observers, especially among partici- pants who engaged in online social activities. Crall et al. (2012) examined general interest in science as a reason for participation in citizen science and suggested that interest was not a driving force for joining a project. Interest in specific nature-based topics, i.e., butterflies, was seen as a driver for engagement and also as a motivator for adding increasingly more complex data protocols to the French Garden Butterflies Watch project (Cosquer et al. 2012). Other research has shown that interest in use of natural resources can be a very strong determinant for future and sustained involvement in the decision-making process about management of natural resources (Danielsen et al. 2009). From these studies, it appears that examining interest in science more broadly may be less effective than measuring specific science topics. However, an audience’s pre-existing interests in specific topics may not change significantly through participation. Self-efficacy for Science and the Environment Another important outcome for studying learning is self-efficacy, i.e., a person’s beliefs about his/her capabilities to learn specific content and to perform particular behav- iors (Bandura 1997). Research has found that self-efficacy affects an individual’s choice, effort, and persistence in activities (Bandura 1982, 2000; Schunk 1991). Individu- als who feel efficacious put more effort into their activi- ties and persist at them longer than those who doubt their abilities. Self-efficacy is sometimes referred to as “perceived competence” (in Self Determination Theory) and “perceived behavioral control” (in Ajzen’s Theory of Planned Behavior, Ajzen 1991). Berkowitz et al. (2005) treat self-efficacy as an essential component in environmental citizenship (along with motivation and awareness), which is dependent on an individual’s belief that they have sufficient skills, knowl- edge, and opportunity to bring about positive change in their personal lives or community. In the context of citizen science, self-efficacy is the extent to which a learner has confidence in his or her ability to participate in a science or environmental activity. In a study involving classrooms, middle school students participating in a horseshoe crab citizen science project showed greater gains in self-efficacy than a control group (Hiller 2012). In an online astronomy project, however, researchers found a significant decrease in efficacy toward science, possibly owing to a heightened awareness of how much participants did not know about the topic (Price and Lee 2013). Crall et al. (2011) determined that self- efficacy is not only important in carrying out the principal activities of a project but also in the potential for individu- als to carry out future activities related to environmental stewardship. Working in a participatory action project with Salal harvesters, Ballard and Belsky (2010) found that the process of co-developing and implementing different experiments increased workers’ self efficacy regarding their skills in scientific research. Although efficacy was not called out directly in the ISE Framework, it can be con- sidered part of the LSIE Strand 6, “identity as a learner” (National Research Council 2009). Self-efficacy also was mentioned by project leaders in our online survey and thus appears to be an important potential outcome from citizen science participation. Motivation for Science and the Environment Motivation is a multi-faceted and complex attitudinal con- struct that describes some form of goal setting to achieve a behavior or end result. The LSIE strands (National Research Council 2009) include motivation to sustain science learning over an individual’s lifetime as an impor- tant aspect of learning in informal environments. The literature on volunteerism frames motivation as an impor- tant factor in effective recruitment, accurate placement, and volunteer satisfaction and retention (Clary and Snyder 1999, Esmond et al. 2004). Of the dozens of theories on motivation, two perspectives seem especially relevant to volunteerism and citizen science. First, the Volunteer Functions Inventory (VFI), developed by Clary et al. (1998), examines how behaviors help individuals achieve personal and social goals. Clary et al.’s (1998) categories of motivation include values (importance of helping oth- ers); understanding (activities that fulfill a desire to learn); social (influence by significant others); career (exploring job opportunities or work advancement); esteem (improv- ing personal self-esteem); and protective (escaping from negative feelings). Wright et al. (2015) studied the motiva- tions of birders in South Africa using a modified version of the VFI and found five categories of motivation to be most important: Recreation/nature; values; personal growth; social interactions; and project organization. The second perspective comes from Self-Determination Theory (SDT), which treats motivation as an explana- tory variable for meeting basic psychological needs (i.e., competency, relatedness, and autonomy) and describes different types of motivations as falling on a contin- uum from intrinsic to extrinsic (Ryan and Deci 2000a, 2000b). According to SDT, individuals are likely to con- tinue pursuing a goal to the extent that they perceive intrinsic value in the pursuit of that goal (i.e., the extent to which they experience satisfaction in performing associ- ated behaviors themselves versus performing behaviors to comply with extrinsic goals such as conforming to social pressures, fear, or receiving rewards). Although SDT can Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 9 of 19 help practitioners better understand the psychological needs behind participation, few published studies have used SDT in the context of citizen science. One excep- tion is a paper by Nov et al. (2014), which used SDT with social movement participation models in an examination of three digital citizen science projects. These researchers found that intrinsic motivation was one of four drivers that influenced quantity of participation, but that it did not affect quality of participation. In the context of citizen science, motivation can serve as both an input and outcome, i.e., to understand the basis of motivation for ISE/citizen science experiences (input) and to sustain motivation to continue participating over long time periods (outcome). However, most studies have examined reasons for participation such as the desire to contribute (see Bell et al. 2008; Hobbs and White 2012; McCaffrey 2005; Raddick et al. 2010; Reed et al. 2013), rather than motivations, which describe the psychological underpinnings of behavior (e.g., “because it makes me feel good”). In an examination of motivation in online projects, Rotman et al. (2012) described a complex and changing framework for motivation that was influenced by partici- pant interest, recognition, and attribution. Although sev- eral studies have purported to examine motivation, it has not been defined nor studied uniformly throughout the field of citizen science. Nevertheless, the major consen- sus appears to be that motivation for citizen science, like other volunteer activities, is dynamic and complex. Content, Process, and Nature of Science Knowledge Included within the ISE Framework’s impact category of “awareness, knowledge, and understanding” are several subcategories such as knowledge and understanding of science content; knowledge and understanding of science processes; and knowledge of the Nature of Science. Knowl- edge of science content refers to understanding of subject matter, i.e., facts or concepts. Knowledge of the process of science refers to understanding the methodologies that scientists use to conduct research (for example, the hypo- thetico-deductive model or “scientific method”). Knowl- edge of the Nature of Science (NOS) refers to understanding the epistemological underpinnings of scientific knowledge and how it is generated, sometimes presented from a post- positivist perspective (Lederman 1992). NOS addresses tenets of science such as tentativeness; empiricism; sub- jectivity; creativity; social/cultural influence; observations and inferences; and theories and laws (see Lederman 1992, 1999, Lederman et al. 2001, 2002). For improving scientific literacy, understanding of NOS and the process of science are generally considered more important than understand- ing basic content or subject matter (American Association for the Advancement of Sciences 1993; National Research Council 1996; NGSS 2013), and knowledge of the pro- cess of science is a regular component of well-established assessments of science knowledge (National Science Board 2014). Despite this recognition, most attempts to measure science literacy within the ISE field fall back on content knowledge, i.e., rote memorization of facts, rather than knowledge of the nature or process of science (Bauer et al. 2000; Shamos 1995). Indeed, citizen science evaluations have typically empha- sized measuring gains in topical content knowledge as opposed to science process knowledge, with mixed results (Ballard and Huntsinger 2006; Bonney 2004; Braschler et al. 2010; Brewer 2002; Devictor et al. 2010; Evans et al. 2005; Fernandez-Gimenez et al. 2008; Jordan et al. 2011; Kountoupes and Oberhauser 2008; Krasny and Bonney 2005; Phillips et al. 2006; Sickler et al. 2014; Trumbull et al. 2000; Trumbull et al. 2005). Overdevest et al. (2004) did not find a significant increase in project participant knowledge about streams and water quality, probably because new volunteers were already highly knowledgea- ble about the subject matter. Price and Lee (2013) actually found a decrease in science content knowledge among project participants, likely owing to exaggerated notions of participants’ self-perceived content knowledge before starting the project and the realization of how much they did not know after participating in the project. However, a few studies have used measures of the process of science to assess impacts of citizen science pro- ject participation. Jordan et al. (2011) and Brossard et al. (2005) used adaptations of the science and engineering indicators and showed no gains in understanding of the process of science as a result of citizen science participa- tion. In contrast, Ballard et al. (2008) used interview data to show evidence that the Salal harvesting project “… increased local people’s understanding of the scientific process and of the ecosystem on which they were a part (p. 14)”. And significant increases in understanding of the process of science before and after participation in a stream water quality-monitoring project were reported by Cronin and Messemer (2013). However, this study had a very small sample size, which may limit generalizability of the results. Likewise, few citizen science projects have attempted to study understanding of the NOS. Jordan et al. (2011) found no evidence for change in knowledge of the NOS using pre-post scenario-based questions in an invasive species project. Price and Lee (2013) found little evidence that project participation influenced epistemological beliefs about NOS, owing to the fact that “epistemological beliefs are personal beliefs and thus harder to change after participating in only one citizen science project” (p. 793). These findings suggest that while citizen science can effec- tively demonstrate gains in content knowledge, it has a long way to go before it can positively establish increases in understanding of science process and the NOS. Skills of Science Inquiry Skills of science inquiry are observable practices that can be transferred to daily life, such as asking and answering questions; collecting data; developing and using models; planning and carrying out investigations; reasoning about, analyzing, and interpreting data; constructing explanations; communicating information; and using evidence in argumentation (National Academies of Science, Engineering, and Medicine 2016; NGSS Lead States 2013). The hands-on nature of many environmentally based citizen science projects makes them particu- larly well suited to influence the development and/or Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 10 of 19 reinforcement of certain science-inquiry skills including asking questions; designing studies; collecting, analyzing, and interpreting data; and discussing and disseminating results (Bonney et al. 2009a; Jordan et al. 2012; Phillips et al. 2012; Trautmann et al. 2012). Top priorities for many practitioners are helping participants learn to follow pro- tocols and exercise accurate data collection skills, because these practices directly influence data quality. The field- wide emphasis on data quality likely comes from the large percentage of contributory, scientist-driven projects, for which a key goal is gathering data of sufficient quality to add to the existing knowledge base through publication in peer-reviewed journals. Consequently, many citizen science projects most effectively influence skills that are related to data and sample/specimen collection, iden- tification of organisms, instrument use, and sampling techniques. Many projects also engage participants in the use of various technological tools such as GPS units, digital thermometers, water conductivity instruments, rain gauges, nets, and smartphones, to name just a few (Figure 4). A few researchers have begun to study skill acquisition in citizen science. Becker et al. (2013) showed an increase in the ability to estimate noise levels with increasing participa- tion in WideNoise, a soundscape project operated through mobile devices. Increases in youths’ self-reported science inquiry skills, such as their perceived ability to identify pond organisms and to develop testable hypotheses before and after participation in Driven to Discover, also have been reported (Meyer et al. 2014). Sullivan et al. (2009) describe the use of communication prompts and strategies to “steer birders toward providing more useful data” and essentially change the birding habits of eBird participants to increase data quality. Using the theory of legitimate peripheral par- ticipation, Mugar et al. (2014) used practice proxies, a form of virtual and trace ethnography, to increase accuracy of data annotation among new members. Additionally, some projects have successfully conducted small-scale studies that compare volunteer- collected data to those collected by experts, thereby creating a baseline metric for assessing their participants’ skills (see Crall et al. 2011; Jordan et al. 2011; Schmeller et al. 2009). Another hallmark of citizen science is the collection of large, publicly available data sets and rich, interac- tive data visualizations. Many projects that provide data visualizations may seek to enhance skills related to data interpretation, i.e., the ability to effectively comprehend information and meaning, often presented in graphical form (Devictor et al. 2010). In one of the few studies exam- ining data interpretation in citizen science, Thompson and Bonney (2007) showed that even the majority of “active users” of eBird did not properly use the extensive array of data- analysis tools. Numerous studies in educational research have shown that assessing the type of reason- ing skills needed for data interpretation requires asking a series of reflective questions to determine one’s justifica- tion underlying the reasoning (e.g., Ayala et al. 2002; Roth and Roychoudhury 1993). Other inquiry skills such as study design, communica- tion, critical thinking, decision making skills, and critically evaluating results are less studied within the citizen science literature. Crall et al. (2012) used open-ended questions to determine whether engaging in an invasive species project improved the abilities of participants to explain a scientific study, write a valid research question, and provide a valid sampling design. These researchers noted positive gains in all but the ability to explain a scientific study. Char et al. (2014) found an increase from pre-post training in the ability of COASST volunteers to correctly weigh evidence to determine whether it con- tained sufficient information for accurately identifying species. These few studies show the potential for studying citizen science participants to evaluate the development of complex science inquiry skills, but such studies are in their infancy. Behavior and Stewardship Behavior change and development of environmental stewardship are among the most sought-after outcomes in science and environmental education programs, both in and out of schools (Bodzin 2008; Heimlich et al. 2008; Kollmuss and Agyeman 2002; Stern 2000; Stern et al. 2008; Vining et al. 2002). Theories examining various determinants of environmental behavior include those espousing the links between knowledge, attitude, and behavior (Hungerford and Volk 1990; Kollmuss and Agyeman 2002; Osbaldiston and Schott 2012; Schultz 2011); attitudes and values (Ajzen 1985; Fishbein and Ajzen 1975); behavior modification and intervention (De Young 1993); and nature exposure (Kaplan 1995; Kellert and Wilson 1993; Ulrich 1993; Wilson 1984). We define behavior and stewardship as measurable actions resulting from engagement in citizen science, but external to the protocol activities and the specific project- based skills of the citizen science project. For example, collecting water quality data may be a new behavior for a project participant, but if the data collection is part of the project protocol it should be measured as a new skill rather than a new behavior. However, somebody decreas- ing their water usage as a result of participating in a water quality monitoring project would be an example of behav- ior change. Our literature review identified five categories of behavior and stewardship that are of interest to the citizen science field and for which we provide definitions below: Global stewardship behaviors; place-based behav- iors; new participation; community or civic action; and transformative lifestyle changes. Global stewardship refers to deliberate changes in behavior that minimize someone’s individual ecological footprint and which collectively can have global influence (e.g., installing low-flow shower heads, recycling, purchas- ing energy-efficient appliances). Place-based behaviors refer to observable actions to directly maintain, restore, improve, or educate about the health of an ecosystem beyond the activities of a citizen science project (e.g., removing invasive species; cleaning up trash; eliminating pesticide use; purchasing locally grown food; engaging in outreach to youth groups). New participation is defined as engagement in science or environmental activities, organizations, or projects spurred on by participation in Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 11 of 19 a citizen science project. Community or civic action refers to participation in civic, governmental, or cultural affairs to solve problems at the local, regional, or national level. Actions could include donating to environmental organi- zations, signing petitions, speaking out against harmful environmental practices, or recruiting others to partici- pate in environmental causes. Finally, transformative life- style changes are efforts that require a strong up-front cost or long-term commitment to maintain, such as investing in a hybrid vehicle, becoming a vegetarian, or pledging to use mass transit whenever possible. Citizen science projects, especially those dealing with environmental topics, are typically hands-on, occur in local environments, and require repeated monitor- ing and data gathering, making them natural conduits for affecting behavior change (Wells and Lekies 2012). However, research has been limited and results have been mixed regarding the actual influence of citizen science on behavior change. For example, in a study examining two different projects, one on pollinators and one on coyotes, Toomey and Domroese (2013) show that partici- pants engage in new activities and change their gardening practices, but otherwise did not take part in advocacy or change their environmental stewardship practices. Crall et al. (2012) found significant differences between current and planned behavior as a result of participating in an invasive species project using self-reported measures, but the actual behavior change was not well described. Using a case-study approach, Oberhauser and Prysby (2008) claim that participants of the Monarch Larva Monitoring Project “work to preserve habitat at many levels, from advocating a more environmentally friendly mowing regimen and insect-friendly pest control, to challenging parking lot, building, and road development projects that threaten monarch habitat (p. 104).” However, the source of these data or accompanying methodologies are not clearly described. Cornwell and Campbell (2012) also used a case study approach and were able to document advocacy and political action by volunteers which directly benefited sea turtle conservation. Evans et al. (2005) docu- mented locally, place-based stewardship in a bird breed- ing program, while other projects showed no change in place-based stewardship practices (Jordan et al. 2011). In a study of human health effects of industrial hog opera- tions, Wing et al. (2008) describe actions being taken by community groups to engage in decision-making that addresses local environmental injustices. Taken together, these examples provide some evidence that citizen science may influence behavior and stewardship, but more robust methodologies are needed to establish causation. Plenty of anecdotal data also highlight other examples of behav- ior change that have not been published or exist only in the gray literature. Discussion Results from research conducted through a systematic review of citizen science project websites and a sur- vey of practitioners who design and implement citizen science projects confirm the relevance and applicability of three ISE documents (Friedman et al. 2008; National Research Council 2009; Bonney et al. 2009a) in framing intended learning outcomes for citizen science partici- pants. Informed by this research along with a systematic literature review, we have modified and contextualized these documents to create a new framework that contains definitions and articulations of learning outcomes for the citizen science field. We believe that the framework provides a robust starting point for setting learning goals and objectives for citizen science projects and designing projects to meet those objectives. Our research has some limitations, however. First, both the co-created and collaborative project categories represented in the online practitioner survey have small sample sizes, so generalizing the types of learning out- comes intended by these project types is challenging. Also, it’s unclear whether the distribution across project types in the online survey reflects the actual distribu- tion of contributory, collaborative, and co-created pro- jects across the U.S. and Canada, or if a disproportionate number of contributory projects received and responded to the survey request. We made no additional effort to recruit additional collaborative or co-created project respondents, thus response bias may be an issue. Also, while we made an effort to ensure that projects which responded to the practitioner survey were included in our website review, project level data from the two sources were not examined together. Doing so may have shown convergence or divergence of intended versus measured outcomes, but was beyond the scope of this work and may have violated confidentiality conditions. Finally, this work is a descriptive study based largely on self-reports in the case of the practitioner survey and published desired outcomes in the case of the website review. More robust inferential studies that can examine Figure 4: Many citizen science project designers hope not only to collect important scientific information but also to help project participants gain skills such as scientific reasoning. Here, a team of volunteers with Public Lab, a non-profit environmental science community, launch a weather balloon. Data collected via the balloon will be used in 3-D mapping surveys, but figuring out how to measure just what participants are learning as they conduct this research is a challenge for the citizen science field. Credit: Alan Kotok/Flickr/CC BY-2.0. https://www.flickr.com/photos/runneralan/14372872424/ https://creativecommons.org/licenses/by/2.0/ Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 12 of 19 field-wide relationships and causal factors between pro- ject characteristics and observed learning outcomes would be a significant next step. Despite these limitations, our findings provide insights into the ways in which learning has been articulated, studied, and measured by citizen science projects. They also provide information about the status of citizen science project evaluation in general. For example, an overwhelming majority of survey respondents expressed positive attitudes toward the importance of evaluation and the evaluation process. However, they also expressed a need for additional support and resources to conduct evaluations. Nearly all respondents reported developing their own evaluation instruments, although most pro- jects measured very similar outcomes. And the fact that very few projects were aware of resources available for guidance in conducting evaluations and locating evalu- ation instruments suggests that work needs to be done to disseminate tools and resources to the citizen science professional community. The comparison of intended learning outcomes described on citizen science project websites and the outcomes actually measured by projects highlights some interesting disconnects. For example, fewer than 5% of project websites stated “increasing interest in science and/or the environment” as an intended outcome, yet interest in science was the most commonly measured out- come (46%) across all projects in the online survey. The frequent measurement of interest in science may result from the relative ease of obtaining instruments to measure this outcome or it may be a proxy for measuring interest in the specific topic addressed by the project (e.g., birds, butterflies, astronomy, weather). Further, despite these reported measurements, few studies have published data about changes in interest, perhaps because they have not actually tried to measure it or because the typical citizen science participant (Caucasian, older, highly educated) already demonstrates a high interest in science when joining a project, making it difficult to detect changes in interest over the course of project participation (Brossard et al. 2005; Thompson and Bonney 2007). However, ample opportunity exists for citizen science projects to increase interest in science and the environment by reaching individuals who are not already engaged, espe- cially underserved audiences for whom access to informal science programming may be limited (Bonney et al. 2016; Flagg 2016). Additionally, projects that reach youth audi- ences via K-12 settings can minimize self-selection bias and carry out quasi-experimental studies to determine whether interest in science is leveraged through citizen science participation (Bonney et al. 2016). As another example of a disconnect, self-efficacy was seldom stated as an intended outcome in the website review and did not emerge as a major category of desired outcomes via the online survey. However, approximately 10% of survey respondents mentioned the concepts of “agency,” “confidence,” or “efficacy” in open-ended com- ments. As stated earlier, self-perceptions of efficacy affect choices of activities that individuals pursue, how much effort they put toward them, and how long they persist in those pursuits (e.g., Bandura et al. 1977; Weinberg et al. 1979). Enhancing perceptions of efficacy may be the single most important outcome for many citizen science projects, thus we have included efficacy in our framework. Yet another disconnect relates to motivation. Few project websites mentioned motivation as an intended learning outcome, and our online survey showed that practition- ers measured motivation primarily to understand reasons for participation. Motivations change over time, however, and sustaining project participation requires an under- standing of changing roles for individuals within a project and motivations for continued participation. More work also is needed to understand how motivations connect to Self-Determination Theory and serve psychological needs within the context of citizen science. For example, the desire to contribute to a project may be associated with a psychological need for competence, and the desire to engage socially with others may serve the psychological need for relatedness. Studies that examine where motivations fall within the intrinsic-extrinsic motivation continuum are needed to understand how motivation might influence sustained participation over time. Our results also reiterate the inclination for practition- ers to expect and measure gains in science content knowl- edge, typically through context-specific instruments that measure mastery of project activities and program content rather than increased knowledge about the process of sci- ence or the Nature of Science. Although some projects have begun to demonstrate outcomes related to “think- ing scientifically” (Braschler et al. 2010; Kountoupes and Oberhauser 2008; Trumbull et al. 2000), a gap remains in our understanding of the potential for citizen science to influence deeper understanding of the process of science and the Nature of Science as well as the more complex facets of science inquiry (i.e., critical thinking, reflection, and reasoning). Future work should focus on the develop- ment of robust and contextually appropriate tools to bet- ter capture deep reflection and rich dialogue about NOS. In perhaps our most surprising finding, nearly 60% of project websites in our study listed data collection as an intended outcome, yet across all projects combined, our online survey showed that skills related to data collection were the least-measured outcome (28%). These findings may reflect the difficulty of measuring attributes such as the acquisition of skills and the relative ease of measur- ing other constructs such as knowledge, interest, and atti- tude. This disconnect also represents a potential tension that exists within the citizen science field, particularly among contributory projects: The need for high confi- dence in data quality versus the dearth of studies that have assessed data collection skills. While several studies demonstrate that volunteers are able to collect data of similar quality to experts, these tend to be isolated exam- ples (Crall et al. 2011; Danielsen et al. 2014). Although a multitude of ways to validate citizen-science data exist (see Kosmala et al. 2016), tools and techniques are needed that can assess changes in participant data collection skills over time. Additionally, the field needs to better understand whether citizen science participation can influence other important inquiry skills such as the ability to make Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 13 of 19 decisions regarding appropriate research methodologies, to use variables and control groups properly, and to evalu- ate evidence. And as attention is increased on the poten- tial for citizen science to democratize science, further work should examine the extent to which it can support devel- opment or reinforcement of critical thinking skills that inform decision making and help to create an informed citizenry. Also, in the new world of “Big Data,” citizen sci- ence is well poised not only to provide the public with large and robust data sets but also to develop support systems so that users can understand how to effectively use these dynamic resources. Such provisioning may facilitate new lines of research to better understand how participants engage with data sets and what meaning they hold for them. Finally, in our website review, environmental steward- ship was mentioned as an intended outcome by 25% of projects—second only to data collection—suggesting a strong desire for citizen science projects to influence individual behavior change. About one-third of survey respondents reported measuring behavior change, but based on several open-ended comments, some practi- tioners equated the act of participating in a project as a change in behavior, meaning that such change was indicated for all participants. Recall, however, that we define behavior change as change that goes beyond pro- ject activities. Further, tacit assumptions may exist about engagement in specific project activities leading to more global environmental behaviors (Kollmuss and Agyeman 2002; Vining et al. 2002) (e.g., the assumption that water- quality monitoring can lead to reducing carbon emissions, recycling, and conserving energy). Intended behavioral outcomes should be directly connected to project con- tent and activities, and the knowledge of how to per- form these targeted behaviors should be made explicit to participants (Phillips et al. 2012; Toomey and Domroese 2013). While citizen science can likely impact behavior change, the development of effective implementation strategies and measurement of those outcomes are still in their infancy. Conclusion Thousands of citizen science projects exist around the word, reaching potentially millions of people, particularly in the observation and monitoring of species and habitats (Theobald et al. 2015). Such projects have the potential not only to engage individuals in the process of science, but also to encourage them to take positive action on behalf of the environment (Cooper et al. 2007; McKinley et al. 2016). If such outcomes are to be achieved, project developers need to better understand how to design pro- jects so that activities and educational learning opportuni- ties support and align with feasible and realistic outcomes (Shirk et al. 2012). This study has resulted in a framework to support citi- zen science practitioners in articulating and measuring learning outcomes for participants in their projects. The framework also should help to build capacity for practi- tioners seeking to conduct evaluations of citizen science projects by helping them to develop their program theory, i.e., to identify underlying assumptions about how project activities affect expected outcomes (Bickman 2000; Chen 2005; Funnell 2000; Funnell and Rogers 2011). In this regard, most evaluators recommend starting with articu- lation of project outcomes, then working backward to determine not only what can be achieved and how, but also what can be reasonably measured (Center for the Advancement of Informal Science Education 2011). Toward that end, work proceeding in parallel to this research is developing generic, yet customizable, evalua- tion scales that are tested as valid and reliable in citizen sci- ence contexts and which align to the framework described above (see DEVISE scales: https://cornell.qualtrics.com/ jfe/form/SV_cGxLGl1AlyAD8FL). By adopting common learning outcomes and measures, the citizen science field can further evaluation capacity and begin to conduct cross-programmatic analyses of citizen science projects to provide funders, stakeholders, and the general public with evidence-based findings about the potential for citi- zen science to impact the lives of its volunteers. Such stud- ies also could provide critical information regarding why and how to achieve outcomes and under what conditions outcomes can be maximized. Future work should support continued development of consistent measures that can be used across studies, par- ticularly those that do not rely on self-reports ( Becker-Klein et al. 2016; Phillips et al. 2012; Wells and Lekies 2012). Continued professional development opportunities for citizen science practitioners to spearhead evaluations of projects will increase capacity for such endeavors, build a steady source of knowledge about impacts, and lead to improved project design, implementation, and sus- tainability for the field as a whole. Initiation of in-depth longitudinal studies that measure persistence of change over time would add understanding of the impacts of such experiences (Schneider and Cheslock 2003). To the extent possible, more effort should be placed on studies that include experimental designs, random assignment, and control groups. Such efforts will increase the field’s ability to provide evidence for causal connections between citi- zen science participation and learning outcomes. Additionally, continued research on learning outcomes should seek to incorporate social learning theories, which may be helpful in understanding how learning happens in citizen science and the mechanisms and processes that enable active learning. Social learning theories such as Cultural Historical Activity Theory (Vygotsky and Cole 1978); Activity Theory (Engeström 1999), Experiential Learning (Dewey 1938; Kolb 1984), Situated Learning Theory (Lave and Wenger 1991), and Communities of Practice (Wenger 1998) are ideally suited for examining learning in citizen science because they emphasize the roles that participation in socially organized activities play in influencing learning (Roth and Lee 2002; National Research Council 2009). Social learning theory may be particularly useful to consider when developing project activities and experiences. Practitioners interested in incorporating social learning theories into citizen science project design, research, and evaluation should refer to the following studies for guidance: Roth and Lee (2004); Brossard et al. (2005); Ballard et al. (2008); Raddick et al. (2009); and Jackson et al. (2014). https://cornell.qualtrics.com/jfe/form/SV_cGxLGl1AlyAD8FL https://cornell.qualtrics.com/jfe/form/SV_cGxLGl1AlyAD8FL Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 14 of 19 Finally, as citizen science continues to grow, it will be important for the field to take a reflective look at its rela- tive impact, and to evaluate whether appropriate ques- tions are being asked by qualified researchers working across projects that involve diverse audiences and issues. Such an analysis will be a first step in gathering critical evidence to demonstrate the potential of citizen science to truly democratize science. Additional Files The Additional Files for this article can be found as follows: • Appendix A. Databases and Search terms used to locate citizen science websites. DOI: https://doi. org/10.5334/cstp.126.s1 • Appendix B. Questions from Online Practitioner Survey. DOI: https://doi.org/10.5334/cstp.126.s1 Acknowledgements The work described in this paper is part of a larger study called DEVISE (Developing, Validating, and Implement- ing Situated Evaluation Instruments). DEVISE is based at the Cornell Lab of Ornithology, with Rick Bonney as Principal Investigator and Tina Phillips as Project Director. We thank the project’s co-PIs, Kirsten Ellenbogen and Candie Wilderman; project consultants Edward Deci, Cecilia Garibay, Drew Gitomer, Kate Haley Goldman, Joe Heimlich, Chris Niemiec, and Gil Noam; and project advi- sory board members Heidi Ballard, Rebecca Jordan, Bruce Lewenstein, and Karen Oberhauser. We also thank the many practitioners who took the time to respond to our survey and queries. Finally, we thank many Lab of Orni- thology staff who helped along the way including Jennifer Shirk, Matthew Minarchek, Marion Ferguson, and Holly Faulkner. DEVISE is supported by the National Science Foundation under Grant No. 1010744. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessar- ily reflect the views of the National Science Foundation. Competing Interests One of the authors of this paper, Rick Bonney, is editor-in- chief of this journal. He was not involved in the process of reviewing the paper other than responding to reviewers’ comments. '"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking text\n",
        "value_text = docs_csguide.iloc[0][\"text\"]\n",
        "value_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzRStMO3z8-m"
      },
      "source": [
        "# 🤩 Here is a data frame "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbEFcvHjjosd"
      },
      "source": [
        "# 🔷 Cleaning Text Data before process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P-uDNlXwfZ-"
      },
      "source": [
        "This tool is develop to develop most of the clean task. Please, do a test before used to decided when you need to apply based on what you need. This tool remove more of the puntuation, spaces and lower case. In my case, I  have divide the cleaning process in 4 steps. \n",
        "\n",
        "1. Keep important words. There are important words in the context of Digital actions- Citizens science projects, like co-design. Also, keep the number. The reason is: After apply Hero or remove spetial characteres, the character \"-\" is removed for the word \"co-design\" and it loose the meaning inside of the corpus, because after cleaning is split in \"co\" \"design\".  To keep the meaning, I remove the character \"-\" for specific words before cleaning tasks\n",
        "\n",
        "2. Keep the meaning of words. Because the unestructure from reports, some words are divide to pass the next line, after the extraction the word loose their meaning.  eg. \"scien-\" \"ce\"\n",
        "\n",
        "3. Remove URL, emails. This bring noise to the model\n",
        "\n",
        "4. Create a variable with the above cleaning text without remove the characters of puntuation, Uppercase, and numbers. It will be useful in advance text analysis, like NER or QA\n",
        "\n",
        "5. Apply the tool texthero\n",
        "https://texthero.org/\n",
        "\n",
        "6. Create a variable with all the text cleaned\n",
        " \n",
        "5. Always run interactively during the cleaning process, methods to evaluate the cleaning process, like a chart with the most frequent words, entities and nouns, extract text and compare with previous steps. This help you to identify outliers of the data (words that don't suppose to be part of the analysis. eg: \"doi\", \".org\", \"pp\", and spetial characteres specific of your text, spetially in reports where people is too creative :/  \n",
        "\n",
        "6. Enjoy the process! it's long! 😎"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAMslt9-U3CX"
      },
      "source": [
        "###  Resources \n",
        "- https://colab.research.google.com/github/hackveda-canada/Data-Science-Essentials/blob/master/Data_Science_Essentials_Day_5_NLP_%26_Text_Mining.ipynb#scrollTo=yjP65tpyLxPW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFjoggfo-YD"
      },
      "source": [
        "## - Keeping important words and meaning and removing urls and emails "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLiJMqsKwD8l"
      },
      "outputs": [],
      "source": [
        "# creating the function for text cleaning\n",
        "def clean_meaning(text):\n",
        "    text = re.sub(r\"co-design\", \"codesign\", text)\n",
        "    text = re.sub(r'co-research', \"coresearch\", text)\n",
        "    text = re.sub(r'co-researcher', \"coresearcher\", text)\n",
        "    text = re.sub(r'co-participation', \"coparticipacion\", text)\n",
        "    text = re.sub(r'co-crear', \"cocrear\", text)\n",
        "    text = re.sub(r'co-creation', \"cocreation\", text)\n",
        "    text = re.sub(r'co-production', \"coproduction\", text)\n",
        "    text = re.sub(r\"- \", \"\", text) # remove \"- \" this appear when the word is cut to pass the next line eg. \"scien- ce\"\n",
        "    text=re.sub(r'\\S+@\\S+', '', text) # removing emails\n",
        "    text=re.sub(r'https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&\\/=]*','',text)\n",
        "    #text = re.sub(r\"\\sd\\s\", \" \", text) # removing single letters\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdv-WibvpTM3"
      },
      "source": [
        "## - Removing puntuation and lowercase. coding and using texthero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJpJcf4Jaiwl"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text=re.sub(r'[^\\sa-zA-Z0-9@\\[\\]]',' ', text) # removing characters: punctuation and other special characters\n",
        "    text=text.lower() # lowercase\n",
        "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUs5WWHDqIoD"
      },
      "source": [
        "## - Creating the variables of \"clean_meaning\" and \"pre-cleantext\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWOWMx7zwN1i"
      },
      "outputs": [],
      "source": [
        "# applying function \n",
        "# \"clean_meaning\" is text for advance analysis\n",
        "docs_csguide ['clean_meaning'] = docs_csguide['text'].apply(lambda x: clean_meaning(x))\n",
        "\n",
        "# \"pre_cleantext\" is my first apporach of clean text for topic modelling\n",
        "docs_csguide ['pre_cleantext'] = docs_csguide['clean_meaning'].apply(lambda x: clean_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsuFpwO4ORiH"
      },
      "source": [
        "## - Removing other characters using TextHero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOwYeBBwizgB",
        "outputId": "ac7c6304-327a-475e-d63b-e5250536e734"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import texthero as hero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbzfm0IKwRxI",
        "outputId": "5b54fa05-e189-4d61-ec70-584a0c46cd91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['linkDrive', 'text', 'URLs', 'count_URLs', 'clean_meaning',\n",
              "       'pre_cleantext', 'cleanText'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# \"cleanText\" is text for topic modeling \n",
        "# remove white spaces,\"\\\", puntuation\n",
        "docs_csguide['cleanText']=hero.clean(docs_csguide['pre_cleantext'])\n",
        "docs_csguide.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCYjo0_OSP3u"
      },
      "source": [
        "# 😎 Testing the cleaning in Text Cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xx9iABRaVt9"
      },
      "outputs": [],
      "source": [
        "a = docs_csguide.iloc[0][\"text\"]\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "aY8O8bEequeo",
        "outputId": "00f73cd9-e132-4f25-8f1c-9aef28cb2934"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science RESEARCH PAPER A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Tina Phillips*, Norman Porticella†, Mark Constas† and Rick Bonney* Since first being introduced in the mid 1990s, the term “citizen science”—the intentional engagement of the public in scientific research—has seen phenomenal growth as measured by the number of projects developed, people involved, and articles published. In addition to contributing to scientific knowledge, many citizen science projects attempt to achieve learning outcomes among their participants, however, little guidance is available for practitioners regarding the types of learning that can be supported through citizen science or the measuring of learning outcomes. This study provides empirical data to understand how intended learning outcomes first described by the informal science education field have been employed and measured within the citizen science field. We also present a framework for describing learning outcomes that should help citizen science practitioners, researchers, and evaluators in designing projects and in studying and evaluating their impacts. This is a first step in building evaluation capacity across the field of citizen science. Keywords: learning outcomes; evaluation; informal science learning * Cornell Lab of Ornithology, US † Cornell University, US Corresponding author: Tina Phillips  Introduction Citizen science, defined here as public participation in scientific research, was originally conceived as a method for gathering large amounts of data across time and space (Bonney et al. 2009b). For decades or even centuries, citizen science has contributed to knowledge and understanding about far-ranging scientific topics, questions, and issues (Miller-Rushing et al. 2012). More recently, citizen science practitioners—those who conceive, develop, and implement citizen science projects—have sought not only to achieve science research outcomes but also to elicit learning and behavioral outcomes for participants ( Bonney et al. 2016; Phillips et al. 2014). Many proponents of citizen science argue that participating directly in the scientific process via citizen science is an excellent way to increase science knowledge and literacy (Bonney et al. 2016; Fernandez-Gimenez et al. 2008; Jordan et al. 2011; Krasny and Bonney 2005); understand the process of science (Trautmann et al. 2012; Trumbull et al. 2000); and develop positive action on behalf of the environment (Cornwell and Campbell 2012; Cooper et al. 2007; Lewandowski and Oberhauser 2017; McKinley et al. 2016). While some projects have demonstrated achievement of a few learning outcomes (see Bonney et al. 2016 for examples), most projects have yet to document robust outcomes such as increased interest in science or the environment, knowledge of science process, skills of science inquiry, or stewardship behaviors (Bela et al. 2016; Bonney et al. 2016; Jordan et al. 2012; Phillips et al. 2012). Several factors may account for the lack of demonstrated and measurable learning outcomes. First, the field of citizen science is still young. Few if any specific outcomes have been defined or described by the field, therefore, project designers may not have clear concepts of what types of learning they are attempting to foster. In addition, measuring learning requires dedicated time, resources, and expertise in conducting social science research or evaluations, which many citizen science projects lack. As a result, citizen science suffers from a lack of quality project evaluations and cross-programmatic research (Phillips et al. 2012). The informal science learning community recently developed guidance including tools and resources for evaluating learning outcomes from participation or engagement in informal science education (ISE) activities (Friedman et al. 2008; National Research Council 2009). These tools and resources are relevant to the field of citizen science, because many citizen science projects operate in informal environments such as private residences, parks, science and nature centers, museums, community centers, afterschool programs, or online. In addition, many citizen science projects are funded through ISE initiatives because the projects are expected to foster lifelong science learning (Crain et al. 2014). Therefore, tools developed to measure learning outcomes resulting from ISE can serve as logical starting points for evaluating outcomes of citizen science participation. Phillips, T, et al. 2018. A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science. Citizen Science: Theory and Practice, 3(2): 3, pp. 1–19, DOI:    Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 2 of 19 The objectives of the research presented in this paper were to determine and describe the types of learning outcomes that are intended by citizen science project developers, to examine the alignment of these outcomes with informal science learning frameworks and guidelines, and to develop and present a new framework for articulating citizen science learning outcomes. We believe that the framework will help citizen science practitioners to design projects that achieve measurable learning. We also hope that the framework will facilitate cross-programmatic research to help the citizen science field show how its projects are impacting science and society. Our research further sought to determine the extent to which citizen science learning outcomes have been evaluated across the field, as a first step toward our overall goal of deepening evaluation capacity for the citizen science community. Citizen Science and Informal Science Learning The educational underpinnings of citizen science— particularly when involving adults—draw heavily from Informal Science Education (ISE), what Falk and Dierking (2003) refer to as “free-choice learning”—lifelong, self-directed learning that occurs outside K-16 classrooms. Two influential documents from the ISE field provided a starting point for our study. The Framework for Evaluating Impacts of Informal Science Education Projects (Friedman et al. 2008), supported by the National Science Foundation (NSF), was the first publication produced by the ISE field that described a “standard” set of learning outcomes (referred to as impact categories) that could be used to systematically measure project-level outcomes. (We will refer to this publication as the “ISE Framework” for the remainder of this paper.) A major goal of the framework was to facilitate cross-project and cross-technique comparisons of the impacts of ISE projects on public audiences. The five impact categories are: • Knowledge, awareness, or understanding of Science, Technology, Engineering, and Math (STEM) concepts, processes, or careers • Engagement or interest in STEM concepts or careers • Attitude toward STEM concepts, processes, or careers • Skills based on STEM concepts or processes • Behavior related to STEM concepts, processes, or careers A second document, Learning Science in Informal Environments: People, Places, and Pursuits (National Research Council 2009), focuses on characterizing the cognitive, affective, social, and developmental aspects of science learners. Termed the “LSIE strands,” these aspects of science participation include: • Interest and motivation to learn about the natural world • Application and understanding of science concepts • Acquisition of skills related to the practice of science • Reflecting on science as a way of knowing, participating in, and communicating about science • Identifying oneself as someone capable of knowing, using, and contributing to science The authors of the LSIE strands noted that while the concepts originated in research, at the time of writing they had not yet been applied or analyzed in any systematic venue. The significant overlap between the LSIE strands and the ISE Framework’s impact categories is shown in Table 1. A third ISE document also contributed to framing this study. In 2009, an inquiry group sponsored by the Center for Advancement of Informal Science Education (CAISE) produced “Public Participation in Scientific Research: Defining the Field and Assessing Its Potential for Informal Science Education” (Bonney et al. 2009a), which was created as a “first step toward developing an organized methodology for comparing outcomes across a variety of Public Participation in Scientific Research (PPSR) projects” (p.20). This paper included a rubric of potential citizen science learning outcomes, based on the ISE Framework, and examined ten NSF-funded citizen science projects to assess whether they reported outcomes similar to those described in the ISE Framework. Figure 1: Participants in citizen science engage in a large number of activities such as designing studies, collecting and analyzing data, and disseminating project results. What do project designers hope that participants will learn from their participation? How are desired learning outcomes designed? How are they measured? Credit: No copyright. Pacific Southwest Region USFWS/ Flickr/Public Domain.   Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 3 of 19 One result of the CAISE report was a realization that citizen science practitioners were measuring project outcomes in varied ways, making it difficult for crossprogrammatic research to study the collective impact of the field. Another result was the development of a project typology based on the level of participant involvement in the scientific process (Bonney et al. 2009a). This typology described “Contributory” projects that are researcherdriven, where participants primarily focus on data collection; “Collaborative” projects that are typically led by researchers, but may include input from participants in phases of the scientific process such as designing methods, analyzing data, and disseminating results; and “Co-created” projects that involve participants in all aspects of the scientific process, from defining a question to interpreting data to disseminating results (Figure 1). This typology allowed projects designed for different reasons and in different ways to be grouped to help researchers understand common outcomes. The three documents described above served as foundations for articulating learning outcomes from citizen science participation, however, they lacked systematic empirical support. This study provides such support by ground truthing and applying the concepts within the ISE Framework, the LSIE strands, and the Bonney et al. (2009a) rubric to the field of citizen science. Methods and Results Our research used two sources of data—a structured review of citizen science project websites and an online survey of citizen science practitioners—to address the following three questions: 1) What are the learning outcomes that are intended or desired by citizen science practitioners, and to what extent do these outcomes align with those described by the field of informal science education? (Data Source: Website Review) 2) What is the status of evaluation of citizen science learning outcomes across the field? (Data Source: Online practitioner survey) 3) How are citizen science learning outcomes measured by different projects? (Data Source: Online practitioner survey) We also conducted a literature review to uncover definitions, descriptions, and elucidations of the learning outcomes that we identified through our research. We used the results of this review, along with our new understanding of the outcomes desired and measured by citizen science practitioners, to develop a framework of common learning outcomes for the citizen science field. Intended Learning Outcomes To describe and understand the learning outcomes that are intended or desired by citizen science practitioners as they develop projects, we first identified individual projects by conducting a semi-structured search of the following citizen science portals: Citizen Science Central (citizenscience.org); InformalScience (informalscience. org); SciStarter (scistarter.com); Citizen Science Alliance (citizensciencealliance.org); and National Directory of Volunteer Monitoring Programs (yosemite.epa.gov/ water/volmon.nsf/). The last portal included 800 projects, from which we sampled every fifth one. If a project Table 1: Comparison of NSF Framework and LSIE strands. NSF Framework Category LSIE Strands Knowledge, Awareness, Understanding: Measurable demonstration of assessment of, change in, or exercise of awareness, knowledge, understanding of a particular scientific topic, concept, phenomena, theory, or careers central to the project. Strand (2), Understanding: Come to generate, understand, remember, and use concepts, explanations, arguments, models, and facts related to science. Engagement, interest or motivation in science: Measurable demonstration of assessment of, change in, or exercise of engagement/interest in a particular scientific topic, concept, phenomena, theory, or careers central to the project. Strand (1), Interest and motivation: Experience excitement, interest and motivation to learn about phenomena in the natural and physical world. Skills related to science inquiry: Measurable demonstration of the development and/or reinforcement of skills, either entirely new ones or the reinforcement, even practice, of developing skills. Strand (3), Science Exploration: Manipulate, test, explore, predict, question, and make sense of the natural and physical world; and Strand (5): Participate in scientific activities and learning practices with others, using scientific language and tools Attitudes toward science: Measurable demonstration of assessment of, change in, or exercise of attitude toward a particular scientific topic, concept, phenomena, theory, or careers central to the project or one’s capabilities relative to these areas. Attitudes refer to changes in relatively stable, more intractable constructs such as empathy for animals and their habitats, appreciation for the role of scientists in society or attitudes toward stem cell research. Related to Strand (6), Identity: Think about themselves as science learners, and develop an identity as someone who knows about, uses, and sometimes contributes to science. Also, related to Strand (4), Reflection: Reflect on science as a way of knowing; on processes, concepts, and institutions of science; and on their own process of learning about phenomena. Behavior: Measurable demonstration of assessment of, change in, or exercise of behavior related to a STEM topic. Behavioral impacts are particularly relevant to projects that are environmental in nature since action is a desired outcome. Related to Strand (5), Skills: Participate in scientific activities and learning practices with others, using scientific language and tools.        Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 4 of 19 was listed on multiple portals, we included it only once. In total, 327 citizen science projects met our criteria for study inclusion: Being open to participation in the U.S. or Canada, having an online presence, and being operational at the time of the search (2011). The complete list of databases and search terms used to locate citizen science projects is available in the supplemental material for this paper (Appendix A). From each of the 327 project websites, we gathered the following information: Project name, URL, contact information, general goal statements, learning objectives or desired outcomes (if any), and potential indicators of learning (if any). Nine percent of project websites did not describe intended learning outcomes (e.g., some projects stated their goals to be purely scientific in nature), but the remaining 92% of projects described at least one. We coded each goal statement and learning objective into one of the major categories outlined in the ISE Framework (knowledge, engagement, skills, attitude, behavior, other) and into sub-codes outlined in the assessment rubric by Bonney et al. (2009a). Several projects described multiple learning outcomes. In these cases, each distinct outcome was coded separately. For example, the Great Lakes Worm Watch states that its goal is “increasing scientific literacy and public understanding of the role of exotic species in ecosystems change.” Objectives are to “provide the tools and resources for citizens to actively contribute to the development of a database documenting the distributions of exotic earthworms and their impacts across the region as well as training and resources for educators to help build understanding of the methods and results of scientific research about exotic earthworms and forest ecosystems ecology.” The text from the goal statement and learning objectives (left) were coded into the outcomes categories on the right: • Increasing scientific literacy and public understanding \\uf0e0 content knowledge • Citizens actively contribute to the development of a database \\uf0e0 data collection and monitoring, data submission • Help build understanding of the methods and results of scientific research \\uf0e0 Nature of Science knowledge Results from our coding of project goals and objectives are presented in Table 2. They reveal that the number of aspirational learning outcomes for projects ranged from zero to as many as seven, with about 40% of projects including at least two. The majority of projects (59%) focus on influencing skills related to data collection and monitoring. Intended outcomes for these projects are often stated as “Volunteers gain data collection and reporting skills.” The second most frequently stated intended learning outcome (28% of projects) was understanding of content knowledge (e.g., “volunteers learn about macroinvertebrates and stream health”). The third most-common intended outcome, increased environmental stewardship—which typically includes some type of behavior change (e.g., “engage watershed residents in protecting water quality”)—was specified by about 26% of projects. Other intended learning outcomes were mentioned much less frequently, Table 2: Count of specified learning outcomes as coded from 327 citizen-science project websites. Percentages represent the proportion of projects that described the stated outcome. Several projects stated more than one outcome. Stated Outcomes on project websites Count of projects stating outcome (N = 327) Percentage of projects stating outcome Data Collection and Monitoring 193 59% Content Knowledge 90 28% Environmental Stewardship 86 26% No Education Goal Specified 29 9% Attitude/Awareness 25 8% Nature of Science 20 6% Data Analysis 14 4% Interest in the Environment 13 4% Civic Action 12 4% Submitting Data 12 4% Interest in Science 10 3% Community Health 9 3% Communication Skills 7 2% Using Technology 6 2% Science Careers 4 1% Designing Studies 2 0.5% Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 5 of 19 including increases in knowledge of the nature of science, data analysis skills, interest in the environment, civic action, data submission, communication skills, use of technology, science careers, study design, and also shifts in attitude/awareness. Considering all projects for which intended learning outcomes were stated, each of the ISE Framework impact categories was represented, suggesting a strong alignment between learning outcomes desired for citizen science participants and those for participants in the informal science learning community more generally. Status of Citizen Science Project Evaluation To uncover the status of evaluation of citizen science learning outcomes across the field, we conducted an online survey of citizen science practitioners in March 2011. Delivered via Survey Monkey™, the survey contained 25 questions, including 20 closed-ended questions with predetermined options including “other.” The remaining five questions were open-ended, providing text boxes for answers. Only one question, which asked respondents to classify their project according to the three-model typology of citizen science developed by Bonney et al. (2009a), required a response. Additional questions focused on the duration of the project, the approximate number of participants, and the type of training that participants received. Respondents also were asked if any type of evaluation had ever been conducted for their project; details about evaluations that were conducted; what learning outcomes described in the ISE Framework had been measured; and what other types of outcomes had been measured. The complete set of survey questions is available in the supplemental material (Appendix B). Following approval by the Cornell University Institutional Review Board (#1102002014), we sent an email invitation to potential respondents describing the goal of the survey and explaining that participation was voluntary and confidential. Two reminder emails were sent approximately two and four weeks following the initial invitation. An informed consent statement was included at the start of the survey. Potential respondents were recruited via the citizenscience.org listserv (citsci-discussion-l), which anyone could join, and which at the time of recruitment had approximately 1,100 members. Not all members of the listserv were project leaders, and multiple list members likely represented a single project, making it difficult to know the actual number of projects represented by listserv members. After the survey was closed, we made sure that all responding projects were included in the previously described website review, to obtain as much overlap between the two datasets as possible. The survey was completed by 199 respondents representing 157 unique projects (some projects had multiple entries, in which case only the first entry was included; other respondents failed to include information about their project name, which was optional). All but ten of the 157 unique projects also were represented in the project website data. The remaining ten projects that responded to the online survey but were not in the website review were either no longer operational, not in the US or Canada, or did not have a web presence. The majority of projects (72 or 37%) had been operating from 1–5 years, and nearly half (49%) had fewer than 100 participants. Because most questions were optional, response rates varied for different survey items. Results revealed that of the 199 respondents, 114 or 57% had undertaken some type of project evaluation. More than half of the evaluations were administered by internal project staff to measure project outcomes or impacts, mostly using data collected through surveys. About one third of respondents reported conducting post-only or pre-and posttest evaluation designs. Reasons for conducting project evaluations included: Gauging participant learning; identifying project strengths and weaknesses; obtaining additional funding or support; promoting a project more broadly; and providing recommendations for project improvement. In addition to asking about project learning outcomes (described in the next section), the survey also asked what other aspects of the project had been evaluated. Two thirds of participants reported measuring satisfaction or enjoyment with the project, followed by motivation to participate (53%) and evaluation of project outputs such as numbers of participants, web hits, journal articles, and amount of data collected (44%). Other measured outcomes included scientific/conservation (39%); effectiveness of workshops and trainings (38%); data quality (37%); community capacity building (23%); and social policy change (3%). Another open-ended question asked respondents “Please do your best to provide the name or description of any instrument (e.g., Views on Science and Technology Survey) used to collect evaluation data, even if you developed the instrument.” Of the 72 respondents to this question, only three had used a pre-existing, validated instrument. The majority of respondents had developed their own instruments in-house or had an external evaluator develop original instruments. A handful of respondents replied with “Survey Monkey” or some other data collection platform as opposed to describing an evaluation instrument. Some mentioned tools such as GPS units or calipers as instruments used by the project, while others stated that they did not understand the question. When asked about their overall satisfaction with their evaluations, more than half of respondents expressed agreement or strong agreement that evaluations were of high quality, that evaluation findings were informative to the project developers, that recommendations from the evaluation were implemented, that the project had improved as a result of evaluation, that they learned a lot about evaluation, and that they felt confident they could personally conduct an evaluation in the future. Survey respondents also were asked about aspects of the evaluation process for which they would like assistance. The highest priority was help with developing goals, objectives, and indicators, followed by creating or finding appropriate survey instruments, help with analyzing or interpreting data, and help with data collection. Participants also were asked what specific resources would be most helpful for conducting evaluations. The most common replies were a database of potential surveys and data collection instruments; sample evaluation reports from citizen science; examples of evaluation designs; and an entry-level guide for conducting evaluations.  Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 6 of 19 Finally, respondents were presented with a list of eight different online organizations that support or provide resources for evaluation and were asked how often they access them. Surprisingly, the majority of respondents had never heard of any of the resources or organizations. The only exception was citizenscience.org, which was used by 46% of respondents, but rarely (as opposed to frequently, sometimes, or never). These results show a range of evaluation efforts and a positive attitude toward evaluation and findings among citizen science practitioners, but also a need for more knowledge of and accessibility to evaluation tools and resources. Measurement of Learning Outcomes Respondents who reported having conducted evaluations (114 or 57%) were asked “For the most recent evaluation of your project, which broad categories of learning outcomes, if any, were evaluated?” Responses to this question were based on the ISE Framework broad impact categories. Aggregated results across all projects revealed that interest or engagement in science was the most commonly measured outcome (46%), followed by knowledge of science content (43%). Behavior change resulting from participation and attitudes toward science process, content, careers, and the environment accounted for 36% and 33%, respectively, of measured learning outcomes. Science inquiry skills (e.g., asking questions, designing studies, data collection, data analysis, and using technology) were the least commonly measured outcomes across all projects (28%). In an open-ended question about other types of learning outcomes, about 10% of respondents also described measuring motivation and self-efficacy or confidence to participate in science and environmental activities. Considering differences in categories of learning outcomes measured within project types, contributory projects (for which there were 69 respondents that had conducted evaluations) reported measuring interest in science most frequently (43%) and skills of science inquiry least frequently (18%). Two-thirds of all collaborative projects (N = 21) measured content knowledge, followed by interest (57%), behavior change (52%), and attitudes and skills (both 43%). Only nine survey respondents represented co-created projects that had conducted evaluations, and of these, skills of science inquiry were measured most often. Responses combined across projects and separated among project types are summarized in Figure 2. Earlier in this paper we showed that a majority of citizen science project websites described intended learning outcomes very similar to those in the ISE framework, although not always using the same language. Results from the online practitioner survey added to our “ground truthing” of the ISE Framework, as respondents described attempts to measure these same outcomes, albeit to varying degrees. Open-ended responses highlighted the need to emphasize efficacy as an important learning outcome in citizen science. Survey respondents also made it clear that additional resources were needed to help formulate and measure learning outcomes. A Framework for Articulating and Measuring Common Learning Outcomes for Citizen Science In addition to synthesizing and comparing empirical results from our website review and practitioner survey to describe intended and measured learning outcomes, we used key word searches to conduct a review of more than 40 peer-reviewed articles focused on defining and measuring these learning outcomes. Our data and review facilitated a re-conceptualization or contextualization of several of the impact categories presented in the ISE Framework to make them relevant to citizen science, in Figure 2: Measured learning outcomes from online survey of citizen science practitioners who reported having conducted some sort of evaluation (n = 99).  Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 7 of 19 particular environmentally based projects. For example, some outcomes uncovered in our research, such as “skills of science inquiry,” map well to the categories and their definitions in the ISE framework. Other outcomes, such as “attitude,” required clarification. Our new framework is thus based on both empirical data and contributions from the literature and includes the following learning outcomes: Interest in Science and the Environment; Selfefficacy for Science and the Environment; Motivation for Science and the Environment; Knowledge of the Nature of Science; Skills of Science Inquiry; and Behavior and Stewardship (Figure 3). This framework should help citizen science practitioners consider some of the more commonly desired and achievable learning outcomes when designing projects. However, we emphasize that no single project should try to achieve and/or measure all, or even most, of these outcomes, as doing so can set up unreasonable expectations for both the project and its evaluation. We also note that the framework is not exhaustive. Indeed, as citizen science continues to expand, new research will inevitably reveal other learning outcomes that are important to articulate and measure. Below we describe each outcome within the framework, highlighting how each has been explained in the broad educational field and also providing examples of how each has been used in published studies of citizen science. These outcomes are not hierarchical but, beginning with interest in science and the environment, build from and reinforce each other. Interest in Science and the Environment We define interest as the degree to which an individual assigns personal relevance to a science or environmental topic or endeavor. Within ISE, Hidi and Renninger (2006) treat interest as a multi-faceted construct encompassing cognitive (thinking), affective (feeling), and behavioral (doing) domains across four phases of adoption: triggered situational interest typically stimulated by a particular event and requiring support by others; maintained situational interest, which is sustained through personally meaningful activities and experiences; emerging individual interest characterized by positive feelings and self-directed pursuit of re-engaging with certain activities; and well-developed individual interest leading to enduring participation and application of knowledge. Our definition of interest is compatible with Hidi and Renninger’s (2006) later phases of interest development, which are characterized by positive feelings and an increasing investment in learning more about a particular topic. Interest in science is considered a key driver to pursuing science careers in youth (Maltese and Tai 2010; Tai et al. 2006) and sustained lifelong learning and engagement in adults (Falk et al. 2007; Hidi and Renninger 2006). Over time, this type of interest can lead to sustained engagement and motivation and can support identity development as a science learner ( Fenichel and Schweingruber 2010; National Research Council 2009). Further, interest is noted as an important precursor to deeper engagement in democratic decision-making processes regarding science and technology (Mejlgaard and Stares 2010). Figure 3: Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science. Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 8 of 19 Although interest is considered to be an attitudinal structure (see Bauer et al. 2000; Fenichel and Schweingruber 2010; Sturgis and Allum 2004), equating interest with attitudes should be avoided because attitude is a very broad construct, encompassing related but distinct sub-constructs such as efficacy, interest, curiosity, appreciation, enjoyment, beliefs, values, perseverance, motivation, engagement, and identity (Osborne et al. 2003). Interest also has been used synonymously with engagement (Friedman et al. 2008), but as McCallie et al. (2009) point out, engagement has yet to be well defined and has multiple meanings within the literature, particularly in ISE. Citizen science projects, especially those for which repeated visits or experiences are the norm, can lend themselves to deeper and sustained interest in science and the environment, yet few studies have looked at interest as an outcome, and those that have find mixed results. Price and Lee (2013) reported increased interest in science among Citizen Sky observers, especially among participants who engaged in online social activities. Crall et al. (2012) examined general interest in science as a reason for participation in citizen science and suggested that interest was not a driving force for joining a project. Interest in specific nature-based topics, i.e., butterflies, was seen as a driver for engagement and also as a motivator for adding increasingly more complex data protocols to the French Garden Butterflies Watch project (Cosquer et al. 2012). Other research has shown that interest in use of natural resources can be a very strong determinant for future and sustained involvement in the decision-making process about management of natural resources (Danielsen et al. 2009). From these studies, it appears that examining interest in science more broadly may be less effective than measuring specific science topics. However, an audience’s pre-existing interests in specific topics may not change significantly through participation. Self-efficacy for Science and the Environment Another important outcome for studying learning is self-efficacy, i.e., a person’s beliefs about his/her capabilities to learn specific content and to perform particular behaviors (Bandura 1997). Research has found that self-efficacy affects an individual’s choice, effort, and persistence in activities (Bandura 1982, 2000; Schunk 1991). Individuals who feel efficacious put more effort into their activities and persist at them longer than those who doubt their abilities. Self-efficacy is sometimes referred to as “perceived competence” (in Self Determination Theory) and “perceived behavioral control” (in Ajzen’s Theory of Planned Behavior, Ajzen 1991). Berkowitz et al. (2005) treat self-efficacy as an essential component in environmental citizenship (along with motivation and awareness), which is dependent on an individual’s belief that they have sufficient skills, knowledge, and opportunity to bring about positive change in their personal lives or community. In the context of citizen science, self-efficacy is the extent to which a learner has confidence in his or her ability to participate in a science or environmental activity. In a study involving classrooms, middle school students participating in a horseshoe crab citizen science project showed greater gains in self-efficacy than a control group (Hiller 2012). In an online astronomy project, however, researchers found a significant decrease in efficacy toward science, possibly owing to a heightened awareness of how much participants did not know about the topic (Price and Lee 2013). Crall et al. (2011) determined that selfefficacy is not only important in carrying out the principal activities of a project but also in the potential for individuals to carry out future activities related to environmental stewardship. Working in a participatory action project with Salal harvesters, Ballard and Belsky (2010) found that the process of co-developing and implementing different experiments increased workers’ self efficacy regarding their skills in scientific research. Although efficacy was not called out directly in the ISE Framework, it can be considered part of the LSIE Strand 6, “identity as a learner” (National Research Council 2009). Self-efficacy also was mentioned by project leaders in our online survey and thus appears to be an important potential outcome from citizen science participation. Motivation for Science and the Environment Motivation is a multi-faceted and complex attitudinal construct that describes some form of goal setting to achieve a behavior or end result. The LSIE strands (National Research Council 2009) include motivation to sustain science learning over an individual’s lifetime as an important aspect of learning in informal environments. The literature on volunteerism frames motivation as an important factor in effective recruitment, accurate placement, and volunteer satisfaction and retention (Clary and Snyder 1999, Esmond et al. 2004). Of the dozens of theories on motivation, two perspectives seem especially relevant to volunteerism and citizen science. First, the Volunteer Functions Inventory (VFI), developed by Clary et al. (1998), examines how behaviors help individuals achieve personal and social goals. Clary et al.’s (1998) categories of motivation include values (importance of helping others); understanding (activities that fulfill a desire to learn); social (influence by significant others); career (exploring job opportunities or work advancement); esteem (improving personal self-esteem); and protective (escaping from negative feelings). Wright et al. (2015) studied the motivations of birders in South Africa using a modified version of the VFI and found five categories of motivation to be most important: Recreation/nature; values; personal growth; social interactions; and project organization. The second perspective comes from Self-Determination Theory (SDT), which treats motivation as an explanatory variable for meeting basic psychological needs (i.e., competency, relatedness, and autonomy) and describes different types of motivations as falling on a continuum from intrinsic to extrinsic (Ryan and Deci 2000a, 2000b). According to SDT, individuals are likely to continue pursuing a goal to the extent that they perceive intrinsic value in the pursuit of that goal (i.e., the extent to which they experience satisfaction in performing associated behaviors themselves versus performing behaviors to comply with extrinsic goals such as conforming to social pressures, fear, or receiving rewards). Although SDT can Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 9 of 19 help practitioners better understand the psychological needs behind participation, few published studies have used SDT in the context of citizen science. One exception is a paper by Nov et al. (2014), which used SDT with social movement participation models in an examination of three digital citizen science projects. These researchers found that intrinsic motivation was one of four drivers that influenced quantity of participation, but that it did not affect quality of participation. In the context of citizen science, motivation can serve as both an input and outcome, i.e., to understand the basis of motivation for ISE/citizen science experiences (input) and to sustain motivation to continue participating over long time periods (outcome). However, most studies have examined reasons for participation such as the desire to contribute (see Bell et al. 2008; Hobbs and White 2012; McCaffrey 2005; Raddick et al. 2010; Reed et al. 2013), rather than motivations, which describe the psychological underpinnings of behavior (e.g., “because it makes me feel good”). In an examination of motivation in online projects, Rotman et al. (2012) described a complex and changing framework for motivation that was influenced by participant interest, recognition, and attribution. Although several studies have purported to examine motivation, it has not been defined nor studied uniformly throughout the field of citizen science. Nevertheless, the major consensus appears to be that motivation for citizen science, like other volunteer activities, is dynamic and complex. Content, Process, and Nature of Science Knowledge Included within the ISE Framework’s impact category of “awareness, knowledge, and understanding” are several subcategories such as knowledge and understanding of science content; knowledge and understanding of science processes; and knowledge of the Nature of Science. Knowledge of science content refers to understanding of subject matter, i.e., facts or concepts. Knowledge of the process of science refers to understanding the methodologies that scientists use to conduct research (for example, the hypothetico-deductive model or “scientific method”). Knowledge of the Nature of Science (NOS) refers to understanding the epistemological underpinnings of scientific knowledge and how it is generated, sometimes presented from a postpositivist perspective (Lederman 1992). NOS addresses tenets of science such as tentativeness; empiricism; subjectivity; creativity; social/cultural influence; observations and inferences; and theories and laws (see Lederman 1992, 1999, Lederman et al. 2001, 2002). For improving scientific literacy, understanding of NOS and the process of science are generally considered more important than understanding basic content or subject matter (American Association for the Advancement of Sciences 1993; National Research Council 1996; NGSS 2013), and knowledge of the process of science is a regular component of well-established assessments of science knowledge (National Science Board 2014). Despite this recognition, most attempts to measure science literacy within the ISE field fall back on content knowledge, i.e., rote memorization of facts, rather than knowledge of the nature or process of science (Bauer et al. 2000; Shamos 1995). Indeed, citizen science evaluations have typically emphasized measuring gains in topical content knowledge as opposed to science process knowledge, with mixed results (Ballard and Huntsinger 2006; Bonney 2004; Braschler et al. 2010; Brewer 2002; Devictor et al. 2010; Evans et al. 2005; Fernandez-Gimenez et al. 2008; Jordan et al. 2011; Kountoupes and Oberhauser 2008; Krasny and Bonney 2005; Phillips et al. 2006; Sickler et al. 2014; Trumbull et al. 2000; Trumbull et al. 2005). Overdevest et al. (2004) did not find a significant increase in project participant knowledge about streams and water quality, probably because new volunteers were already highly knowledgeable about the subject matter. Price and Lee (2013) actually found a decrease in science content knowledge among project participants, likely owing to exaggerated notions of participants’ self-perceived content knowledge before starting the project and the realization of how much they did not know after participating in the project. However, a few studies have used measures of the process of science to assess impacts of citizen science project participation. Jordan et al. (2011) and Brossard et al. (2005) used adaptations of the science and engineering indicators and showed no gains in understanding of the process of science as a result of citizen science participation. In contrast, Ballard et al. (2008) used interview data to show evidence that the Salal harvesting project “… increased local people’s understanding of the scientific process and of the ecosystem on which they were a part (p. 14)”. And significant increases in understanding of the process of science before and after participation in a stream water quality-monitoring project were reported by Cronin and Messemer (2013). However, this study had a very small sample size, which may limit generalizability of the results. Likewise, few citizen science projects have attempted to study understanding of the NOS. Jordan et al. (2011) found no evidence for change in knowledge of the NOS using pre-post scenario-based questions in an invasive species project. Price and Lee (2013) found little evidence that project participation influenced epistemological beliefs about NOS, owing to the fact that “epistemological beliefs are personal beliefs and thus harder to change after participating in only one citizen science project” (p. 793). These findings suggest that while citizen science can effectively demonstrate gains in content knowledge, it has a long way to go before it can positively establish increases in understanding of science process and the NOS. Skills of Science Inquiry Skills of science inquiry are observable practices that can be transferred to daily life, such as asking and answering questions; collecting data; developing and using models; planning and carrying out investigations; reasoning about, analyzing, and interpreting data; constructing explanations; communicating information; and using evidence in argumentation (National Academies of Science, Engineering, and Medicine 2016; NGSS Lead States 2013). The hands-on nature of many environmentally based citizen science projects makes them particularly well suited to influence the development and/or Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 10 of 19 reinforcement of certain science-inquiry skills including asking questions; designing studies; collecting, analyzing, and interpreting data; and discussing and disseminating results (Bonney et al. 2009a; Jordan et al. 2012; Phillips et al. 2012; Trautmann et al. 2012). Top priorities for many practitioners are helping participants learn to follow protocols and exercise accurate data collection skills, because these practices directly influence data quality. The fieldwide emphasis on data quality likely comes from the large percentage of contributory, scientist-driven projects, for which a key goal is gathering data of sufficient quality to add to the existing knowledge base through publication in peer-reviewed journals. Consequently, many citizen science projects most effectively influence skills that are related to data and sample/specimen collection, identification of organisms, instrument use, and sampling techniques. Many projects also engage participants in the use of various technological tools such as GPS units, digital thermometers, water conductivity instruments, rain gauges, nets, and smartphones, to name just a few (Figure 4). A few researchers have begun to study skill acquisition in citizen science. Becker et al. (2013) showed an increase in the ability to estimate noise levels with increasing participation in WideNoise, a soundscape project operated through mobile devices. Increases in youths’ self-reported science inquiry skills, such as their perceived ability to identify pond organisms and to develop testable hypotheses before and after participation in Driven to Discover, also have been reported (Meyer et al. 2014). Sullivan et al. (2009) describe the use of communication prompts and strategies to “steer birders toward providing more useful data” and essentially change the birding habits of eBird participants to increase data quality. Using the theory of legitimate peripheral participation, Mugar et al. (2014) used practice proxies, a form of virtual and trace ethnography, to increase accuracy of data annotation among new members. Additionally, some projects have successfully conducted small-scale studies that compare volunteercollected data to those collected by experts, thereby creating a baseline metric for assessing their participants’ skills (see Crall et al. 2011; Jordan et al. 2011; Schmeller et al. 2009). Another hallmark of citizen science is the collection of large, publicly available data sets and rich, interactive data visualizations. Many projects that provide data visualizations may seek to enhance skills related to data interpretation, i.e., the ability to effectively comprehend information and meaning, often presented in graphical form (Devictor et al. 2010). In one of the few studies examining data interpretation in citizen science, Thompson and Bonney (2007) showed that even the majority of “active users” of eBird did not properly use the extensive array of dataanalysis tools. Numerous studies in educational research have shown that assessing the type of reasoning skills needed for data interpretation requires asking a series of reflective questions to determine one’s justification underlying the reasoning (e.g., Ayala et al. 2002; Roth and Roychoudhury 1993). Other inquiry skills such as study design, communication, critical thinking, decision making skills, and critically evaluating results are less studied within the citizen science literature. Crall et al. (2012) used open-ended questions to determine whether engaging in an invasive species project improved the abilities of participants to explain a scientific study, write a valid research question, and provide a valid sampling design. These researchers noted positive gains in all but the ability to explain a scientific study. Char et al. (2014) found an increase from pre-post training in the ability of COASST volunteers to correctly weigh evidence to determine whether it contained sufficient information for accurately identifying species. These few studies show the potential for studying citizen science participants to evaluate the development of complex science inquiry skills, but such studies are in their infancy. Behavior and Stewardship Behavior change and development of environmental stewardship are among the most sought-after outcomes in science and environmental education programs, both in and out of schools (Bodzin 2008; Heimlich et al. 2008; Kollmuss and Agyeman 2002; Stern 2000; Stern et al. 2008; Vining et al. 2002). Theories examining various determinants of environmental behavior include those espousing the links between knowledge, attitude, and behavior (Hungerford and Volk 1990; Kollmuss and Agyeman 2002; Osbaldiston and Schott 2012; Schultz 2011); attitudes and values (Ajzen 1985; Fishbein and Ajzen 1975); behavior modification and intervention (De Young 1993); and nature exposure (Kaplan 1995; Kellert and Wilson 1993; Ulrich 1993; Wilson 1984). We define behavior and stewardship as measurable actions resulting from engagement in citizen science, but external to the protocol activities and the specific projectbased skills of the citizen science project. For example, collecting water quality data may be a new behavior for a project participant, but if the data collection is part of the project protocol it should be measured as a new skill rather than a new behavior. However, somebody decreasing their water usage as a result of participating in a water quality monitoring project would be an example of behavior change. Our literature review identified five categories of behavior and stewardship that are of interest to the citizen science field and for which we provide definitions below: Global stewardship behaviors; place-based behaviors; new participation; community or civic action; and transformative lifestyle changes. Global stewardship refers to deliberate changes in behavior that minimize someone’s individual ecological footprint and which collectively can have global influence (e.g., installing low-flow shower heads, recycling, purchasing energy-efficient appliances). Place-based behaviors refer to observable actions to directly maintain, restore, improve, or educate about the health of an ecosystem beyond the activities of a citizen science project (e.g., removing invasive species; cleaning up trash; eliminating pesticide use; purchasing locally grown food; engaging in outreach to youth groups). New participation is defined as engagement in science or environmental activities, organizations, or projects spurred on by participation in Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 11 of 19 a citizen science project. Community or civic action refers to participation in civic, governmental, or cultural affairs to solve problems at the local, regional, or national level. Actions could include donating to environmental organizations, signing petitions, speaking out against harmful environmental practices, or recruiting others to participate in environmental causes. Finally, transformative lifestyle changes are efforts that require a strong up-front cost or long-term commitment to maintain, such as investing in a hybrid vehicle, becoming a vegetarian, or pledging to use mass transit whenever possible. Citizen science projects, especially those dealing with environmental topics, are typically hands-on, occur in local environments, and require repeated monitoring and data gathering, making them natural conduits for affecting behavior change (Wells and Lekies 2012). However, research has been limited and results have been mixed regarding the actual influence of citizen science on behavior change. For example, in a study examining two different projects, one on pollinators and one on coyotes, Toomey and Domroese (2013) show that participants engage in new activities and change their gardening practices, but otherwise did not take part in advocacy or change their environmental stewardship practices. Crall et al. (2012) found significant differences between current and planned behavior as a result of participating in an invasive species project using self-reported measures, but the actual behavior change was not well described. Using a case-study approach, Oberhauser and Prysby (2008) claim that participants of the Monarch Larva Monitoring Project “work to preserve habitat at many levels, from advocating a more environmentally friendly mowing regimen and insect-friendly pest control, to challenging parking lot, building, and road development projects that threaten monarch habitat (p. 104).” However, the source of these data or accompanying methodologies are not clearly described. Cornwell and Campbell (2012) also used a case study approach and were able to document advocacy and political action by volunteers which directly benefited sea turtle conservation. Evans et al. (2005) documented locally, place-based stewardship in a bird breeding program, while other projects showed no change in place-based stewardship practices (Jordan et al. 2011). In a study of human health effects of industrial hog operations, Wing et al. (2008) describe actions being taken by community groups to engage in decision-making that addresses local environmental injustices. Taken together, these examples provide some evidence that citizen science may influence behavior and stewardship, but more robust methodologies are needed to establish causation. Plenty of anecdotal data also highlight other examples of behavior change that have not been published or exist only in the gray literature. Discussion Results from research conducted through a systematic review of citizen science project websites and a survey of practitioners who design and implement citizen science projects confirm the relevance and applicability of three ISE documents (Friedman et al. 2008; National Research Council 2009; Bonney et al. 2009a) in framing intended learning outcomes for citizen science participants. Informed by this research along with a systematic literature review, we have modified and contextualized these documents to create a new framework that contains definitions and articulations of learning outcomes for the citizen science field. We believe that the framework provides a robust starting point for setting learning goals and objectives for citizen science projects and designing projects to meet those objectives. Our research has some limitations, however. First, both the co-created and collaborative project categories represented in the online practitioner survey have small sample sizes, so generalizing the types of learning outcomes intended by these project types is challenging. Also, it’s unclear whether the distribution across project types in the online survey reflects the actual distribution of contributory, collaborative, and co-created projects across the U.S. and Canada, or if a disproportionate number of contributory projects received and responded to the survey request. We made no additional effort to recruit additional collaborative or co-created project respondents, thus response bias may be an issue. Also, while we made an effort to ensure that projects which responded to the practitioner survey were included in our website review, project level data from the two sources were not examined together. Doing so may have shown convergence or divergence of intended versus measured outcomes, but was beyond the scope of this work and may have violated confidentiality conditions. Finally, this work is a descriptive study based largely on self-reports in the case of the practitioner survey and published desired outcomes in the case of the website review. More robust inferential studies that can examine Figure 4: Many citizen science project designers hope not only to collect important scientific information but also to help project participants gain skills such as scientific reasoning. Here, a team of volunteers with Public Lab, a non-profit environmental science community, launch a weather balloon. Data collected via the balloon will be used in 3-D mapping surveys, but figuring out how to measure just what participants are learning as they conduct this research is a challenge for the citizen science field. Credit: Alan Kotok/Flickr/CC BY-2.0.   Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 12 of 19 field-wide relationships and causal factors between project characteristics and observed learning outcomes would be a significant next step. Despite these limitations, our findings provide insights into the ways in which learning has been articulated, studied, and measured by citizen science projects. They also provide information about the status of citizen science project evaluation in general. For example, an overwhelming majority of survey respondents expressed positive attitudes toward the importance of evaluation and the evaluation process. However, they also expressed a need for additional support and resources to conduct evaluations. Nearly all respondents reported developing their own evaluation instruments, although most projects measured very similar outcomes. And the fact that very few projects were aware of resources available for guidance in conducting evaluations and locating evaluation instruments suggests that work needs to be done to disseminate tools and resources to the citizen science professional community. The comparison of intended learning outcomes described on citizen science project websites and the outcomes actually measured by projects highlights some interesting disconnects. For example, fewer than 5% of project websites stated “increasing interest in science and/or the environment” as an intended outcome, yet interest in science was the most commonly measured outcome (46%) across all projects in the online survey. The frequent measurement of interest in science may result from the relative ease of obtaining instruments to measure this outcome or it may be a proxy for measuring interest in the specific topic addressed by the project (e.g., birds, butterflies, astronomy, weather). Further, despite these reported measurements, few studies have published data about changes in interest, perhaps because they have not actually tried to measure it or because the typical citizen science participant (Caucasian, older, highly educated) already demonstrates a high interest in science when joining a project, making it difficult to detect changes in interest over the course of project participation (Brossard et al. 2005; Thompson and Bonney 2007). However, ample opportunity exists for citizen science projects to increase interest in science and the environment by reaching individuals who are not already engaged, especially underserved audiences for whom access to informal science programming may be limited (Bonney et al. 2016; Flagg 2016). Additionally, projects that reach youth audiences via K-12 settings can minimize self-selection bias and carry out quasi-experimental studies to determine whether interest in science is leveraged through citizen science participation (Bonney et al. 2016). As another example of a disconnect, self-efficacy was seldom stated as an intended outcome in the website review and did not emerge as a major category of desired outcomes via the online survey. However, approximately 10% of survey respondents mentioned the concepts of “agency,” “confidence,” or “efficacy” in open-ended comments. As stated earlier, self-perceptions of efficacy affect choices of activities that individuals pursue, how much effort they put toward them, and how long they persist in those pursuits (e.g., Bandura et al. 1977; Weinberg et al. 1979). Enhancing perceptions of efficacy may be the single most important outcome for many citizen science projects, thus we have included efficacy in our framework. Yet another disconnect relates to motivation. Few project websites mentioned motivation as an intended learning outcome, and our online survey showed that practitioners measured motivation primarily to understand reasons for participation. Motivations change over time, however, and sustaining project participation requires an understanding of changing roles for individuals within a project and motivations for continued participation. More work also is needed to understand how motivations connect to Self-Determination Theory and serve psychological needs within the context of citizen science. For example, the desire to contribute to a project may be associated with a psychological need for competence, and the desire to engage socially with others may serve the psychological need for relatedness. Studies that examine where motivations fall within the intrinsic-extrinsic motivation continuum are needed to understand how motivation might influence sustained participation over time. Our results also reiterate the inclination for practitioners to expect and measure gains in science content knowledge, typically through context-specific instruments that measure mastery of project activities and program content rather than increased knowledge about the process of science or the Nature of Science. Although some projects have begun to demonstrate outcomes related to “thinking scientifically” (Braschler et al. 2010; Kountoupes and Oberhauser 2008; Trumbull et al. 2000), a gap remains in our understanding of the potential for citizen science to influence deeper understanding of the process of science and the Nature of Science as well as the more complex facets of science inquiry (i.e., critical thinking, reflection, and reasoning). Future work should focus on the development of robust and contextually appropriate tools to better capture deep reflection and rich dialogue about NOS. In perhaps our most surprising finding, nearly 60% of project websites in our study listed data collection as an intended outcome, yet across all projects combined, our online survey showed that skills related to data collection were the least-measured outcome (28%). These findings may reflect the difficulty of measuring attributes such as the acquisition of skills and the relative ease of measuring other constructs such as knowledge, interest, and attitude. This disconnect also represents a potential tension that exists within the citizen science field, particularly among contributory projects: The need for high confidence in data quality versus the dearth of studies that have assessed data collection skills. While several studies demonstrate that volunteers are able to collect data of similar quality to experts, these tend to be isolated examples (Crall et al. 2011; Danielsen et al. 2014). Although a multitude of ways to validate citizen-science data exist (see Kosmala et al. 2016), tools and techniques are needed that can assess changes in participant data collection skills over time. Additionally, the field needs to better understand whether citizen science participation can influence other important inquiry skills such as the ability to make Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 13 of 19 decisions regarding appropriate research methodologies, to use variables and control groups properly, and to evaluate evidence. And as attention is increased on the potential for citizen science to democratize science, further work should examine the extent to which it can support development or reinforcement of critical thinking skills that inform decision making and help to create an informed citizenry. Also, in the new world of “Big Data,” citizen science is well poised not only to provide the public with large and robust data sets but also to develop support systems so that users can understand how to effectively use these dynamic resources. Such provisioning may facilitate new lines of research to better understand how participants engage with data sets and what meaning they hold for them. Finally, in our website review, environmental stewardship was mentioned as an intended outcome by 25% of projects—second only to data collection—suggesting a strong desire for citizen science projects to influence individual behavior change. About one-third of survey respondents reported measuring behavior change, but based on several open-ended comments, some practitioners equated the act of participating in a project as a change in behavior, meaning that such change was indicated for all participants. Recall, however, that we define behavior change as change that goes beyond project activities. Further, tacit assumptions may exist about engagement in specific project activities leading to more global environmental behaviors (Kollmuss and Agyeman 2002; Vining et al. 2002) (e.g., the assumption that waterquality monitoring can lead to reducing carbon emissions, recycling, and conserving energy). Intended behavioral outcomes should be directly connected to project content and activities, and the knowledge of how to perform these targeted behaviors should be made explicit to participants (Phillips et al. 2012; Toomey and Domroese 2013). While citizen science can likely impact behavior change, the development of effective implementation strategies and measurement of those outcomes are still in their infancy. Conclusion Thousands of citizen science projects exist around the word, reaching potentially millions of people, particularly in the observation and monitoring of species and habitats (Theobald et al. 2015). Such projects have the potential not only to engage individuals in the process of science, but also to encourage them to take positive action on behalf of the environment (Cooper et al. 2007; McKinley et al. 2016). If such outcomes are to be achieved, project developers need to better understand how to design projects so that activities and educational learning opportunities support and align with feasible and realistic outcomes (Shirk et al. 2012). This study has resulted in a framework to support citizen science practitioners in articulating and measuring learning outcomes for participants in their projects. The framework also should help to build capacity for practitioners seeking to conduct evaluations of citizen science projects by helping them to develop their program theory, i.e., to identify underlying assumptions about how project activities affect expected outcomes (Bickman 2000; Chen 2005; Funnell 2000; Funnell and Rogers 2011). In this regard, most evaluators recommend starting with articulation of project outcomes, then working backward to determine not only what can be achieved and how, but also what can be reasonably measured (Center for the Advancement of Informal Science Education 2011). Toward that end, work proceeding in parallel to this research is developing generic, yet customizable, evaluation scales that are tested as valid and reliable in citizen science contexts and which align to the framework described above (see DEVISE scales:  jfe/form/SV_cGxLGl1AlyAD8FL). By adopting common learning outcomes and measures, the citizen science field can further evaluation capacity and begin to conduct cross-programmatic analyses of citizen science projects to provide funders, stakeholders, and the general public with evidence-based findings about the potential for citizen science to impact the lives of its volunteers. Such studies also could provide critical information regarding why and how to achieve outcomes and under what conditions outcomes can be maximized. Future work should support continued development of consistent measures that can be used across studies, particularly those that do not rely on self-reports ( Becker-Klein et al. 2016; Phillips et al. 2012; Wells and Lekies 2012). Continued professional development opportunities for citizen science practitioners to spearhead evaluations of projects will increase capacity for such endeavors, build a steady source of knowledge about impacts, and lead to improved project design, implementation, and sustainability for the field as a whole. Initiation of in-depth longitudinal studies that measure persistence of change over time would add understanding of the impacts of such experiences (Schneider and Cheslock 2003). To the extent possible, more effort should be placed on studies that include experimental designs, random assignment, and control groups. Such efforts will increase the field’s ability to provide evidence for causal connections between citizen science participation and learning outcomes. Additionally, continued research on learning outcomes should seek to incorporate social learning theories, which may be helpful in understanding how learning happens in citizen science and the mechanisms and processes that enable active learning. Social learning theories such as Cultural Historical Activity Theory (Vygotsky and Cole 1978); Activity Theory (Engeström 1999), Experiential Learning (Dewey 1938; Kolb 1984), Situated Learning Theory (Lave and Wenger 1991), and Communities of Practice (Wenger 1998) are ideally suited for examining learning in citizen science because they emphasize the roles that participation in socially organized activities play in influencing learning (Roth and Lee 2002; National Research Council 2009). Social learning theory may be particularly useful to consider when developing project activities and experiences. Practitioners interested in incorporating social learning theories into citizen science project design, research, and evaluation should refer to the following studies for guidance: Roth and Lee (2004); Brossard et al. (2005); Ballard et al. (2008); Raddick et al. (2009); and Jackson et al. (2014).   Phillips et al: A Framework for Articulating and Measuring Individual Learning Outcomes from Participation in Citizen Science Art. 3, page 14 of 19 Finally, as citizen science continues to grow, it will be important for the field to take a reflective look at its relative impact, and to evaluate whether appropriate questions are being asked by qualified researchers working across projects that involve diverse audiences and issues. Such an analysis will be a first step in gathering critical evidence to demonstrate the potential of citizen science to truly democratize science. Additional Files The Additional Files for this article can be found as follows: • Appendix A. Databases and Search terms used to locate citizen science websites. DOI: https://doi. org/10.5334/cstp.126.s1 • Appendix B. Questions from Online Practitioner Survey. DOI:  Acknowledgements The work described in this paper is part of a larger study called DEVISE (Developing, Validating, and Implementing Situated Evaluation Instruments). DEVISE is based at the Cornell Lab of Ornithology, with Rick Bonney as Principal Investigator and Tina Phillips as Project Director. We thank the project’s co-PIs, Kirsten Ellenbogen and Candie Wilderman; project consultants Edward Deci, Cecilia Garibay, Drew Gitomer, Kate Haley Goldman, Joe Heimlich, Chris Niemiec, and Gil Noam; and project advisory board members Heidi Ballard, Rebecca Jordan, Bruce Lewenstein, and Karen Oberhauser. We also thank the many practitioners who took the time to respond to our survey and queries. Finally, we thank many Lab of Ornithology staff who helped along the way including Jennifer Shirk, Matthew Minarchek, Marion Ferguson, and Holly Faulkner. DEVISE is supported by the National Science Foundation under Grant No. 1010744. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation. Competing Interests One of the authors of this paper, Rick Bonney, is editor-inchief of this journal. He was not involved in the process of reviewing the paper other than responding to reviewers’ comments. '"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b = docs_csguide.iloc[0][\"clean_meaning\"]\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "yTdQvjaORo2e",
        "outputId": "c6e9e7ea-8ba8-4153-e27a-778cccf6aa52"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a framework for articulating and measuring individual learning outcomes from participation in citizen science research paper a framework for articulating and measuring individual learning outcomes from participation in citizen science tina phillips   norman porticella   mark constas  and rick bonney  since first being introduced in the mid s  the term  citizen science  the intentional engagement of the public in scientific research has seen phenomenal growth as measured by the number of projects developed  people involved  and articles published  in addition to contributing to scientific knowledge  many citizen science projects attempt to achieve learning outcomes among their participants  however  little guidance is available for practitioners regarding the types of learning that can be supported through citizen science or the measuring of learning outcomes  this study provides empirical data to understand how intended learning outcomes first described by the informal science education field have been employed and measured within the citizen science field  we also present a framework for describing learning outcomes that should help citizen science practitioners  researchers  and evaluators in designing projects and in studying and evaluating their impacts  this is a first step in building evaluation capacity across the field of citizen science  keywords  learning outcomes  evaluation  informal science learning   cornell lab of ornithology  us   cornell university  us corresponding author  tina phillips  introduction citizen science  defined here as public participation in scientific research  was originally conceived as a method for gathering large amounts of data across time and space  bonney et al  b   for decades or even centuries  citizen science has contributed to knowledge and understanding about far ranging scientific topics  questions  and issues  miller rushing et al     more recently  citizen science practitioners those who conceive  develop  and implement citizen science projects have sought not only to achieve science research outcomes but also to elicit learning and behavioral outcomes for participants   bonney et al    phillips et al     many proponents of citizen science argue that participating directly in the scientific process via citizen science is an excellent way to increase science knowledge and literacy  bonney et al    fernandez gimenez et al    jordan et al    krasny and bonney    understand the process of science  trautmann et al    trumbull et al     and develop positive action on behalf of the environment  cornwell and campbell   cooper et al    lewandowski and oberhauser   mckinley et al     while some projects have demonstrated achievement of a few learning outcomes  see bonney et al   for examples   most projects have yet to document robust outcomes such as increased interest in science or the environment  knowledge of science process  skills of science inquiry  or stewardship behaviors  bela et al    bonney et al    jordan et al    phillips et al     several factors may account for the lack of demonstrated and measurable learning outcomes  first  the field of citizen science is still young  few if any specific outcomes have been defined or described by the field  therefore  project designers may not have clear concepts of what types of learning they are attempting to foster  in addition  measuring learning requires dedicated time  resources  and expertise in conducting social science research or evaluations  which many citizen science projects lack  as a result  citizen science suffers from a lack of quality project evaluations and cross programmatic research  phillips et al     the informal science learning community recently developed guidance including tools and resources for evaluating learning outcomes from participation or engagement in informal science education  ise  activities  friedman et al    national research council    these tools and resources are relevant to the field of citizen science  because many citizen science projects operate in informal environments such as private residences  parks  science and nature centers  museums  community centers  afterschool programs  or online  in addition  many citizen science projects are funded through ise initiatives because the projects are expected to foster lifelong science learning  crain et al     therefore  tools developed to measure learning outcomes resulting from ise can serve as logical starting points for evaluating outcomes of citizen science participation  phillips  t  et al    a framework for articulating and measuring individual learning outcomes from participation in citizen science  citizen science  theory and practice        pp     doi     phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  the objectives of the research presented in this paper were to determine and describe the types of learning outcomes that are intended by citizen science project developers  to examine the alignment of these outcomes with informal science learning frameworks and guidelines  and to develop and present a new framework for articulating citizen science learning outcomes  we believe that the framework will help citizen science practitioners to design projects that achieve measurable learning  we also hope that the framework will facilitate cross programmatic research to help the citizen science field show how its projects are impacting science and society  our research further sought to determine the extent to which citizen science learning outcomes have been evaluated across the field  as a first step toward our overall goal of deepening evaluation capacity for the citizen science community  citizen science and informal science learning the educational underpinnings of citizen science  particularly when involving adults draw heavily from informal science education  ise   what falk and dierking    refer to as  free choice learning  lifelong  self directed learning that occurs outside k  classrooms  two influential documents from the ise field provided a starting point for our study  the framework for evaluating impacts of informal science education projects  friedman et al     supported by the national science foundation  nsf   was the first publication produced by the ise field that described a  standard  set of learning outcomes  referred to as impact categories  that could be used to systematically measure project level outcomes   we will refer to this publication as the  ise framework  for the remainder of this paper   a major goal of the framework was to facilitate cross project and cross technique comparisons of the impacts of ise projects on public audiences  the five impact categories are    knowledge  awareness  or understanding of science  technology  engineering  and math  stem  concepts  processes  or careers   engagement or interest in stem concepts or careers   attitude toward stem concepts  processes  or careers   skills based on stem concepts or processes   behavior related to stem concepts  processes  or careers a second document  learning science in informal environments  people  places  and pursuits  national research council    focuses on characterizing the cognitive  affective  social  and developmental aspects of science learners  termed the  lsie strands   these aspects of science participation include    interest and motivation to learn about the natural world   application and understanding of science concepts   acquisition of skills related to the practice of science   reflecting on science as a way of knowing  participating in  and communicating about science   identifying oneself as someone capable of knowing  using  and contributing to science the authors of the lsie strands noted that while the concepts originated in research  at the time of writing they had not yet been applied or analyzed in any systematic venue  the significant overlap between the lsie strands and the ise framework s impact categories is shown in table   a third ise document also contributed to framing this study  in   an inquiry group sponsored by the center for advancement of informal science education  caise  produced  public participation in scientific research  defining the field and assessing its potential for informal science education   bonney et al  a   which was created as a  first step toward developing an organized methodology for comparing outcomes across a variety of public participation in scientific research  ppsr  projects   p    this paper included a rubric of potential citizen science learning outcomes  based on the ise framework  and examined ten nsf funded citizen science projects to assess whether they reported outcomes similar to those described in the ise framework  figure   participants in citizen science engage in a large number of activities such as designing studies  collecting and analyzing data  and disseminating project results  what do project designers hope that participants will learn from their participation  how are desired learning outcomes designed  how are they measured  credit  no copyright  pacific southwest region usfws  flickr public domain    phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  one result of the caise report was a realization that citizen science practitioners were measuring project outcomes in varied ways  making it difficult for crossprogrammatic research to study the collective impact of the field  another result was the development of a project typology based on the level of participant involvement in the scientific process  bonney et al  a   this typology described  contributory  projects that are researcherdriven  where participants primarily focus on data collection   collaborative  projects that are typically led by researchers  but may include input from participants in phases of the scientific process such as designing methods  analyzing data  and disseminating results  and  co created  projects that involve participants in all aspects of the scientific process  from defining a question to interpreting data to disseminating results  figure    this typology allowed projects designed for different reasons and in different ways to be grouped to help researchers understand common outcomes  the three documents described above served as foundations for articulating learning outcomes from citizen science participation  however  they lacked systematic empirical support  this study provides such support by ground truthing and applying the concepts within the ise framework  the lsie strands  and the bonney et al   a  rubric to the field of citizen science  methods and results our research used two sources of data a structured review of citizen science project websites and an online survey of citizen science practitioners to address the following three questions    what are the learning outcomes that are intended or desired by citizen science practitioners  and to what extent do these outcomes align with those described by the field of informal science education   data source  website review    what is the status of evaluation of citizen science learning outcomes across the field   data source  online practitioner survey    how are citizen science learning outcomes measured by different projects   data source  online practitioner survey  we also conducted a literature review to uncover definitions  descriptions  and elucidations of the learning outcomes that we identified through our research  we used the results of this review  along with our new understanding of the outcomes desired and measured by citizen science practitioners  to develop a framework of common learning outcomes for the citizen science field  intended learning outcomes to describe and understand the learning outcomes that are intended or desired by citizen science practitioners as they develop projects  we first identified individual projects by conducting a semi structured search of the following citizen science portals  citizen science central  citizenscience org   informalscience  informalscience  org   scistarter  scistarter com   citizen science alliance  citizensciencealliance org   and national directory of volunteer monitoring programs  yosemite epa gov  water volmon nsf    the last portal included  projects  from which we sampled every fifth one  if a project table   comparison of nsf framework and lsie strands  nsf framework category lsie strands knowledge  awareness  understanding  measurable demonstration of assessment of  change in  or exercise of awareness  knowledge  understanding of a particular scientific topic  concept  phenomena  theory  or careers central to the project  strand     understanding  come to generate  understand  remember  and use concepts  explanations  arguments  models  and facts related to science  engagement  interest or motivation in science  measurable demonstration of assessment of  change in  or exercise of engagement interest in a particular scientific topic  concept  phenomena  theory  or careers central to the project  strand     interest and motivation  experience excitement  interest and motivation to learn about phenomena in the natural and physical world  skills related to science inquiry  measurable demonstration of the development and or reinforcement of skills  either entirely new ones or the reinforcement  even practice  of developing skills  strand     science exploration  manipulate  test  explore  predict  question  and make sense of the natural and physical world  and strand     participate in scientific activities and learning practices with others  using scientific language and tools attitudes toward science  measurable demonstration of assessment of  change in  or exercise of attitude toward a particular scientific topic  concept  phenomena  theory  or careers central to the project or one s capabilities relative to these areas  attitudes refer to changes in relatively stable  more intractable constructs such as empathy for animals and their habitats  appreciation for the role of scientists in society or attitudes toward stem cell research  related to strand     identity  think about themselves as science learners  and develop an identity as someone who knows about  uses  and sometimes contributes to science  also  related to strand     reflection  reflect on science as a way of knowing  on processes  concepts  and institutions of science  and on their own process of learning about phenomena  behavior  measurable demonstration of assessment of  change in  or exercise of behavior related to a stem topic  behavioral impacts are particularly relevant to projects that are environmental in nature since action is a desired outcome  related to strand     skills  participate in scientific activities and learning practices with others  using scientific language and tools         phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  was listed on multiple portals  we included it only once  in total   citizen science projects met our criteria for study inclusion  being open to participation in the u s  or canada  having an online presence  and being operational at the time of the search     the complete list of databases and search terms used to locate citizen science projects is available in the supplemental material for this paper  appendix a   from each of the  project websites  we gathered the following information  project name  url  contact information  general goal statements  learning objectives or desired outcomes  if any   and potential indicators of learning  if any   nine percent of project websites did not describe intended learning outcomes  e g   some projects stated their goals to be purely scientific in nature   but the remaining   of projects described at least one  we coded each goal statement and learning objective into one of the major categories outlined in the ise framework  knowledge  engagement  skills  attitude  behavior  other  and into sub codes outlined in the assessment rubric by bonney et al   a   several projects described multiple learning outcomes  in these cases  each distinct outcome was coded separately  for example  the great lakes worm watch states that its goal is  increasing scientific literacy and public understanding of the role of exotic species in ecosystems change   objectives are to  provide the tools and resources for citizens to actively contribute to the development of a database documenting the distributions of exotic earthworms and their impacts across the region as well as training and resources for educators to help build understanding of the methods and results of scientific research about exotic earthworms and forest ecosystems ecology   the text from the goal statement and learning objectives  left  were coded into the outcomes categories on the right    increasing scientific literacy and public understanding   content knowledge   citizens actively contribute to the development of a database   data collection and monitoring  data submission   help build understanding of the methods and results of scientific research   nature of science knowledge results from our coding of project goals and objectives are presented in table   they reveal that the number of aspirational learning outcomes for projects ranged from zero to as many as seven  with about   of projects including at least two  the majority of projects     focus on influencing skills related to data collection and monitoring  intended outcomes for these projects are often stated as  volunteers gain data collection and reporting skills   the second most frequently stated intended learning outcome    of projects  was understanding of content knowledge  e g    volunteers learn about macroinvertebrates and stream health    the third most common intended outcome  increased environmental stewardship which typically includes some type of behavior change  e g    engage watershed residents in protecting water quality   was specified by about   of projects  other intended learning outcomes were mentioned much less frequently  table   count of specified learning outcomes as coded from  citizen science project websites  percentages represent the proportion of projects that described the stated outcome  several projects stated more than one outcome  stated outcomes on project websites count of projects stating outcome  n     percentage of projects stating outcome data collection and monitoring    content knowledge    environmental stewardship    no education goal specified    attitude awareness    nature of science    data analysis    interest in the environment    civic action    submitting data    interest in science    community health    communication skills    using technology    science careers    designing studies     phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  including increases in knowledge of the nature of science  data analysis skills  interest in the environment  civic action  data submission  communication skills  use of technology  science careers  study design  and also shifts in attitude awareness  considering all projects for which intended learning outcomes were stated  each of the ise framework impact categories was represented  suggesting a strong alignment between learning outcomes desired for citizen science participants and those for participants in the informal science learning community more generally  status of citizen science project evaluation to uncover the status of evaluation of citizen science learning outcomes across the field  we conducted an online survey of citizen science practitioners in march   delivered via survey monkey   the survey contained  questions  including  closed ended questions with predetermined options including  other   the remaining five questions were open ended  providing text boxes for answers  only one question  which asked respondents to classify their project according to the three model typology of citizen science developed by bonney et al   a   required a response  additional questions focused on the duration of the project  the approximate number of participants  and the type of training that participants received  respondents also were asked if any type of evaluation had ever been conducted for their project  details about evaluations that were conducted  what learning outcomes described in the ise framework had been measured  and what other types of outcomes had been measured  the complete set of survey questions is available in the supplemental material  appendix b   following approval by the cornell university institutional review board      we sent an email invitation to potential respondents describing the goal of the survey and explaining that participation was voluntary and confidential  two reminder emails were sent approximately two and four weeks following the initial invitation  an informed consent statement was included at the start of the survey  potential respondents were recruited via the citizenscience org listserv  citsci discussion l   which anyone could join  and which at the time of recruitment had approximately   members  not all members of the listserv were project leaders  and multiple list members likely represented a single project  making it difficult to know the actual number of projects represented by listserv members  after the survey was closed  we made sure that all responding projects were included in the previously described website review  to obtain as much overlap between the two datasets as possible  the survey was completed by  respondents representing  unique projects  some projects had multiple entries  in which case only the first entry was included  other respondents failed to include information about their project name  which was optional   all but ten of the  unique projects also were represented in the project website data  the remaining ten projects that responded to the online survey but were not in the website review were either no longer operational  not in the us or canada  or did not have a web presence  the majority of projects   or    had been operating from   years  and nearly half     had fewer than  participants  because most questions were optional  response rates varied for different survey items  results revealed that of the  respondents   or   had undertaken some type of project evaluation  more than half of the evaluations were administered by internal project staff to measure project outcomes or impacts  mostly using data collected through surveys  about one third of respondents reported conducting post only or pre and posttest evaluation designs  reasons for conducting project evaluations included  gauging participant learning  identifying project strengths and weaknesses  obtaining additional funding or support  promoting a project more broadly  and providing recommendations for project improvement  in addition to asking about project learning outcomes  described in the next section   the survey also asked what other aspects of the project had been evaluated  two thirds of participants reported measuring satisfaction or enjoyment with the project  followed by motivation to participate     and evaluation of project outputs such as numbers of participants  web hits  journal articles  and amount of data collected      other measured outcomes included scientific conservation      effectiveness of workshops and trainings      data quality      community capacity building      and social policy change      another open ended question asked respondents  please do your best to provide the name or description of any instrument  e g   views on science and technology survey  used to collect evaluation data  even if you developed the instrument   of the  respondents to this question  only three had used a pre existing  validated instrument  the majority of respondents had developed their own instruments in house or had an external evaluator develop original instruments  a handful of respondents replied with  survey monkey  or some other data collection platform as opposed to describing an evaluation instrument  some mentioned tools such as gps units or calipers as instruments used by the project  while others stated that they did not understand the question  when asked about their overall satisfaction with their evaluations  more than half of respondents expressed agreement or strong agreement that evaluations were of high quality  that evaluation findings were informative to the project developers  that recommendations from the evaluation were implemented  that the project had improved as a result of evaluation  that they learned a lot about evaluation  and that they felt confident they could personally conduct an evaluation in the future  survey respondents also were asked about aspects of the evaluation process for which they would like assistance  the highest priority was help with developing goals  objectives  and indicators  followed by creating or finding appropriate survey instruments  help with analyzing or interpreting data  and help with data collection  participants also were asked what specific resources would be most helpful for conducting evaluations  the most common replies were a database of potential surveys and data collection instruments  sample evaluation reports from citizen science  examples of evaluation designs  and an entry level guide for conducting evaluations   phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  finally  respondents were presented with a list of eight different online organizations that support or provide resources for evaluation and were asked how often they access them  surprisingly  the majority of respondents had never heard of any of the resources or organizations  the only exception was citizenscience org  which was used by   of respondents  but rarely  as opposed to frequently  sometimes  or never   these results show a range of evaluation efforts and a positive attitude toward evaluation and findings among citizen science practitioners  but also a need for more knowledge of and accessibility to evaluation tools and resources  measurement of learning outcomes respondents who reported having conducted evaluations   or    were asked  for the most recent evaluation of your project  which broad categories of learning outcomes  if any  were evaluated   responses to this question were based on the ise framework broad impact categories  aggregated results across all projects revealed that interest or engagement in science was the most commonly measured outcome      followed by knowledge of science content      behavior change resulting from participation and attitudes toward science process  content  careers  and the environment accounted for   and    respectively  of measured learning outcomes  science inquiry skills  e g   asking questions  designing studies  data collection  data analysis  and using technology  were the least commonly measured outcomes across all projects      in an open ended question about other types of learning outcomes  about   of respondents also described measuring motivation and self efficacy or confidence to participate in science and environmental activities  considering differences in categories of learning outcomes measured within project types  contributory projects  for which there were  respondents that had conducted evaluations  reported measuring interest in science most frequently     and skills of science inquiry least frequently      two thirds of all collaborative projects  n     measured content knowledge  followed by interest      behavior change      and attitudes and skills  both     only nine survey respondents represented co created projects that had conducted evaluations  and of these  skills of science inquiry were measured most often  responses combined across projects and separated among project types are summarized in figure   earlier in this paper we showed that a majority of citizen science project websites described intended learning outcomes very similar to those in the ise framework  although not always using the same language  results from the online practitioner survey added to our  ground truthing  of the ise framework  as respondents described attempts to measure these same outcomes  albeit to varying degrees  open ended responses highlighted the need to emphasize efficacy as an important learning outcome in citizen science  survey respondents also made it clear that additional resources were needed to help formulate and measure learning outcomes  a framework for articulating and measuring common learning outcomes for citizen science in addition to synthesizing and comparing empirical results from our website review and practitioner survey to describe intended and measured learning outcomes  we used key word searches to conduct a review of more than  peer reviewed articles focused on defining and measuring these learning outcomes  our data and review facilitated a re conceptualization or contextualization of several of the impact categories presented in the ise framework to make them relevant to citizen science  in figure   measured learning outcomes from online survey of citizen science practitioners who reported having conducted some sort of evaluation  n       phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  particular environmentally based projects  for example  some outcomes uncovered in our research  such as  skills of science inquiry   map well to the categories and their definitions in the ise framework  other outcomes  such as  attitude   required clarification  our new framework is thus based on both empirical data and contributions from the literature and includes the following learning outcomes  interest in science and the environment  selfefficacy for science and the environment  motivation for science and the environment  knowledge of the nature of science  skills of science inquiry  and behavior and stewardship  figure    this framework should help citizen science practitioners consider some of the more commonly desired and achievable learning outcomes when designing projects  however  we emphasize that no single project should try to achieve and or measure all  or even most  of these outcomes  as doing so can set up unreasonable expectations for both the project and its evaluation  we also note that the framework is not exhaustive  indeed  as citizen science continues to expand  new research will inevitably reveal other learning outcomes that are important to articulate and measure  below we describe each outcome within the framework  highlighting how each has been explained in the broad educational field and also providing examples of how each has been used in published studies of citizen science  these outcomes are not hierarchical but  beginning with interest in science and the environment  build from and reinforce each other  interest in science and the environment we define interest as the degree to which an individual assigns personal relevance to a science or environmental topic or endeavor  within ise  hidi and renninger    treat interest as a multi faceted construct encompassing cognitive  thinking   affective  feeling   and behavioral  doing  domains across four phases of adoption  triggered situational interest typically stimulated by a particular event and requiring support by others  maintained situational interest  which is sustained through personally meaningful activities and experiences  emerging individual interest characterized by positive feelings and self directed pursuit of re engaging with certain activities  and well developed individual interest leading to enduring participation and application of knowledge  our definition of interest is compatible with hidi and renninger s    later phases of interest development  which are characterized by positive feelings and an increasing investment in learning more about a particular topic  interest in science is considered a key driver to pursuing science careers in youth  maltese and tai   tai et al    and sustained lifelong learning and engagement in adults  falk et al    hidi and renninger    over time  this type of interest can lead to sustained engagement and motivation and can support identity development as a science learner   fenichel and schweingruber   national research council    further  interest is noted as an important precursor to deeper engagement in democratic decision making processes regarding science and technology  mejlgaard and stares    figure   framework for articulating and measuring individual learning outcomes from participation in citizen science  phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  although interest is considered to be an attitudinal structure  see bauer et al    fenichel and schweingruber   sturgis and allum    equating interest with attitudes should be avoided because attitude is a very broad construct  encompassing related but distinct sub constructs such as efficacy  interest  curiosity  appreciation  enjoyment  beliefs  values  perseverance  motivation  engagement  and identity  osborne et al     interest also has been used synonymously with engagement  friedman et al     but as mccallie et al     point out  engagement has yet to be well defined and has multiple meanings within the literature  particularly in ise  citizen science projects  especially those for which repeated visits or experiences are the norm  can lend themselves to deeper and sustained interest in science and the environment  yet few studies have looked at interest as an outcome  and those that have find mixed results  price and lee    reported increased interest in science among citizen sky observers  especially among participants who engaged in online social activities  crall et al     examined general interest in science as a reason for participation in citizen science and suggested that interest was not a driving force for joining a project  interest in specific nature based topics  i e   butterflies  was seen as a driver for engagement and also as a motivator for adding increasingly more complex data protocols to the french garden butterflies watch project  cosquer et al     other research has shown that interest in use of natural resources can be a very strong determinant for future and sustained involvement in the decision making process about management of natural resources  danielsen et al     from these studies  it appears that examining interest in science more broadly may be less effective than measuring specific science topics  however  an audience s pre existing interests in specific topics may not change significantly through participation  self efficacy for science and the environment another important outcome for studying learning is self efficacy  i e   a person s beliefs about his her capabilities to learn specific content and to perform particular behaviors  bandura    research has found that self efficacy affects an individual s choice  effort  and persistence in activities  bandura     schunk    individuals who feel efficacious put more effort into their activities and persist at them longer than those who doubt their abilities  self efficacy is sometimes referred to as  perceived competence   in self determination theory  and  perceived behavioral control   in ajzen s theory of planned behavior  ajzen    berkowitz et al     treat self efficacy as an essential component in environmental citizenship  along with motivation and awareness   which is dependent on an individual s belief that they have sufficient skills  knowledge  and opportunity to bring about positive change in their personal lives or community  in the context of citizen science  self efficacy is the extent to which a learner has confidence in his or her ability to participate in a science or environmental activity  in a study involving classrooms  middle school students participating in a horseshoe crab citizen science project showed greater gains in self efficacy than a control group  hiller    in an online astronomy project  however  researchers found a significant decrease in efficacy toward science  possibly owing to a heightened awareness of how much participants did not know about the topic  price and lee    crall et al     determined that selfefficacy is not only important in carrying out the principal activities of a project but also in the potential for individuals to carry out future activities related to environmental stewardship  working in a participatory action project with salal harvesters  ballard and belsky    found that the process of co developing and implementing different experiments increased workers  self efficacy regarding their skills in scientific research  although efficacy was not called out directly in the ise framework  it can be considered part of the lsie strand    identity as a learner   national research council    self efficacy also was mentioned by project leaders in our online survey and thus appears to be an important potential outcome from citizen science participation  motivation for science and the environment motivation is a multi faceted and complex attitudinal construct that describes some form of goal setting to achieve a behavior or end result  the lsie strands  national research council   include motivation to sustain science learning over an individual s lifetime as an important aspect of learning in informal environments  the literature on volunteerism frames motivation as an important factor in effective recruitment  accurate placement  and volunteer satisfaction and retention  clary and snyder   esmond et al     of the dozens of theories on motivation  two perspectives seem especially relevant to volunteerism and citizen science  first  the volunteer functions inventory  vfi   developed by clary et al      examines how behaviors help individuals achieve personal and social goals  clary et al  s    categories of motivation include values  importance of helping others   understanding  activities that fulfill a desire to learn   social  influence by significant others   career  exploring job opportunities or work advancement   esteem  improving personal self esteem   and protective  escaping from negative feelings   wright et al     studied the motivations of birders in south africa using a modified version of the vfi and found five categories of motivation to be most important  recreation nature  values  personal growth  social interactions  and project organization  the second perspective comes from self determination theory  sdt   which treats motivation as an explanatory variable for meeting basic psychological needs  i e   competency  relatedness  and autonomy  and describes different types of motivations as falling on a continuum from intrinsic to extrinsic  ryan and deci a  b   according to sdt  individuals are likely to continue pursuing a goal to the extent that they perceive intrinsic value in the pursuit of that goal  i e   the extent to which they experience satisfaction in performing associated behaviors themselves versus performing behaviors to comply with extrinsic goals such as conforming to social pressures  fear  or receiving rewards   although sdt can phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  help practitioners better understand the psychological needs behind participation  few published studies have used sdt in the context of citizen science  one exception is a paper by nov et al      which used sdt with social movement participation models in an examination of three digital citizen science projects  these researchers found that intrinsic motivation was one of four drivers that influenced quantity of participation  but that it did not affect quality of participation  in the context of citizen science  motivation can serve as both an input and outcome  i e   to understand the basis of motivation for ise citizen science experiences  input  and to sustain motivation to continue participating over long time periods  outcome   however  most studies have examined reasons for participation such as the desire to contribute  see bell et al    hobbs and white   mccaffrey   raddick et al    reed et al     rather than motivations  which describe the psychological underpinnings of behavior  e g    because it makes me feel good    in an examination of motivation in online projects  rotman et al     described a complex and changing framework for motivation that was influenced by participant interest  recognition  and attribution  although several studies have purported to examine motivation  it has not been defined nor studied uniformly throughout the field of citizen science  nevertheless  the major consensus appears to be that motivation for citizen science  like other volunteer activities  is dynamic and complex  content  process  and nature of science knowledge included within the ise framework s impact category of  awareness  knowledge  and understanding  are several subcategories such as knowledge and understanding of science content  knowledge and understanding of science processes  and knowledge of the nature of science  knowledge of science content refers to understanding of subject matter  i e   facts or concepts  knowledge of the process of science refers to understanding the methodologies that scientists use to conduct research  for example  the hypothetico deductive model or  scientific method    knowledge of the nature of science  nos  refers to understanding the epistemological underpinnings of scientific knowledge and how it is generated  sometimes presented from a postpositivist perspective  lederman    nos addresses tenets of science such as tentativeness  empiricism  subjectivity  creativity  social cultural influence  observations and inferences  and theories and laws  see lederman     lederman et al       for improving scientific literacy  understanding of nos and the process of science are generally considered more important than understanding basic content or subject matter  american association for the advancement of sciences   national research council   ngss    and knowledge of the process of science is a regular component of well established assessments of science knowledge  national science board    despite this recognition  most attempts to measure science literacy within the ise field fall back on content knowledge  i e   rote memorization of facts  rather than knowledge of the nature or process of science  bauer et al    shamos    indeed  citizen science evaluations have typically emphasized measuring gains in topical content knowledge as opposed to science process knowledge  with mixed results  ballard and huntsinger   bonney   braschler et al    brewer   devictor et al    evans et al    fernandez gimenez et al    jordan et al    kountoupes and oberhauser   krasny and bonney   phillips et al    sickler et al    trumbull et al    trumbull et al     overdevest et al     did not find a significant increase in project participant knowledge about streams and water quality  probably because new volunteers were already highly knowledgeable about the subject matter  price and lee    actually found a decrease in science content knowledge among project participants  likely owing to exaggerated notions of participants  self perceived content knowledge before starting the project and the realization of how much they did not know after participating in the project  however  a few studies have used measures of the process of science to assess impacts of citizen science project participation  jordan et al     and brossard et al     used adaptations of the science and engineering indicators and showed no gains in understanding of the process of science as a result of citizen science participation  in contrast  ballard et al     used interview data to show evidence that the salal harvesting project    increased local people s understanding of the scientific process and of the ecosystem on which they were a part  p      and significant increases in understanding of the process of science before and after participation in a stream water quality monitoring project were reported by cronin and messemer     however  this study had a very small sample size  which may limit generalizability of the results  likewise  few citizen science projects have attempted to study understanding of the nos  jordan et al     found no evidence for change in knowledge of the nos using pre post scenario based questions in an invasive species project  price and lee    found little evidence that project participation influenced epistemological beliefs about nos  owing to the fact that  epistemological beliefs are personal beliefs and thus harder to change after participating in only one citizen science project   p     these findings suggest that while citizen science can effectively demonstrate gains in content knowledge  it has a long way to go before it can positively establish increases in understanding of science process and the nos  skills of science inquiry skills of science inquiry are observable practices that can be transferred to daily life  such as asking and answering questions  collecting data  developing and using models  planning and carrying out investigations  reasoning about  analyzing  and interpreting data  constructing explanations  communicating information  and using evidence in argumentation  national academies of science  engineering  and medicine   ngss lead states    the hands on nature of many environmentally based citizen science projects makes them particularly well suited to influence the development and or phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  reinforcement of certain science inquiry skills including asking questions  designing studies  collecting  analyzing  and interpreting data  and discussing and disseminating results  bonney et al  a  jordan et al    phillips et al    trautmann et al     top priorities for many practitioners are helping participants learn to follow protocols and exercise accurate data collection skills  because these practices directly influence data quality  the fieldwide emphasis on data quality likely comes from the large percentage of contributory  scientist driven projects  for which a key goal is gathering data of sufficient quality to add to the existing knowledge base through publication in peer reviewed journals  consequently  many citizen science projects most effectively influence skills that are related to data and sample specimen collection  identification of organisms  instrument use  and sampling techniques  many projects also engage participants in the use of various technological tools such as gps units  digital thermometers  water conductivity instruments  rain gauges  nets  and smartphones  to name just a few  figure    a few researchers have begun to study skill acquisition in citizen science  becker et al     showed an increase in the ability to estimate noise levels with increasing participation in widenoise  a soundscape project operated through mobile devices  increases in youths  self reported science inquiry skills  such as their perceived ability to identify pond organisms and to develop testable hypotheses before and after participation in driven to discover  also have been reported  meyer et al     sullivan et al     describe the use of communication prompts and strategies to  steer birders toward providing more useful data  and essentially change the birding habits of ebird participants to increase data quality  using the theory of legitimate peripheral participation  mugar et al     used practice proxies  a form of virtual and trace ethnography  to increase accuracy of data annotation among new members  additionally  some projects have successfully conducted small scale studies that compare volunteercollected data to those collected by experts  thereby creating a baseline metric for assessing their participants  skills  see crall et al    jordan et al    schmeller et al     another hallmark of citizen science is the collection of large  publicly available data sets and rich  interactive data visualizations  many projects that provide data visualizations may seek to enhance skills related to data interpretation  i e   the ability to effectively comprehend information and meaning  often presented in graphical form  devictor et al     in one of the few studies examining data interpretation in citizen science  thompson and bonney    showed that even the majority of  active users  of ebird did not properly use the extensive array of dataanalysis tools  numerous studies in educational research have shown that assessing the type of reasoning skills needed for data interpretation requires asking a series of reflective questions to determine one s justification underlying the reasoning  e g   ayala et al    roth and roychoudhury    other inquiry skills such as study design  communication  critical thinking  decision making skills  and critically evaluating results are less studied within the citizen science literature  crall et al     used open ended questions to determine whether engaging in an invasive species project improved the abilities of participants to explain a scientific study  write a valid research question  and provide a valid sampling design  these researchers noted positive gains in all but the ability to explain a scientific study  char et al     found an increase from pre post training in the ability of coasst volunteers to correctly weigh evidence to determine whether it contained sufficient information for accurately identifying species  these few studies show the potential for studying citizen science participants to evaluate the development of complex science inquiry skills  but such studies are in their infancy  behavior and stewardship behavior change and development of environmental stewardship are among the most sought after outcomes in science and environmental education programs  both in and out of schools  bodzin   heimlich et al    kollmuss and agyeman   stern   stern et al    vining et al     theories examining various determinants of environmental behavior include those espousing the links between knowledge  attitude  and behavior  hungerford and volk   kollmuss and agyeman   osbaldiston and schott   schultz    attitudes and values  ajzen   fishbein and ajzen    behavior modification and intervention  de young    and nature exposure  kaplan   kellert and wilson   ulrich   wilson    we define behavior and stewardship as measurable actions resulting from engagement in citizen science  but external to the protocol activities and the specific projectbased skills of the citizen science project  for example  collecting water quality data may be a new behavior for a project participant  but if the data collection is part of the project protocol it should be measured as a new skill rather than a new behavior  however  somebody decreasing their water usage as a result of participating in a water quality monitoring project would be an example of behavior change  our literature review identified five categories of behavior and stewardship that are of interest to the citizen science field and for which we provide definitions below  global stewardship behaviors  place based behaviors  new participation  community or civic action  and transformative lifestyle changes  global stewardship refers to deliberate changes in behavior that minimize someone s individual ecological footprint and which collectively can have global influence  e g   installing low flow shower heads  recycling  purchasing energy efficient appliances   place based behaviors refer to observable actions to directly maintain  restore  improve  or educate about the health of an ecosystem beyond the activities of a citizen science project  e g   removing invasive species  cleaning up trash  eliminating pesticide use  purchasing locally grown food  engaging in outreach to youth groups   new participation is defined as engagement in science or environmental activities  organizations  or projects spurred on by participation in phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  a citizen science project  community or civic action refers to participation in civic  governmental  or cultural affairs to solve problems at the local  regional  or national level  actions could include donating to environmental organizations  signing petitions  speaking out against harmful environmental practices  or recruiting others to participate in environmental causes  finally  transformative lifestyle changes are efforts that require a strong up front cost or long term commitment to maintain  such as investing in a hybrid vehicle  becoming a vegetarian  or pledging to use mass transit whenever possible  citizen science projects  especially those dealing with environmental topics  are typically hands on  occur in local environments  and require repeated monitoring and data gathering  making them natural conduits for affecting behavior change  wells and lekies    however  research has been limited and results have been mixed regarding the actual influence of citizen science on behavior change  for example  in a study examining two different projects  one on pollinators and one on coyotes  toomey and domroese    show that participants engage in new activities and change their gardening practices  but otherwise did not take part in advocacy or change their environmental stewardship practices  crall et al     found significant differences between current and planned behavior as a result of participating in an invasive species project using self reported measures  but the actual behavior change was not well described  using a case study approach  oberhauser and prysby    claim that participants of the monarch larva monitoring project  work to preserve habitat at many levels  from advocating a more environmentally friendly mowing regimen and insect friendly pest control  to challenging parking lot  building  and road development projects that threaten monarch habitat  p      however  the source of these data or accompanying methodologies are not clearly described  cornwell and campbell    also used a case study approach and were able to document advocacy and political action by volunteers which directly benefited sea turtle conservation  evans et al     documented locally  place based stewardship in a bird breeding program  while other projects showed no change in place based stewardship practices  jordan et al     in a study of human health effects of industrial hog operations  wing et al     describe actions being taken by community groups to engage in decision making that addresses local environmental injustices  taken together  these examples provide some evidence that citizen science may influence behavior and stewardship  but more robust methodologies are needed to establish causation  plenty of anecdotal data also highlight other examples of behavior change that have not been published or exist only in the gray literature  discussion results from research conducted through a systematic review of citizen science project websites and a survey of practitioners who design and implement citizen science projects confirm the relevance and applicability of three ise documents  friedman et al    national research council   bonney et al  a  in framing intended learning outcomes for citizen science participants  informed by this research along with a systematic literature review  we have modified and contextualized these documents to create a new framework that contains definitions and articulations of learning outcomes for the citizen science field  we believe that the framework provides a robust starting point for setting learning goals and objectives for citizen science projects and designing projects to meet those objectives  our research has some limitations  however  first  both the co created and collaborative project categories represented in the online practitioner survey have small sample sizes  so generalizing the types of learning outcomes intended by these project types is challenging  also  it s unclear whether the distribution across project types in the online survey reflects the actual distribution of contributory  collaborative  and co created projects across the u s  and canada  or if a disproportionate number of contributory projects received and responded to the survey request  we made no additional effort to recruit additional collaborative or co created project respondents  thus response bias may be an issue  also  while we made an effort to ensure that projects which responded to the practitioner survey were included in our website review  project level data from the two sources were not examined together  doing so may have shown convergence or divergence of intended versus measured outcomes  but was beyond the scope of this work and may have violated confidentiality conditions  finally  this work is a descriptive study based largely on self reports in the case of the practitioner survey and published desired outcomes in the case of the website review  more robust inferential studies that can examine figure   many citizen science project designers hope not only to collect important scientific information but also to help project participants gain skills such as scientific reasoning  here  a team of volunteers with public lab  a non profit environmental science community  launch a weather balloon  data collected via the balloon will be used in  d mapping surveys  but figuring out how to measure just what participants are learning as they conduct this research is a challenge for the citizen science field  credit  alan kotok flickr cc by      phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  field wide relationships and causal factors between project characteristics and observed learning outcomes would be a significant next step  despite these limitations  our findings provide insights into the ways in which learning has been articulated  studied  and measured by citizen science projects  they also provide information about the status of citizen science project evaluation in general  for example  an overwhelming majority of survey respondents expressed positive attitudes toward the importance of evaluation and the evaluation process  however  they also expressed a need for additional support and resources to conduct evaluations  nearly all respondents reported developing their own evaluation instruments  although most projects measured very similar outcomes  and the fact that very few projects were aware of resources available for guidance in conducting evaluations and locating evaluation instruments suggests that work needs to be done to disseminate tools and resources to the citizen science professional community  the comparison of intended learning outcomes described on citizen science project websites and the outcomes actually measured by projects highlights some interesting disconnects  for example  fewer than   of project websites stated  increasing interest in science and or the environment  as an intended outcome  yet interest in science was the most commonly measured outcome     across all projects in the online survey  the frequent measurement of interest in science may result from the relative ease of obtaining instruments to measure this outcome or it may be a proxy for measuring interest in the specific topic addressed by the project  e g   birds  butterflies  astronomy  weather   further  despite these reported measurements  few studies have published data about changes in interest  perhaps because they have not actually tried to measure it or because the typical citizen science participant  caucasian  older  highly educated  already demonstrates a high interest in science when joining a project  making it difficult to detect changes in interest over the course of project participation  brossard et al    thompson and bonney    however  ample opportunity exists for citizen science projects to increase interest in science and the environment by reaching individuals who are not already engaged  especially underserved audiences for whom access to informal science programming may be limited  bonney et al    flagg    additionally  projects that reach youth audiences via k  settings can minimize self selection bias and carry out quasi experimental studies to determine whether interest in science is leveraged through citizen science participation  bonney et al     as another example of a disconnect  self efficacy was seldom stated as an intended outcome in the website review and did not emerge as a major category of desired outcomes via the online survey  however  approximately   of survey respondents mentioned the concepts of  agency    confidence   or  efficacy  in open ended comments  as stated earlier  self perceptions of efficacy affect choices of activities that individuals pursue  how much effort they put toward them  and how long they persist in those pursuits  e g   bandura et al    weinberg et al     enhancing perceptions of efficacy may be the single most important outcome for many citizen science projects  thus we have included efficacy in our framework  yet another disconnect relates to motivation  few project websites mentioned motivation as an intended learning outcome  and our online survey showed that practitioners measured motivation primarily to understand reasons for participation  motivations change over time  however  and sustaining project participation requires an understanding of changing roles for individuals within a project and motivations for continued participation  more work also is needed to understand how motivations connect to self determination theory and serve psychological needs within the context of citizen science  for example  the desire to contribute to a project may be associated with a psychological need for competence  and the desire to engage socially with others may serve the psychological need for relatedness  studies that examine where motivations fall within the intrinsic extrinsic motivation continuum are needed to understand how motivation might influence sustained participation over time  our results also reiterate the inclination for practitioners to expect and measure gains in science content knowledge  typically through context specific instruments that measure mastery of project activities and program content rather than increased knowledge about the process of science or the nature of science  although some projects have begun to demonstrate outcomes related to  thinking scientifically   braschler et al    kountoupes and oberhauser   trumbull et al     a gap remains in our understanding of the potential for citizen science to influence deeper understanding of the process of science and the nature of science as well as the more complex facets of science inquiry  i e   critical thinking  reflection  and reasoning   future work should focus on the development of robust and contextually appropriate tools to better capture deep reflection and rich dialogue about nos  in perhaps our most surprising finding  nearly   of project websites in our study listed data collection as an intended outcome  yet across all projects combined  our online survey showed that skills related to data collection were the least measured outcome      these findings may reflect the difficulty of measuring attributes such as the acquisition of skills and the relative ease of measuring other constructs such as knowledge  interest  and attitude  this disconnect also represents a potential tension that exists within the citizen science field  particularly among contributory projects  the need for high confidence in data quality versus the dearth of studies that have assessed data collection skills  while several studies demonstrate that volunteers are able to collect data of similar quality to experts  these tend to be isolated examples  crall et al    danielsen et al     although a multitude of ways to validate citizen science data exist  see kosmala et al     tools and techniques are needed that can assess changes in participant data collection skills over time  additionally  the field needs to better understand whether citizen science participation can influence other important inquiry skills such as the ability to make phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  decisions regarding appropriate research methodologies  to use variables and control groups properly  and to evaluate evidence  and as attention is increased on the potential for citizen science to democratize science  further work should examine the extent to which it can support development or reinforcement of critical thinking skills that inform decision making and help to create an informed citizenry  also  in the new world of  big data   citizen science is well poised not only to provide the public with large and robust data sets but also to develop support systems so that users can understand how to effectively use these dynamic resources  such provisioning may facilitate new lines of research to better understand how participants engage with data sets and what meaning they hold for them  finally  in our website review  environmental stewardship was mentioned as an intended outcome by   of projects second only to data collection suggesting a strong desire for citizen science projects to influence individual behavior change  about one third of survey respondents reported measuring behavior change  but based on several open ended comments  some practitioners equated the act of participating in a project as a change in behavior  meaning that such change was indicated for all participants  recall  however  that we define behavior change as change that goes beyond project activities  further  tacit assumptions may exist about engagement in specific project activities leading to more global environmental behaviors  kollmuss and agyeman   vining et al     e g   the assumption that waterquality monitoring can lead to reducing carbon emissions  recycling  and conserving energy   intended behavioral outcomes should be directly connected to project content and activities  and the knowledge of how to perform these targeted behaviors should be made explicit to participants  phillips et al    toomey and domroese    while citizen science can likely impact behavior change  the development of effective implementation strategies and measurement of those outcomes are still in their infancy  conclusion thousands of citizen science projects exist around the word  reaching potentially millions of people  particularly in the observation and monitoring of species and habitats  theobald et al     such projects have the potential not only to engage individuals in the process of science  but also to encourage them to take positive action on behalf of the environment  cooper et al    mckinley et al     if such outcomes are to be achieved  project developers need to better understand how to design projects so that activities and educational learning opportunities support and align with feasible and realistic outcomes  shirk et al     this study has resulted in a framework to support citizen science practitioners in articulating and measuring learning outcomes for participants in their projects  the framework also should help to build capacity for practitioners seeking to conduct evaluations of citizen science projects by helping them to develop their program theory  i e   to identify underlying assumptions about how project activities affect expected outcomes  bickman   chen   funnell   funnell and rogers    in this regard  most evaluators recommend starting with articulation of project outcomes  then working backward to determine not only what can be achieved and how  but also what can be reasonably measured  center for the advancement of informal science education    toward that end  work proceeding in parallel to this research is developing generic  yet customizable  evaluation scales that are tested as valid and reliable in citizen science contexts and which align to the framework described above  see devise scales   jfe form sv cgxlglalyadfl   by adopting common learning outcomes and measures  the citizen science field can further evaluation capacity and begin to conduct cross programmatic analyses of citizen science projects to provide funders  stakeholders  and the general public with evidence based findings about the potential for citizen science to impact the lives of its volunteers  such studies also could provide critical information regarding why and how to achieve outcomes and under what conditions outcomes can be maximized  future work should support continued development of consistent measures that can be used across studies  particularly those that do not rely on self reports   becker klein et al    phillips et al    wells and lekies    continued professional development opportunities for citizen science practitioners to spearhead evaluations of projects will increase capacity for such endeavors  build a steady source of knowledge about impacts  and lead to improved project design  implementation  and sustainability for the field as a whole  initiation of in depth longitudinal studies that measure persistence of change over time would add understanding of the impacts of such experiences  schneider and cheslock    to the extent possible  more effort should be placed on studies that include experimental designs  random assignment  and control groups  such efforts will increase the field s ability to provide evidence for causal connections between citizen science participation and learning outcomes  additionally  continued research on learning outcomes should seek to incorporate social learning theories  which may be helpful in understanding how learning happens in citizen science and the mechanisms and processes that enable active learning  social learning theories such as cultural historical activity theory  vygotsky and cole    activity theory  engestr m    experiential learning  dewey   kolb    situated learning theory  lave and wenger    and communities of practice  wenger   are ideally suited for examining learning in citizen science because they emphasize the roles that participation in socially organized activities play in influencing learning  roth and lee   national research council    social learning theory may be particularly useful to consider when developing project activities and experiences  practitioners interested in incorporating social learning theories into citizen science project design  research  and evaluation should refer to the following studies for guidance  roth and lee     brossard et al      ballard et al      raddick et al      and jackson et al        phillips et al  a framework for articulating and measuring individual learning outcomes from participation in citizen science art    page  of  finally  as citizen science continues to grow  it will be important for the field to take a reflective look at its relative impact  and to evaluate whether appropriate questions are being asked by qualified researchers working across projects that involve diverse audiences and issues  such an analysis will be a first step in gathering critical evidence to demonstrate the potential of citizen science to truly democratize science  additional files the additional files for this article can be found as follows    appendix a  databases and search terms used to locate citizen science websites  doi  https   doi  org   cstp  s   appendix b  questions from online practitioner survey  doi   acknowledgements the work described in this paper is part of a larger study called devise  developing  validating  and implementing situated evaluation instruments   devise is based at the cornell lab of ornithology  with rick bonney as principal investigator and tina phillips as project director  we thank the project s co pis  kirsten ellenbogen and candie wilderman  project consultants edward deci  cecilia garibay  drew gitomer  kate haley goldman  joe heimlich  chris niemiec  and gil noam  and project advisory board members heidi ballard  rebecca jordan  bruce lewenstein  and karen oberhauser  we also thank the many practitioners who took the time to respond to our survey and queries  finally  we thank many lab of ornithology staff who helped along the way including jennifer shirk  matthew minarchek  marion ferguson  and holly faulkner  devise is supported by the national science foundation under grant no    any opinions  findings  and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the national science foundation  competing interests one of the authors of this paper  rick bonney  is editor inchief of this journal  he was not involved in the process of reviewing the paper other than responding to reviewers  comments  '"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c = docs_csguide.iloc[0][\"pre_cleantext\"]\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "oTPm6TnlsG4E",
        "outputId": "6d074fd1-55aa-4485-e91f-139a0ed0fc7e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'framework articulating measuring individual learning outcomes participation citizen science research paper framework articulating measuring individual learning outcomes participation citizen science tina phillips norman porticella mark constas rick bonney since first introduced mid term citizen science intentional engagement public scientific research seen phenomenal growth measured number projects developed people involved articles published addition contributing scientific knowledge many citizen science projects attempt achieve learning outcomes among participants however little guidance available practitioners regarding types learning supported citizen science measuring learning outcomes study provides empirical data understand intended learning outcomes first described informal science education field employed measured within citizen science field also present framework describing learning outcomes help citizen science practitioners researchers evaluators designing projects studying evaluating impacts first step building evaluation capacity across field citizen science keywords learning outcomes evaluation informal science learning cornell lab ornithology us cornell university us corresponding author tina phillips introduction citizen science defined public participation scientific research originally conceived method gathering large amounts data across time space bonney et al b decades even centuries citizen science contributed knowledge understanding far ranging scientific topics questions issues miller rushing et al recently citizen science practitioners conceive develop implement citizen science projects sought achieve science research outcomes also elicit learning behavioral outcomes participants bonney et al phillips et al many proponents citizen science argue participating directly scientific process via citizen science excellent way increase science knowledge literacy bonney et al fernandez gimenez et al jordan et al krasny bonney understand process science trautmann et al trumbull et al develop positive action behalf environment cornwell campbell cooper et al lewandowski oberhauser mckinley et al projects demonstrated achievement learning outcomes see bonney et al examples projects yet document robust outcomes increased interest science environment knowledge science process skills science inquiry stewardship behaviors bela et al bonney et al jordan et al phillips et al several factors may account lack demonstrated measurable learning outcomes first field citizen science still young specific outcomes defined described field therefore project designers may clear concepts types learning attempting foster addition measuring learning requires dedicated time resources expertise conducting social science research evaluations many citizen science projects lack result citizen science suffers lack quality project evaluations cross programmatic research phillips et al informal science learning community recently developed guidance including tools resources evaluating learning outcomes participation engagement informal science education ise activities friedman et al national research council tools resources relevant field citizen science many citizen science projects operate informal environments private residences parks science nature centers museums community centers afterschool programs online addition many citizen science projects funded ise initiatives projects expected foster lifelong science learning crain et al therefore tools developed measure learning outcomes resulting ise serve logical starting points evaluating outcomes citizen science participation phillips et al framework articulating measuring individual learning outcomes participation citizen science citizen science theory practice pp doi phillips et al framework articulating measuring individual learning outcomes participation citizen science art page objectives research presented paper determine describe types learning outcomes intended citizen science project developers examine alignment outcomes informal science learning frameworks guidelines develop present new framework articulating citizen science learning outcomes believe framework help citizen science practitioners design projects achieve measurable learning also hope framework facilitate cross programmatic research help citizen science field show projects impacting science society research sought determine extent citizen science learning outcomes evaluated across field first step toward overall goal deepening evaluation capacity citizen science community citizen science informal science learning educational underpinnings citizen science particularly involving adults draw heavily informal science education ise falk dierking refer free choice learning lifelong self directed learning occurs outside k classrooms two influential documents ise field provided starting point study framework evaluating impacts informal science education projects friedman et al supported national science foundation nsf first publication produced ise field described standard set learning outcomes referred impact categories could used systematically measure project level outcomes refer publication ise framework remainder paper major goal framework facilitate cross project cross technique comparisons impacts ise projects public audiences five impact categories knowledge awareness understanding science technology engineering math stem concepts processes careers engagement interest stem concepts careers attitude toward stem concepts processes careers skills based stem concepts processes behavior related stem concepts processes careers second document learning science informal environments people places pursuits national research council focuses characterizing cognitive affective social developmental aspects science learners termed lsie strands aspects science participation include interest motivation learn natural world application understanding science concepts acquisition skills related practice science reflecting science way knowing participating communicating science identifying oneself someone capable knowing using contributing science authors lsie strands noted concepts originated research time writing yet applied analyzed systematic venue significant overlap lsie strands ise framework impact categories shown table third ise document also contributed framing study inquiry group sponsored center advancement informal science education caise produced public participation scientific research defining field assessing potential informal science education bonney et al created first step toward developing organized methodology comparing outcomes across variety public participation scientific research ppsr projects p paper included rubric potential citizen science learning outcomes based ise framework examined ten nsf funded citizen science projects assess whether reported outcomes similar described ise framework figure participants citizen science engage large number activities designing studies collecting analyzing data disseminating project results project designers hope participants learn participation desired learning outcomes designed measured credit copyright pacific southwest region usfws flickr public domain phillips et al framework articulating measuring individual learning outcomes participation citizen science art page one result caise report realization citizen science practitioners measuring project outcomes varied ways making difficult crossprogrammatic research study collective impact field another result development project typology based level participant involvement scientific process bonney et al typology described contributory projects researcherdriven participants primarily focus data collection collaborative projects typically led researchers may include input participants phases scientific process designing methods analyzing data disseminating results co created projects involve participants aspects scientific process defining question interpreting data disseminating results figure typology allowed projects designed different reasons different ways grouped help researchers understand common outcomes three documents described served foundations articulating learning outcomes citizen science participation however lacked systematic empirical support study provides support ground truthing applying concepts within ise framework lsie strands bonney et al rubric field citizen science methods results research used two sources data structured review citizen science project websites online survey citizen science practitioners address following three questions learning outcomes intended desired citizen science practitioners extent outcomes align described field informal science education data source website review status evaluation citizen science learning outcomes across field data source online practitioner survey citizen science learning outcomes measured different projects data source online practitioner survey also conducted literature review uncover definitions descriptions elucidations learning outcomes identified research used results review along new understanding outcomes desired measured citizen science practitioners develop framework common learning outcomes citizen science field intended learning outcomes describe understand learning outcomes intended desired citizen science practitioners develop projects first identified individual projects conducting semi structured search following citizen science portals citizen science central citizenscience org informalscience informalscience org scistarter scistarter com citizen science alliance citizensciencealliance org national directory volunteer monitoring programs yosemite epa gov water volmon nsf last portal included projects sampled every fifth one project table comparison nsf framework lsie strands nsf framework category lsie strands knowledge awareness understanding measurable demonstration assessment change exercise awareness knowledge understanding particular scientific topic concept phenomena theory careers central project strand understanding come generate understand remember use concepts explanations arguments models facts related science engagement interest motivation science measurable demonstration assessment change exercise engagement interest particular scientific topic concept phenomena theory careers central project strand interest motivation experience excitement interest motivation learn phenomena natural physical world skills related science inquiry measurable demonstration development reinforcement skills either entirely new ones reinforcement even practice developing skills strand science exploration manipulate test explore predict question make sense natural physical world strand participate scientific activities learning practices others using scientific language tools attitudes toward science measurable demonstration assessment change exercise attitude toward particular scientific topic concept phenomena theory careers central project one capabilities relative areas attitudes refer changes relatively stable intractable constructs empathy animals habitats appreciation role scientists society attitudes toward stem cell research related strand identity think science learners develop identity someone knows uses sometimes contributes science also related strand reflection reflect science way knowing processes concepts institutions science process learning phenomena behavior measurable demonstration assessment change exercise behavior related stem topic behavioral impacts particularly relevant projects environmental nature since action desired outcome related strand skills participate scientific activities learning practices others using scientific language tools phillips et al framework articulating measuring individual learning outcomes participation citizen science art page listed multiple portals included total citizen science projects met criteria study inclusion open participation u canada online presence operational time search complete list databases search terms used locate citizen science projects available supplemental material paper appendix project websites gathered following information project name url contact information general goal statements learning objectives desired outcomes potential indicators learning nine percent project websites describe intended learning outcomes e g projects stated goals purely scientific nature remaining projects described least one coded goal statement learning objective one major categories outlined ise framework knowledge engagement skills attitude behavior sub codes outlined assessment rubric bonney et al several projects described multiple learning outcomes cases distinct outcome coded separately example great lakes worm watch states goal increasing scientific literacy public understanding role exotic species ecosystems change objectives provide tools resources citizens actively contribute development database documenting distributions exotic earthworms impacts across region well training resources educators help build understanding methods results scientific research exotic earthworms forest ecosystems ecology text goal statement learning objectives left coded outcomes categories right increasing scientific literacy public understanding content knowledge citizens actively contribute development database data collection monitoring data submission help build understanding methods results scientific research nature science knowledge results coding project goals objectives presented table reveal number aspirational learning outcomes projects ranged zero many seven projects including least two majority projects focus influencing skills related data collection monitoring intended outcomes projects often stated volunteers gain data collection reporting skills second frequently stated intended learning outcome projects understanding content knowledge e g volunteers learn macroinvertebrates stream health third common intended outcome increased environmental stewardship typically includes type behavior change e g engage watershed residents protecting water quality specified projects intended learning outcomes mentioned much less frequently table count specified learning outcomes coded citizen science project websites percentages represent proportion projects described stated outcome several projects stated one outcome stated outcomes project websites count projects stating outcome n percentage projects stating outcome data collection monitoring content knowledge environmental stewardship education goal specified attitude awareness nature science data analysis interest environment civic action submitting data interest science community health communication skills using technology science careers designing studies phillips et al framework articulating measuring individual learning outcomes participation citizen science art page including increases knowledge nature science data analysis skills interest environment civic action data submission communication skills use technology science careers study design also shifts attitude awareness considering projects intended learning outcomes stated ise framework impact categories represented suggesting strong alignment learning outcomes desired citizen science participants participants informal science learning community generally status citizen science project evaluation uncover status evaluation citizen science learning outcomes across field conducted online survey citizen science practitioners march delivered via survey monkey survey contained questions including closed ended questions predetermined options including remaining five questions open ended providing text boxes answers one question asked respondents classify project according three model typology citizen science developed bonney et al required response additional questions focused duration project approximate number participants type training participants received respondents also asked type evaluation ever conducted project details evaluations conducted learning outcomes described ise framework measured types outcomes measured complete set survey questions available supplemental material appendix b following approval cornell university institutional review board sent email invitation potential respondents describing goal survey explaining participation voluntary confidential two reminder emails sent approximately two four weeks following initial invitation informed consent statement included start survey potential respondents recruited via citizenscience org listserv citsci discussion l anyone could join time recruitment approximately members members listserv project leaders multiple list members likely represented single project making difficult know actual number projects represented listserv members survey closed made sure responding projects included previously described website review obtain much overlap two datasets possible survey completed respondents representing unique projects projects multiple entries case first entry included respondents failed include information project name optional ten unique projects also represented project website data remaining ten projects responded online survey website review either longer operational us canada web presence majority projects operating years nearly half fewer participants questions optional response rates varied different survey items results revealed respondents undertaken type project evaluation half evaluations administered internal project staff measure project outcomes impacts mostly using data collected surveys one third respondents reported conducting post pre posttest evaluation designs reasons conducting project evaluations included gauging participant learning identifying project strengths weaknesses obtaining additional funding support promoting project broadly providing recommendations project improvement addition asking project learning outcomes described next section survey also asked aspects project evaluated two thirds participants reported measuring satisfaction enjoyment project followed motivation participate evaluation project outputs numbers participants web hits journal articles amount data collected measured outcomes included scientific conservation effectiveness workshops trainings data quality community capacity building social policy change another open ended question asked respondents please best provide name description instrument e g views science technology survey used collect evaluation data even developed instrument respondents question three used pre existing validated instrument majority respondents developed instruments house external evaluator develop original instruments handful respondents replied survey monkey data collection platform opposed describing evaluation instrument mentioned tools gps units calipers instruments used project others stated understand question asked overall satisfaction evaluations half respondents expressed agreement strong agreement evaluations high quality evaluation findings informative project developers recommendations evaluation implemented project improved result evaluation learned lot evaluation felt confident could personally conduct evaluation future survey respondents also asked aspects evaluation process would like assistance highest priority help developing goals objectives indicators followed creating finding appropriate survey instruments help analyzing interpreting data help data collection participants also asked specific resources would helpful conducting evaluations common replies database potential surveys data collection instruments sample evaluation reports citizen science examples evaluation designs entry level guide conducting evaluations phillips et al framework articulating measuring individual learning outcomes participation citizen science art page finally respondents presented list eight different online organizations support provide resources evaluation asked often access surprisingly majority respondents never heard resources organizations exception citizenscience org used respondents rarely opposed frequently sometimes never results show range evaluation efforts positive attitude toward evaluation findings among citizen science practitioners also need knowledge accessibility evaluation tools resources measurement learning outcomes respondents reported conducted evaluations asked recent evaluation project broad categories learning outcomes evaluated responses question based ise framework broad impact categories aggregated results across projects revealed interest engagement science commonly measured outcome followed knowledge science content behavior change resulting participation attitudes toward science process content careers environment accounted respectively measured learning outcomes science inquiry skills e g asking questions designing studies data collection data analysis using technology least commonly measured outcomes across projects open ended question types learning outcomes respondents also described measuring motivation self efficacy confidence participate science environmental activities considering differences categories learning outcomes measured within project types contributory projects respondents conducted evaluations reported measuring interest science frequently skills science inquiry least frequently two thirds collaborative projects n measured content knowledge followed interest behavior change attitudes skills nine survey respondents represented co created projects conducted evaluations skills science inquiry measured often responses combined across projects separated among project types summarized figure earlier paper showed majority citizen science project websites described intended learning outcomes similar ise framework although always using language results online practitioner survey added ground truthing ise framework respondents described attempts measure outcomes albeit varying degrees open ended responses highlighted need emphasize efficacy important learning outcome citizen science survey respondents also made clear additional resources needed help formulate measure learning outcomes framework articulating measuring common learning outcomes citizen science addition synthesizing comparing empirical results website review practitioner survey describe intended measured learning outcomes used key word searches conduct review peer reviewed articles focused defining measuring learning outcomes data review facilitated conceptualization contextualization several impact categories presented ise framework make relevant citizen science figure measured learning outcomes online survey citizen science practitioners reported conducted sort evaluation n phillips et al framework articulating measuring individual learning outcomes participation citizen science art page particular environmentally based projects example outcomes uncovered research skills science inquiry map well categories definitions ise framework outcomes attitude required clarification new framework thus based empirical data contributions literature includes following learning outcomes interest science environment selfefficacy science environment motivation science environment knowledge nature science skills science inquiry behavior stewardship figure framework help citizen science practitioners consider commonly desired achievable learning outcomes designing projects however emphasize single project try achieve measure even outcomes set unreasonable expectations project evaluation also note framework exhaustive indeed citizen science continues expand new research inevitably reveal learning outcomes important articulate measure describe outcome within framework highlighting explained broad educational field also providing examples used published studies citizen science outcomes hierarchical beginning interest science environment build reinforce interest science environment define interest degree individual assigns personal relevance science environmental topic endeavor within ise hidi renninger treat interest multi faceted construct encompassing cognitive thinking affective feeling behavioral domains across four phases adoption triggered situational interest typically stimulated particular event requiring support others maintained situational interest sustained personally meaningful activities experiences emerging individual interest characterized positive feelings self directed pursuit engaging certain activities well developed individual interest leading enduring participation application knowledge definition interest compatible hidi renninger later phases interest development characterized positive feelings increasing investment learning particular topic interest science considered key driver pursuing science careers youth maltese tai tai et al sustained lifelong learning engagement adults falk et al hidi renninger time type interest lead sustained engagement motivation support identity development science learner fenichel schweingruber national research council interest noted important precursor deeper engagement democratic decision making processes regarding science technology mejlgaard stares figure framework articulating measuring individual learning outcomes participation citizen science phillips et al framework articulating measuring individual learning outcomes participation citizen science art page although interest considered attitudinal structure see bauer et al fenichel schweingruber sturgis allum equating interest attitudes avoided attitude broad construct encompassing related distinct sub constructs efficacy interest curiosity appreciation enjoyment beliefs values perseverance motivation engagement identity osborne et al interest also used synonymously engagement friedman et al mccallie et al point engagement yet well defined multiple meanings within literature particularly ise citizen science projects especially repeated visits experiences norm lend deeper sustained interest science environment yet studies looked interest outcome find mixed results price lee reported increased interest science among citizen sky observers especially among participants engaged online social activities crall et al examined general interest science reason participation citizen science suggested interest driving force joining project interest specific nature based topics e butterflies seen driver engagement also motivator adding increasingly complex data protocols french garden butterflies watch project cosquer et al research shown interest use natural resources strong determinant future sustained involvement decision making process management natural resources danielsen et al studies appears examining interest science broadly may less effective measuring specific science topics however audience pre existing interests specific topics may change significantly participation self efficacy science environment another important outcome studying learning self efficacy e person beliefs capabilities learn specific content perform particular behaviors bandura research found self efficacy affects individual choice effort persistence activities bandura schunk individuals feel efficacious put effort activities persist longer doubt abilities self efficacy sometimes referred perceived competence self determination theory perceived behavioral control ajzen theory planned behavior ajzen berkowitz et al treat self efficacy essential component environmental citizenship along motivation awareness dependent individual belief sufficient skills knowledge opportunity bring positive change personal lives community context citizen science self efficacy extent learner confidence ability participate science environmental activity study involving classrooms middle school students participating horseshoe crab citizen science project showed greater gains self efficacy control group hiller online astronomy project however researchers found significant decrease efficacy toward science possibly owing heightened awareness much participants know topic price lee crall et al determined selfefficacy important carrying principal activities project also potential individuals carry future activities related environmental stewardship working participatory action project salal harvesters ballard belsky found process co developing implementing different experiments increased workers self efficacy regarding skills scientific research although efficacy called directly ise framework considered part lsie strand identity learner national research council self efficacy also mentioned project leaders online survey thus appears important potential outcome citizen science participation motivation science environment motivation multi faceted complex attitudinal construct describes form goal setting achieve behavior end result lsie strands national research council include motivation sustain science learning individual lifetime important aspect learning informal environments literature volunteerism frames motivation important factor effective recruitment accurate placement volunteer satisfaction retention clary snyder esmond et al dozens theories motivation two perspectives seem especially relevant volunteerism citizen science first volunteer functions inventory vfi developed clary et al examines behaviors help individuals achieve personal social goals clary et al categories motivation include values importance helping others understanding activities fulfill desire learn social influence significant others career exploring job opportunities work advancement esteem improving personal self esteem protective escaping negative feelings wright et al studied motivations birders south africa using modified version vfi found five categories motivation important recreation nature values personal growth social interactions project organization second perspective comes self determination theory sdt treats motivation explanatory variable meeting basic psychological needs e competency relatedness autonomy describes different types motivations falling continuum intrinsic extrinsic ryan deci b according sdt individuals likely continue pursuing goal extent perceive intrinsic value pursuit goal e extent experience satisfaction performing associated behaviors versus performing behaviors comply extrinsic goals conforming social pressures fear receiving rewards although sdt phillips et al framework articulating measuring individual learning outcomes participation citizen science art page help practitioners better understand psychological needs behind participation published studies used sdt context citizen science one exception paper nov et al used sdt social movement participation models examination three digital citizen science projects researchers found intrinsic motivation one four drivers influenced quantity participation affect quality participation context citizen science motivation serve input outcome e understand basis motivation ise citizen science experiences input sustain motivation continue participating long time periods outcome however studies examined reasons participation desire contribute see bell et al hobbs white mccaffrey raddick et al reed et al rather motivations describe psychological underpinnings behavior e g makes feel good examination motivation online projects rotman et al described complex changing framework motivation influenced participant interest recognition attribution although several studies purported examine motivation defined studied uniformly throughout field citizen science nevertheless major consensus appears motivation citizen science like volunteer activities dynamic complex content process nature science knowledge included within ise framework impact category awareness knowledge understanding several subcategories knowledge understanding science content knowledge understanding science processes knowledge nature science knowledge science content refers understanding subject matter e facts concepts knowledge process science refers understanding methodologies scientists use conduct research example hypothetico deductive model scientific method knowledge nature science nos refers understanding epistemological underpinnings scientific knowledge generated sometimes presented postpositivist perspective lederman nos addresses tenets science tentativeness empiricism subjectivity creativity social cultural influence observations inferences theories laws see lederman lederman et al improving scientific literacy understanding nos process science generally considered important understanding basic content subject matter american association advancement sciences national research council ngss knowledge process science regular component well established assessments science knowledge national science board despite recognition attempts measure science literacy within ise field fall back content knowledge e rote memorization facts rather knowledge nature process science bauer et al shamos indeed citizen science evaluations typically emphasized measuring gains topical content knowledge opposed science process knowledge mixed results ballard huntsinger bonney braschler et al brewer devictor et al evans et al fernandez gimenez et al jordan et al kountoupes oberhauser krasny bonney phillips et al sickler et al trumbull et al trumbull et al overdevest et al find significant increase project participant knowledge streams water quality probably new volunteers already highly knowledgeable subject matter price lee actually found decrease science content knowledge among project participants likely owing exaggerated notions participants self perceived content knowledge starting project realization much know participating project however studies used measures process science assess impacts citizen science project participation jordan et al brossard et al used adaptations science engineering indicators showed gains understanding process science result citizen science participation contrast ballard et al used interview data show evidence salal harvesting project increased local people understanding scientific process ecosystem part p significant increases understanding process science participation stream water quality monitoring project reported cronin messemer however study small sample size may limit generalizability results likewise citizen science projects attempted study understanding nos jordan et al found evidence change knowledge nos using pre post scenario based questions invasive species project price lee found little evidence project participation influenced epistemological beliefs nos owing fact epistemological beliefs personal beliefs thus harder change participating one citizen science project p findings suggest citizen science effectively demonstrate gains content knowledge long way go positively establish increases understanding science process nos skills science inquiry skills science inquiry observable practices transferred daily life asking answering questions collecting data developing using models planning carrying investigations reasoning analyzing interpreting data constructing explanations communicating information using evidence argumentation national academies science engineering medicine ngss lead states hands nature many environmentally based citizen science projects makes particularly well suited influence development phillips et al framework articulating measuring individual learning outcomes participation citizen science art page reinforcement certain science inquiry skills including asking questions designing studies collecting analyzing interpreting data discussing disseminating results bonney et al jordan et al phillips et al trautmann et al top priorities many practitioners helping participants learn follow protocols exercise accurate data collection skills practices directly influence data quality fieldwide emphasis data quality likely comes large percentage contributory scientist driven projects key goal gathering data sufficient quality add existing knowledge base publication peer reviewed journals consequently many citizen science projects effectively influence skills related data sample specimen collection identification organisms instrument use sampling techniques many projects also engage participants use various technological tools gps units digital thermometers water conductivity instruments rain gauges nets smartphones name figure researchers begun study skill acquisition citizen science becker et al showed increase ability estimate noise levels increasing participation widenoise soundscape project operated mobile devices increases youths self reported science inquiry skills perceived ability identify pond organisms develop testable hypotheses participation driven discover also reported meyer et al sullivan et al describe use communication prompts strategies steer birders toward providing useful data essentially change birding habits ebird participants increase data quality using theory legitimate peripheral participation mugar et al used practice proxies form virtual trace ethnography increase accuracy data annotation among new members additionally projects successfully conducted small scale studies compare volunteercollected data collected experts thereby creating baseline metric assessing participants skills see crall et al jordan et al schmeller et al another hallmark citizen science collection large publicly available data sets rich interactive data visualizations many projects provide data visualizations may seek enhance skills related data interpretation e ability effectively comprehend information meaning often presented graphical form devictor et al one studies examining data interpretation citizen science thompson bonney showed even majority active users ebird properly use extensive array dataanalysis tools numerous studies educational research shown assessing type reasoning skills needed data interpretation requires asking series reflective questions determine one justification underlying reasoning e g ayala et al roth roychoudhury inquiry skills study design communication critical thinking decision making skills critically evaluating results less studied within citizen science literature crall et al used open ended questions determine whether engaging invasive species project improved abilities participants explain scientific study write valid research question provide valid sampling design researchers noted positive gains ability explain scientific study char et al found increase pre post training ability coasst volunteers correctly weigh evidence determine whether contained sufficient information accurately identifying species studies show potential studying citizen science participants evaluate development complex science inquiry skills studies infancy behavior stewardship behavior change development environmental stewardship among sought outcomes science environmental education programs schools bodzin heimlich et al kollmuss agyeman stern stern et al vining et al theories examining various determinants environmental behavior include espousing links knowledge attitude behavior hungerford volk kollmuss agyeman osbaldiston schott schultz attitudes values ajzen fishbein ajzen behavior modification intervention de young nature exposure kaplan kellert wilson ulrich wilson define behavior stewardship measurable actions resulting engagement citizen science external protocol activities specific projectbased skills citizen science project example collecting water quality data may new behavior project participant data collection part project protocol measured new skill rather new behavior however somebody decreasing water usage result participating water quality monitoring project would example behavior change literature review identified five categories behavior stewardship interest citizen science field provide definitions global stewardship behaviors place based behaviors new participation community civic action transformative lifestyle changes global stewardship refers deliberate changes behavior minimize someone individual ecological footprint collectively global influence e g installing low flow shower heads recycling purchasing energy efficient appliances place based behaviors refer observable actions directly maintain restore improve educate health ecosystem beyond activities citizen science project e g removing invasive species cleaning trash eliminating pesticide use purchasing locally grown food engaging outreach youth groups new participation defined engagement science environmental activities organizations projects spurred participation phillips et al framework articulating measuring individual learning outcomes participation citizen science art page citizen science project community civic action refers participation civic governmental cultural affairs solve problems local regional national level actions could include donating environmental organizations signing petitions speaking harmful environmental practices recruiting others participate environmental causes finally transformative lifestyle changes efforts require strong front cost long term commitment maintain investing hybrid vehicle becoming vegetarian pledging use mass transit whenever possible citizen science projects especially dealing environmental topics typically hands occur local environments require repeated monitoring data gathering making natural conduits affecting behavior change wells lekies however research limited results mixed regarding actual influence citizen science behavior change example study examining two different projects one pollinators one coyotes toomey domroese show participants engage new activities change gardening practices otherwise take part advocacy change environmental stewardship practices crall et al found significant differences current planned behavior result participating invasive species project using self reported measures actual behavior change well described using case study approach oberhauser prysby claim participants monarch larva monitoring project work preserve habitat many levels advocating environmentally friendly mowing regimen insect friendly pest control challenging parking lot building road development projects threaten monarch habitat p however source data accompanying methodologies clearly described cornwell campbell also used case study approach able document advocacy political action volunteers directly benefited sea turtle conservation evans et al documented locally place based stewardship bird breeding program projects showed change place based stewardship practices jordan et al study human health effects industrial hog operations wing et al describe actions taken community groups engage decision making addresses local environmental injustices taken together examples provide evidence citizen science may influence behavior stewardship robust methodologies needed establish causation plenty anecdotal data also highlight examples behavior change published exist gray literature discussion results research conducted systematic review citizen science project websites survey practitioners design implement citizen science projects confirm relevance applicability three ise documents friedman et al national research council bonney et al framing intended learning outcomes citizen science participants informed research along systematic literature review modified contextualized documents create new framework contains definitions articulations learning outcomes citizen science field believe framework provides robust starting point setting learning goals objectives citizen science projects designing projects meet objectives research limitations however first co created collaborative project categories represented online practitioner survey small sample sizes generalizing types learning outcomes intended project types challenging also unclear whether distribution across project types online survey reflects actual distribution contributory collaborative co created projects across u canada disproportionate number contributory projects received responded survey request made additional effort recruit additional collaborative co created project respondents thus response bias may issue also made effort ensure projects responded practitioner survey included website review project level data two sources examined together may shown convergence divergence intended versus measured outcomes beyond scope work may violated confidentiality conditions finally work descriptive study based largely self reports case practitioner survey published desired outcomes case website review robust inferential studies examine figure many citizen science project designers hope collect important scientific information also help project participants gain skills scientific reasoning team volunteers public lab non profit environmental science community launch weather balloon data collected via balloon used mapping surveys figuring measure participants learning conduct research challenge citizen science field credit alan kotok flickr cc phillips et al framework articulating measuring individual learning outcomes participation citizen science art page field wide relationships causal factors project characteristics observed learning outcomes would significant next step despite limitations findings provide insights ways learning articulated studied measured citizen science projects also provide information status citizen science project evaluation general example overwhelming majority survey respondents expressed positive attitudes toward importance evaluation evaluation process however also expressed need additional support resources conduct evaluations nearly respondents reported developing evaluation instruments although projects measured similar outcomes fact projects aware resources available guidance conducting evaluations locating evaluation instruments suggests work needs done disseminate tools resources citizen science professional community comparison intended learning outcomes described citizen science project websites outcomes actually measured projects highlights interesting disconnects example fewer project websites stated increasing interest science environment intended outcome yet interest science commonly measured outcome across projects online survey frequent measurement interest science may result relative ease obtaining instruments measure outcome may proxy measuring interest specific topic addressed project e g birds butterflies astronomy weather despite reported measurements studies published data changes interest perhaps actually tried measure typical citizen science participant caucasian older highly educated already demonstrates high interest science joining project making difficult detect changes interest course project participation brossard et al thompson bonney however ample opportunity exists citizen science projects increase interest science environment reaching individuals already engaged especially underserved audiences access informal science programming may limited bonney et al flagg additionally projects reach youth audiences via k settings minimize self selection bias carry quasi experimental studies determine whether interest science leveraged citizen science participation bonney et al another example disconnect self efficacy seldom stated intended outcome website review emerge major category desired outcomes via online survey however approximately survey respondents mentioned concepts agency confidence efficacy open ended comments stated earlier self perceptions efficacy affect choices activities individuals pursue much effort put toward long persist pursuits e g bandura et al weinberg et al enhancing perceptions efficacy may single important outcome many citizen science projects thus included efficacy framework yet another disconnect relates motivation project websites mentioned motivation intended learning outcome online survey showed practitioners measured motivation primarily understand reasons participation motivations change time however sustaining project participation requires understanding changing roles individuals within project motivations continued participation work also needed understand motivations connect self determination theory serve psychological needs within context citizen science example desire contribute project may associated psychological need competence desire engage socially others may serve psychological need relatedness studies examine motivations fall within intrinsic extrinsic motivation continuum needed understand motivation might influence sustained participation time results also reiterate inclination practitioners expect measure gains science content knowledge typically context specific instruments measure mastery project activities program content rather increased knowledge process science nature science although projects begun demonstrate outcomes related thinking scientifically braschler et al kountoupes oberhauser trumbull et al gap remains understanding potential citizen science influence deeper understanding process science nature science well complex facets science inquiry e critical thinking reflection reasoning future work focus development robust contextually appropriate tools better capture deep reflection rich dialogue nos perhaps surprising finding nearly project websites study listed data collection intended outcome yet across projects combined online survey showed skills related data collection least measured outcome findings may reflect difficulty measuring attributes acquisition skills relative ease measuring constructs knowledge interest attitude disconnect also represents potential tension exists within citizen science field particularly among contributory projects need high confidence data quality versus dearth studies assessed data collection skills several studies demonstrate volunteers able collect data similar quality experts tend isolated examples crall et al danielsen et al although multitude ways validate citizen science data exist see kosmala et al tools techniques needed assess changes participant data collection skills time additionally field needs better understand whether citizen science participation influence important inquiry skills ability make phillips et al framework articulating measuring individual learning outcomes participation citizen science art page decisions regarding appropriate research methodologies use variables control groups properly evaluate evidence attention increased potential citizen science democratize science work examine extent support development reinforcement critical thinking skills inform decision making help create informed citizenry also new world big data citizen science well poised provide public large robust data sets also develop support systems users understand effectively use dynamic resources provisioning may facilitate new lines research better understand participants engage data sets meaning hold finally website review environmental stewardship mentioned intended outcome projects second data collection suggesting strong desire citizen science projects influence individual behavior change one third survey respondents reported measuring behavior change based several open ended comments practitioners equated act participating project change behavior meaning change indicated participants recall however define behavior change change goes beyond project activities tacit assumptions may exist engagement specific project activities leading global environmental behaviors kollmuss agyeman vining et al e g assumption waterquality monitoring lead reducing carbon emissions recycling conserving energy intended behavioral outcomes directly connected project content activities knowledge perform targeted behaviors made explicit participants phillips et al toomey domroese citizen science likely impact behavior change development effective implementation strategies measurement outcomes still infancy conclusion thousands citizen science projects exist around word reaching potentially millions people particularly observation monitoring species habitats theobald et al projects potential engage individuals process science also encourage take positive action behalf environment cooper et al mckinley et al outcomes achieved project developers need better understand design projects activities educational learning opportunities support align feasible realistic outcomes shirk et al study resulted framework support citizen science practitioners articulating measuring learning outcomes participants projects framework also help build capacity practitioners seeking conduct evaluations citizen science projects helping develop program theory e identify underlying assumptions project activities affect expected outcomes bickman chen funnell funnell rogers regard evaluators recommend starting articulation project outcomes working backward determine achieved also reasonably measured center advancement informal science education toward end work proceeding parallel research developing generic yet customizable evaluation scales tested valid reliable citizen science contexts align framework described see devise scales jfe form sv cgxlglalyadfl adopting common learning outcomes measures citizen science field evaluation capacity begin conduct cross programmatic analyses citizen science projects provide funders stakeholders general public evidence based findings potential citizen science impact lives volunteers studies also could provide critical information regarding achieve outcomes conditions outcomes maximized future work support continued development consistent measures used across studies particularly rely self reports becker klein et al phillips et al wells lekies continued professional development opportunities citizen science practitioners spearhead evaluations projects increase capacity endeavors build steady source knowledge impacts lead improved project design implementation sustainability field whole initiation depth longitudinal studies measure persistence change time would add understanding impacts experiences schneider cheslock extent possible effort placed studies include experimental designs random assignment control groups efforts increase field ability provide evidence causal connections citizen science participation learning outcomes additionally continued research learning outcomes seek incorporate social learning theories may helpful understanding learning happens citizen science mechanisms processes enable active learning social learning theories cultural historical activity theory vygotsky cole activity theory engestr experiential learning dewey kolb situated learning theory lave wenger communities practice wenger ideally suited examining learning citizen science emphasize roles participation socially organized activities play influencing learning roth lee national research council social learning theory may particularly useful consider developing project activities experiences practitioners interested incorporating social learning theories citizen science project design research evaluation refer following studies guidance roth lee brossard et al ballard et al raddick et al jackson et al phillips et al framework articulating measuring individual learning outcomes participation citizen science art page finally citizen science continues grow important field take reflective look relative impact evaluate whether appropriate questions asked qualified researchers working across projects involve diverse audiences issues analysis first step gathering critical evidence demonstrate potential citizen science truly democratize science additional files additional files article found follows appendix databases search terms used locate citizen science websites doi https doi org cstp appendix b questions online practitioner survey doi acknowledgements work described paper part larger study called devise developing validating implementing situated evaluation instruments devise based cornell lab ornithology rick bonney principal investigator tina phillips project director thank project co pis kirsten ellenbogen candie wilderman project consultants edward deci cecilia garibay drew gitomer kate haley goldman joe heimlich chris niemiec gil noam project advisory board members heidi ballard rebecca jordan bruce lewenstein karen oberhauser also thank many practitioners took time respond survey queries finally thank many lab ornithology staff helped along way including jennifer shirk matthew minarchek marion ferguson holly faulkner devise supported national science foundation grant opinions findings conclusions recommendations expressed paper authors necessarily reflect views national science foundation competing interests one authors paper rick bonney editor inchief journal involved process reviewing paper responding reviewers comments'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d = docs_csguide.iloc[0][\"cleanText\"]\n",
        "d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfH7lI7AwECq"
      },
      "source": [
        "# 🚀 Exporting the dataframe as a  CSV File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4tF1whe1jXX"
      },
      "source": [
        "#### Install pandas 1.1.5. This code doesn't work with the update pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGTGdcWv0lb8",
        "outputId": "a9780921-c93e-4c4a-9f04-bbdb1698eb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proceed (y/n)? "
          ]
        }
      ],
      "source": [
        "!pip uninstall pandas -qq\n",
        "!pip install pandas==1.1. -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO8wvw9Q0zZU"
      },
      "outputs": [],
      "source": [
        "# exporting the csv file \n",
        "docs_csguide.to_csv('docs_csguide.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fxZwfdj1Uc1"
      },
      "source": [
        "If you want to download in a Google Drive Folder here is the code: \n",
        "\n",
        "```\n",
        "docs_csguide.to_csv(r'/gdrive/MyDrive/Colab Notebooks/HEIDI_docs_report_articles/guide_articles_clean_pages/textCS.csv', index=False, header= True)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1UiJ2v3K6UE"
      },
      "source": [
        "## Option 3. List of Token lemmatized "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBlVrXZ0LRwn"
      },
      "outputs": [],
      "source": [
        "fullText= docs_csguide['lemma_pipe'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unzzTpE5Cu4x"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfbS5hDaVi2w"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 🔷 Topic Modelling - LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn0P1f-AdwES"
      },
      "source": [
        "## 📙 Libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry-h_ZU4dgkJ"
      },
      "outputs": [],
      "source": [
        "# Install\n",
        "!pip install -qq -U gensim\n",
        "!pip install spacy -qq\n",
        "!pip install pyLDAvis -qq\n",
        "!pip install -qq -U gensim\n",
        "!python -m spacy download en_core_web_md -qq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij2cdfb5Ex4f"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import pandas as pd\n",
        "\n",
        "# NLP\n",
        "sns.set()\n",
        "import spacy\n",
        "import en_core_web_md\n",
        "\n",
        "#visulization libraries\n",
        "from spacy import displacy\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models # don't skip this\n",
        "pyLDAvis.enable_notebook()# Visualise inside a notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Topic modelling\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Hide warnings \n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUeLsvaleY-g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tlyKozB1Jlt"
      },
      "source": [
        "Feed the LDA model into the pyLDAvis instance\n",
        "- lda_viz = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
        "- lda_viz = pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, dictionary)\n",
        "\n",
        "- https://github.com/Poojaajitkumar/nlp_masterthesis/blob/master/LDATopicModelling_test.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKgd2_iL1dAB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjIJUvQkX9Ux"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "text = docs_csguide.copy()\n",
        "text.head()\n",
        "text.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Eo3GVw9Lc_u"
      },
      "outputs": [],
      "source": [
        "# Our spaCy model:\n",
        "nlp = en_core_web_md.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3b8I2UFL7aE"
      },
      "outputs": [],
      "source": [
        "# Tags I want to remove from the text\n",
        "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cJepi0pNGgJ"
      },
      "outputs": [],
      "source": [
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(text['summary']):\n",
        "   proj_tok = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
        "   tokens.append(proj_tok)\n",
        "\n",
        "# Add tokens to new column\n",
        "reports['tokens'] = tokens\n",
        "reports['tokens']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWHWVUovW89-"
      },
      "source": [
        "# Option 1. LDA Thesis Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRA150NlYgDJ"
      },
      "outputs": [],
      "source": [
        "docs_csguide.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuTSwkoSYZov"
      },
      "outputs": [],
      "source": [
        "# Convert to list\n",
        "corpus = docs_csguide['cleanText'].values.tolist()\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoMFGFanQgng"
      },
      "source": [
        "## Tokenize words with Gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOY2GR7qYOIX"
      },
      "outputs": [],
      "source": [
        "Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rb11ioDMSQD"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3Dj29g6YI_5"
      },
      "outputs": [],
      "source": [
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8yaGIFVRacV"
      },
      "outputs": [],
      "source": [
        "docs_csguide.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWfQtkrPgxYg"
      },
      "source": [
        "# Create list of word tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGqsDhUlnh2K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZX460rTBUVT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSC2TEC57u7H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWEe0MeTkfE9"
      },
      "source": [
        "## Creating a list from appen string. \n",
        "\n",
        "note: the output is different than the other\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6F1euwglQWb"
      },
      "outputs": [],
      "source": [
        "# creating text string\n",
        "mylist = []\n",
        "for string in docs_csguide['cleanText']:\n",
        "    mylist.append(string)\n",
        "mylist\n",
        "\n",
        "corpus= str(mylist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WXFytOAqCrl"
      },
      "outputs": [],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQyd1QCraov7"
      },
      "outputs": [],
      "source": [
        "# tokens using gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsiClu-LX2rb"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF8SvOu68hQv"
      },
      "source": [
        "## 🔽"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arH2CYAjZyDd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKgQeZR3X3sB"
      },
      "outputs": [],
      "source": [
        "tokenslist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfV5RW-yZ4pH"
      },
      "outputs": [],
      "source": [
        "print(tokenslist [:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_D4aIYDkgbN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NOp3_WMcJJ3"
      },
      "source": [
        "# Building the models N-Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdtcBtJP3Gbd"
      },
      "source": [
        "- bigram = gensim.models.phrases.Phrases(texts)\n",
        "- texts = [bigram[line] for line in texts]\n",
        "- texts = [bigram[line] for line in texts]\n",
        "- Check: https://www.kaggle.com/faressayah/text-analysis-topic-modelling-with-spacy-gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44vM42wqcCFq"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(tokenslist, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[tokenslist], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[tokenslist[0]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UtnkOBpcSSS"
      },
      "outputs": [],
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6iyJURZcxyI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = tokenslist.copy()\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "#python3 -m spacy download en\n",
        "#nlp = spacy.download('en')\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Uak0dREALDd"
      },
      "outputs": [],
      "source": [
        "bigrams = pd.DataFrame(data_words_bigrams .sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H71KQ9lHAAYl"
      },
      "outputs": [],
      "source": [
        "# creation of the bigrams dataframe\n",
        "bigrams_series = (pd.Series(nltk.ngrams(txt_tokens, 2)).value_counts())\n",
        "bigrams = pd.DataFrame(bigrams_series.sort_values(ascending=False))\n",
        "bigrams = bigrams.reset_index().rename(columns={'index': 'bigram', 0:'count'})\n",
        "bigrams['bigram'] = bigrams['bigram'].astype(str)\n",
        "bigrams['bigram'] = bigrams['bigram'].str.replace(\"(\", '')\n",
        "bigrams['bigram'] = bigrams['bigram'].str.replace(\")\", '')\n",
        "bigrams['bigram'] = bigrams['bigram'].str.replace(\",\", '_')\n",
        "bigrams['bigram'] = bigrams['bigram'].str.replace(\" \", '')\n",
        "bigrams['bigram'] = bigrams['bigram'].str.replace(\"'\", '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSEJiT2T_Rgj"
      },
      "source": [
        "❗ REVISAR LA ORIFIAL FORMULA DE DATA_WORDS_NOSTOPS\n",
        "\n",
        "- # Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_MnrkvjN_j2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUb2ipsP_OKI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6cu4uwcz6Gj"
      },
      "source": [
        "# Othe rcpde\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNmPepMo0BhJ"
      },
      "source": [
        "https://www.kaggle.com/morrisb/compare-lda-topic-modeling-in-sklearn-and-gensim\n",
        "\n",
        "Compare LDA (Topic Modeling) In Sklearn And Gensim\n",
        "\n",
        "\n",
        "https://www.kaggle.com/morrisb/compare-lda-topic-modeling-in-sklearn-and-gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xGXqGRnz9Zp"
      },
      "outputs": [],
      "source": [
        "# Remove rare words\n",
        "minimumWordOccurrences = 5\n",
        "texts = [[word for word in comment if wordFrequency[word] > minimumWordOccurrences] for comment in preprocessed_comments]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfk82-zDz9vk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-e24umJB_6e"
      },
      "source": [
        "____________\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4azX0CL7IVq"
      },
      "source": [
        "# Some task Natural Language in Python using spaCy\n",
        "\n",
        "https://colab.research.google.com/github/DerwenAI/spaCy_tuTorial/blob/master/spaCy_tuTorial.ipynb#scrollTo=RvSFyDvPyueg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzI6Ojf_B-d2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jocWgnY03ZFK"
      },
      "outputs": [],
      "source": [
        "text1 = \"The rain in Spain falls mainly on the plain.\"\n",
        "doc = nlp(text1)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMuhwJJVPBeY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "    \n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glwLurmpPECJ"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOCPO8iHPFge"
      },
      "outputs": [],
      "source": [
        "text2 = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in. Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n",
        "\n",
        "doc = nlp(text2)\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(\">\", sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBZ6qiGCPTcC"
      },
      "outputs": [],
      "source": [
        "for sent in doc.sents:\n",
        "    print(\">\", sent.start, sent.end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TlWcotsPWDg"
      },
      "outputs": [],
      "source": [
        "doc[48:54]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiX6TrIYPeRF"
      },
      "outputs": [],
      "source": [
        "token = doc[51]\n",
        "print(token.text, token.lemma_, token.pos_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQVr7EzaQM91"
      },
      "source": [
        "## Natural Language Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IRCtYw8Pi2K"
      },
      "outputs": [],
      "source": [
        "text3 = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
        "doc = nlp(text3)\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FllAjK8vP3HE"
      },
      "outputs": [],
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoyRNMeYP-xj"
      },
      "outputs": [],
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEcI4VkQVkb"
      },
      "source": [
        "# wordnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eY4c1EdQx9R"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_d0KD9GQBV6"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp9yOJfEQXYf"
      },
      "outputs": [],
      "source": [
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "print(\"before\", nlp.pipe_names)\n",
        "\n",
        "if \"WordnetAnnotator\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\n",
        "    \n",
        "print(\"after\", nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzE6PrDkVoca"
      },
      "source": [
        "❗ NOTA: develop the pipeline scattertext\n",
        "\n",
        "- https://colab.research.google.com/github/DerwenAI/spaCy_tuTorial/blob/master/spaCy_tuTorial.ipynb#scrollTo=pC3XvyOC_0sx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3uwJcYvKmj5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZwF7OgmJMuQ"
      },
      "source": [
        "# 🔼 Option 3. Parallelize the work using joblib\n",
        "# Option 4. Sets vs. Lists (using stopword list vs set : better performance)\n",
        "Conclusions\n",
        "\n",
        "https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqsB5iR0TWwH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwMYFtMMRUjh"
      },
      "source": [
        "## 🔼 Name Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phk-dqr7Sd99"
      },
      "outputs": [],
      "source": [
        "# creating the function for text cleaning\n",
        "def clean_text1(text):\n",
        "    #text=text.lower() # lowercase\n",
        "    #text = re.sub(r'\\d+', '', text) # remove numbers\n",
        "    #text=re.sub(r'[^\\sa-zA-Z0-9@\\[\\]]',' ', text) # removing characters: punctuation and other special characters\n",
        "    text=re.sub(r'\\S+@\\S+', '', text) # removing emails\n",
        "    text=re.sub(r'https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&\\/=]*','',text)\n",
        "    text = re.sub(r\"\\sd\\s\", \" \", text) # removing single letters\n",
        "    text = re.sub(r\"- \", \"\", text) # remove \"- \" this appear when the word is cut to pass the next line eg. \"scien- ce\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zLq_ndwSh8V"
      },
      "outputs": [],
      "source": [
        "# applying function \n",
        "docs_csguide ['textNER'] = docs_csguide['text'].apply(lambda x: clean_text1(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3bKZM99Rz7B"
      },
      "outputs": [],
      "source": [
        "textNER = docs_csguide.iloc[0]['textNER']\n",
        "textNERx =textNER[2000:3500] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srb1ns_pU-D5"
      },
      "outputs": [],
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jEFGuK2UHC4"
      },
      "outputs": [],
      "source": [
        "text= nlp(textNERx) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-onVRpPETwXJ"
      },
      "outputs": [],
      "source": [
        "displacy.render(text , style = \"ent\",jupyter = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK9Bd7IjTvT8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okFU3XiXUe4N"
      },
      "source": [
        "# examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cU9-6u8Nysl"
      },
      "source": [
        "# 😎 Workflow- pipelines for extractions using txtai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WfVUW2-m09V"
      },
      "outputs": [],
      "source": [
        "# workflow that extract the text for each link \n",
        "workflow1 = Workflow([Task(lambda x: textractor(x))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oTufpoF9zZ4"
      },
      "outputs": [],
      "source": [
        "# Data to run through the pipeline\n",
        "link =['https://www.epa.gov/sites/default/files/2019-03/documents/508_csqapphandbook_3_5_19_mmedits.pdf',\n",
        "       'https://www.epa.gov/system/files/documents/2021-11/aphl-epa-citizenscience-qualityassurance-orientationguide.pdf',\n",
        "       'https://cs4rl.github.io/viewer/9788794233590-cs4rl-skilling-v1-screen.pdf'\n",
        "       \n",
        "]\n",
        "\n",
        "\n",
        "#'https://www.ceh.ac.uk/sites/default/files/hp1114final_5_complete.pdf',\n",
        "#'https://www.ceh.ac.uk/sites/default/files/citizenscienceguide.pdf',\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZO0qzuSm4Y2"
      },
      "outputs": [],
      "source": [
        "# Workflows are generators for efficiency, read results to list for display\n",
        "file_links = list(workflow1(link))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWSbZhcF49YE"
      },
      "outputs": [],
      "source": [
        "# Create a Pandas Dataframe to hold the filenames and the text\n",
        "df = pd.DataFrame(list(zip(link, file_links)), columns = ['url', 'text'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvgsvuF3f05_"
      },
      "outputs": [],
      "source": [
        "df.to_csv('text_guide1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3mwPPHJf9JT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNbeJVUHNglm"
      },
      "source": [
        "# 😎 Removing all the substrings in a string for all the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3T2krjzFMVT"
      },
      "outputs": [],
      "source": [
        "# create a df\n",
        "data = {'x':  ['a', 'b','c'],\n",
        "        'y': ['tengo References Reference dasd', 'i love you References asdadadadlkasjd adald  adad', 'no tengo ref']\n",
        "        }\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# remove all the words inside after a string \n",
        "\n",
        "# removing characther after the word \"References\"\n",
        "def removeReferences(doc):\n",
        "  doc = doc.split(\"References\", 1)\n",
        "  return doc[0]\n",
        "\n",
        "# USE str.split() TO REMOVE EVERYTHING AFTER A CHARACTER IN A STRING\n",
        "df['count'] = list(map(lambda x: x.count(\"References\"), df['y']))\n",
        "df['no ref']= df['y'].apply(removeReferences)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCJbGLq_XyYm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwIM4s7GGLsx"
      },
      "source": [
        "## 🟢 Link to Resources\n",
        "\n",
        "#### Apache tika\n",
        "- Colab TIKA https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/Processing%20documents%20with%20Apache%20Tika.ipynb#scrollTo=VpReBPj6817U\n",
        "- Coding extract text from folder:  https://pretagteam.com/question/how-to-extra\n",
        "\n",
        "\n",
        "ct-text-from-pdfs-in-folders-with-python-and-save-them-in-dataframe\n",
        "- Using tika: https://medium.com/analytics-vidhya/data-extraction-from-pdf-documents-using-apache-tika-and-python-b56e4bc79245\n",
        "- Library  https://www.libhunt.com/r/tika-python\n",
        "\n",
        "#### PaperAi\n",
        "- Github: https://github.com/neuml/paperai\n",
        "\n",
        "#### Repository Awesome colab-notebooks\n",
        "- https://github.com/amrzv/awesome-colab-notebooks\n",
        "- https://www.libhunt.com/r/awesome-colab-notebooks\n",
        "\n",
        "#### Respository Code question\n",
        "https://github.com/neuml/codequestion \n",
        "\n",
        "### TXTAI\n",
        "\n",
        "Article explaining what you can do with txtai\n",
        "\n",
        "- https://towardsdatascience.com/introducing-txtai-an-ai-powered-search-engine-built-on-transformers-37674be252ec\n",
        "\n",
        "Github libray\n",
        "- https://pythonawesome.com/an-ai-powered-index-over-sections-of-text-with-python/\n",
        "\n",
        "Distributed embeddings cluster\n",
        "- https://github.com/neuml/txtai/blob/master/examples/15_Distributed_embeddings_cluster.ipynb\n",
        "\n",
        "Run pipeline workflows\n",
        "- https://github.com/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb\n",
        "\n",
        "\n",
        "Extract sentences and paragraphs with txtai\n",
        "- https://dev.to/neuml/extract-text-from-documents-3kbb\n",
        "- https://github.com/neuml/txtai/blob/master/examples/10_Extract_text_from_documents.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr2rGqZyIDkp"
      },
      "source": [
        "## 🟢 NOTE: Shortcuts in Google Colab 🔜\n",
        "https://towardsdatascience.com/10-tips-for-a-better-google-colab-experience-33f8fe721b82#0d57\n",
        "\n",
        "\n",
        "- Undo last action (inside a cell): ctrl + m + z\n",
        "- Find and replace: ctrl + m + h\n",
        "- Insert code cell above: ctrl + m + a\n",
        "- Insert code cell below: ctrl + m + b\n",
        "- Delete cell: ctrl + m + d\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRvTmFSRo9cb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyQI61QYdpS"
      },
      "source": [
        "🟢 # Markdown in Google Colab\n",
        "\n",
        "https://colab.research.google.com/notebooks/markdown_guide.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zjDE19PYbJc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HAPWy3hSi31tDrfOj7Y9zQaG_4QFhwkz",
      "authorship_tag": "ABX9TyN1Te7SfnrLaSCCgblYQ/S8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}